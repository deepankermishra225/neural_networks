{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><b>SOFTMAX CLASSIFICATION IN DNN USING NUMPY</b><br><center>\n",
    "<hr>\n",
    "<ul>\n",
    "<li>https://cs231n.github.io/neural-networks-case-study/#grad</li>\n",
    "<li>https://dustinstansbury.github.io/theclevermachine/derivation-backpropagation</li>\n",
    "<li>https://ai.plainenglish.io/gradient-descent-update-rule-for-multiclass-logistic-regression-4bf3033cac10</li>\n",
    "<li>https://blog.yani.ai/backpropagation/</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><b>GENERATING SAMPLE DATA</b></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAksAAAGwCAYAAAC5ACFFAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy81sbWrAAAACXBIWXMAAA9hAAAPYQGoP6dpAAEAAElEQVR4nOzdd5xcVd348c+909v23je76b0n9BIIvShNVLBgQfFRo6Lo86A++hPbo6igWFCwUkRQegkEAqT3tr2X2Z2ZnbLT2/39sWSTYWY2bTdbct6vV16w99y598yWme+c8z3fIymKoiAIgiAIgiCkJI93BwRBEARBECYyESwJgiAIgiCMQARLgiAIgiAIIxDBkiAIgiAIwghEsCQIgiAIgjACESwJgiAIgiCMQARLgiAIgiAII1CPdwemgng8Tk9PDxaLBUmSxrs7giAIgiAcB0VRGBwcpKSkBFlOP34kgqVR0NPTQ3l5+Xh3QxAEQRCEk9DZ2UlZWVnadhEsjQKLxQLA47/4J0aDaZx7IwiCIAjC8fAHfNz8xRuG38fTEcHSKDg89WY0mDCJYEkQBEEQJpVjpdBMqgTvt956i6uvvpqSkhIkSeKZZ5455mM2bNjAkiVL0Ol01NbW8sgjjySd8+CDD1JVVYVer2flypVs3bp19DsvCIIgCMKkNKmCJZ/Px8KFC3nwwQeP6/zW1lauvPJKLrzwQnbv3s2XvvQl7rjjDl5++eXhcx5//HHWrVvHt7/9bXbu3MnChQtZu3Yt/f39Y/U0BEEQBEGYRCRFUZTx7sTJkCSJp59+muuuuy7tOV//+td5/vnn2b9///CxW265BZfLxUsvvQTAypUrWb58OQ888AAwtLKtvLycL3zhC3zjG984rr54PB4yMzN59ncvimk4QRAEQZgkfAEfV3/6ctxuNxkZGWnPm9I5S5s2bWLNmjUJx9auXcuXvvQlAMLhMDt27OCee+4ZbpdlmTVr1rBp06a01w2FQoRCoeGvPR7P6HZcEARBEE4XGWT11Cx7E48qED/160zpYMlqtVJYWJhwrLCwEI/HQyAQwOl0EovFUp5TV1eX9rr33Xcf3/3ud8ekz4IgCIJwumhz1WgsqvHuxpiKDMYIO6KndI0pHSyNlXvuuYd169YNf+3xeESdJUEQBGFSORwohZ1RYsE4TMqknBFIoNLLaLOHQp1TCZimdLBUVFREX19fwrG+vj4yMjIwGAyoVCpUKlXKc4qKitJeV6fTodPpxqTPgiAIgjDmZIYDpYg7Nt69GTPx0NBz02arCTujJz0lN6lWw52o1atXs379+oRjr776KqtXrwZAq9WydOnShHPi8Tjr168fPkcQBEEQpprDOUqx4Cgk9Exwh5/jqeRlTapgyev1snv3bnbv3g0MlQbYvXs3HR0dwND02G233TZ8/mc/+1laWlq4++67qaur49e//jVPPPEEX/7yl4fPWbduHb///e959NFHOXToEHfeeSc+n4+Pf/zjp/W5CYIgCMJpN9Wm3lIZhec4qabhtm/fzoUXXjj89eG8odtvv51HHnmE3t7e4cAJoLq6mueff54vf/nL/OIXv6CsrIw//OEPrF27dvicm2++GZvNxr333ovVamXRokW89NJLSUnfgjCZWHLN6HP0gETUF8HV62aSVgkRBEEYd5O2ztJEIuosCROFrJIpmVfM5s07+OdjzxIMBDn/orO44UNXM9A0QGAwON5dFARhApC1EoYSLYGeMPHw1A4DRnquos6SIJyBCmcW8L1v/x+7dxwpxPrYX57mhf+8xsN/u5/QPivx+NR+YRQEQRhtkypnSRCE9LQGLT191oRA6TCPe5C/PfoUWaVZp79jgiAIk5wIlgRhisjMt/DyC2+kbV//8ltoLdrT2CNBEISpQQRLgjBFKHEFrVaTtl2j1YBIURQE4TR75e2XuO7OqwhHwgnH/+fn3+QHD31/nHp1YkSwJAhThKvfw1XXXpq2/ZoPrCXoDKVtFwRBGAvnr7iQWDzOuzvfGT7mdDvZvGcTl593xTj27PiJYEkQpohoOIpZZ+LqD6xNaqusLuOa69bi6nOPQ88EQTiT6bQ6Ll69hpfeemH42GvvvkJBbiGLZi8ex54dP7EaThCmkP4mGx++9YNcftXF/OuJ5/D7A1xy+QXMnzcb68H+M6MAnSAIE86VF1zFnd/+DLYBG/k5+by88UUuO/dyJOnkq2qfTiJYEoSpRIG++n40Og0f//AtKBIE3UG69/aMd88EQTiDTa+aQU1FDa++/TLL5i+nrauNH3zlsvHu1nETwZIgTEGRUAR7x8B4d0MQBGHYFedfxVMvP4nNaWPJvKUU5E6enTJEzpIgCIIgCGPu4rPWYB+w8cKG5yZNYvdhIlgSBEEQBGHMmY1mzl1+Pga9gbOXnjve3TkhIlgSBEEQBOG0sDttXLz6ErSayVUgVwRLgiAIgiCMqUHfIBu3v8WeQ7u59pLrx7s7J0wkeAuCIAiCMKY+/d+fxOsb5FM3f4aK4orx7s4JE8GSIAiCIAhj6h8/f2K8u3BKRLAkCBOEOceEuchMJBZBrVGjBOI4u9xEQpHx7pogCMIZTQRLgjABZJdlYXXb+OadP6DPagNgyYoFfP1/voCrxU3QGxznHgqCIJy5RIK3IIwznUmHL+7n7v/67nCgBLBz614+94mvkzMtexx7JwiCIIhgSRDGWUaxhQd+/seUbQ7bAIcONmDMMJzmXgmCIAiHiWBJEMaZxqDh0P6GtO07tu0RwZIgCMI4EsGSIIyzWCRGQVFe2vbyilIioehp7JEgCIJwNBEsCcI489v83P7Jm1O2qVQqzrtwNW6b5zT3ShAEYeJ65tV/8aEv38TaT6zhc9/+DIeaD47p/USwJAjjbNDhZdmyRVzzgbUJx/V6HT998Lv4rL5x6pkgCEJ6KrXMtIUVLFs7j+Vr57B87TymLaxApR7b0OKNzev5zd8f5LbrP8Zvv/cHaipq+fqPv4rT7Ryze4rSAYIwAfQc6OXmG67j1ts/SGNDCyaTifLyEjzdHjz9g+PdPUEQhAQqtczCC2cjeW1Eeo+s4s02Wci6cDZ73jhELBofk3s/+eITXHHBVVx+3hUAfPnjX2Hznk28+Nbz3Hr1R8bknmJkSRAmAgXsrQ76D/STr85B79fQs7cXr0OMKgmCMPFUzi1D8tqI+hI/zEV9g0heG5Vzy8bkvpFohIa2BpbOXTZ8TJZlls5dysGmA2NyTxDBkiBMKIoCIX9YJHQLgjCh5RRlJAVKh0V9g+QUZYzJfd2DbuLxGNmZifXnsjNyGHANjMk9QQRLgiAIgiCcIImRp9iO1T7ZiGBJEARBEIQTohwjfDhW+8nKtGQiy6qkZG6nZ4CcrJwxuSeIYEkQBEEQhBM0YPWgNllStqlNFgasY1PuRKPWMKNqBjsP7hg+Fo/H2XlgJ3Nq547JPUEES4IgCIIgnKD2A10o5vykgEltsqCY82k/0DVm977x8pt4fsNzvLzxRdq727j/kf8jGApw2Xur48aCKB0gCIIgCMIJiUXj7HnjEJVzy8gpLkAijoLMgNVD+7axKxsAcOGqi3ENuvjTU3/E6R6gpqKWH33tp+Rkjt00nAiWBEEQBEE4YbFonJY9HbTsOf33vv6SD3L9JR88bfcT03CCIAiCIAgjEMGSIAiCIAjCCCZdsPTggw9SVVWFXq9n5cqVbN26Ne25F1xwAZIkJf278sorh8/52Mc+ltR+2WWXnY6nIgiCIAjCJDCpcpYef/xx1q1bx0MPPcTKlSu5//77Wbt2LfX19RQUFCSd/69//YtwODz8tcPhYOHChdx4440J51122WX86U9/Gv5ap9ON3ZM4Q0myRHZJFrpsHbFYDCku4bX68LnEdh6CIAjCxDapgqWf/exnfOpTn+LjH/84AA899BDPP/88f/zjH/nGN76RdH5OTmJm/GOPPYbRaEwKlnQ6HUVFRWPX8TOcSqOiZF4xf/3zP3nu6VcIh8KUVZTwpbs/TUF5Ps7OsdspWhAEQRBO1aSZhguHw+zYsYM1a9YMH5NlmTVr1rBp06bjusbDDz/MLbfcgslkSji+YcMGCgoKmDlzJnfeeScOh2PE64RCITweT8I/Ib386Xn899fv41+PPUc4NDTS19XRw1fv+g79HhumLOM491AQBEEQ0ps0wZLdbicWi1FYWJhwvLCwEKvVeszHb926lf3793PHHXckHL/sssv485//zPr16/nRj37Em2++yeWXX04sFkt7rfvuu4/MzMzhf+Xl5Sf3pM4Aao0Kj3eQg/vqU7bf/+PfYS4yn+ZeCYIgCMLxm1TTcKfi4YcfZv78+axYsSLh+C233DL8//Pnz2fBggXU1NSwYcMGLr744pTXuueee1i3bt3w1x6PRwRMaegtBnbv2Z+2vaujZxKF7IIgCMKZaNK8TeXl5aFSqejr60s43tfXd8x8I5/Px2OPPcYnP/nJY95n2rRp5OXl0dTUlPYcnU5HRkZGwj8htVg4Sn5+btp2rU6LrJo0v4aCIAjCGWjSvEtptVqWLl3K+vXrh4/F43HWr1/P6tWrR3zsk08+SSgU4iMf+cgx79PV1YXD4aC4uPiU+yxAwBtk9twZaHXalO1XXncJIWdoTO6dVZxJycJiTJUmMmszKZ5XhDHTMCb3EgRBEKauSTUNt27dOm6//XaWLVvGihUruP/++/H5fMOr42677TZKS0u57777Eh738MMPc91115GbmzjC4fV6+e53v8sHP/hBioqKaG5u5u6776a2tpa1a9eetuc11Q12D/J/D3yXr9z17eEEb4DZ82bw0Y/dSNfu7lG/Z35NHpt37OChOx8dvmdufg73/exbWNRmBh3eUb+nIAiCMPb21O3m8ecfo7GtHofLwf9+8f9xzrJzx/SekypYuvnmm7HZbNx7771YrVYWLVrESy+9NJz03dHRgSwnDpbV19fz9ttv88orryRdT6VSsXfvXh599FFcLhclJSVceumlfO973xO1lkbRoN2LJc/M3576DQcPNGC3OViwcA6Z5gx69vWixJVRvZ/BoqfHbuWXP/l9wnGHbYC77riHvz31GxEsTQGSLJFflkNGrhklrjDQ58FpdY13twThjKFSy5TWFmHO1BMLRVDptHjdAbqbrGO6kW4wFKSmoobLz7+Cb//iv8fsPkebVMESwF133cVdd92Vsm3Dhg1Jx2bOnImipH4zNhgMvPzyy6PZPSGNQbuXQbuXYnM+ZTXFBHsD9EaOvYrxZFiKLPz0279J2RYOhdnw+jssn7UIt02UfJisDBY9NfPLad24l8Z/tyCrZCpWzmbu6uk07GwlEoqOdxcFYUpTqWVmLa/h4LPv0F/XOXy8YHY5c646m7ptzWMWMK1cuIqVC1eNybXTmTQ5S8LUEPAG8Q54iUbSl2Y4VSqtTGdb+qm9hrpmtMbUOVTCxCdJEjXzy3nnwWdo23SQsC9I0OOn4dUd7PjLy9QsrBjvLgrClFdaW5QUKAH0H+rk4HPvUFo7tQo9T7qRJWFyMFgMmPKMgEJ4MILHPnja7h0LxamqqeDQ/oaU7XPmziTkG5ukcmHs5ZZm077pAGFfMKlt0OrEZx3AmGnE7/aPQ+8mJq1eQ9mMInQ6NbFwFLVBi9vho6e5b9SnwYUzgzlTnxQoHdZ/qJPZV5zekZ+xJoIlYVTJskTR7EIaW1r5zc8eIRgMccnl53P+hWdhq7cR8oePfZFT5On18Nkv3M4XP/OtpDaDQc8556+kZ0/vmPdDGBsZ2Ub2vtyatr1ndxP5S+eIYOk9WoOWGYsr2fm313B324cOSlC2ZDozz19M/baWtKkKxyKrZAoqcsnKtyABPk8Qa7uNcCAyek9AmJBioZF/xrHw1PodEMGSMKoKZhbw85/9jnff2jp87ND+Bp7427/59R9+RNfubk7ydfm4BX0hcgty+Ma3/4tf/vT3+H0BAErKivj+T+7B3SFylSazeBzUWk3adpVWfdJv/lNR5awStj3yEt5+15GDCnTtaERj0JFfWUp/u/2Er6szapm+qJLG9TvYv6sZJR4nr6aEOVevpqfdgdt2+kaThdNPpUv/NwigGuFvdDISwZIwarQGDf0Oe0KgdJi1p5+nn3qBS84+H2eva8z74mgdYFZ5LY/+4wG8Ph8qtQqdSoun24PfExjz+wsnT1bJaA1aouEo0XByorajz03lqjns//c7KR9fsXIO7U22se7mpCCrZKR4LDFQOkr75kOcs2L2SQVL0xaUs/n3zxFwHllZam/u4e0HnuG8L30Qr9NPLDp2uYnC+PK6gxTMLqf/UPJUXMHscrzu5GnyyUwES8KoycjP4NHHHk/b/uKz67nyikvgNM2Aufs8uPvEKNJkodaoqJxbilqCwX4nhsxsNCYD7XU9BAaPvPAO2gcpW1lDTlUhA22JFf3Ll88kJstEjjFFMNmpNCoKK3IxmPVEIzFs3c6U044anRq/M/0ITzwaI34SAY0py4in05YQKB19zcbXdlKwYDq9Lf0nfG1hcuhusjLnqrOBdxICpqNXw42VQNBPd9+RRTy9tl6a2huxmDIozCsc4ZEnTwRLwmkjZkaEdFRqmZnLp7H7H+txdR4ZFdJZDKz61FW0HkoMmBp2tDLrmnOI+vz07GpEVqspWzaTSAxa93eNx1M4bbILMymuzKXhtR0MtPaitxipuWARxdV5NO/pgKP+ziKhKKbc9NsxqTQqZLXqhPtgzjLRvzf1AgoAW2MXFefMP+HrCpNHLBqnblsz5WcvYvYVq4iFI6i0Grzu4JiWDQCob61n3Q++OPz1b/7+AABrz7mMr3/mm2NyTxEsCaPCYDGg1qi55gOX8crzG1Kec8U1FxP2jH2CtzD5FFXl0/Dy1oRACSA0GGDLH55n2SeuoH77kaTuWDRO4842dEYdmbNriMcVWuusKaftphK9WU9+kYW37n8KJT70ZhQaDLDz7+upWDGL8vm1dNYfGbqNx+LEkLAUZTNodSZdr+qseTis7hPuRzQaQ2vWp23XmvRj+mYpTAyxaJyOup7Tft9Fsxfz+l/eOq33FHWWhFOiM2opXVBCf9jOnx97Ap1ey1nnrUg6r6ikgOs/eAWu05CvJEw+GTkmevemXuEW9PiRlTgzl1QwZ1UNJbWFSJIEQMgfor/djr3TMeUDJYDiqjz2P/32cKB0tI6tdVgyDUiylHC8/VAPy25bS07Vkbo3kixRuXoOxUtm0N/pOOF+OHtdlC+fnbZ92rkLsPe4Tvi6gjBRiZEl4aSpNCryZuTx+Tu+Tl/v0IjAqy++yTe/9yWuvG4N//z7swSDQS65/ALOv/As+uttYipOSCkejY24gm2w18Ghl7cT8vgpWzaDmecueG/J+2ns5ASgM2hw96QPbgbarBgshoT8pUgwQv2OVqatXcE8k45YOIJar8VpG6Rhe0vCtN3xisfieNwB5ly1ioPPbU5oK5xbiaW8gN7t6cs7nAqdUUtRVT4GkxZJkhh0+bG22c+IYFkYPyJYEk5aVkkmD97/x+FACYa2E/nO3T9mWm0V9//2ewz2ewkPhunalVhRW5Ylskqz0FqGKmmH3GFcvS5RIO8MJalUqPVaosHU07TG3AzC3gCKotC5rR61TkNBVRl9bWfWqrfDI2rpqDRqlBS1zCKhKO0HTm3DalOWkcLyXLQGDeFghL4OB5bsLM7/yk3YGjqJhSPkzygnHFVo3Nl2SvdKJzPfQnFlLgeeeXsouV+CgpnlzLlqNS0HuhPy2gRhNIlgSThp2gwtb65/N2VbS1Mb2zfvocxSTGAwcam+wawnZ3oOf33kn7z6wgYkSeLyqy/mlo9cj63eTsgvqmufafq7ncxYsyRplAIgr7aUQauTeOzI1NPhJe9nWrA06A5QMLOM/vrkJHZJksiqKKB7U9Oo37dyTilKIMCBpzbg7XdhLshixiVLkY1aDm1txpxjRjbqadrXfULlAiy5Zooq8lCpJCRZwmkfpL/dkfCzPkxWyZTVFPDW/U8dWcGnQH9dJ65OG2d9/joOvNs4Wk9ZEBKIYEk4afF4nHiK3InDvF4fclZyWlzu9Fw+dds6BuxHEk6f/Pt/eHP9u/zmjz+ha/epfQIWJp+BHifTFpQz95rVNLy2k4g/NPTmuHQ65UtnsPnhFxPOP54l71mFmegMWsLBCM4+10lNN0001lYbc64+G3f3fwh5Ez+EzP/gufR3DYz6PfPKcvD39CcEst5+Fzv/tp5Zl68gtzQH+0nct7S2EDkSZtdfXibg8iLJMqVLapl90RLqt7cmTavll+fS/OaelD/3sC9I/8E2MvMzxAbZwpgQCd5CAkmWyCnNIr82j/xpuRgshvQnR6FmelXa5kVL5iWNKmUVZvLic+sTAqXD+vvsbHxrMxn5lpPtvjCJteztJKI1svLTV3PuFz/Imns+hEqtYtPvXyD2vjdOWSUja1Ivec/IszB3dS0qv5eBvQ3gcTF3VQ1ZhZmn42mMqWg4SsvBblbfeQ3zrz+H4nlVVJ89j/O+fAOKwYitc/SDpYLSbOpf3payreHVHRSUZZ/wNc05ZtSxCDv/9hoB11CtJiUep2t7A7v+9hrVc0uTHmM063A0p195ZWvowpQxwuuVIJwCMbIkDDNlG8msyOQff32aLe/sIDsnkw9/7AZqZlfRW2dN+mTu6fHw1W99ns9/4utJI0znX3wWGkWdNJyuNqvTTt0BrH/5LZbfvQjOrNkV4T1Oqwun1QVA7eJKHK3WlCMJFStnMdCfXGzRmGmkuCyLjff/k1jkvcfthab1u1j5qSuJhqN4nb6xfApjLuAJcGBTExm5FvIWzyISiVG/sz3l1NVoiIUjR76X7xOPxoiFTrwcSFFFLnv+/mrKNne3HaJR1Fp1wuhSLBpDZzHiH0hdZFOfYSSapp+CcKrEyJIAgEanwVBk4KM3fI5/PfYc3Z297N9Txz1f/j5//vPj5NfkJT0mMBjEENPx8N/vZ9mqRWi0GgqL81n3jc/yX1+6A1tzii0UFDAY09dnMZqMU2K6RDh17Yd6WHrbpWRXFBw5KEHpkumUr55Hf1vy71fptHx2/m190pt7PBZn519fo7SmIOkxk5XHMYi11Yaja2DMAiUASTXy24SkOvGilmq1nDboAXB12tCbdQnHbN1Opp2/IO1jKlfPxdGbPGItCKNBjCwJAGSWZnD/T35HMJicXP3cM69y80euR1bJSS/KbqsHjV7Dl7/4GbQmLbFIjIA9QNfe1MPlPrufmz98HXt3HUzZftOt1zJoS95CQTjzRIIR6re3Mf2K1eiN2uEl726H772yAclRtUolpd3eI+QNICmiUOKJisYUjLkZ+B3JuUDGbAux2El8upGGSo+kG7HSZ5kYtCVu3xIYDKKeVkDl6jm0bzrq9UOCOVeuwusNp72eIJwqESwJAGhMGrZt2pW2ffM721k2cyEeR/IbUSQYwd5yfIXtAp4AM+ZO47wLV/HWG4krny65/HzKiouxHupL82jhTBMJRU5o+5JUxRqPFo/GQUKMXp6A7qZ+ln70EjY99GxCaQe1TsPS2y6ho/HE938b6PNQsXI2rW/vT2pTadRYinLpbEseJWrZ00H5nGlUrZ6Lo6UHSSWTO60Ye4+b7kbrCfdDEI6XCJaEYbIsp13dplKrRiwaeCJ6D/Xxubs+wa2338CLz69HJctcduVFWAxm+upEoCScPEmtTluvSaUZquV0pgZKKrVMdnE2arWM1+3HO3B8uVuBwQBdzTbOvut6nO29eLrtZBTnkT2tmI76XvyewLEv8j62rgFmr56Lp9eBo/nI9iwqrZqVn7yCrqb0gU9nfS+SLGHMMKBEwLpp7DZsFSaev//nr2zc/hYdve3oNDrmTp/Hp275LBXFFWN6XxEsCQBEBiOce9Eq3nwtdfL16rOXYTuYIgfpJChxhb66fjQ6NTdddQ2g4Orz0B8WWd3CqbG2O5h37VnsfnxDUtusK1aOydL6U5VdlEVGthFFgYE+95gkoJdOL8Rs0dG5tY6QN0DBrAoqVtXSsq+ToC91XTNjhoHi6nw0GhlJlvG5/QTiavTlpQz6Q/SeQj0nJa5Qt7WF2rWrmK0aSurWZxgx5GbS1dTPoGPkqXglruBz+Uc8Rxh7KrVMTmUOskHG5/NjMpmIB2IMtA+M2d6Ae+p2c+2a65k5bRbxWIw/PPk77v7RV/jTD/+MQT92qyElZbSGC85gHo+HzMxMnv3di5gMpvHuzklRaVQUzM7nM7d/BedA4saan/jMh7jw3HMZaJ94bzSC8H7F0wowGdU0vLqdwd4BzAVZTF+zlIgiJ2wyO950Jh21Cyro3dtM795mZLWKytVzyCgtoGFn26glbRdX5xMZcHLo+S0Jxw1ZJlZ++mrqtibvEJ9TnEVunom9T72Ft98FQO60YuZdfy5tdT343Sc+mpSOrJLRm/VEw1HCAbHR9ukiayUMJVoCPWHi4RMPA1RqmZL5xfzkvgfZtHH78PHV5y7ja/d8np59vadlM2WXx8UHPn8NP//WL1k4a1HKc0Z6rr6Aj6s/fTlut5uMjIy09xEjSwIAsUgMV6ubP/z157z5xibeeWsrObnZ3PShazBpjKlXtgnCBNTb0o/WoKXywqXDRSl7OhxpR1BGmznHREFZDlqdhoA/RF+bPenekgS1CyvY8rtnCbiOjCS5Ot8kt6aYmVecReOutlPuiyRBdkEGb/7lpaS2gMtH42vbKZg3nd6WI3lHaq2awrJsNt7/VMLUu6Oll02/+fdQpexRrBIej8UT9rITJoecypykQAlg08bt/IQH+cLn06yIHmW+wNAoZIYpfaAzGkSwJAwLDAbo2hVgce08VixaghRXcFk9+MOJL2RqjYqs0iw0Fg0oEA1EcXe7CQcj49RzQUgUDoTprBu7USSDxUBBeQ5anZqgP0xfh51wIMK0hRWE7E72P/EG/gEPmaV5zLp8Jf5gLCEgySnJpnProYRA6TBHcy9Rnx+dUXfKW/+Ys83YGjrTtvfsbqbmgsUMOn2oNWr8Hj95pTk0vLojZY5i2B/C1tBJRp4Fjz390n/h5EgSGDKMoCgnlQt2OskGOSlQOmzTxu188aufHvM+xONxHvzrr5g3Yz7V5dPG9F4iWBKSeOyDkOYDgSHDQGZVBg/+/I9sfGMziqKwZPkCvvz1zxK2hY47aVQQJqvKuaUQDNH0wiZ8Ax4yS/KYuXYZqNX07m6k5c29w+e6Om1s/t1zLPnwxQlbcWTlWtjz8qa09+ja0UDO/OmnvPedJEvEQ+mX0ytxBb1Jh1mjEHA7KZtWgqUgi7p/pU+wdjT1kL9k1qQLlmSVTElNIRnZRqKhMGqdlkGXn+6mvjGtU3W8SqcXkpljYqDNiqxSUT2nBnuvm772iTmq7/ONPBp4rPbR8ItHf05rVyu//J8HxvxeIlgSTkhuTTZ3fOTLCXlNO7ft5Y4Pf4m//PPX+N2BCfHCIwhjobAyD09rNw2v7Bg+Zm/qxt7UzZp7PkTLW3tTPu7Av99l+R1XDgdLCsrQMEIakiyNyupTr9PH9IXl8OLWlO1500vo2tHIwReG8pma1u9i0c0XYMg0ExpMPbJhyDYTmWT1jGSVzOwVNdS9uIVd+1uHjxfNrWT2Fas5tLV5XF+3queVYT/Qwu6N+44clGD2FasonV40IcsimEzGY7Z7Gbuaeb949Ods3v0u93/rV+TnjH2xWVHBWzhuGfkW3npjc1ICOEAoFOZvj/6TrJLJv/+WIKSTV5xF42s7k45rjDpcXba0ZQlC3gAcVZbD2T9I+fKZae9Tvnwmrr7kv7MTFY/FCYWiFC9InqKQ1SpmXbqMlrf3JRxv3rCH2gsXpu/bitk4e06+UrY5x8z0xZXMXFrF7BXTKJtRhFo7tp/bS2sLqXtxM9ajAiUA64F2Dj33LmXTi8b0/iPRm3QQCtGyMfHngAKHnt+MyaRBnWYfxPEUD8RZfe6ylG2rz11GPDA2waeiKPzi0Z/z9o6N/N8991NcUDIm93k/ESwJx01r0vLOxtSfUAG2bd6NSj/x/qgFYTSoNCoCrkGUeHJEFI/GUGlGfsOXj9o2xGl1UbJoOub85A8XRfOqQKMdtRzA9oM9VF+4mIU3XUBGcQ46i5GypTO48Ks3UvfyDoKexOmSwT4n0WCEmvMTAyZZJbP4lgsZ6PMQT/E9OB5FVXnk5ejZ9ddXePsXT/HWz5+kZ9M+Zi6rQmfUHfsCJ8mSZcS6vy1lW9+hDsyZ47cBb15pNs1v7knb3v7uAXJLT3yz4rE20D7A1+75fFLANLQa7q4xWz39i0d/zmvvvsp/33kvRr2RAZeDAZeDUHhsF3CIaTjhuCkxhdy8nLTtWdmZIGbghEkktzSbgtJs4pEoslpFwB+mp7k/ZaCixBXkNJ/wY+EoskpGo9cSSVEQM6s8n0Ag8ZoNO9tYcttaXO1WenY3odKoqVg1B12Whb4Ox6gkeMPQJ/HGHW2Ys03MvOYcVCqZaCxOf30X9qbulI/Z/c83WXPPrZQtn4m7y/ZeVe0crO0OHF0nl0OjN+sxmzRs+u1zR3VuKFjxWAdY/vErOLR1bApMxsIjB54nsxnwaFFrVIQG0+f3BD0+VOqJ9yE0Fo3Ts6+XL3z+Dr741U+/V2fJSDwQp2dfz5iVDfjP+mcA+PIP/ivh+N2fuofLzrt8TO4JIlgSToDb6ubGD13Ny8+9nrL9Ix+/Aa9NJHgLk0PV3FJ8Xf2888AGYu/tbp9dUcDCmy+keW9yscZ4LI7GaECt0xANJb/5du1qYuUdl/Pub55NyH/RmvQsvOlCmvclrkqLBCMc2tKMOcdM6TmL0Oo1mLNMuNqtSG4XxWX56HNKaTvQPSplD7xO33DBS5VapnZ+WdpzzflZeN0Bmvd2oDfqUJQgHa2nNlJQVJnLoedTJ7UHnF58did6s56gN3hK90lFdYxpPpVOM+r3PF7+wRC5NSV4bamnXfNqS/EPjv73ZDTEovGE8gBjmaN02Ot/eWvM75GKmIYTjls0EsMg6/nMF25Larvq+kupqaoiMMGXuwpTgyRL5JfnUjG7hNLaQrSGE3uzs+SaCTs9HHx+83CgBODs6GfL75+jak5pysf1NPez9CNrkOTE5Gy1Xkv1OfNxDgQ578s3MueqVVSsmMXCmy5g9Z3X0nqoh5A/9eiFd8CLy+ZBr1Wx8edPsusfr9Pw2k62PfIy2x5+gZoF5Wj1o/tmHovGicQU8qanfp5zrzkLa5sdFAj6Qmn7fiIMZv1QXlcaAy29GDPGZjrM6w5SMLM8ZVv+jDJ8nvELRuzdA1SfuwA5xeiRxqCjaEHNqOSvCadGjCwJJ8Te4uDs5Su58N/nsG3zLsKRCCtXL0GKSFjFvm7CaZCRZ6F8eiHt7x6gq6UHfYaJaecvJKJIdBzqOa5rFFXksusvL6dsC7h8hNxetAZtUkVpt30QtVbF+etupHtXEz67i6yKQgpmVdBe14vX6aO/w05GngVTTQXuwQA9m49dwLF8ehFbH34hacQq6Pax54kN1Fy2kvYDqafMTlbbgW7mXHsOvbsaaX1nP9FgmMySXGZftRqvP4JvlAtFRsNRdBZj2iknQ7aFYIoRu9HQ3dTHnGvPRnnmbWwNRzZmzp9eytzrzqFuW8uY3Pd4xGNxupr6OPvz17L/6bdxdvQP923ONWfROso/d+HkiGBJOGHOTidSF8wpn4EkSTgbnCed8CkIJ0Jn1FFSmcPG+/9J7L3l657eAfrrO5mxZgnF0woTij+mo1KrUhaEPMzdbUdvMqXcfsPR42Kg10VWUTY5xfkEfeGkitYnUoNIkiWIRtMGEc72PowmHUiQU5SFyWIgGoth73YSOYUk8HgszqEtTeQW57Dqs9cgSRKhQJiedjuBMZj26e92UnPBQg4+mzwVJ8kyBTMrOHAcgeXJiEVj1G1tofL8xcy5ejXRYAS1ToPPG6JuW8vw79J4cdsGCfrC1F6+CoNx6Gft9QRo3N1J5CQDSLVWDYpCdJKVeZioRLAknBRFYUw2/BSEkRRX57HvXxtTvrk1vLaTC75603EFS/F4HK1JT9iXOigwF2TR35/+91tRwNnrGv5akiC/PJfcokyUWBxkGYfVja3LkbacwGGySk6ZFP6+DjN39XS6dtTTuqUbndlA9bkLCMc57tG01E8EHD1OHO+VAlBrVBRU5FFaUzCUj9LtHLW/c3e/h5nLplGyqJae3UeCIpVGxbLb19LbNrbFF2PRGO2n8r0aYyF/iLZRGEUqqMglvzQbn909tO9elgVrm40Bq5jKOxUiWBIEYdIwGLW4OtPnvTg7+jFY9MccGbF1u6i5YGHS5rIwlCdiKsgmeJwJzZIkMXNZNT0763nnyfXEIkNlBKrPncfMpdNp2NHCSPUlY5EYhiwzSKQMrNR6LXqLgdd/8gTRo4Iq68F2ai9eTElNET3Nxw4QjyW7MJPiyqGtThwtvegsBmrOX0RxdRVNu9pGfA7Hq2FHK+XLZzP9osW4u+1ojFqMOZn0tNpw9XtO/QYnQK1VU1iZhyXbiKIoDPR6sPcMpCwNMVlUzCrB39PPm4+9Ovw8VBoVC2+6AHVFLv0djuQHpa+NOnWMwnOcdAneDz74IFVVVej1elauXMnWrenr/jzyyCNIkpTwT6/XJ5yjKAr33nsvxcXFGAwG1qxZQ2Nj41g/DUEQTsrIr3qSJB1zJAeG6hzl1JZTuWp2wiX1mSZWf+YqOuqPf1+5kpoC2t7eS/Obe4dHvGKRKE2v76ZrywGKqo9dXdhp91K5cnbKtjlXrKB986GEQOmwpvW7yMqzjFQM/LjoTToKijN46/5/0buvlbAvyKDVye7H36Dj7b2Uzxqdwn+KotBxqIdD21txuMP0dLo5uKX5tAdKllwLMxZV0LtlH2///J9sevAZAl09zFlVi+Y4V8bpjFosuWa0Bu0Y9/b46IxaNHKcQy9sSQj4YpEYO/+2ntzCjIRaX/Hoe8GUftKFASfs8HM8/JxPxqQaWXr88cdZt24dDz30ECtXruT+++9n7dq11NfXU1CQ+gUpIyOD+vr64a+l972q/PjHP+aXv/wljz76KNXV1fzP//wPa9eu5eDBg0mBlSAI48vvDZFTVchAW4rFBNJQPaPu7uOr1dOws5WSWdVUnzOfoMePWq9BkWTa6q0nlLOTmWdm9/b6lG3tWw5x3tnzjjk12NPcx/SVczDlZ9H8xm5C3gDGHAuzLltB7rRiXv3+X9M+1tHSgzHTiM918gnZRVX57H/mHZR4cm2crp2NTDt/IbIsHXduoiRLI47QKHFlTEoEHA+VRkXF9AI2/uKpI8FtOErr2/uxNXSx+MOXjJjwbcgwUDmrmKDTw6DVSV55LrosM+0HewiM03MCyC/Lpen15Oryh7VvPkhOVSn2zvdGTOMQGYyhzR4KA2LB+HF90JhUpKFASZutJjIYO6U6gJMqWPrZz37Gpz71KT7+8Y8D8NBDD/H888/zxz/+kW984xspHyNJEkVFqUvZK4rC/fffz3//939z7bXXAvDnP/+ZwsJCnnnmGW655ZaxeSKCIJyU3jYb864/l3d//e+klWOzL1+B/UTyMhToaeqjp6kPWSWjxOMnNdUUDYbTvskocYXY8SToKtC4o42swkyWfeIKZLVMJBylv2MA87EKU47CG5zeqB1xWb+jpQdDhhHfCEnxSFAyrZDsfAshrx+1TkssrtDV2DeuQcRhaq2ailnFWLJNHHphS8q8N2+/C7/dhcFiIJBibzy9SUf17GI2//a5oS1sDh/PMLLq01fRtLdzVMosnAyNVoV/IP3CAr/DQ9aMyoRjYcdQ2YzDAdNUFRmMDT/XkzVpvkPhcJgdO3Zwzz33DB+TZZk1a9awaVP63bu9Xi+VlZXE43GWLFnCD37wA+bOnQtAa2srVquVNWvWDJ+fmZnJypUr2bRpU9pgKRQKEQodeQHzeE7vELIgnAk0eg06o45wIDy8Ki0cCNPZ3M85//UBurbX42jpRZ9ppPqcBQSCUboaTm7D0VPZRPWY25ycwL5erj53Uk0dnyeDvJoS7M2pk5Nza4rp33pqS98libQ5UwAqrQbFN/KqqhlLqunaepA9fz44fB1jjoXlH1tLW50V/zjWYFNr1cxcVs3uf6yn9oKF9B/qSHuudX8rWXNqUgZLZdOL2PHnVxICJYCgx8/Ov73G7OvPo2VvZ9LjToegP0xWeX7agCm7spCALzmQCzuihJ1RZPXUTF6KR5VR2Vli0kxW2u12YrEYhYWFCccLCwuxWlO/QM6cOZM//vGP/Pvf/+avf/0r8Xics846i66uoTobhx93ItcEuO+++8jMzBz+V16eutiZIAigNWgpm1HMtAXllM0oOmaBRb1Jx6zl0yivykEfD1FSlsmsFdMwWIamxb0DPg5ubkKVn0fVxcvIWziT1jrrSQdKpyoUipJZmpeyLbuigIDv1GoHWdtszLnmLNQpcmlqLliI2+E75eRr94CPojlVqRslyKkqGjHYyS7KYqCxg/ZNBxMCLv/AIJt++xyVs4pPrYOnqGx6IXuf3ICr00YsHEVjSL8PndaoTxs8azRy2krbnt4BdLrxG3/o73RQe/GSpFQTGKpgXrK4FmefK/WD4xAPK1Py32htwTVpRpZOxurVq1m9evXw12eddRazZ8/mt7/9Ld/73vdO+rr33HMP69atG/7a4/GIgEkQUiipLcSoV9H42g68/S4sRTlMX7MUrzecMo9Ha9BQs6CcLb9/noDryNYJWpOe1Z+9mpYDPQS9QRQFBnqcjM1WnSemq8HK4lsvZtufXsRnPzLKbC7IYtEtF1G/o+2Urh8ORuhs6ufcL36Qji0HsTf1oLMMlQ6Iy6pRWW7e325n1hUrcXX2J22su+D6c+nvco74+IKybLb87u3U/fcFCQx40Bm14zZFZTTpGGgdCqY7dzRSuWoWh17clvLcsqUzaNiTenQoFhl5KiceHb+aRrFIjP4uFys/dSV7//nm8AhTRnEOC2+6gI5669TLSTqNJk2wlJeXh0qloq8vMbGzr68vbU7S+2k0GhYvXkxT01CNj8OP6+vro7j4yCefvr4+Fi1alPY6Op0OnW7sdsgWhKkgpzgLvF42PXJkL6egxz+URPuhi8gqzMDVlziFXVpTyO7HXk8IlGDoDXf7Iy+z4NY1NO1qPx3dP27hYISmPR0svPUSiEbxD3gw5WagyGoadrWddFHBo3mdPg5saiKnpIDa6ZXEojG62gZSFs08GdFIjOZ9Xaz89NU4W3uxN3ahyzBRtmwGbocPW2v6fCYYmsaLjJBb5e13oTXoxy1YOnojXXtTN9MvXEhuTTGO5sRVjzPXLmdwMJh2ZEmt16ZNXpdV8rjuMQdDNbOCfhOLPnwJKllCkiVCoSht9X3jllA/VUyaYEmr1bJ06VLWr1/PddddBwwVllu/fj133XXXcV0jFouxb98+rrjiCgCqq6spKipi/fr1w8GRx+Nhy5Yt3HnnnWPxNE4LQ4YBjUaNfzBANHxqSW2CcLIKK3J551f/Stm2/+m3WfW5a5OCJb1Rk7aOks/hQa2amHkVIX+Yxp1tqNQqNDo1Vuvo77quKAqObicORh7lOVlBb5CDm5sw55jIWTCDaCRGw66O48rnisWUEbcyySjJpatt/MYBVfrE5f1bH3mFhTecS+35C7E1dKE1GyiYXYnb6ae7Mf22TQN9HqrPnkfLxn1JbdPOX4i9N/0CA41eQ35ZNmqNmoA3hL17bGo6+Vw+GneJgsGjbdIESwDr1q3j9ttvZ9myZaxYsYL7778fn883vDrutttuo7S0lPvuuw+A//3f/2XVqlXU1tbicrn4yU9+Qnt7O3fccQcwtFLuS1/6Et///veZPn36cOmAkpKS4YBsMrHkmbGUWti/r45+q52Fi+eSn5mHrdEmSt4Lp108FE7YpPZokWAYJcWURvwYv6exUARJklBGo0LiGIhFY8TGcSpmNHgHfHgHTuzNtr9zgBlrlrDv6eSpOEOWGa3FSDg4fntHepx+ihdMo3fvUCJ8LBJl5z/eQGc2UHvhIjKmlVC3o+2YwYu11Ubt4hnoM000vbGbsC+IzmJg+kVLMJcX0rw79ahn+cxi9FqZ5jf3EHB5yakqZM7Z8+lq6sdtEwuEJoNJFSzdfPPN2Gw27r33XqxWK4sWLeKll14aTtDu6OhAlo/krDudTj71qU9htVrJzs5m6dKlvPvuu8yZM2f4nLvvvhufz8enP/1pXC4X55xzDi+99NKkq7FkyTPjlf185gNfI3LUkPOM2TX88Gf/Q/fubrF/m3BaSfLI60ckOVUiqgZZrUqZ+yFJElqzYcIGSmcyt81D7oJyZq5dRtPru4aX5WeV57PolotoGqcVYof1NFqZeelyVBoV3TubUBQFSZYomF1B7swK6rY2H/coT9OuNrKKMll+x5VDtadiCv3dTvrSBEqFlXkErDZ2HlUt3tPjoGNrPWd97hpCgbCYIpsEJEW88pwyj8dDZmYmz/7uRUwG07j0oXRhCbd+8LOEQ8k5AdfccBk3XXM1js6xGb4XhFRmLqtm2x+eT1pmDWDIMrHktsto2NmWcDy/PJeo3UHDqzuSHlO5eg5ZM6roaR6/EQphZPnlOeSVZBOPRJDVKgK+CD3NfYRPYcPf0SLJEsXV+WTmWYiHI8haDS77INZW25hucTJ3dS1v/vSJlEF+Rkkus649l9Z94xtMnsl8AR9Xf/py3G43GRkZac+bVCNLQmp6s55DhxpSBkoAL/5nPR/56A0g/h6F06i7pZ8lH76Yzb9/IaEytKxWsfhDF9OdYjWcrdPBtIUVzDPpaXxtJyFvAI1RR835C8mdUUHDztbT+RSEE2TrHMDWORHWKCZT4go9zf2jso/e8dIZtXh6HWlHQz09jnEtNyAcP/FTmgLUGhWO7vSjRpFwhFhscudRCJOPd8CHQ6vh/HU30LWzgUGrk4ySXEoXT6e7uT/t9hwtezrILMgYqmStklDiYOtx0rBDBErC5HJc8zYTc82C8D4iWJoCAt4g8xem3oQToLS8GEkRf5HC6ee0unD1ucguyqWgtJhQIMyBzU3HrPfi7vfgPs2bqwrCaAsHwmQUV6RdlJBZmkdoAkxRCsc2aSp4C+nFIjHMBhMLlsxN2f7lr38Gr1UsJRXGh6LAQK8La2s/TqtLFMYTzii2LidzrlmddFylUbPghvPobRm5htXR1Fo1pdMLmb64kur5ZZhzzKPZVWEEYmRpirA12vjO97/G439/hn//80WCwRAVVaV86e7PkGfOxXmKyd2SLJFdlIVKLeNzB/C7T36Hc0EQhDNFf6eD0umFnP3562h9ey8Bp5fs6iIqV86ho8FK0HeMjZLfk1uSRUFJFvWvbGOgxYouw0jN+QspXlJF0662U97yRhiZWA03CibCajgYWlqdXZKJLltPXFEgpuDt9eI7gcBGkiWMGUYURRkOiLLLslBZVDz/n9ew9ds557yVzJ03C3+fH7VFjaySiQaiuHrcKXfyFqY2nVFLYWUeRoueWCSGrceVtBlsOtlFmRRV5BKPRJFUMrGYQneLTQTjwpSj1qrJK81GrVER8IUY6HEddxkMg1lP+bQ83n3oP0kr98qWTqdwySw6DqXeaFkYmVgNd4YwZhgwF1nQ6FVEQlH8/X4Gul0nfiEJ8qtzUXSwc9tetDotS1cuRKvScOBAHffc/v+GT331hTcpLM7nVw/fx3e+/mN6u/tYsmIhn/zMrQRtQQZt3hFuJEwl+eU5ZOeYOPT8ZgbarejMBqadt4BZK6ZRv711xCXZJbWF4Pfx7q//TTQ4tJLTmG1hyYcvprdThceRevd0QZiMouEo1mNsG5NOUXU++57ZmPLvqWtHI9POW5h2GxZhdIicpUksf1ouTsXNt775/7jh6k/y1S9/hw53D4UzC074WsWzivjHk8/w4Q/cyf/d9xvu+84vuPGKT/Lyy29gybLw4we+TVZ25vD5fb02HvrFo8xdMAvngJv1L73FbTfdhZQhozNqR7iTMFUYMwxkWHS88+tnGGgb2qQzNBjg0PNbaHh5K5WzS9M+VmfUYdDJ7HnizeFACcDvHNqlvnxG4el4CoIwKej0ajw9jrTtznYrBovhNPbozCOCpUkqqyiTnQf2c/d//S8tjUOVY7s7e/nuPT/hpVdeJ7ss67ivZcwwUN/UxLP/ejnhuKIoPPh/fyQSjvKnh/7B9376DSTpyKq6N197l7MvWDn8dTQS5ac/+DWZpZkIU19RVT4H/v1OyoTtvgPt6A1DU7SpFFbk0vDK9pRtsUiUvgNtZOanHxIXhDOJJEkjlhhQaTUJtcyE0SeCpUnKmG/k1z//Y8q2v/7xSQy5x/6UkZmfQV5lLgXT83n8b8+kPe+1l96kvLKUbZt3seqcpcPHY7EYRcX5Cece2FuHrBO/VmcCrVbFYF/6hQPONisGS+ptg3QGDYPW9I91d9vFCKUgvMfj9FM4uyJlmyRJZJUXEBgUW6aMJfGuNkl5/T78vuRtJACi0RgOhzPl3lsAxiwjpYtK2Fa3m1///k/87a9P8bl1H+fK6y9Jef6A3Yklw8zLz73B+WvOHj4+Z/5MPO5BZs+bPnxMluWE0SdhYlFpVGj0muGvtQYt2UVZWHJPfAmyJEsj/qw1Bh2xaOpPu+FgFHN++hHIjOKcCbFFhiBMBF6Xn4UfPA99hjGpbeFN59PXMTGrpk8lIsF7ktJoRv7R6fVaQvHkTxo6oxZjsYGP3fIFvINHai89+bf/cPe3v4DHPcjG1zcnPGbhkrm8u3E7kUgU1XvTKiqVijs+/2H+8ednWHX2Mg7tbwTg3ItWERkUb3ITgSQdqSCckWehdFoBYa+faDCMuTAblUaNt9+JvbELfYaJitW1WNsdOHqOr8yEe8BL8YJqeva0JN9blskszaersynlY/s67My4ZClbHn4xqU1WqyiaP40Dm1I/VhDOJFqDhtLqXDY//CJLb72YwT4njtbeob/ZFTMJBmN0p9nEdzSptWpMWUbi0TiDA2feIh4RLE1SatRUTSunrSV5w7ecvGyMOiNukisgZ5Zm8r/3/jQhUIKh/KSf3/cQP33wOwnBUnZOJvMWzeahXzzKB26+kr27DnLWecv50Mc+wGOPPo3fFyBWUQxAbn4Od33pk/QdPH17LwmJdCYd5bWFqDUysXAUrVFPMBiGUJh3H3yaaOhIIFu5eg65VYU0v7kXgEPPbWbpR9egFGcx0Os65r362hzMWrsCV5cdv+PI75okSSz9yMX0ttvTPjboCxFBxdxrz6LuhS3DJSd0FgNLP3IJ3adx/y5BmMiKqvLZ//Q7eHocvPPQs2SW5pFZmotvwMObv/gX533phjG9v0qtonp+GUQi2Ju6MRj1VKyqob/bOWH3ARwLIliapNydbr7/029y58e+xqDnSJSv1+v48S/uxd2Zus6NolaGR4HeLxwK43J6yM7JxOX0sOqcpdx2x0386Lu/Iic3i5tvu47GuhbqDjbxrS//AI97kP/+f+toONTMPd/+IkuXL8TRPCBqLY0Tg1nPtHmlbP/zK3j7XUMHJahaPYeCGeVEw4kjfu2bDpJRmE12ZSHO9j4URWHHX1/j/K/cdFzBUiwao3FXO0tvX0vA4cHe2IUhy0zhnEqsnQPHvEZnfS95pdmc818fJBoKI6tk4sh0t/Tjc4qK84IAYDLrcbT2Dn/t7rbj7j7yQcTV0Y/Boh+bnCUJZi6vZu8TG3C29yUcX3zzheSX55wxAZMIliapoC+E3KPiT3//JXt2H+DAvjpqZ1SzYuVi3J0efJ7U+UzxY6yY0Om0/OnJX+Ed9LHl3Z38/Ie/5byLVnPBJWdz913fpb21a/jcBYvnsHzZImZXTic4GKR7jyiKNp4qZxez+XfPEfQcVdBRgbZ3D6I16ChZMC1pyqxl4z5mrFky/EKoxBWc7VaMGQb8aX6HjhYOhKnb2oLepMNUXkwkHOPAlubj3tLE3u3EPsIm0IJwplOO8cekxONDc+5jILc4m+4dDYmB0lCn2PX4G5z/lZuwdQ0c19+73qynuCoPnV6DJEl4PQGsbXYiocmRtiGCpUnM7/bj3+OnMquUmZfXEAqE6N7TO+JjNLKG0vIiujutSW2SJFFdXUH75g4suWZWL13K6mVLiQfj6A06PvHZD/Off72EJElc+8HLmTWjls7d3cSiYiRpNJmyjBRV5aFRy0iyjM8TpLfVNuKLikanITzoTwyUjtLyzgGWffjipGDJPzCIzpKYNBr2BlGpU69iSyfoCx33tg2CIBy/oD9MdkUBzo7UU9NZlYX0bGkek3vnlmSx5T9vpW5UoP9QBxm5Fjz2kQvIZhdmUlCcwb6n3x4eFcufXsrca8+m5WAPgeP4YDbeRLA0Bfjc/uPe0mSwd5BvfPuLfPHT30oaZfroJ28i5Bp6wxt0eBl0HJXE1wklmYWs++JnAfA7/PTsHzkwE05cXlkOmRYt+x5bj88+lAeUV1PCvOvPGfFFRWvQ4LOn32IkGgynXB2ZVZ6Ptz9xZCe3poSm/d2n8CwEQRgtva125l1/Du/++t9JKQ61Fy3G2T943CO5kixRWJlHdoGFeCSGpFJh73Vh70o9lSbLUkLR2PcLe/2o8kfe4kulVlFclctbP38qoRaUrbGbd3/9b876/HWTYjGHCJbOMD6Xn8yiDB55/Fc8/NDf2L+3jsKifD726VuoKCqlryF9Yq3f7Rd7do0hjU5DboGZt3/1dMKLn725h00PPcuqz17Dwc2pX1RC/jBFtfkp2wA0Rh3xFCOAM9YsYd+/3x3+Oq+2lEhcEXlngjBBhPwhetoHOO9LN9D6zj4cLUMr4aadv4CYpKb94PF9sJFVMrNWTKP1zT3s/1sDSjyOSqum5oKFTF9cSeOu5BV1gcEgOdVFDLQmz0QA5M0op6stfWVxgIKKXJrW70pZNDPsD9Ff10FG3rFHp8abCJbOQG6rB/WAmk994qOodDJKTGHQ6h0xUBLGXmFlHvUvbUv5KTHkDeBs68WcbcKbIvk5Go6iNuox5ljwDyS/6MxcsxRH25G8A32GkQUfOBevzYXf4UFj0DHt3PmULp3BgXdTLwAQBGF8eOyDHBjwkldZRt7cGqLRGF1tTsKB9KM+71c+s5hDz22i/1DH8LFYOErDKzuIR+MUVOTT3zEU+OhNOgor8jBm6llw/Tm8+fOnkjb9zSzJRW3UEw6MnHNkMOsSEtTfz9HUTcHyuSJYEiamaDiK4xifCITTy2jWJSdSHsXR3EPWnJqUwRJA28FuVnzyCnY/9jquzqENO2WVzLTzFpBZXYLeH+a8xdOJx2IoSDj6PGRXlnDpvR8lEghha+iiZ3cTs1dMo6fFhrMv/bSeIAinlxJXsHWe5Gu2BCaLLiFQOlrrxr2c/YUP0N/hoKSmAINORf1Lm3H3OKhaPYfz/ut69j7zDs72PlQaNRUrZ1F51jzqt7ce89bRSAx9hpGgO/Xrlj7TRHQSjGSLYEkQJohIOIohy5x2CxFjjoVIOJr28SF/mMZd7Uy/8iwMRg3xSAyVVoPd6qZhR/KLms6ko7Akkzd+8gSRwJHk7IZXtrPik1cQjcYZdEzsT3uCIBybWqMm4EpfSDIWiRGPxsgsyECOhNj86OvDbS0b92Hd38biWy5El2kiEorgsHo4uLkJJX7sZClbt5OaCxax4y+vpmyvWDWHhl2pg7iJRGx3MgWZc0zk1+aRX5tHdnHWWK0qFUbZ4ReVlCQoWVSLyzryaE84GKFtfxeHtrZSv6uDg1ua6U9THLJkWgG7H3sjIVACiMfi7PjLq5TWpM+BEgRh8ohFY2hNI6xwlUDWqCiuzOXAUTmMh/mdg7z722eJx+LUbWvF1uk4rkAJIOAJoM4wU3X23MRbShILPnAu7gE/8djE3wRYjCxNIbJKpnhuETt37uXJX/yHYCDEBWvO4roPXo6jyUlgcOIvzzyTeZ0+iiorqVw1h/bNB4ePy2oViz90Ef3drqS8gVOh16tx96Qe1o8EQsQCoaFCkZPghUwQzlSZBRnkFmYiyRKDTh+2roGkQEaJK8QVCVNexvAq26MVzq5k0OnHZNYR9qUubqnEFYIuLyq16oTLxbTs6aBsRhWVq+bgbOtD1qjIKi+gv2sAW8vkyJUVwdIUUjAznx/+v1+yffPu4WN/efhJnnv6FX7/l5/Tszd43J8GhPHRtLudstnVVJ09F3e3HbVOiyk/C2ubjYFjjCqdqGMFQdFwdKjcwMRPJxCEM45Ko2Lm0mr6D7Wz74kdxCNRiuZVMfecBbTs70oqKtvZYGX5xy5j68Mv4ncemV7PKs9n9lWrqdvWwoxFFSPfU6s+ZmHjdLoarUiShMGiJ+6L0d018csFHE0ES1OE1qClz2ZLCJQOcw64+cdf/sV1l1/OQJeoljzRDb2ogM6oIx4PEm5Ov8faKZFltCZ9yk+SkiRhzLEQq0+fcC4IwvipWVDBnsePLOaAoWr93bubOeeu6znwvpyioC9E874uFt92KUokSsDlxZSXSTSmUL+9lVgkRigUJaM4B09vct0ljVGHpNWc0gduRVGOa2eAiUjkLE0RGXkWXnxufdr2V17YgDZDexp7JJwKRRl6cTuRpcEnytruYN5156Rsq7loEY5RHskSBGF06E06wu7BhEDpsIg/ROvGveSX5ya1BX0hGna00XKwB5vdT+PuDpr3dBANR8nIs2DKNLLk1ovRGHQJj5NVMss+eik9Lcn3O1OIkaWpQlFQq9P/ODUa9ajmuwiTn9vmwTCtgNWfvZr6l7bi7rZjzM1gxpqlqCxmWvd1jncXBUFIISPPQs/u9NNYPXtbWLp0Jv3JdSYBiEXjxKJHPogZMwwUl2fz5k8fx1yQzcqPr8XVZcPd4yCjOIfCOVV0NvUn7upwhhHB0hTh6vdw1bWX8Oy/Xk7ZftX1awm7xN5dZxJJlsgrzcGcaSAeV3BYXXgHEmudWFv60Rl1VF+yAr1RSyQUpb9zAF9b4nSt3qwnK9+CooCzzz2mI16CIIwsHldQaTVp21Ua9QlNl5XUFLDz768Ri8Rwd9t5+9f/IbuiAFNeBo6WXnJqy3Gd4XXXxDTcFBENR8kwWrjs6ouS2soqSrjug5fjFNMqZwxjppG5q2oJdPSw/4nXaXjuHTJ0EjOXT0N+3x5xIX+IjkM9NOxoo3V/V8I+gyq1ihlLqyksNGHfXc/AvkbKKrOpWViRcq85QRDGnqvPTdmymWnbK1fNxtGfvOotHbVKwu9IPN/Z0U/XziasB9pRIpEzvgSNGFmaQmyNNj52+y1cec0l/POxZ/H7A1xy+QUsWTKfvoN9x73ZojC5qdQy1XNKePuX/yJy1CaYe558k4JZ5Sy84XyC/hBelx9rm53oCIUupy+pYv9TbyZUFu/Z00zRvCpqLlhM0+6JX0xOEKaaaDhKJKZQsXI2HVsOJbRlluRSOH8aB09gc9pjjULFY3GQpKFkyjOUCJamEEWBvvp+tAYtn/74R0GCoCdI956e8e6acIr0Jh1F1fnoDRqQJNx2L/0ddmLR5GW8BRV5NL62IyFQOqy/rpPBXge7Ht9AZnkesy9fRXerDbctuVK3JdeMq60n5RYs1v1tVKyYjc6oI+QX07uCcLq1HeymatF0ypZMp3N7HbFQlOJFNRhyMmnY0XZC15JU6VfGymoVGpP+jC87I4KlKSgcCGNvF/u+TRU5xVnkFVo48O93cHXakGSJkoU1zLpkGY272pPyhzJzzezfl37Ppr5D7eRUFdK7vw17Yw/n/tcH8LkDSSNMOQUZND6fXM33sI7NBylYNofeSVJUThCmFAXaDnSj1qrJmVGNJEnYbIMEWpKX/R9Lb5uD+defw46/vpbUNvuKlfR3ipIzImdJECYwjU5DYWkW7/7638PLhJW4QveuJrb+4XlqFpQnPUZRFGRV+j9tWaUa/pQYj8aof3krhZV5SedJkjRipd5YNIYkXkEEYVxFw1H62+30tdkIDKauvn0sbpuHMCrOvus68meUobMYya0uZtWnrkSXn4O9+8QDsKlGvNQJaelNOrIKMjFYRthTSBhThZW51L24NeUQuH9gEK/VgSHDkHB8oM9D+QjJn4VzKrE1dQ9/3XeoE0uWMek894CPkkU1aa9TumQ6bvuZu5RYEKYSa5ud1jorZecsZPFta6m8eCld7QN01veOd9cmhEkXLD344INUVVWh1+tZuXIlW7duTXvu73//e84991yys7PJzs5mzZo1Sed/7GMfQ5KkhH+XXXbZWD+NCUOlUZFXnUvRvEJyZ+ZQPLeIvIocShcU48TDhm3vYg3YKF1YjDHFG+phWoOGghn55M/JJ7M2k5L5RWQVZ57GZzI1GS0G7M3pc85s9Z2Ys0wJx+zdA1SePQ9jbkbS+VWr5+DqtBE7aspNY9CmzH1yWl2ULJqO4X3XB7AUZZNRko/P5U9qEwRhcooEI3Q1WGne00FnXS8hvygRctikyll6/PHHWbduHQ899BArV67k/vvvZ+3atdTX11NQUJB0/oYNG/jQhz7EWWedhV6v50c/+hGXXnopBw4coLS0dPi8yy67jD/96U/DX+t0uqRrTUVavYaC2QX8/McP8c6bW1EUhZy8bL74tU/Rs62P3/7y0eFzzRYTv/7jj1Bi8aShXr1ZT1Z1Jt/55o85tL8RAIPRwKc+/1FWL1tKf9OZW/X1VMWiMXQWAwFn6hEcfaYpKddIiSs07mpn2ccuw93Rh3V/K2qDjrJFNfgcHvb/JzEPqeqsudh7XSmv37yng5Wfvpqu7fX07GpCUklUrJhFwdxqGne2jcZTFARhktIatBRV5mHM0BOPxbH1uHCmeS2Z7CRlEpV1XrlyJcuXL+eBBx4AIB6PU15ezhe+8AW+8Y1vHPPxsViM7OxsHnjgAW677TZgaGTJ5XLxzDPPnHS/PB4PmZmZPPu7FzEZkj+FT1Ql84r58hf+h66O5JGL//7+Op74279pONQ8fKywKJ9f/uYHWA8mro4qWVDMnZ/8Gvb+5Hntb9/3NSqzS8/oyq+nIiPPgl4Jc+DfqROtz193I4e2t6ZdqWLONmHJMWHJNuNpt7Lv6Y0J7dkVBSy4+cLhZcbZRVmYLHpisTj2bieRUAQkyC3JJjPHjKIoOG2DZ3yBOkE40+WWZJNXZOHQ85txNPeiNeqoPnc+hfNrqN/WcsyNuicKX8DH1Z++HLfbTUZG8mj8YZNmGi4cDrNjxw7WrFkzfEyWZdasWcOmTZuO6xp+v59IJEJOTk7C8Q0bNlBQUMDMmTO58847cThGXkkWCoXweDwJ/yYbjV5Db19fykAJ4E+//Qc33Hp1wrE+q41gJHFUSWfU0drakTJQAvjtrx7FmJ9++k4Ymcc+SE5NKYVzKxMbJFh00wU4+jwjLun1On30NvfTsL0FyWLi/HU3Mn3NEqrPmcfqz1zF0o9eQiwao3pBOfPPnYnkcdP+xg5sOw9ROT2fitkloICj20nLvk5a93eJQEkQznB6k47cfBPv/OoZHM1DOU1hf4j6l7dz4OmNVM0tG+cejr5JMw1nt9uJxWIUFhYmHC8sLKSuru64rvH1r3+dkpKShIDrsssu4wMf+ADV1dU0NzfzzW9+k8svv5xNmzahUqlSXue+++7ju9/97sk/mQnAaDHw7t5tadu7O3vJzctOOh7wB5FkafgNWm/WsXX7zrTXsfb0I6nO8NKvp6h+eyuV5y5ixsVLsbf0oNFpyakuxu3yIYWi5JXnMtDjPOYnud4WG9Y2O5n5WVQuK8Tb42DLH17A7xwku7yA6RcvJuj2MdBmHTp/fxu1Fy+mpKaInmZRHkAQhCFFVfkc+M+7KfcbdTT3MEtSUGlUxCLpV9NONpMmWDpVP/zhD3nsscfYsGEDev2R1V233HLL8P/Pnz+fBQsWUFNTw4YNG7j44otTXuuee+5h3bp1w197PB7Ky5OXcE9k4VCEktLitO1mi4lwOJJwTJZlCkry8SpenD2uoesEwlRNq0h7nZzcrNHo7hlNiSu07e9CVsmYMo3EZQ0KEB1wYavvQpdhZMaK2fi8YTQ6NTqtTCwaQ63XYe910d9uT7iW0aKn/e19tL69f/i4vbkHe3MPyz6yBk+vA2fHUHDUtH4X53/lRnpbbGIjZkEQANAbtcOlTFJxNHdjyjThsScXu52sJk2wlJeXh0qloq8vMV+mr6+PoqKiER/705/+lB/+8Ie89tprLFiwYMRzp02bRl5eHk1NTWmDJZ1ON+mTwAOeAHMXzkSn0xIKJa94uPaGy3jpP68nHLvupsv559+fxWQycOmaC+g5aCUwGGTOwpmYzEZ83uSVUR/95E0E+gNj9jzOJPFYnMBggIqZRWx66FlCg0e+3+2bDrLwg+fiHfTT8MoOAGSVTM0Fi6hZWEnzniPbkmTnmdnzyP6k6wMceG4zc69exfa/HClOZ2/qxpRlxOv0pXyMIAhnGGmosnc8TR02jVFPMDw5cpaO16TJWdJqtSxdupT169cPH4vH46xfv57Vq1enfdyPf/xjvve97/HSSy+xbNmyY96nq6sLh8NBcXH6UZepwtM1yC9//wNM5sScovMuWs01N1xOY30LADl52Xzuyx9nzvyZ/OPRf/H7B//Kiy+/TnZpFgCudhe//uOPyCs4kgsmSRIfvOUqzjl7JW7b5MvpmqgKq/Kpe3FLQqB02J5/baRwRjm8N+sZj8VpXL8Tb3c/mflDiYtag5bBvvTVeAMuLxrD+z4IxJXhawqCIAzVcpuRulGC3Gkl+FxT68PVpBlZAli3bh233347y5YtY8WKFdx///34fD4+/vGPA3DbbbdRWlrKfffdB8CPfvQj7r33Xv7+979TVVWF1TqUi2E2mzGbzXi9Xr773e/ywQ9+kKKiIpqbm7n77rupra1l7dq14/Y8Txevw4spbuSRv/+Knl4rLqeH2hlVyBGZQLePB//4Q5qb2vF6fTz71Mvs2LJn+LF/f+QpLrviIugGn8uPMW7g17/7Ed6AH78vQEFhHmF3mJ79oqDZaMrINrL7QFvqRgWcnf1kFOXg6T2ScN+4fhfLPn45bpuHeCyOSjvyn70sJ36Gyq0tw7Yj/fYpgiCcWeydDuacv4iBVmvihy8JFt9yEX2dU2+7rUkVLN18883YbDbuvfderFYrixYt4qWXXhpO+u7o6Eh4of/Nb35DOBzmhhtuSLjOt7/9bb7zne+gUqnYu3cvjz76KC6Xi5KSEi699FK+973vTfpptuPlc/rxOf2otWoK9Xk46gdQ4goanRpbvZOvfeE7KR8XDIYIBo+sjPN7AvgPBJAkkGSZ3j7raXoGZxZFUWCE1KF4JIqsTlyYEBr0I7+XZB8NR9FnWdImX+ZUFeHqPpKLUHX2XAbd/jN+E82pRGvQUD2/nIxsA6AQVySsrXa6G5M3TBaEVOJxhfrtrSz80MWEPT5sDZ3oLEYK51Rh63FOye1RJlWwBHDXXXdx1113pWzbsGFDwtdtbW0jXstgMPDyyy+PUs8mt2g4mlDcMBaJkVOSlfZ8SZJSBpSKAsokqa8xGQX9YbIrCoYTsN8vu7KQuvdylg7TmQ3EY0eCnd42G0s/cgnbHnk5IWlba9Kz8IZz2ffMO+TVljLtvAWg09G6r3Nsnoxw2ulMOhacN4OYo5tw7+FRX4miolyyCqdz4O3Gce2fMLFk5FlQa9X43X6CvlBCWyQUoX57KzqjFlNJEdFIjINbm0f8MDeZTbpgSTg94nEFi9FEcWkhvd3JnzjPu2g1UV80xSOFsdTbamfedefwzoPPJJUKKFtSi7OjPynpsvbixfR3Hxkqd/V5kFXZnP+VG+nZ24zf7iFnWjE51cU4HV6qLlxKNBqnp8tJyJ/4AimMHlOmkYrZxehNOmLRGN1N/Ti6x3Z395nLqoj2txOPHL3SVSHqtmPIKSS3NHvM+yBMDOZsE8XV+RzeczsSidPTaiPgCZBTnEVxZS79dZ0E7HYKa0ox5JTSsr+LcCBxQVDIHz4jtkWZVBW8J6rJWsH7WPQmHZnVmXzxs9+ir/fI1MzcBbP43g+/Ts++3pR7igljK6swg9LqfBpe24GjuRedxUDtBYvIry3hnd8+h/e9HAJJlpl23nwK5tfQtKs96TqSBJkFmWh0GgKDAbHabRRJsoTeqCMajREJRpLaK+eWUFBiIersIx4OIckq1Jm5RCQje9+qG5NpT7VWzeILaglbk38XhvqsQskqY8+G46tbJ0xeuSXZZGXp2fPkhuGtlMwFWSy6+UK83jBaYmx79OWE30NjjoWVd1zJoW0to1I/Sa1Vo1KrCAfD4zrNf7wVvEWwNAqmarAEQ/u+ZVdm4fK46euzUVFZil6lx95sJzqFCo5NNiq1ioKKXIwWPbFoDFu3k1gkRtn0QtRqmXgkhtqgw2F109d2JNBVaVSgKGmDXL1ZT9Z7K+dc/e6koXdhZJIsUbu4kqw8E0okCCo1cUVF894u3P1Dq0Iz8izMXFRMuD95elNlysAT0tO0M3VAcypMWUZmLSokOpA+n1BVUM3O1w6O+r2FiUOlVjFzSQVv/fyppNppslrFJd+8lVd/8PeUZQGK51dTsHQ2PU0nn99mzjZRNr2QiNdPyBvEUpiN3x+m81AP8XEImo43WBLTcMKIgt4gvQesqNQy+docPM0eXHGx3cV4i0Vj9LYk5y017e5IcTbkl+eQX5pD0O1FlmU0ZgO9bfbhrUtklUzNwgoiHi+d2w8BUL5sJpqMYpr3dEyafZ7GlQQLzp+FKuAg3HvUm4kkM3NxBY27JZx9birnFBNJE7DEfB6yS/ITquQD5JXlUDazCLUKkCQGB/y07u9OmhIZSTgQQdakX7giySpi4uc85eWX59D0xu6URWb1FiOO1t609ZOs+9uYfukyeppO7t6WXDMl5Tls/s1/iASOfBArnFPJzMtXUre1mYk6fCOCJeG4xKJxYtGpPy89FZXPLCbY7+Ctn61HiQ+9Gaq0ahbffCGa8hxsnQNMX1zJoWffGd7nCYZeGPOml7Lopgs4sKkpYQGAMESlVlE6vZC80izUOg0E3IT876tarMQJWduZtrCaHa+40WrVRKPJU3PDp4eD6Aza4VG9mkUVZOeoiTjaibz38zPpDSy8YCb7324iMHh8RV8joQjhCEhqDUqK+6szc2lrFNvaTHV6oxZ3V+rq2/osE7ER/s4VRUGJnXw0U1ZbyLsPPE00lPj713ewHUthNrml+di7JuZKuklTlFIQhBOnM2rRquDgs5uGAyWAWDjK9r++Sn5xFuZsE76+gYRA6TB7YzeO5h7mnT0DjU58tjqaRqdh8cWzyc+OEetvRU2E8IA99cmKghQNYcwwoByjwKekUg1Pk5qzTeTk6YnYe+Hon18wQMTaxqwV1SfU5/rtbWgLK5F1+oTj6swcgnHdhH2jEkZPJBzDmJN6umn25Ssw52emfawpL5PoSeap6k06vH0DSYHSYW3vHiCvOP29x5t49ROEScaSa6aoIheVWkU8HsfW7cJpdaU8N788l6bX02x0rEDHloOUrJhL3TMb096vY1sdsXCEeWfP4MCmphOa+pnKZq2cRmygi3hoaARIkiSU+Ah5fLEIGp0GR4+bvEwLUW+KfbNkmaiiIvLeG0r5rCKirtT5IUosipooerOeoDeY8hyVRoXWoCUSjBANRwl6g7TX9VExqxyNRiIWiRGLgbXdQVdDw4l9A4RJydbpoObCRfTXJ+bM5c8oY6ClF5VOQ9HcKqwpit/Ou+5setvS7wk3Eo1eQ8CZfq+4dEHURCGCJUGYRKrmlhJxetj1l5cJuHxojTpqLljIjKXVNOxsTapxotGq8TvSv0B5bW5UKol4LP2bfDwaJ+wL4re7mbG4kgObGidsXsHpotFp0Oslwq4jeRfxaARZqyMeTp0UL2kNBAZ78Lp85F88BzkcIh4+KvCUJHSFldTtOPImpjdoiftGCE4jAQzvBUtqrRq9WU8kFCEejTNjeRVGowYlEkJSa4jGZdRaNYrfTdTWQUySkM3ZRCXdUP7bGf4zPVOEgxGCoThzrz2LQ89tHs5HLJlfTfPGffgdHlZ8bC1ZZfm0vrOfkDdAVnk+c65YiaLV4UsTvB9L0BuksDY/bbsh20w0MnFz5kSwJAiTRG5pNr7ufg4+t3n4WNgf4tALW6lY6aFk9rSkVSohf4jMsjz8aT7RZVUU4Hb6KFlcm7bQZcn8aqwH2jFmW/A195BTcmbU4lFpVOhNOiKhaNJomsGiJx5KzBUKOWzo84vwdyevZJO1OkJhhfB7ZQT2vlnHrBU16LIV4uEgkkqNotbTsLsrYaf2SDiKOk2OEQBqHbGYm3nnzsBgUBEPB5A0OvQWM77OFsKeIyNO5urp+DrbEq4VC/Ui6w3MWV3LvrfqT/h7JExOXY1W8spyOPfLNxJ0DSLJMpaCLJo27CEei7P54RcpmFnG/OvPRqPXMtjvont3Mxkzqk76npFQFFmvw1KYnXJ/ytmXr5zQ26SIYEkQJomC0mzeffDNlG0dW+uYdu6CpGCpv3OA2ouX0Ls/edRJpVFTung6BzY3MWdlLaa8THz2xJWO5vxMssryOfDcZmZeupSDL2xhye2XTelgSa1VM3N5NUaTemhURqUhqqho3Nk+XIsqEooiqTQJj4uFgsSCAYwl5QT6rcNBicqcgWwpYP+bR+oXhQMR9r5Zh9agxWDREwlF8buTN0furO9j+vy8oZyl95NldJmZzF1tIdjbRtg9FBjpC4oJ9HYSDx0JlDSWTCIeV8qgKx4MoM9QMJj1BNJM5wlTj71rAHvXAGqNCgXIdAYpWVxL42tD0/b99V3013cNn7/0tkuwneKm6K37u1l6+1rqXtg8NM2nDO0wMOuKFWAwMNgxcfcSFcGSIEwS8Ug0/by+AkG3D5VaTqihFA1HsVs9rPzE5ex96i0C7+0Ebi7IYtEtF9LZ2AcKNO1uZ/VnrqJ7RwMd2xuQJChdVEv+9FK2//U1LEXZhAb9QytljpGgPJmp1DILL5xFfKCb8OCRwEGSVcxZWc2Bza34XD4CgwEUtW6osudRc5IhRz8qgwlTeRWRiEI0EsPe5aJ728GUhfzCgfCIOWBumwd/sABjVh4R15HkcUmtxlRaScznJurzJgRGaqOJYH/im47GbCFoT7/STQl4yC7KJNAkgqUzzeF6eQNWJ3NXz6BrR8NwocrDsisKMORkEmg5tQUAkVCEum0tFC2fy4xLlxOPxVGQ6Ot04K6fuIESiGBJECYNWaMasV2t1ybsAXeYo8dJ9pIqzr3rOsK+IJJKRmPU09c5MDzlEw5G2LuxngXnzx6athsYxHqwnfrXdpBVms+C689h66MvU7K4Frdj6lb6Lp1eBIO2hOADQInHCPe1Ubu4gj1vDNWhat7dyYwlVYSs7Qkr1ZAkfIMR9rxx6KRzu4yZRqrmlmDJNqLRqogF1ehzcoZWNCoKSjxGoK8HQ2EJAWt34oPjyXkfynv9SkeBlHV3hDOIMlSnbeUdV9G7t5me3U3IKpmKVbPJriqhYVfbqNwmFonR3Wil+9inTigiWBKEScLvC5NdWYizPTnBUp9hRJHllG94M5ZW0/jKVvoOHMmlkSSJhTeeT0FlHv3tQyMWSlxh75uHmHf2DHRmI8acDGZctJjBPidbHnkJFJh27gIObj7JinSTQF5ZFtG+1pRtSiyGVqOg1qiIRmK4+j3Ube+kZmElKjkGsSiSRo/L4aN5Q11CoCTLEiUziigsz0GSFOIK9LY6sDb3JQVUeaXZTJtXTDzgRo6rGGweKjSq0unR5eTj7z1qFVOqAEeSQJJBORI0RTxutJnZBG2pi2HKxiwcPS3H900SpqyQP8SBTY1kF2Ux69pzUVAY6PNg3do83l0bdyJYEoRJoqe5n4U3XcCW3z83PJ0GoNFrWf6Jy2lvSAyiZFkiuzgLV1tPQqAEQ6MIu5/YwPlfuRFbh2M4yFLiCgfebWT64kqCgwHa3t1PNBShfMUsypbOpHlf17hsSXC6HGuGUYnGUGnUGDONlE4vRKNT4bIN0tduJx5TCPlDSftcyaqhqT054CTa915AIkmUFOeQXzqLfW8dCaxklcy0BaWEupsxV9bg7TgSuKkMRiK+xER9RVGGksNjRwoJhlwD6HPzCdqP/D5EfYPo8wtRefTE3jdqpjJZ8HkjoiSEMMxpTV+O5EwlgiVBmCQiwQjNeztZ9vErCLm9uLvtmPOzMBVm01HfS8AztDpLo9NQObsElaQQ9gbQZOew9NaLOPDcZoKexCTint1NZBXmJLwwxmNx6re3YsoyUXPZKiRJYtDl58Cmxim/vDwaiSOpVChpSilIGi0zllWhU0WIeWzE/RGyjSbyV0+j7VAf/SkSpKvnlyF57USPruytKETdDjQZCqXTi+hqGBrxKazOJ+ZxgCS/N+V2ZHRIiQ/17WihATv6/MKEqbiI24mmpBx9QTEhR//Qc5EkQh43+tJqIoODxP1uJFkCYxZ+X4xDm8XIgSCMRARLgjCJBH0h6ra1oDVo0ZtM9Nv9BNuOJF2qtWpmLq1ix19ewdN75LilMJsVt69lyyMvETpqe4yg24+lsCDlvXwuHz7X1M1POkxr0FI2vRBDhp5oJIYxp5CwrSfpPJXRjIKMJuom4jqyajDm9xLze6maVcWg0zcctB6WU5RBuCd1Ib+oZ4DCqmnDwZI5y0A85Boa4nrfFFtk0IO5opqw88jy6qhvELXJjLG4nIDdihKJgCQT9geQjNkoWVpUqqHp2a4WO31v7sKYaSC7KBMlpmDvbiPkP7XNkvUmHWUzizBlGggHo3Q39OEZobaXIExGIlgShEko3SqqsumF7HnijYRACWCwz8m+f7/DzEuWsvdfbw8fz59Zjn1g6gdE6ZTPKqaoMouYy0bc50TW6tCYy5FV5YTsvSjRKEgSmowc4vpMNFoZvzf1RtKRASuVc0qoO2qURlbJaNQSI01wSUcN14X8YSyZGgiHkNTqxNV2SpyIz4s+vygh9yjY34smKxdDaQ0hf4h4TKGz2YatszNlSpPP5cfnSi5TcDJKZxRSWp1D1NVPzNGPXqNlxsICAqEiDrw79UcihTOH2BtOEKYQg0nHQFvqCrvOjn4yinKGvzbmWDAVZh/3RqxTTU5xFsXlZsK9bcQCPpRYjFjAj6+1HgVQ5VWgKqhGzq3E7Vej0alQgukDy3goSE6hBYP5yL5rWoMWWT3yKkZZfeQzq7XVhsqSB0DY6UCfX5RwbsjehxKLYamZhSa3CHVWPtriarwhLZuf283O1w6y+41D9HfYx7zKuiXXTEllFqHeNmKBoeArHgkTsfegl3xUzS0b2w4IwmkkgiVBmELikfQ7hgPEIlEkWaJ0cS0r77iSln1dSJJEZn4G2UVZqLVnzmDz9CWVhG2pa7uE+rpAgZ2vHaRuexvZ+UbC1g6QR04Bl+Ix5p9bizHTCEBeWQ7xaASV3pDyfLXZklAXKxyI4LB60eQUEnY7QVEwlVejNpmRNVrU5kwwZNC4u5OGfXaa693ser2eui0tSYnlY61qTgkRR+rvX9TjJL8sa0rX5BLOLGfOK6MgnAFUOs3Qhq4pl5SDuSCbc790Ay67l0Nbm8kvzyUn30LfwXYiwRBVsytRZBWt+7uG94yaikyZRtQaCKbb+FZRkCUFJKh8LyiIh0OodHreSyhKeojGkkHE5yHsdDBzWRW71h8kpyiDiMeJsbgMX09iVW2VwYQhvxi7NTG/p2VvJ4GaAipmTUdWhpKzDSWVhPwhrK0OercdGi4kOJ60ejVRd/rNT5VICK1emzBdbMwwUDm3BINJBxI4+wbpqrcObxwsCBOVCJYEYQpx2b2ULZtB57bkfb7KlkzH1uOku3Fomq6kpoCIw8Wbf3lp+JzmDXvIrSlh7nXnULe1hdzSbHKLMpFkCd9gELfdS8ATmPRvbhWzi1MWbzyaSqsCBYxmHbH+oSAnNGDHWFKGvydxx3ZJrUGXV4ivvQUlHkMtRdGZdOgMGtRY8Ha2YSgoRtZoiEejyBoNsVCQSMCHtS15P6zswgxiPg+hQSfEYyjxOJrMHHKKM+lqGN1Kx/kVuZRNL0SlAiQJZ7+X9gPdRMMjj1Iea9hIkuWEgDu/Ipfq2QVEBnqJ9Q8llWcbzeRdNIv9bzedsdPBwuQggiVBmEJ6m/uZee4C1DoN7ZsPEY/Ghqvwlq+aS/22oTo/skomI9vIxkdfTLqGo7kHe30HCy6YTcuG3Wx+egOxcJTCuZXMuXwFkXAERVLRdqiHkO/UVlKNF71JRzwSQdZoiUeS068llRq1Tps0LRnxuJBkmYza2YRcDpRoFLXBhKzV4u9qR3lvpEqJhNAZtCixGJJaQqXV4e/pACQklYwSiyFrdZgqanDbEpftF03Lx6gOExlIXEEXcTlQm6NUzSujdW9isHayZq6YhsUYJ2JvJ/5emYJMg4lFF81m31v1GMwGZLWM1+lLWlDgsg2SaTARC6TI45JkYopqOODS6NRUzy0i1J1Y+DLm9xIPtTF71TR2vnpgVJ6TIIwFESydQTILM9Bn65FkiYg3grPbNaWnWs5EiqJQv62F/MpSzlkxm3gshqxWM9DnoX5ry/D0XFZhJj27GtNep/Xt/ZhzM2jZuG/4mHV/G7b6Ts7+7DXseuI1lt22lsbd7YQDk2+UKRqNEfLaMZaU4+1oSVymL0mYyqqIuJ0svXQe/sEgGqOJmH8oKAi7BtCYM4iHh4KH4EDy9iiSRk/QF0JBwtfdiam0Am1WNmGXE0VR0GZkotIbCAfCSblGJdPyidjaUvfb6yavpGZUgqWswkwsZojYExcExAI+1KZBll06l5BzAOIxJH0BwZDCoc0twwFQx6FeFl00i3hf29CqwSPPHl1hOY17jtR+KqktJOZKXT5BiUWRY0HMOWa8A96U5wjCeBPB0hlApVFRMq+YV15+g6f+8RyBQJCzz1/JJz9zK54Oz6gtIxYmBkWB/nb78DYmqajUMv4RdpgP+1O3xSIxmjfupWBGObv/8TozrjqL1v1dKc+dyLob+qmZlUXQ1oe5qpaod5BYMICs06MxZ4AsEbL3YyzRolN86Ior8LU1DlfKDjntaMwZBPqS6zFJajXRmEw4EKavzUFhQRa+rjZknR6tJRMkibDHDd5BXAF90uNlGWIjLWWLRZFV8il/0KmYWUTUmdx/bXYuskqFt6XuyEG3A7VOz4LzZ7Hrtf0oytCmqPs3NjJ71TTkeBgiAVBpkPRmmvd14+w7UmLBkm0k5h1h+jDsx5xpEMGSMGGJYOkMUDgjn//++n0c3Hckj+Xl517nrdc38chjvyToDRGLjn/CqHD6DA74KJhbSeeOhpTthbMqGEixBx1Af10nC288j5aN+9AbNWPZzVGlN+kon1WMOctILBZHMuqRIgG8rY1Dq820OmKhIGqjiZCtH0mjJh6LEXHZUZQ4hvJaIoNuCPtBpUY2ZaLNjREe6B8emVLpDahzS9m7cWjUrrvRSk7xLLSZEhHPwPAWJOqMLOL6bNq21iV3dIQNb4duoiJ+jHyrkZhzTFTNLcWSY8Tren9ekoQ2MxtvW/L+f/FQEHXITV55LraOoTyrgDfIztcOYsw0YrToiYQGcacYFQuHomg0GpRQmtcZlZpwKH3wLgjjTZQOmOJ0Ri3dVmtCoHRYwB/g4Yf+SlZp5jj0TBhPQW8QY24W5oKspDZZrWLWZcvTLkXXGHTE3puKiYUnR5BdWJXHgnNqsKgHidtakVydSBE/hsJSzFW1aDOy0Zgs6LNzCdr7iPq86LLzCLuGintG3QNEwxEa9tnotsZpbfGz+T+76GjzocqvQlNcjaZ4Gj4y2b2hnuB7o3aKAvveqqO7Jzx0XlE16sJp9Nlgz+uHUo4OOfsGURlNKZ+HrNXhHwyfdLHHgso85iwvR+O3okTDQ8NYR1EbTUR96atvRz0DFFfnJx33u/3YuwZw2zwpH9fd2I86Iy/tdSVDptiLTJjQxMjSFGfJtfDc00+nbX/r9c3c8emPnsYeCRNFf7eTVZ+8nKYNe+jcXk8sEiN/Rhkz1yxh3zPvMHPNElrf2Z/0hl65chZdOxuRVTJqffqRpbyyHPKKs4aSmdUqXHbvexvOnt48OYNZT+XMAkK9RzalJR5HpVETtPej0uuHRnzi8eE94TQZWUgq9XCxRQAlGsHvCeCxHwkm+lpt9LWmzsUZfpwylHjf29x/XP1tP9jDogtno8S6EnKhJLUGTX45h95KPRp4LGqtmqo5RYS6hxLKw64BdNl5hBxH9UuWU++LJ0los3JQ6Q0oWj0Gi57A4PGPBPlcPrx+BWNGNlGP8+gLo80vpaux/7TXiRKEEyGCpSlOiccxmY1p2w1GfeqaPMKUl51v4d3fP09uVRHLb7sUSZZwdvSz7S+vEhoMkFWeT/6MMvoOdQw/Jre6mKyKAg69tI3pFy/G5029kUftokoGGjvZ9PQGoqHIcCHM2RctpW5bc0IhxrFWMbuYqNOadFySJMJOO5rMbEylVcQjYZRoBJXBSNTvw9/dnni+SnVapquj4Sh7NtQxY1kVxmwN8UgIWaMlFIqzd2MjwZNcgVg8LZ+o+0hgF/G4MFVUE49kEfG4AIgF/eiycwkNHMl3UxvNGAqLCTkdhOz9SGo1c1dW4POe2Aa8hzY1UTW/jPzSGuLhAJIko6h0tNX1Dk/rHabVazBlm4hFYkP7zImXKGGciWBpinNa3Vxx1cU8/pdnUrZfd+MVBAdErsCZSKVWEXT56NzeQOf25NGKoMfHghvOp2PLISKBEPnTS4kGw+x+YgNzrlxJZkku7i4rc1fX0nqgG/97G8jmlebgaOig4ZXtw9dS4gpdOxoJenxUX7ycttOYFG7M0BOzJQdL8VgUWaMl4nYScTsxFJUSC4cJ9PcmbWIra7QE/NHTNvoRCUU48E4jKrUKrV5DJBQ55UKUpiwj8VDiKJivow19fiG6qukosShxSYUiq5D1RuJBP5Jag76giMG2piPfk0iYWMCPMSObaQvLadkztDJPkiRUannEfrbt66J9fzd6s454XEkqPaHWqpm1choGvUw85EOSVUj6Sjob+rC2jDyCJ0wuao2KgopcDCYdkUgMW7czaRPqiUQES1NcPBZHHVdz2ydv4s8PP5HQNn3mNK6+di2duyffaibh1PkHA+TVlNBfn3oZev6MChp2tqK1ZJAzzYI5x0QsFGbB9efQtqWOg89vAaDpjd2cdee11O9sIxqOkl+axTtPv5HymvbGHuZcNUKV8TGgKCRuSPue0IAdfX7hcIHJQH8v5ooaYgEfseCRF21JrUFTUMGhjelLLYyVWDRGwDs6o1khfxiTUUs0cnSpB2VoU16bFXVWHu1tfpy9LhaePwu1fhCNTpsyeIShLU1yS2robbExbWE5RpMW4lFQaRjoG6R1TwfxFMGloigpp/AkWWLBBbNQnN2E3Ue391FeXQpw3AFTRp6FqnklaLUqJEki4IvQur9LrPydIHKKsygsz6bxtZ0426zoM03UXrgYVVUeLaNUQ2y0iWDpDGBvcbB2zYVceMk5PPfvVxn0DHLJ2vOpqqyg94BVDHGfofra7cy6fAW2xq6kEZPDm+x2trfgdwdw97uZvWIab/7fP5OuE/YFqXtxM0Ur5tHdaCUeiw0ngKfid3hQ69REgqenPpO1zUFpSTZRz0DC8VjAT9ycgbGkgkBfN0oshq+zBWNpJZJKQ8jrHfpvKM6+jY3DSduTVU9zP3lnVYM/9WbAsjGTga4u4nGFnesPkFeaw4xllcT6ulOeD6BEgyw4bwZhazthz5Ep2UyThQUXzmbPG4eOezQuvyIXOeAikmJVXNjWTfmMmuMKloqn5VNWk0PE3kPkvVIPGo2WuSuraNrbw0Cv67j6I4wNQ4aB3DwTG+9/avh3I+jxs/3Pr1B97nxKasvpaUq9Enc8ndBquD179vD973+fX//619jtiTVcPB4Pn/jEJ0a1c8Losbc48LV7+cBll/OxW24mI26mZ3/vcWxpcPJMWUaKZheQNyuXwnkF5NfkodGJ+HyiiISi9HU5Oftz15JVPrTCSZJlSpdMZ8V7m+weZsw04WhOXyfHerCdjJyhFVyyRo1qhA15TXkZVM4qwZydesXXaOtvs4ExG1mboqaRRkssEsZYUoG5YhqmsipikoZtrxxk/+ZOdr7RwN436wlM8kAJIOQLMWDzo8kpTGyQZLSF5XQ19B0ZCVLA3jVAwDtyfpTOZCZkbUuqgh7zDaIKuylKsXIuneKqPCIJyd+JlKAPS655xGuotWrKZxQQ7usYrokFEI+ECVnbqFlUjnSs0gzCmCquymffvzamDKJbN+4jK888ITdgPu53rldeeYWrr76a6dOnMzg4yL333suTTz7JhRdeCEAgEODRRx/lj3/845h1Vjg1sWgce9fAsU8cBZnFmbgjHr79xR/T3TmUL7J89WK+9s3P42pxTYk3n6lgwOrGPxhk+hWrMVr0qDVqgoEwLfu7Era3kGSIp1oldZgCOp2aBedMR5ZlVn/qCpS4Qsvb++ndd2QVmiHbjN/h4eDzW5h9+Qqy8i10NSTnE42meFxhz4Y6Vl61ECIhIr5BJFmF2mgmNGAj4nFxOCTQZubQbY0mbe3xfhl5Foz5BlQaFRF/FHePh5B/4m/90ry7g5LaAkpqaiAWQZIkYopMy4FeHD3JgcpAr5u8DEvqcgKyDEocJZJ6hDDqcVJUXXXcqwBllTy85UoqSjyGSq0a8RpF0/KJetIUY1UUFJ+LnJJsHN2n53VQSKbVqhjsSx8Uu7ts6E36CTeSe9wjS9/5znf46le/yv79+2lra+Puu+/mmmuu4aWXXjr2g4UpQ6PXkJFnwZBhSHuO1qAlqovyxc98azhQAti2aRd3fuJucmpyTkdXheMUDUfRG3UM9jjY9/RGml/dTmGhmdrFlcjy0Ec8n8tPXm1p2mvk1hTTvaeZ9T98jK2PvoIsq9j1+BvkTy+l5tz5ABiyzCz7yBoOPr8Fn93N9r+8iuL1kVmQMebPUaPXEPF6h2actQZcYZk+u4eQzoKcW4hszkRlMqPNK8TaYkNv0lE4s4D82XnkzMimaE4hpmwTkgSl84vZ31bHF+78Jjdecwff+/7PULIhuyxrzJ/HaOhp6mf7y/vZ/VYzOzc0svO1gykDJYDuxj7krEIkVXKQos0tIhoc4Q1NUZBPYIRg0OlDZUi/clfWm/C5Uk8hHmbK0KOMUNxSiQQxZSSPMAqnj3SMXwpZrUI5haKrY+W4g6UDBw4MT7NJksTdd9/Nb3/7W2644Qaee+65Mevg+z344INUVVWh1+tZuXIlW7duHfH8J598klmzZqHX65k/fz4vvPBCQruiKNx7770UFxdjMBhYs2YNjY2nP5FzPJhzTBTOKqBkQTGFswowZaV/odLoNZTMKyaWEWfz/h10e62ULCzGnJM8lZJZksGv7/9Tyus4bAPs3XMAU2b6ewmn1/Sl1ex94g12/n09/XWdWA+0se1PL9H86namLawAhlaz+QZDVKyYlfR4lUbF7LXLh/eRc3fb2f3UW8y8ZBl7//U25ctmsPrTVzL/2rPY9dgb+OxHtsE4+Pxmiipyx/w5qrUqkFU4B0P87e8v8MXP/DdNTe088/Rr3HPPL/nHf97BFtXS2zNATkkWhlIj//OtH3LrBz7LR2/8PP/1uW8yKPsomV/Cw3/4O/9332/o77OjKAoH99Vz58e+Rr/XkfLvYaKKhqPEjrHCLhqOcuCdJtT5VahzClGbLKgzstAWV2PvDxOXRpickGViseNPiOys60WdXZT6UnoDPl+USGjktIGAN4Sk1aY/Qa0j6Bt51FAYW77BILnTilO2SbJMRlEuIf/E+xkdd7Ck0+lwuVwJx2699Vb+8Ic/cPPNN/P0CIUPR8vjjz/OunXr+Pa3v83OnTtZuHAha9eupb8/9TDvu+++y4c+9CE++clPsmvXLq677jquu+469u/fP3zOj3/8Y375y1/y0EMPsWXLFkwmE2vXriU40iemKaBwZgEdrh6+8qV7ufHqT/K1dd/FGrBRUJucY6DSqCicXcBXvngvX/jUPfz2V3/mu/f8hI988E6ixljSG4TGoGH/3hTbOLxn25bdGEWwNCFk5FlwNHbi7k6euuiv6yDq9aMz6QDoqO+hZPlslt12KTlVhZjyMqhYMZOzP3sNh17eRtBzZKWRp8eBMceCJEs0vL4LW0MXWx99Ba/NnXCPsC8IY/wpUmvQUD27lJ7Wfn703Qd4+Df/4Etf/zTf/9bP+OsfnuDGD18DisIP7v0Fb7+9FWOxic9+7Ks01rcMX6Ov18brr25EUSu8+J/1Ke/zfz/4NeaikXNqJiO/J8D2V/bTdHCAPqeK7u4oO1+vp3VfJ77BMLIu9UiNJjOP7sbjm4KDodV6rQd60ZVMQ2V47zVFllFn5SNlFFO/pSXpMcYMAwWVeeSUZCPJEr0tNlQZ6fOkVKYs7F2OtO3C2LO22ph3/blojLqktoU3noe1c2L+fCTlONfvXnrppVx66aV89atfTWr7xz/+we23304sFiM2Ul7DKVq5ciXLly/ngQceACAej1NeXs4XvvAFvvGNbySdf/PNN+Pz+RJGvlatWsWiRYt46KGHUBSFkpISvvKVrww/L7fbTWFhIY888gi33HLLcfXL4/GQmZnJs797EZNh4n+yzC7L5q0tm/jDr/+a1PaFr97B8jkLcfYeeVPLq87ldw//hTfXv5t0vtFk4NHHHqBn75Hk38JZBXzlS9+muzN1QvAdn/sI5y5ZiavPnbJdOH0q55TQ8Ow7eHpT53AUza0if+lsrC1H3vQMFj35pTlkF1hoXL+Tzh2NxFMUa1z20TXsefItjDkWShfXDpcaeL/z1t3Ioa3Jb4QnSpYlCirzyM63oMTiaAxaJJWMWiUTi0Tpczq5+drPcMOtV+MccLH+pY386uH7+Mn3HiAcjvDtH36NtzdsQavR8Kff/iPh2l/5788RDATR6XT87Ae/SduHv/7zNzjqJ+aL/VhQa9UsunAWiqf/SF6TJKHJysMf0XLw3Sa0Bi3lM4uw5JiIx+L0ttixdzlSVSMAOHJ+rpl4NEZPsw1H90DC+Tqjljmra1ERgZAXZDWSMZOeZtvQh7tiA2H7kZIHkqxCk19Ke4NjxA2mhdPDmGmgem4Z/YfacTT3YMgyU7FyNgO2QfraTu/PxxfwcfWnL8ftdpORkT4l4LhHlu688066u1MvIf3Qhz7EI488wnnnnXfiPT1O4XCYHTt2sGbNmuFjsiyzZs0aNm3alPIxmzZtSjgfYO3atcPnt7a2YrVaE87JzMxk5cqVaa8JEAqF8Hg8Cf8mE0Ounkd+/1jKtt8/8BfMxWbyynOHVyupTWreej3198PvC9DV1YtGd2TbC7/Nz22fuCnl+bIss+ay83D3i0BpIpAkacTtR+KxWNK+roHBIB11PYRDUTq2NaQMlAD0FiPRUISs8vyEUaej5c8ow+s+9UJ0skpm1soaXHWtbLz/KTb+4ine+vk/6dpah62xi33/fpfGhqFE81XnLGPDq++yfPVidm3fR0dbN+u+eSff+fqPCfqD1B8a2kTWkmHmv+7+FI88+SsMBj2P/fmZEavhA2g0Z9Zqz2g4yq7XD+HwalEXTUNTVI0qv4rO9gAH320ivzyHRedPJ1PnR7G3Ibm7qKw2sfjiuag0qZO1w4Ewzbs72L3+IHvfrMfelRgoqdQyC86bCa5uIrYuIh4XEZedcE8zJRVmIsEIbQ1OVPlVaIur0RZPg5xyGvdaRaA0QfjdAQ6820hIpaNg6RwMFSXU72w/7YHSiTjuv+zrr7+e66+/njfeeGN4BdzRbr31VgYH02/AeKrsdjuxWIzCwsRlr4WFhdTVpZ7ysVqtKc+3Wq3D7YePpTsnlfvuu4/vfve7J/wcJgq3y0M0knruPxgMYR8Y4D9Pv8y8BbNZtnwBcZQRCwi6nG5KjAVE3lsMNOjwsmTxAq66/lKee/qV4fM0Wg3f+/E3CNqCaT9VCqeX2+GjdHEt9S9vT9letnQGDlvqv2tnv4fKlbNo23QwqS23uhiPdQAkqD5vAdFAhPbNh4gd9XtnzLEw97pzqN8+FMTIKpm8shx0Bg2hQARH98Axt0UxWAwUVOSQnZ+BEotTtXIW5UunEwtHaXx9N/Wv7mDGxYsxZJkIa4Y+G8qyRCwWY/U5y/jPv16muLQQj3uQvl4bDoeT4pJCLBlmfvSre3noF4+SkWHh9w/+BYdtgPyCXHQ6LaFQck7FyrOXEvWPXSmOiSoWidG2r4u2fYnFbXVGLdVziwn1HDVqGI/z/9m77/g26vOB4587bdka3ntm70lCwg4BAhTKKLvsEvamrAIto+xSSkuh/Jgtq+yyIcwwQgIJ2c5yvG15a2/d/f4wcaJYcpaT2M73/XrlBb7v3enOsqRH3/E8EWcrstHLyGnlrPpuffeiEVVRcbW4tvmc5w3JRvG09khXABBubaRw+BB+/HglrXUdXZOJVVW83/RT7jZPXK3F/myHvwbNmTOHq666invvvRedrqs3oa2tjfPPP59vv/2Wiy++uM8vsr+55ZZbuO6667p/drvdFBUV7cUr2jE6ffLipwABf5D33/6U99/+FIs1lade/Aujx49g9fK1CfcfNqKMjnXxq2kaVzdxxqkncta5J1Oxaj2pFjNl5SV4Gz24HAOrJ24w63Q4GTNzGHWL1uLvjH/TshVkYs5Oo66mKuGxzdVtjD1sIlkjCnHVtdKwrBJfm5ucUcUMmzWJZW/MZ/rvjsVR0044FOPAq0/GVd+Cv82FvSQHgzWVDUtriIajZBakk5Vvo+rbFXiaOkjNTWP4QeNpc7horUs8RFgyugApHGL9Bwvwtrmw5qUzfNYkWjc0UL94PeNPPgijLYXKb1Yw7byjSAkGMJlNrF6xjinTJ3Sfp7i0gPVruj7Qv/t6EX/9190UFOXxr8deYPmSVZzzu1NpbupKhvjSc29y6z3XctfND8dNOcjOyeSGWy6jpWJz0kRbjhWtWYukSnhbvTtd022gKhyRuCYfgBIMYLZrGXvgcExmGTXgAUlGGpdPu8PDxmW1CY8DyCpMJ9qa+G8SQA35SbGb8Tn9ojiv0Gd2OFj68ssvOeecc5g3bx4vv/wyVVVVXHjhhQwfPpylS5fuhkvskpmZiUajobk5PrNnc3MzubmJV1Dk5ub2uv+m/zY3N5OXlxe3z8SJE5Nei8FgwGDoOTltoDDqDOTkZtHs6JkNt6ikgJbmzV2hHreXP1x/L3+6/0bOOfnyHvsfcMh0NDFNzzcltSsRpiRJFKTmokQVGpclT2oo7D0bltYw7aJjaVpWScPP65E1MsXTR5E+tJD1i+M/lFLsKWh1GiLhCKWjC3CsqMKxYiMavY5xvz4Ae1EW0XBXnqJxp86isaqFcDBCXmkWakzBVphFSk4GjVWteNd2zYOyZFqwpGiZ/+gb3dnknfWt1C9ex/QLjiZrxlCaa9vpaOzEkmEhqzANs8VIyOVjyWvfEXB6AeiocvDDMx8x8ZRDsOZn8NN/5nHgZcfjWFUNQOO3q3nwkT9wz51/47Y/X8fLz77J7DkH8/Xn3zNt5mQAIuEI77/1Kb+94Dc89tD/dV1Lh4vc/GwcjS0s/G4xRqOBvz97H4sXLqO1uY39ZkxizOgRtK5rIxqOdmUoHpLOJx98wfyvfiAl1cwZZ59E0ah8mtc07zO9HNb0FGKtySd4680GYu4Gwg7v5o3ONtJsGQydXMKGJTVJjuz9F6gqCrK8Q/mWBWGbdvgvaubMmSxdupSxY8cyefJkTjzxRK699lq+/vprSkpKdsc1AqDX65kyZQqff755JYqiKHz++efMmDEj4TEzZsyI2x9g3rx53fuXlZWRm5sbt4/b7WbhwoVJzzkYuOpcPPC3OzCnxOdKslhTufGPV/DCU/+N275xfQ06nY77H72dgqKuoDIl1cy5F53OdTdeQnt1B9aMrtphW2deVVWVoDe4zSR/wt4T8odZ9f16VIuVUScezPDjDsSPjoofNnQXRc3IT2PM/kOx6FU0fg+lw7Lxt3Sw7tOf6KhupnVdPT888xFL35iPx+mn4scq1v9cDcCwCUXUfLmY+X99na8ffo3V73xDYXkm1kwLAAXlWSx/c37Pz0AVlrz8BWGXl0hzK5MOG02qTmXZi/P48v5XqfhoEfudcwSHXvcbZl7yKyadeggpmTZWf/ADQw4ah6qo1P60jvwJ5cgamc6NDpT1rTz1wsP4XF4uu+58Djx0OpFwhBGjh6I3dC05//SDr2hr29xT+tZ/P+Cs80/u/vnrz7/n8vNu4qeFSwmHI4wbN5q6nxsI+kLIGpm0cju/++01PPX4f1izaj2LFy7jhiv+yPPPv0pWgtWmA1lqegppufbuFZNbUmJKVzbTBDTmFGJ+LzG/t0db1NVOWmYK2iRZ4L3OwLbzMblEDTihb+3UbMR169bx008/UVhYSGNjI2vXrsXv95OSsntXgl133XWce+65TJ06lWnTpvHoo4/i8/k4//zzATjnnHMoKCjgvvvuA+Dqq6/mkEMO4S9/+QvHHnssr776Kj/99BNPPfUU0DW59ZprruGee+5h2LBhlJWVcfvtt5Ofn88JJ5ywW+9lbwp4gshamX//93F+XrKCdWsqGTVuOBmZafz9oaepr23scUzIHyJTl84jj94Fmq7fXbAjSNgXxlKSyg/fL8FkMjB95hSCnSGc9c49f2PCLul0OOl0OHtszyhII0Uv8fVfXuueu7b2k59IK8lh+gVz+P5f73f3LDpWVlN24Dh0Bh2RUFfv07ePvUU0uDlY7qxp5vvH3+HQG04lGskh5g8w4eSD0Gg1rP9qGa3rNs99iQRCSJJE5VfLcKysYsJJB8X1JM1/7G2mnXska+YtRlUUJp5yMBUf/4is6fqQ9jR3Urb/6O5zWjPtNH65AktUYcghxbQ4OvnbU39m3dpK7nv0Nm6/4T78vgDaLTJFr16xlqN+dRjnzj2NRQt+5te/OZrMrHQ0GpmSkiLaNnT1xJosRux5Np7918t0dvRcwPDRu59z6hm/RqPVEEsyKX6gSM+zUz6+EMJ+1FgESZ9NJCazZuHG7hw5TVVtFBWnEXX1XB2ot6YR6khe503xO8nIT6O5uuc+tWuamHDQUGKBnisoNSkWXO3+XhctCMLO2OFg6f777+ePf/wjc+fO5aGHHmLDhg2cffbZjB8/nhdffHG39sicdtpptLa2cscdd+BwOJg4cSIff/xx9wTt2trauO7XmTNn8vLLL3Pbbbdx6623MmzYMN555x3Gjh3bvc+NN96Iz+dj7ty5OJ1ODjzwQD7++GOMxsGd5dXX6cfX6acso4iRRwzFWmjlpDnnJUz9YLGmkmI201jZFJdBN39sHk88/hxffPpt3P5X/f4i9pswkY6a5CnthYEjtyidr//yeo9J/p01zThW1ZA3rpzGZZXd2xuWrMM2pIRYJEbT0vVxgdImsUiMyq+XoUQVahd1LdDQGnRMPOVgDKlG6pds2LzzL4/ra3MT9AQ4+KoTiUWiBJxe1n66mNUfLmT47MkseeVLfnjmI2Ze/CuUX3rELNlp2EuyWfbWN6SV5JA3tpSaH9cx/MiJLP1ydXcx34yMNCy5Vl56+0nqaxuxpKYy/YApLPxuMQB/ve9Jbr/3esZPGs3fHniK2uoGTCYjJ552LKf/9kRCwSArlq8hVZ/KvI++Tvq7/PzT+Rx54KEJg9KBwp5jY+i4PEKOjWw5pijr9Iw/eAQ/f76aaCRGa207hSPGIAd9KFtl1daYzL1maZZUBY0mca9UyBdi44pGyscPIeZqIer3IWu0aKwZhBUd6xeu65sbFfY6SZbILEgnPceKJEt4Onw017R193jvSTscLP3tb3/jnXfe4eijjwZg7NixLFq0iFtvvZVDDz2UUGj3TmK84ooruOKKKxK2ffXVVz22nXLKKZxyyilJzydJEnfddRd33XVXX13igOJz+vE5/aCBsy88heef6plS4Oob5+Jrji8zkJqeys/LVvYIlAAee+j/eOG1f3TVehLf8AY0S3oqrevrk66GrFlYwZSzDo8LljaV9zJbDNQt7tlLuUl7ZRMl+4/q/jkairD4pS844LLjaVy2ESWmoDMZ4h67fsl6LLlpVH69nNRsO5PPmMXSN77GYOkalomFo7SsqSV9SB6SJDFs1kSaVlcz85LjMFjNhP1hbCNLWb2wMu5v09PuxdPe1WOl1WsJ+YPcfNuV/OWBJ/j2q4WMGjsMgOsv/WP3MYFAEEVV+PTjL3n8L8+iqioP/P0OoGvS+AmnHkNBYR7Njhbeef0jNq5PNgdnYCkfV0CouYatJ18pkTCKu5mC4bnUrGpAVVWWf1nByP2HYLLLKEEfkkaDZEyl3eEhxWwh6k5So81owdmWOFUNQFtDJ85WD/lDs7FlFhAOx2hY0TJgVlYJ26Y36hg+uZTq71ay4M0vUCIxcsaUMnLOftSsa8bb0XMId3fa4WBpxYoVZGZmxm3T6XQ89NBD/OpXv+qzCxP2rI66TuYcdTjlQ0t55okXaahrYuiIci696jwyUtJor4l/UzNnmHj54TeTnu9/b37EScccS/seKtwr7B4anYZQZ/LVi9FQpHvIa5PCKcOoqWwjPdeOoZdM7QaLmchWxWdVVaVx+UZyRpfQtLKKcSccwIavl3e3aw06lF/SD3hbnCx87mOmnnU4WxYhc9a3oTXq2e/8ObQ5XAQlA+tXNBANb9+y/mg4SjQcpX5ZI5defD5XXHshOoOWKy68JW4/c4qJ6TOncO3Ft3VvW/jdYm7/8/WoqsJLz71JVWUtRSUFnHHuibhdHqZMmYC7duB+oOuMOmRixJIUvI16PWTkl1GzqivQiUZirPxmHXqTjhRbCrFoDHd7NbIkMeWoseB19sjgLusNRFUt/m3MO4qGo9SuTh6MCwPbkAnFLHr2o7jySI6VVbSuq+Pgq09m9aKNe/TL+A4HS1sHSls65JBDdulihL2reU0z+fZsHnjodjR6LdFgFHeTh/a2ngGPpJFwJpiXsUl7WyeSZgeqaAr9ks/lp3hoPhu++Dlhe1pxNt4WZ/fPOaNKULU6IsEI7Y2dlB8wlsallQmPLZsxmlXv/9DzMdvd5I8vo/zAsdQtXh83h6lw8lCWvbm5NzPsCxL2hwi6Nn/LTM22kzd+CBtXNeDZhZ4GJabQtrFrPlLWqCyaGuJX1h5y+EzmffhV3Da304PP6+PBu/7B2AkjufOBGzGZjSBJDBlWSlZWJu6NAzd1hkarAaX3IZBEr/pwIEI44Oz+WVFVKn7YyKj9y1HcbUS9LiRJRmNNA6ON5fMTpykR9g1mmxlPY1tcoLRJLBxlw1dLyRpRlnBO2+6yb6WbFbape1huG6KBKNNmTOTTDxPPzzjwkOkE3YO7vt6+IBKMIBuN2PIzcDVuNVFXgjHHz2D9F0vJKM+jZMYYjJk2NvzcNdwUDUfxB6KMOmY6FR8tjFvtNuSQ8QQ9/h65nQAyynLRGg0sfPZjoqFI9/bCKcMIeQKEPPF/n742F521Ld3XVDhlBMu/WdunOXa0Wg1arYboFhOzrTYLVZXxQ2vHnDCbP1x3H0cccygHz9qfR+57kvbWri8bOblZ3PnQTVizLbhbBmbvUsgfQtInn88p6XSEgtvXg+ft9LFk3iryyrNJzy9GURQaqttpq0s+7CvsGyxpKTiWJQ+YW9bUUjBtNM1J9+h7IlgSeqXVa7FmWFBRcbe6u7PrOhtcnD/3TL787Hsi4UjcMdk5mUyaPI6GZaKLfDCoWlnPpN8eQfV3K6hbtIZYJIa9MIvRx88gEFYpmDEOVYV2hxP/VpP669c5yCnJ5NDrT6Wzthk1ppJelotGq+Hz+1/p8VgavZbsMWWEAmGmnnsUjUvXI2s15I0pw9vSyfK3es6RM6VbqPlxLRq9lklnzKKl0blLgZJWr8WWZ0XWyahRFVeTi7ArzBHHHBpXRHfDuiomTB7Dou/je90kCY7/zVFc/bs/oGwxxNTsaOWK82/ixTefwNPmHZAJE1VFpaPZgy3VSszbs4dMl55H5ZLkc422Fosq1K9zUL8uecUEYd+jKF3zFZPRmQx7fD6sCJaEhCRZIntYFm6/hw8++wytRsPhRx2MQdLTuqGNWDSG3+Hn6Rf/yl8feJKli1ei0WiYddSBXHz5ubSt33eKiQ520XCU1T9sIKukgJnTRoPUlZupbmMbQe+2ew+ba9pormnDZDUhSRKOhZVkFWaw33lHseKtb7rrxqVm25l0+ixq1zbhafeiN+qwDSnBaDXh73Sx4n89CznrU4xklOUx4TcHo0sx01TTirN554e50grtRPUx/vXUv9m4oZrSISWcP/cM1JjK7y75LevXbGTDLzXmFi9cxoWXncnrL72Lx901DCjLMnOOm8U7r30YFyh1/y6jMd549T1+fdRRdDQ4d/o696aNS2sZd/AI9Okmoq521FgUjcGIJi2Hphpn90R5QdhZHU1Ohk4f1b1SdmtlB4ylrcm5R69JUkV/5y5zu93YbDbee+ojUky7N9fUnpI/No+//+3pHgV0Tzz1GE495de0rOsaK9ab9NgLbMhGGUmSCLvDOBtcAz6PjLD7WTJSySvNQkYBSSISUWjc2ELA0zMAK59QjK++BVWJoTMZcDe2017tYOJph1G7voWAO7DLf3OWzFQaPc3cdsN9Pdr+dP+NlOcUY0oz0trRwfKlq8jOyWLyfuMJhUI89OfH+emHpdx2z3VEIhH+++L/qK5MXLJj3MRR3HzjVbRVDewvFGm5dgqHZ6PVafG7A9SuaUJVVAqG52JM0RNwB6lf3ywS0go7pWhEHu6qhh7zJbOGFzLyVzNYs6hnnq2d4Qv4OG7u0bhcLqxWa9L9RM+S0IPZZmb1mnU9AiWAt1/7kMOPPBidsWsSbzgQpmXDnptkJwweWy7X3yZVxZSWyoYvfibo9pFRnsfU387GUdfRZ0uILXkWHvj9bQnbHrrnHzz/yt9pXN6EzqBl+pjJRMJRan6sRWfQcd01l6AxadDqNGj0Wpoam9Hrdayr6Dm5vaSsCDU68L+jbp3EtHxCMRnZJmLuNhRfCIPJSMbBQ3DUOKlbI0odCTumbm0ThcPyOfjaITQt30gsHCFnbBmqRsO6xdV7/HpEAR2hB3OmiVf/83bS9tdfeRdbTvIIXBD6UsGwXJqXrefH5z+hs7aFgNNH/ZINfPHga9jTzBhT+yaBrNfv6x5O25rP68ft6WqLhKK42zwE3IFffo7QWtlG65o2YiGFVkcbRqOBI445hH++8CCzjjqo+zySJHHaWSfQ2ejsk2vuL3JKM0lP1xBuriUW8KPGYsQCPsJN1eQWpmLPse3tSxQGoPr1DtYuqQG7HV1uDjXrmqlcWrtX8veJniWhB0kCn9eXtN3r8SZeHywIfUySwJZuZul3q3q0qYrCyre/ZcRxB1C1sj7B0Tv6WL3/Ucty8nZJgoLxedxxywOsWFqxxTEyf7jnWkaMGsKa1Rs45rjDUTyxQZestXBYDpGWqoRtkfYmSkcXsbQ5eaoRQUhGiSk4+8HfjgiWhB6i/hgHHbo/Lz2fOOnkobMPJOTZdqZ2a6aFlJwUwtEwOp2OqDdKZ4OT2F5IVS8MTCaLic6a5JXrXQ1tGEy6Pnkss9FEemYaHW09y/TY02ykmMy4STx53J5n583X3kdVVf78yK3duZXS0m34vH7qahrIK8ghLd2O0WhAckgDcjVcIpIkIctqj4zem6ixGHqd+HYlDGwiWBJ6cDY5OfGUY3n3rU96DEtk52Qy84D9qF/a+/LgjLIM1lVt4B+3PkNHuxNJkjjosP25+oa5NFe0dNfkEoTeqIqKrO19tsC2eoS2l7vBw53338i1l9wWl09Jo9Hwp/t/j6cxeW4kY7qRtvZOTjv7hO7cSlfccCH1tY2889pH3fu9/NybHHTodK66di6NKwfHPB6VxEGSxpSCIT0TWaMBnYGs4gza6tqTxVQ7xZZlpXRsAXq9DKhEo1CzupGOPbxSShj8RLAk9KCq0FnVyTMvPcrTT7zIF/O+RSPLHH38bM4+/ze0rmvr9fjU9BSqG+u46w9/2eKcKvO/WEBNVR0PP3rnoPmgEHavgDeIfVwhkiQlTFSYNawAryvQJ4/l6/Rhzbbw4ltP8vZrH7B+7UaGDi/jxFOPJdQaxNWWPCWBpJH41YlHcNWFt6IoCvY0G4XF+fzj4Wd67PvNVws57IgDKbYXbFcC2H5PhXBIQdJoUH8pxG3MzkPWaAm0NKJGIiDLlJRmkD8km+Vfr8GYYqBkdD5mS9d8s45mN/VrHdtdkgYguySTkhGZRFrrCG8qAC7LDPnlvCJ3k9CXROqAPjAYUwcAaLQy9nw7BltXcrBgZxBnoxNlG8MHuaNz+PCjzxg9bgSSJLHo+yV88M5nBPxdH2r/ePp+ZKdEJNSzd8meZ8OcZcbj9aLVajFo9bgbPNusEyUMXllF6WhCQVa89U3cdr3ZwMxLf826pbUJ/5Z2liRL2HNtaPQySlih0+Ha5pBZyZRinvjHc3zy/pcAHP+bOXg9Pr745JuE+w8fNYQ//en3tG7o/YvHQGHJSGXU1CJCjhq0KanoUq0EHD17nzUpFsJaO2azlmhHE7FQV5oIbYoF2Z5D1cpGlJiC3+Un6Es+1K/VaZg8ezShhg0J2w355Sz5Yu0OBV/CvkmkDhB2WSyq0F67Y4VwjalG9Kl6HI0tvPz8W6iKwiGzZ/LIk3fx4F3/oGpDDatXrmW/kRN7fMBlDclk4ZIlPHnp84RCXblZcvKyuO+R20iRU/B1Jp90LgxerXUd5JVnc9DVJ1O7cDVBl4+MIQVkjyqmalVDnwZK0DX0t6Or1UK+IJXrqrt/NpmMNDcln2vl9fgG1SIJT7uXyhUOyseXozdo8NUmrgeoBP3YSwvwVMYnG5RkGb1eYsioNGKBIBiyiCoaKn6oJOTfnKdJb9IzbEoJljQzMXfyPFUxdxu5ZVnUrxU92ELfEMGS0Kcyh2Vw4RnXxE2S/fzjb/hxwVIe+PsdXH7eTeTl5/T4gDNbTTS0NvG3B5+K297c1MrlF9zEf17/pwiW9mFNG1to1shklBVh02nwuQOsWpC4V2FviASiTJk+gd+cdTzZuZkYDHqOOPZQmptaqd5Y12P/Aw+ZRsw/uHo92hs7aW/qZNqcsd3DcVvTp2UQbI4vg6RNtaC3peGtWr/l2ZB1OsYfPIKfv6ggGo6iN+mZcOgIoq11qD47Sih59nglHMJksffBXQlCFxEsCX3GlmXly8++TbiayO3y8M0XP3DYEQcwctQwGpfHf+NLzUnlgdv+kfC8gUCQBd//xJji4aKUwj5MiSm01vXPrNexUIwjjz2Uu2/9S3dwVFxawDU3X8J///MOC79b3L1vqiWFU8/8NY3LB+GcGhVivSTc1BhMhDvjn0NjZg7emp49UUokguJqpmBYDjWrGhgysYhoWz1KOEQsEkY2GMGf+AuUrDcSaBeFvIW+I5JSCn1Gl6Jl/pc9s35vsuj7Jcy96hzc9T0nymqNGqo3Ji4PAVCxah0Gc/LCioKwt0iyhDHbyCXn/D6uF6m2uoEbr7yTS689j5TUFDQaDYfPOZhnXn6UzqpdK/bbn7U1OtGmWhK2qYqCpN2c6kHS6lAi4aRpB6I+Dxl5XQktU23G7t6kiNuJ3pae9Bo01gyaNorKAkLfET1LQh+SSLUkn+Ceak1BDkt4O3p+G4yGYpSWF3cXKd3aqDHDCfm3ndtJEPY0e56N119+l0i459ypaCTKu298wqv/+xdBX4iwJ0xrRdugzjVWv85B1qzRyOEwSniL16wsg9aI1pZJuKUriagky0mH7LYka2TYsjCxqhJqbyGlqAx/Ux1qtGtIU9Jo0GUWULe+ZVD/joU9T/QsCX3G0+LhtLNOSNp++m9PpLPWmfhYh5eLrzwnYZvJZGTGzKliCE7olzQGDSuXr0Gr0zJsZDlDh5chy11vreYUE16Pl6A7ROOKJtqq2+M+xHUGHZllGWQPyySzNAOtfuB/f41FYiz7ag1Rczb6nFK0aTnosgvRZJaw6oeN+AIqWksaAEokjMZoSnouWacnFIx0ZTyXNXFtEY+LYFsz5txCUkuGkFo2AtVWyLqlTTRVil4loW8N/Fem0G/ozQZySrP567/u4cP/fcaXn37Tndxv9pxDKC8poWl14nkaAXeAgiF5XHPTXJ54tOdqOGft3k93L+y7ZI1MWoEdrVmLGlMItAfx/FLAV42qnH7uiWRkprFq2Ro0Wg1jJ4wiGo0SDoXZsLaaivXrGD1hOJ5GD+7WruPSi9MJEOTxJ5+lpqqeoSPKOP+i05ECEihgyjQRDAYxGAzEfFE66pzEogOjtyQSirBi/jp0Bi3GFCORUKQ7FcDq7zdQPr6IjPwhKCE/qiqhs9qJuJ09zqNNz2X94q5eqHaHG3uKlZhv8zB+LODHV1+NzpZBkyNCw/rmPXJ/wr5H5FnqA4M1z9L20mg15I3J5fvvf+TN/75PMBDksCMO5FcnHcmC+T8yaco4zDoTretbt5m9d1OeJbfHi04n8iwJe19qegopBak8/3+vsOj7JVjtVs44+0SmTB5P4yoHuSNzeO31d3nj5XeBrozfD//zTv7zzGss/WklQ0eUodVqqa2u5877byRdbweg0lHDPbc/0uPxXv7fk3z71SKef+oV/L4AkiRx4KHTufb3F9O0qnnQ5A6SZAlTqhFVhWGTitFrwkRdbajRKBqjCW1aDg0bO7oDIFkjM+HQkcghJ1G3E1BBktDaMghjZsU3a0mSTFwQktrePEsiWOoD+3qwlDcml3vufITlP6+O226zW3nuv4/RtMJBNDQ43uCFfYvOoMNWbuWCM67u7u3c5ODD9ufq6y6msqqKG6+6q3v7YUceSHZuJsFAiCOPPZRVy9YQCoUZN3EUGzfUcNDB+4MKZ59yGcFg/Dy8Aw6ZxtgJI/nXY//ucS3DRw3hnntvSdo7O9DZc2wUDstBq9fgcwWoW9PUIzGlJEvkD80hpzgdSQJFhaaNrTiqWkWgJOwUkZRS2CMMZgP1jY09AiUAl9PNS8+9wQlHH01Hfc90AoLQ39kLrDz68FM9AiWA+V/+wJW/v4gXn3sjbvuRxx7KquVrSU+3c/l5N8W1HTp7JtNmTCIUCPcIlACOO/ko/nzbXxNey7qKSjwBLxqtTCyqJNxnIHM2u7ZZXV5VVBrWOWgQpUyEPUxM8BZ2iTUzlY/e/yJp+2cfz0dn6Zuq8IKwp2lTdCz6/uek7S3NrbRvlVfMYDCw3/4TeeLR53vs/9Vn37Pyl3lNieh02h7Fq7dUtaEGvVG/fRcvCEKfEcGSsEtURUWvTx4M6fU60T0uDFgSoNUl74AP+INMnT4x/hgJvpz3bdJjXv3PO9jTrFisqT0fT5LQ9fJ6ysnPJjJI5iwJwkAigiVhlzhb3Bx/4lFJ248/eQ4hp8iPJAxMQWeIo445LGl7YWE+Z517MgbD5t6epYtX0dnuYvyk0dz/2O088q+7+csTd/LIv+7mgEOm4exwoURU/njf77tTDGwy/4sF/OrEIxI+VnqGnZysrEEzwVsQBhIxZ0nYJdFwlHRzOseecAQfvDMvrq10SDHHHncEdT/X76WrE4Rd42x0ccHFZ7D4x2U4GuML41585TlE3BHC3jD/9+Jfefi+f7J8ySoWfreYuVeeTcAf5P4/Poazs2sejsls4rLrzufgw2fi7/CTaUzjpTef4M3X3qeqsobho4Zywm+Oxmgw4GhsYcE3P3U/VlZ2Bo8+eQ/OGueevP3dxmwzk1+ehUYn42zx0lLbNmgzmguDg1gN1wf29dVwSJAzPJsOt5O3XnufQCDIUcfOYvSo4bSsbSEiVsIJA5jBrCdzWCZLl67k6y++Jz3dzgm/OQZ9TEtbdQcAepMee4ENySAjyaDRazjt2IuIJchO/cyrjxJtjhL0BpFlCXueHVknoYQVOptcSLJEZnkGilalvraB9Iw0bBYrzjoXAXdgT99+n5IkidEzh2IyguJpR43FkM2pyKnprFlY1Z27ShD2FJE6YA/a54OlX+gMOmy5XX9sfmdA5EYSBpUUewomqxFVUXE6XEkTRKbnp/HBF5/x6n/eTtg+46CpXHnZ71BV0KZq8Pn8mM0mlIBCR01H90o3SQKtXkcsGuvKYD0IDJ9aSqo+QMy7VX1IWcaQV86SzyvEMKOwR4nUAcIeFwlFaKvpn1XhBWFX+Zw+fM7EVe63JOkk1q/bmLS9od5BanYKf73/X3z12Xds+r468+D9uOHmy2lc0UQsGkNVu15Tg4VGq8GWYSbclCDLtqIQczVTMCyHmlUNe/7iBGEbRLAkCL9IK0zDlG7E4/ViMOjRKBo6a52EAz1z7AhCInqTHnOamcuuPR9npwudTsdnH33Nu29+0r3P2Reewt8e+j8ikQgXXHomXq+Pzz+ez/fzf+T+6GNcc/VcWje07cW72D0s6akoweTBZtTrIS23TARL+whZI5NVnEF6lgVVhVAgTFN1G0FvcG9fWkIiWBIEIG9ULh989BkvPf8G0UjXMMDQ4WX8+eFbcVa5+u0LWOg/jCkG7OV2brruLtav7epZ0uq0nHzGr7jtnuu457au0ib7zZhEYVEe3329iAXf/IQ9zco1N11Mi6ONxx76P6SbBuci5a4eNKmXPSS2WQ9JGBR0Rh3DJ5dS+eXPrH55PUpMwVaQydhfH0B7m4/2xv6XxFjMWeoDYs7SwGbLtrK8cjWP3P9kj7bM7HSeePohGlc07YUrEwaS/HF5XPa7G2lt6TkUPfeqc1hXUcmcX82ifFgJF55+TY/kkyedfixWm4WZB+5HsC4w6OIGSZaY/qsJaIihKgqSJKHGYgRaHSihIFqLHUeLIorh7gNG7FfG0hfn4W2Nz9guSRIHXHkCVRVNhAN7Zgh6e+csDc6vMMI+T9bIWDMtWDMsSPJW32a3+tGUaeK5p15JeJ62lg7q6hvRm0QWciE5vVFHY6MjYaAE8N9/v8M1N13M0JIyHnvw/xJm6X7r1Q+YMn0CVmvqoAuUAIZPLSPm6cRbXYmvdiPemkr8jgbMuQVoUizIlkyaNrbu7csUdjNDioFQp6dHoARdvY8VH/xAbknWXriy3g2YYKmjo4OzzjoLq9WK3W7nwgsvxOtNvsy0o6ODK6+8khEjRmAymSguLuaqq67C5eoZyW7979VXX93dtyPsJpIEWUMzsQ2xsXD1zyxev5zMkRlkDckke3gWWaOzMBWZyR2XQ9aQTGSNjIJCZ0fymlSV66swmAx78C6EgUZvNlC5oTppu8vpJuAJosgK3329KOl+K36uwKA17oYr3Lss6alYLTLh9ha2TOmvRiN466ow5RWx4pt121z1J8kSOoMOSeptOE/oz1LtKbSuq0va3r6xCbO1/70GBsycpbPOOoumpibmzZtHJBLh/PPPZ+7cubz88ssJ929sbKSxsZGHH36Y0aNHU1NTwyWXXEJjYyNvvBFf+PK5555jzpw53T/b7fbdeSvCbpQzMofnn/8vH737Wfe2f/zlGc6/5HTyC/PiipQecMh0rr/pUqKRKDa7FZfTneiUlA8tISQmeQu9CAdClJYXJ223WFPRSPI2S/8YjQbcTYn/DgeyopG5RJ1Jeo0UhZDTiUaXuF4edOW6Gjq5BHOKDjUWRdLo8DgDbFhSQzSSOIWD0D9FI1GMCUr9bKI3G/tloegB0bNUUVHBxx9/zNNPP8306dM58MAD+fvf/86rr75KY2NjwmPGjh3Lm2++yXHHHceQIUOYNWsWf/7zn3nvvfeIRuPzeNjtdnJzc7v/GY39L6oVts1kMbGxujouUNrkuSdfxWq1YLNvHpP+7uuF/OPRZ9Agc/aFpyQ8Z3qGneLiQrEiTuhVOBChqDCf9Ax7wvazLziFQHuAqD/K/gdOSXqe6TMm42odfMGS3qhDifTyGoqGMJgSFwjWm/SMP3gE+mAr4aYqIi11hJs2YlJdTDhsVK9BltD/uFvd5IwtTTrXv/yg8bT2wwneAyJYWrBgAXa7nalTp3Zvmz17NrIss3Dhwu0+z6YJXFptfIfa5ZdfTmZmJtOmTePZZ59lW3PeQ6EQbrc77p+we+gMOjKL08kszcCaael139QsMy+98GbS9k8++JJDjzggbtuX874lKsU4fNZBnHPRaWg0m994i0oK+MczD+Csdu7SPQj7hs7qTh5/5gGKSwu6t2k0Gk4583gOO+xAnA4XnfVOrr5hLimp5h7H/+aM4yDEoCz7EfKHkfW9DGXrjAR9iWtIDp1UTLS9HiUc364E/eBpoWhkXl9eqrCbqSq0NnQy6YxZPYZTs4YXkj22DKcj+bSIvWVADMM5HA6ys7Pjtmm1WtLT03E4HNt1jra2Nu6++27mzp0bt/2uu+5i1qxZmM1mPv30Uy677DK8Xi9XXXVV0nPdd9993HnnnTt+I8IOyR6ahTPg5un/vIzb6ebAQ/bn4ENn0L6hnUCipfyyhMvpSXo+V6eL3Lz4vyNVVQlHw6xfu5H9pk/kpNOOxevxocZU9LIOZ7Uz6Zu4IGwp4AmiVqs89MifiKgRgsEQNpuVUGeIxpVdqymj4SieOg8v/Pcf/O+tj/jhu8XY02ycec5J5GXl0Ly2ZRuPMjDVrnEwemoB4Zaec1UkjQZFNuBPVMpFghSrgXBj4tQdUZ+HzPxyqlfE159MsadQNq4A4y8LM4KBCNUrG/B2bjupqLD7tdZ1kFmQziE3nErHxibC/iCZQwuIxFTW/pg8oevetFeDpZtvvpkHHnig130qKip2+XHcbjfHHnsso0eP5k9/+lNc2+233979/5MmTcLn8/HQQw/1GizdcsstXHfddXHnLyoq2uXrFDbLLMvg3Q8/4ZUX3uretnjRcv7z3Ov864WHCa1o7jEZNBqIMW3GJOpqEie1mzxtPEsWLY/bJssyzk43t/3+PqCr2OlDj/0Rc8xIR1tHH9+V0N/IGpmMkjQ0KVp8Pj8pKWZivhjtNR07VWIk6AvhqNi89N1Hzw9nvytAYFmAIw44hDmzZ4ECnhbPoA2UoCv7eWdHCHtGLpGOFlC7frey3oAuq5BVCxJ/QGo0MiSorxdnq5GAjII0hozNJdLWQMTVtfxcq9Uxer9iqiqaaa0Tr+v+oK2hg7aGDlJsZmS9iQ0rGpKWEOoP9mqwdP3113Peeef1uk95eTm5ubm0tMS/kUSjUTo6OsjNze31eI/Hw5w5c7BYLLz99tvodL0vAZ8+fTp33303oVAIgyFxt7HBYEjaJuw6WZZQDWpcoLRJR1snT//zRc4+/RTaa+Pf9JxNTk7/7Yl8+L/PCATiv4mmpduYMGUs//ePF+O2Hz7nYL7/5sfunwP+ANdcehuvvP0vPG2iqOdgpNVpiEUVJFmiYHw+j/31//j6s+9RVRVJkjh09gFcdd3vqF/WuNtqsqkqdDY5d8u5+6sNS2rIKsqgaEQJGhlUScLnDlIxf33S3ttYVAFNLx9TkoSqbh7KkWWJ8vGFhBoq2XrVXchRTdnYIbQ3OgdNrb3BwDdAaoju1WApKyuLrKxt51OYMWMGTqeTxYsXM2VK1+TIL774AkVRmD59etLj3G43Rx11FAaDgXfffXe7Jm4vXbqUtLQ0EQztRZYMC/O//CFp++efzOd3F5/VY7uqqLhqXDzz8qM89pen+eHbn5BlmUNnH8AVN1zI8//anBJCkiRmHXUgRx8/i5uuujvuPNFIlIXfL2Z00XBRBX2QkDUyGWXpSAaJZkcrNruVtDQbz/3fK3w177vu/VRV5ct536IoChf/7mxaN4pah32pta6d1rod+512Nnuwmi3E/D2H2HW2dBqrNpeGySrORPF2kGzZYczTQU5pJk2Vg7cXT9g9BsScpVGjRjFnzhwuuuginnzySSKRCFdccQWnn346+fn5ADQ0NHD44Yfz73//m2nTpuF2uznyyCPx+/28+OKLcROxs7Ky0Gg0vPfeezQ3N7P//vtjNBqZN28e9957LzfccMPevN19niRJxKLJK4/HYkrSFdg+p59wMMxNN1+JbJKRgFhIoW1tO6f/5gTOPPsk/IEAtnQrb7/2ITdddTeRcM9MsXW1DUwYOqZvbkjYqzb1ID38wD/5/pccR0ajgZmHTOP8i0/n559WUFsdP3T79effc+lV5/XZNVgyUknNSyUQ7JqXYzKa8DR4xBya7bBxeR0TDhuFVqsh6nZ2bZQktLYMQphprFzbvW+KzYgaTt5ToYaDmHtZti4IyQyIYAngpZde4oorruDwww9HlmVOPvlkHnvsse72SCTC2rVr8fu7XihLlizpXik3dOjQuHNVVVVRWlqKTqfj8ccf59prr0VVVYYOHcojjzzCRRddtOduTOjB0+HhoEP355knE+fQOuiw/Yl4EqfC1+q15I7K5c3X3+Ot1z4g4A8y/YApXHHtBQTbgng2dvUUmbRGfvjmp4SBEsCkKePxexJMOBUGnPTCNJ57+hW+/3oRGo2GS645l6HDy1m+ZBXffLmQ+/52O++99QmvvvB23HE+X98MD9jybLT62rnuvD/i7Oxa5ZOeYeeOe2/AlmvF7RCraRMxWYyk5dpBVVnzQyUZBWlkF5chAYoKDRtbcVTVxXUiBXxh0lL1QOLnTtLqCDlFGhBhx4nacH1A1Ibre9nDsnj2+VeY9+FXcdvNKSaefflvtK/rIBru2fuUPzaXW2+8t7uQ6SY6vY7nX30MZ6WLaDiKyWIkYAhx9cV/6HGOrOwMnnjmQRqWi3pwA40hxYC9wAo6CVmWiXgiGGwGTj/hIqLRGLffez3fz1/E5x9/E3fcb393Cnqdjmef2Bygv/K/p2hZuWvDNRqtBtsQK+ecckWPlCSyLPPim/+kfe3OTSYfrDQ6DaNnDMWgV8HvQmMyo7XYUVRorm6nfm0T4WDiLzkanYbJh48k3Jh4wrg+r4yfv1yX8L1D2DeJ2nDCgNa6oZWLLvotf/7LrYwZP5KikgJO/e2veeG1f+Ct9yZ8szOYDdTWN/YIlAAi4QhPPPY89nwb0LXMO91s589/uZXM7PTu/abuP5F/Pvsg7ZVixcxAY8myoM/Rc+edf+H0E+Zy+glzeeq5/yDrZXLysigfVkI4FO4RKAG8+PTrjB43glRL15edaTMnQXDXv0fa823857k3EuZuUxSF/770P9LybLv8OIPJ2AOHow22EmltwJCRhaqq+GrWE6jbQGamhslHjCGnNPFc11gkRkNlG/qsQpC2+HiTZPRZBTRVJ/6SJQjbMmCG4YR9i6pC02oHWdYM/nDL1SBLRP1RmpY5kiYNtaSn8OG7nyc95w/fLebq6zbn2eqo7SQ3PZt/PvUgoUgInV6HElBoXdNKJCTeUAcSjU6DIVPPuadeQfSX5ceKovD1Z9+zavka7rj3BtasWs//3vg46Tk+/eArDpk9k6aGZm6+7SoaV25fDrfeSDqJynVVSdvXr92IdJL4zrqJJSMVHSEiwQDm/GKCrQ6iAT/mvCIkWSbidRNztTJ0fB4ZeTYqfqjs8X7QuL6ZkD9MyagSNHJXW0yRqFrjoK1efAkSdo4IloR+LeAOEEiUrC6BWEzBnpa8G9ViSUFR4oc7vB1evGLF24Bnz7fx9BMvdQdKW2pr6aChronCkvzuOUOJuJwubv3TNfg7AzSudBDrg5pjalSltLyIyvXVCdvLhhSjRsVMiE1yijNQfE4kjQZJqyHq95FSWEqos52ob/NquLCzA5PVxsj9y6lYUNnjPO0NnbQ3dCLLEkiSGOYUdpkIloRBw9Xs4sijD+PfT7+WsP3k048j2JY4E/D2SLGbsRZYCEbCqIrStaKpyYu3XQRbe5vWrGXJj8uTti/8bjHnzD2Nk047ltz8HFJSTUSjMT776Gu++ORbYrEYBxw8nfbKDjx9+Hw6G12cfcGpfP5Jz6E/SZI44+wT6Vzv7LPHG+gkuSt9g8ZgIurzoTGaUGLRuEBpk6jbRUqODYPZQMifOE+Toqhss3qxIGwH0f8rDBqKoqL6Fa684Xc92sZOGMkxv5qNs3nnag5ZsiyEzREun3sz55xyOeeediUXnXsdrcF27AVizklf05t0ZJZlkDkkk/TCtK4egl6oUYW0JEVsAbKyM8nOzMSebuPRB/7F9Zf+kT/d9BDpGWnc/9jt5ORlMfPA/TYHShLYc2xkFWeSYu9Zx217xSIxpIDEPQ/fElcPzmJN5f5HbyPmjolejy20NbiQzTZUVUHSaNBZ7YSdyYfOVJ+TrKL0pO2C0FfEarg+IFbD9S/pRWloLBq+mPctbreHgw+dQXZmJi1rW3cqnb4kS+SMyeasky5JOMzz/H8fw1frFxNH+0jWkEw6fS7+/exrtLa0MX7iGM44+0R8Tb6kWdVT01OoaqvjntsfSdj+5sfP8s0XP/Dog0/1aDtk9kyuu+VSmlc2E/AGseVaMWWZ+OyTr6mrbWTqtIlMnDSWjo2dBHYynYQ1y0JKbgoerxdJkrCkpOJp8uwTWeJtWVaKR+WhN2hQkWhvdFK/Lvkw55Qjx6K012EuKCEW8BFydqCEEvcIa82ptLp11K5u3J23IAxi27saTgRLfUAES/2PJEvYsqzIGhlvhzfpUuPtkZZv56MvP+eVf7+dsP2Q2TO56Lzf0l4jJo/uqrRCO9/9tIh//f3fcdt1eh1PPv8Q0dYowURFlIH8sXk89a//xKWbkCSJG/5wGTMO3I+zf3MZPm/i/Duv/O8pWla1YMlMpT3UyU1X3x03vy09w84Tzz1E+7oOIqGd/1va15SOLSQr10Skw4H6S6JZbYoFyZbD8q/XEA70/F0azHrGHjQcrRRF1miIBbyE2hKncNBl5LFueSvu9uQFtAWhN9sbLIk5S8KgpCrqTg+5bU2j17C2ouck0k02rK1C1okR7b5gyjDy1D/+02N7JBzh/jsf4577biHiMyPrZdSYitvhIRzoSjLYtKqJC849g3MuOIWfFi3DZDIyeep4Au0BPG5v0kAJoL6mgRS9GUuehUt/c2OPhQAd7U7uv+sxfn/D5bRWtiU5i7ClFLuZrLwUws21cdujPg9yJMTIaUNY/vWaHseF/GEWf7KS9Pw0ikflYUnPIuzs6A62NpF1ehSNSQRKwh4hgiVB2AYlolA2pDjpBOLS8iKxoqkPGMwG1qzeQH5hDqefcyIFRXkgSWxcX81r//kfOflZqHqVF599g5XL15Cbn825F55KVn4mrZVtqCq0bGhF1siMLx2Foqg0Le9a/p8zNrvXx7bZrSjtCrW1DYRCiTM8//zTCmSTCIq3V/GoPKKdzQnblHAYQ5qK3qRL2LsE0NHYSUdjJyaLkfEHjyDmaSfqcQISGosdyWRj+fx1u+8GBGELIlgShC1JoNVpUWJK98RbZ5OTk0/7FW+/9mGPHgeAc393Oq6mfa9khS3biinThKyRifgiuBpduzTcCSrpWWnccNsV/POvz7J+TVdy0THjR/KnB29EkuCsX19KLNY116W+tpGffljKJVedy8wp+9FZ7wRAiSk9aq4pAYXpMyez8PslPR41PTMNu81GR3sn/m2UOIn2QTqBfYXRrEdpTbxKDbrqtJlSTUmDpU0CniCLPlpOdnEmWYX5qKpKQ1UHbfV1iEkkwp4iviYJg57eqMOaYcGYYki6jyRLZA3JJHt0FgFTCGOhibwxuZgsRlQV9Oh49b2n+Ou/7uaiK88mIysdk8nIbXdfhwnjPjWPRdbIFE4sYNGqn7n0d7/ntBMu4uG//hNjgQlrbvIx/22JhKJk52Zy45V3dgdKAKuWr+H6S/+IVqtLmJD0qX/8B1OGqddzd9Y5ufG2KykuLYjbbrGm8ugTd+OqdREKhBg2ojzpOfIKctBpxPfL7RWLKkiyJvkOGt12v25URaW5upWV365j1Xfraa1rF4GSsEeJV74waBnMejKGZNDY1MyKiqUUFeczbMIQnDVOfM7NPQiSBAXj8nn8b8/wxbxvu7dnZqfz13/eTZ49l5deeJP/vfERgUCQydPG89cn78ZqsdBZ66Sjdt+a2J09LIsH/vx3Fm3RS7Nk0XIuPPMannzhYYwpBoK+5D0Kydjzbbz47BsJixsH/AG+/PRbZh68H99+tTCuTVEUNm6oxmpKTdpLEQ1Haa5o4S9/u5PWjg7Wr6mkoDCPktIinDVO/L8kPo36opx46jG8/dqHPc7x+z9cjrdp8K9e6ysNG1ooG5pBpDPB5GxJQtUaun/vgtDfiWBJGJR0Bi2ZwzO46uI/0Fi/uWxFqiWFx5+5H5NiJODuWlVlz7Pz9psfxAVK0JX5+fILb+bev/6BV/+zeSXckkXLueC0q3j2lccIOAfmm70sS9jz7cgGGWLgcriJbMcQmkarwRf2xwVKmyiKwl/u+yd/uvP3BNe17vA1aQwali1embR95fI1TJwyhu/n/4heryMY3ByQaTQa1C3m/0oSpBWkYUgzEA5HuvZvD9K40oFWr2VsyUjCwQiNWxVLbq/q4MwzTmLMuJG88PR/aXG0MnrcCC675nz0UR3Ohr5ZNLAvaG/spHBYDpoUKzHfFsPUsowht4R1P9ftvYsT9ji9SUduaRbmVCOSBO4OH801bQNmaFsES8KgZC+0c++df4sLlAC8Hh/XXnYHTz7zEIFfan8Z0428+er7Cc/jdnlobWknMzudtpbNPUjRaIx//u05rrzsQtqq2nfLPehNOuwFdjBALBpDp9XhbfLtdHkWWSOTlm/DnGlG1mv4aeFS3njpPdIz7fz2/FPINGbQtjH5vWh0GrKLM1ldvQ5ZlhPO31q7esNOrwxUYyqZ2RnU1TQyedp4LNZUNq6vpnpj14fqmPEjmHnwNCZOHYff6yc9M43vvlrI6y+/R0lZEY3LugIfSZYoGJfPa6/+j7df/5BwKIzJbOK03/6aY489gsZVTXg7k+fEalztoCyzmIf+8kdkrUw0GMXV6MYf6H0+k7AVFZbPX8uQCcWk5WehhoOg0RBTNKz5qQ53m1jFtq+wZqZSUJbFqne/o72yCUmSyB1byog509i4sp6AZ+crK+wpIlgSBiXJKLFkUeLVax1tnbg8HmRZQlFUYrEYgUDyF6ujsYW0dHtcsASw6Psl6G+8tE+vexOz1URqUSr3/ulRViytACArO4Mb/nA5eYXZ3ZOZt5fJYiR9SDovPPNf5n34FZIkMWvOQVxzy8XcfevDXHb+jVx+7QVMnzCZzob4c8uyRNawLHxhP/O+mY/RbODvz97HvA+/5p2thqu0Oi2yvHPBkqfZw5XXX4jX52fBNz/R0dbJaWefQHZuJm+8/B7H/Ho2119yBy3NXUv3JUnimBNm89x/H8PXvHlCd2ZpOv/65wt8ukW+pYA/wPNPvUrAH+S4o46kfRtDp6JmYN9QFZUNP9cgyRJ6kx4lGhNFqvcxskameFgu8x99ozsRqaqqNK2ooqPawYxLjmfVgg17+Sq3TQRLwqAU3sbEUbfLg1GrRwlHkWWZ9Aw7He3OhPuWDinC0dhz3oXFmpqwd6UvpJWl8buzr40r/Nra0s5NV9/Fv154GINZT8ifeIl7IhlDM7jo7Gvj7vGDt+ex8LvF3PngTVx+3k088bfnOfTdmXQ2xB+bPy6fhx94nAXf/BS3/dJrzuO0s0/gv/95p3vbkcccStC5c98SDRYj9XVN3Hb9fd0TuT/832fk5GXx5L8f4tJzb+wOlKDrDfeDt+dRNqSYqSMmdG/XpGjjAqUtvfnqe5x82q+gNmGzsJuoikpoJ+axCQNfVlEGG75eljBje8gToHVdHZZMC55+3tMoVsMJg5LJZIyrxbW1wqK87vIkgZYAv7v0twn3Ky4tIBKO4nH37GX4zRnHEWzv++7jFHsKPy9ZERcoAciyjNFo4PG/PYc1b/tXndlzbHz0/ucJg8G2lg6WLFrO1P0noigK69dWoTfpu9utmRa+/XZhj0AJ4IlHn+eAQ6ahN3TtX1icz4UXn4Wzsee8HluWlaziDGxZya87NTeFO37/QI8Vb81Nrfztwf9j1pEHJTzuP0+/jimrazWcJEt0diafVxSNxvBtIz2AIAh9x5xqoG19fdL21jV1pFp7X83aH4hgSRiUAq0B5l5+dsK2OcfNIurbPBTganUzeeJ4rrj+Qswpm1+002ZO4m9P3Us43LMHZ/yk0Rx73BE4HX0/4ddsNbHkp81DiAVFefzx/t/zyJN3cfu913PJ1eeQkmFGknovLruJNkXL1198n7T9h28XM27iKKAr2NiSMcMYN7l9a9/NX8Q1N83lwcfu4K//uJvmipa4wrCpGSnkT8jjx4qlPPPSy/y0Zhn5E/JITY8vC2RKNbJq5druHEpbm//5AqbuPyFhm8u5efKwqqik9hIkAxiNyVNICLuJ1BV4p+elxQXjwuAXiykYUpMHQwaLidgAKCYthuGEQcnpcDFt8mRuv+d6/vX3F2hpbsNiTeW0s09gzpxZNK6IL7zZvLaFaWMmcvArMwgGg+gNBhR/lMZlTQwvLOeVd55i/lcLcDvdHHjIdNLtaT3O0VciwQglpYUAlJQVctOfruLPtz1CQ93myepzjpvFRRefTcPyhoT5ZgwpBgxGPUF/EFR67WVLSTURCobRaDQMHVbWPVEauuYbbBmMbK2jzclZp55M0wYHjSviV5aZbWaUVJXfnnxpd1bszz/+hqf/+SKPP/sA5pgZv8vf/Tg+j6/H+TdRFAVFSZxYJzsnE7Zo0qGjdEgx1ZU9x9omTBkDYjRoj8oty6JoRA5q0AtKFMmQTSgMFT9U9pi/pDfpkDUaQr5QwpxawsDT1tBJ+cETWPLSZwnbS2aMYcPKhoRt/YnoWRIGrZYNrZRlFvH3J+7l1Xee4pn/PMpBk6fTsLwxYYDR2eSiaYWDzvVOmlc207qxnWg4Sme9k9bVLUwfPYmjDjoMtV3FsbqZWHT3fBtytrqYdcRBaDQaLr32fG6/4f64QAng4/e+4O03P8CeZ4/bbraaKJiQhxM3iyqWEjJHMNqNnPe705M+3tHHz+bLed9yzU1z8bfGD1FF/VH2239S0mMPOWwGzVUtCfMq2Qqt3Hrdn3uUDwkGQ9x6/Z+xFlq6twU8AcZPGp30ccqGluB2JQ7arv/DZWh1GtJHpJM+PB1ZJ/O3J+8hryAnbr/SIcXccff1dNR2Jn0coW9ll2RSNMROuLGSSEczEWc74eZaNP4Wxh8yEvmXnsz0PDtTjhjD+APKGDutkKlHjaF8QhHb2Xkq9GN+dwBDupWiqcN7tI08eho+XzjhfKb+RvQs9UNavZaCYTlk5NtAhVAwQs3qRrwdyb95C4l52r142nd9VZOqdg3X7REq+Jr9PPqve3C53LS3Jl659ear73P8CXPglw4uk8WIIdfI+WdeHTfHKicviydeeIjjTz6Kd9/8JO4cs48+BHualbsfvBmjaqC9Jv6xnA1O5l5xDt/NX0Q0Et8LUFCUy8iRQ2nYKlfRJsFIkNaWxKkImptaCUU2B1GKoqJVtMw68kC++DQ+35UkSdx42+Xk5ecwauxwKlZ21QPryqB+LWkZdq685NbuNBG5+dncfs91PPHMQzQ3t9DY0ExRST4WcyrNFa3dc9WE3a9kVC7hxp5FqJVQENnfSXZpFrFIlNJRWYSbq9jyW4zdYmfMgcNZ+Y2o/zbQbVhaQ/GkEZQdNJ7WdfXIWpnMYYV0NLupX+fY9gn6AREs9TMGs57xB49AcTUTba4CQK/TM2pqIY1VThrWJy5MKQwu7mY3WSWZNDYmf74DgSDR2OYPfnuxjSsuvqXHZPTmplZuv+F+/vzQrRx3wlF8+lFX6oCjj5uFzWbD3x7A1ejCF+4ZjEcjMYItQZ5+8a/87aGn+PmnFWh1Wub8ahbnX3QGLWuSJ5+MRHoPSmLR+G+TrRvauPzKC5g4ZRwvPf8G7W2djJ80miuuvRDZL9G0wsEf/3g9ilYlFAqTmpKCKqucecIlcYGco7GFqy76A/9585/E2mLkm7MJ1gfwRcWXjT0pxZ6CGko+mT7q6SS3tASdXkO4sRJ9WgZ6q70rYJJlYsEAGkXBmmkROZkGOhVqKxqRNTIpNjOqolKxsHJAlawRwVI/M2r/IURba1Aim5e+K5EwYUctBeWltDc6d6qUhDDwdDZ2UlJcmLTdZreSakmBoYAKUZS4pfVbWrV8DX6/n7AjzK+PPKqrp6zWjTuy7Q8hb5sXvU/Pjb+/HI1RiyRByBmmcXlT3GTurVktFgwGfY9hOACT2URqagpuNvfWGVMMBF0hDpg+jZkz9kNFJeKP4GrYXKC3eYvM4JoCmXc/+aRHjxdALBbj5eff4LSTTqSjbt8qR9NfaLQyqtIVEEs6HYa0TDRGE2o0StjZTtTvQ6vXogbdmAtKiAX8eGs3dvcuaUxmzLmFFI/KY+U3IlgaDJSYgmeA5i8Tc5b6EWOqEa0ciwuUthR1tlA0Mm8PX5Wwt0QjMdLtdopKCnq0lZQV8uR/HuZ/b3/M7bfdz/0PPMbGjTX86YHfo9Ul/g4UDkWIhqO0N3TS0di5Q/METKlGlKhCxBuhvbKD9rqOXgMlAF+rn8uvuyBh2xXXXdCdSFKj05A/No92xckTzzzPP/75DA1NTShRhbaq9u5AaWsavYbVK5MP0VSsWo+sF5Ne9ha/y49sSEFvTyclr4io142vtopgqwOdxUZKcTnhYBSNTocSDhHqaI0bhosF/PgaqrFutXJSEPYG0bPUj5itJuil2zoW8JOSmZO0XRh82je289d/3s09dzzChnVVzDrqIHJyszj2hNn87oxraGvdPFl5+c+rOeCQaVx788U8dPfjcecxmYyYzSZc7FiqA2Oqkcxh6Xzx2bfM/2IBqZYUTjvrBHKLsmle1xK3Cm1rriYXU8dP4K9P3M1Tj/+b6o31lJYXcfGV55BlyeguE5M3Jpfbb76f1SvWdh/75affcvCsGVx+xQU4KhIPRaoxlaKSgrjjtlRYnA9ietJeE43ECIZiWCy2rh6jXyiRMIHmRnS2NKKxFLR2C77q9QnPoYTDqNEIWr1WzDUT9ioRLPUj0VAUNLqk7ZJWvGHsa8KBCM2rW7jj7hvwBLy8/Nxb/PDtT3g9Xu7+y6089di/+fmnFd37f/f1Io4/eQ42uzVuyf8Fl5xJoG3Hiv7KGpmMoelcdM71dLRtDsoWfPMTx590FKefdiKtGxIP+23SurEdc6qJ22+/Ho1eQywcw9Pkoa2tK1CyZKSy4PsfEwY8879YwImnHIPJbEiYrdzZ5OKMs0/kk/e/SPjYZ19wSsIEmcKeo0QVAo7EE3gjrk5S8tMBuofrEu7n96M36sR7314gyRIZ+WmYU41EIzHaGjqS9vQOdmIYrh9xt3vAkAIkHjrQ2jKpXycmeO9rUtPNLFq4hHNPvoJP3v+CFUsreOWFt7nmoj9wzkWnUVAUPzT7w3c/MeOgqUBXPblb77yGgw7YH2fTjgUO9nwbzz31SlygtMm7b31CmDAa7bbfQoLeIK0b2mhb34YaVEjNSMWYagTAaDfw9lb15bb01msfYM2yJGyLRWLIQYk/3vt7DIbNiQ71Bj03//EqjKpBfMDuZQazDiWSvCyPGg4Q9Eegl3qCkla/z35A703WDAujpw8hWO9gw4ff41i0kuIhmRSPyt/bl7ZXiJ6lfqZ6ZQPlY4oJNdfGjd9rUm2EYnqcLeKb8r4mJSeFhy9+vMf2UCjMPx5+mjPOO4mHtxh202g0XHntRZx3welISPhb/DSv7VnbbluMdgOffvRV0vbPP/mGIw44hE6Hc5vnyhqaiT8a4NU3/4ff7+fIow+jfFwJsXCMcDj5B2E4HEbtZdpRZ72T0uxCXnzjCVpa2lBVlZzcLHzNvh5pEIR+SNbQuLGVgoIMIp09V1ZKWh3hCCLo3cMMZgP5ZRl8s0XxW1djO80VtYw4cip5ZVk0VSVfCTsYiWCpn2lr6CQWUykbV4asRkCJIelNdDR72LhU5BvZ1+gMOqqr6ohGEw9TVK6vpqAwvmfp6GNnUf1TNdFdTvQmofZSKDhREWFJkrBmWpA1Mp4OL9FwlKwhmbz97oe8/tK73ft9/vE3DB81hIce+yNHHH0Izz75csLHOOa42fg7ex8+dLd4cLd4unu5GpsT530S9rxOh4e01FRivgQroCQJVdLRVNlMVsEIdJY0op7NvZiy3oAuq4jl3ySezyTsPnllmax865uEi0DWzvuJQ284TQRLwt7X6XDS6XBuTv3vD6EmKfUg7H0pdjPmTDNIXRmvnU2uPnu+JAmUbSQjUbeYZT3nuFmY9Wa8kV3PKRRyhzj8qIP55IMvE7bPOvJg3PWbl3SnFaWhs2r5+svv8Xn9HHDQdLLsmXgDvrhAaZN1FZV88L95nHTar3jvrU96JLAcMqyU0WNG0Jgk6eXWdldGdWHn1a1tInPWKJRQDWo0vgdRn11E5aoGUGHF/LUUjconu2gIkhoDWcbnDrF6/jpCIlXKHmc06+msTdIbrYKztgVTqpGAt+8LifdXIljqx8KBCCDG6vsrSZbIG53LytVrePXxt/F6vBxw8DROOfPXOKuc+F07NqE6kXAwQtmIYjQaTcIisyVlhYRDYWYcNJVTz/w1+dm5NK/tm3ltzgYnv7v0t/zw3eIe9eGOOOZQzDoj3l/yNKUVpbFk1XIefeBf3fv855nXmTR1LFfdODfpY7z92ofMPuIQnnz+Yd59+2M+/fArtFotvz55DofNPhDHKjFHbyCLhqOs+GYdo/YfgpYIhH2osh7ZlEr16iY6Gp1A14yD2tWN1K5uRJIYUMkKByMpybzZLXbY50iqqFa4y9xuNzabjfee+ogUk8gJsq/IGZHNP//5HPO/WBC3PSXVzLOv/I3WirY+qXlkL7Dx4/Kl/OORZ+K2a7Ua/vncg6Sn2gj5I3haPAlXje0Kk8VIxtB03v/fPL758gdSrSmc/tsTKS8pwVHhQFW7gsb04en89uRLE55j7pVns3TxShZ9/3OPNos1laeffwTHmhbScm3oLF0TtYOdQdytIhHhYGK2mjDbzERCka7SQeKTp98qGV1A5ScL6axJ8GVFgkNvOI1VCzbs+QvbDXwBH8fNPRqXy4XVak26n+hZEoSdoNVp8AZ9PQIlAJ/Xz7/+8W8uPOdM2qoT10bbEc4GFzP2m8ro50bwn2dfo6mxhXETRnHmOScRaAnSULv7aisFPEHqlzZy6LSZzD7sYFDA1+qjafXmx0zLsfHxB58nPcdb//2AS685P2GwdMjhM4l4o6B2FTJGTDcatPzuAH73jvW26gw6yscX/pKYUkFFpq2hk5rVjWJqwm7kqG5l3EkH8t3j/yO21eT6kXOm0d7k3DsXtheJYEkQdoIlw8InnyWeywPw9Wffc+nl5/XZ47VtbEdn1HHFJReCBpSQQtua9m1m0e4TKjgdyVdharQaOjuSt7tdXjKy0ntsT7WkcO6Fp+FYKYbaBiNZI5NTlkVmvh1VVWmubqetvn27h9h0Rh0TDx1JrLORcNPm4DzDZsN+6EiWfbVGBEy7Scgfpn5jGwddfTJ1P66hY2MTRquZsoPGEwwr1K3d977VDJg8Sx0dHZx11llYrVbsdjsXXnghXm/vNWYOPfRQJEmK+3fJJZfE7VNbW8uxxx6L2WwmOzub3//+90SjYpmq0DtVVdHpkycQ1Wg1ff6YkWCEtup22irb6ajv3DOB0nbwdHg5+LAZSdunz5zMkCGlXHbN+eTmZ2OzW/n1b+bw3KuP4ax29pv7EPqO2WZmypFjyM/VILvr0XobKSkzM/mIseiMyV83Wxo2uYRYez2xQHxVg6jHhSbsJn+oqGawO3navaz+YQO6nCzKjphG1uRRVK9r3icDJRhAPUtnnXUWTU1NzJs3j0gkwvnnn8/cuXN5+eXES443ueiii7jrrru6fzabzd3/H4vFOPbYY8nNzeX777+nqamJc845B51Ox7333rvb7kUY+NxtHmYdcSDPP/Vqwvajj5tF0LlvrOIJ+kKUjyuhtLyI6o11cW1anZZLrjyX2sV1TBsziYMfmwEyRL0RHCuaRaA0CEmyxJgZQ4g4qlB/WZSgAhFnG7Lew5iZw1j6xeptniPFYiDclHi1VdTdSU5JKQ3rdt8QtNA10b69oZNdn0ww8A2InqWKigo+/vhjnn76aaZPn86BBx7I3//+d1599VUaGxt7PdZsNpObm9v9b8sJXJ9++imrV6/mxRdfZOLEiRx99NHcfffdPP7444TDfTtRVhhclJiCJqrhtN+e0KMtJzeLcy44FecvK332Ba3r2njkH3dxypnHYzQaAJi6/0SefflvhFpDRMNRXC1umte20FzRQntd/+kZE/pWVnEGiq+zO1DakhIOoZXCpNjNCY7cTKvT9kg1EE/dFxdkCXvRgOhZWrBgAXa7nalTp3Zvmz17NrIss3DhQk488cSkx7700ku8+OKL5Obmctxxx3H77bd39y4tWLCAcePGkZOzuTv3qKOO4tJLL2XVqlVMmjQp4TlDoRCh0OZeA7fbnXA/YXBr29jOCccfzWGzD+D1V97F7fZy2OwDmL7/ZFrX7aH5RP1EJBShflkjxx95JKee/muQIOKL4Kx1ERGlKvYp6TlWFH/ymoFqwIM9y4rPmbxoeDQSRdL2MlwnSSK9gLBHDYhgyeFwkJ2dHbdNq9WSnp6OI0mRRoAzzzyTkpIS8vPzWb58OTfddBNr167lrbfe6j7vloES0P1zb+e97777uPPOO3f2doRBpGV9KzqDjgvOPhMkCHlC1C/tvbdzsFIVlY4GJzQ49/alCHuRElNA1pAsR5wkywlzhm1JVVS87iAmo4lYsOcKOq01naY+WGkqCNtrrw7D3XzzzT0mYG/9b82aNTt9/rlz53LUUUcxbtw4zjrrLP7973/z9ttvU1lZuUvXfcstt+Byubr/1dXVbfsgYdCKhCK01bTTVt2Op733RQeCMNg1VbWhSU1L2i6Z7bQ3bLtu34YlNcjpBWi2yl2ntaYR1VpoqtzxeoeCsLP2as/S9ddfz3nnndfrPuXl5eTm5tLSEv/CiEajdHR0kJubu92PN336dAA2bNjAkCFDyM3NZdGiRXH7NDd3LWPu7bwGgwGDwbDdjysIgrCv8LR7iaiFaEwpxALxZXe01jRcHQEioW2vOI6Eoiz9ooLSsQWk5eUAKqoq01LXQd2aNYh8ysKetFeDpaysLLKysra534wZM3A6nSxevJgpU6YA8MUXX6AoSncAtD2WLl0KQF5eXvd5//znP9PS0tI9zDdv3jysViujR4/ewbsRBEEQgO4SJ+bcTNSAGyQZyWSls9XHhp+rt/s80XCUDUtq4rYZU40MnVxCitVEKBimfm0zng7RoyvsXgNiztKoUaOYM2cOF110EU8++SSRSIQrrriC008/nfz8fAAaGho4/PDD+fe//820adOorKzk5Zdf5phjjiEjI4Ply5dz7bXXcvDBBzN+/HgAjjzySEaPHs3ZZ5/Ngw8+iMPh4LbbbuPyyy8XPUeCIAg7IMVuJq8sC41WprPFw+oFG9AZtFgzraiqirO5aZfL/xSNzCO32E7M1UKsvRmjTs+IiTn4AjlULNi16RWC0JsBkToAula1jRw5ksMPP5xjjjmGAw88kKeeeqq7PRKJsHbtWvz+rhUWer2ezz77jCOPPJKRI0dy/fXXc/LJJ/Pee+91H6PRaHj//ffRaDTMmDGD3/72t5xzzjlxeZkEQRCE5CRZYuxBwxk1JR+rzoNZ7aCo2MjUo8aiM+hoq2unvb5jlwMlW5aV3EILYUd1d6JKJRIm0taIWRuieFR+X9yOICQkCun2AVFIVxCEfdXIaeWYNV5ivvjCx5KsQZdXxpJ5q4lFd72g9PhDRiC7G1Fjiec76fOH8OPHK3f5cYR9y/YW0h0wPUuCIAhC/6LVa7HYDT0CJQBViaG4W8kbkp3gyB2nM2iSBkoAajSMzjAgZpYIA5D4yxIEQRgE9CY9uWWZ6A063J0+Wmvbey00K2tkcsuysKSbiQSjNFa2EPTtWIkea0YqajD55Oqox01mfgn1fVJPrPec3ZKsQYmJgRJh9xDBkiAIwgA3dFIJaVkmYp52iAawFaVQMmos636qwdXas8JAep6doROLiLnbUIIdyGYdGTNKcTnDrPuparsft2sSRy9BjESfLfF3t/uwmMw9CusCIMtEYlKfDPcJQiJiGE4QBGEAKxqZh82iEnbUEPN5iYWCRF3thBsrGTGlCIM5fmWvMdXI0AkFhBoriXqcKJEwUb+PcEstFnN0hyZKu1rdSCZL0natJY3m2r7JtF2zugFtej6SZqvv+JKEIaeYjctEcmBh9xHBkiAIwkAlQW5pBlFna882VSXa6aB4VF7c5pLR+UTbG0lUXC3qbCOnJH1bI17dlJhCe7MHrTW956XpdEip6bRUJ68TtyPCgQgrv9uAnFmMLrMAnS0dbXoOutxyNixvwt3Wc96UIPQVMQwnCIIwQJlSjSjhnrXTNokF/Fiy4+tfpliNxFqCSY9RwwFMKUYC3uT7bGnj0lqGTSnFnluK4usERUEypqJoTayYvw6ll3lTO8rvDrBk3ipSbGZMFiPhoBt3W3WfnV8QkhHBkiAIwkClgrS93UDbS5J3eJ7R+sXV6Aw6MovS0Wg1uFpbdmudRJ/Lj8+VYO6SIOwmIlgSBEEYoALeIJLemLRdYzLjao+vz+Zq92FLNlEaQGvY4VVx0FVQumlD8w4fJwgDgZizJAiCMADpDDrKxhcha2RSy4ZjLihBYzRt3kGW0abnUbcmftl+3ZpGtOl5SLKm5zkz86hfLwIeQdia6FkSditrpoXSMfnoDRpAIhiIULWiAZ/Tt81jBUFIzJRqZOyBw4g5mwjWdU2glnV6TLkFRIMBYtEYsslGxaIqwoFw3LHhQITVP1Qxcno5BN2ooQBotMgpaThqO3BsTDBZXBD2cSJYEnabnLIsiodlEGltIPJL5l2tTseY6cVUrnDQ3ti5l69QEAamUfuXE2mpRo1uzmitRML46qowFw+lZlUzzdW1kGTqkbfTx08fr8CeY8OSZiYcitBWt0bkKRKEJESwJOwWGp2GkhHZhBo3xm1XIxFCjhrKJwyhw+HsNcOwIAg9pdjNyEqYWDRx6Y9QWxMpVnPSQGlLzmYXzmZXH1+hIAw+Ys6SsFvklmV1ZRNORFVRfZ1kFvbMzSIIQu/MVjNEkq8EiwX8pKaZkrYLgrDjRLAk7BYpViNKqLdcLkHMluSreARBSCwSioCsS9ou6/REgskLzgqCsONEsCTsFkFvGFmvT76D1kDQF07eLghCQq4WV68lRjTWDOrWiRVtgtCXRLAk7BZNVS3Ilsyk7ZrUNFrr+qZmlCDsS1QVaiqa0OcUgxSfkFJrSSMQkvF27L6EkIKwLxITvIXdIhKK4qjpJLeggHBbE6hKV4Mso88qpGZtM0pM2bsXKQj9jDHFQP6wHAxGHT5XgMbKFqLhnkNqLTXtRCMxysaUIamRrhIjeiOtDU6qV67fC1cuCIObCJaE3aZuTRMBbzrFI0vQ/JL/LhJR2bCikU6HWIEjCFsaOrmEtAwjMXc7SjiMyW4i9/CRVK920FLTsxhtR6OTjkYnepMOWZYJ+UOJauMKgtAHRLAk7FZt9R201XcgSRKg7rY389T0VMrHF2IwaAAVRZGoW+egpUYM9Qn9X8GwHOwWUANujBlZIElEfR5CjVWUjijC7w7g7UycyDUciOzhqxWEfY8IloQ9YkcLc+6I9Dw7Q8fnE26pI/xL8kskieLyHFJtZjYur9ttjy0IfSF/SDZ6vUTE3Ym/qQ5VUdFZbVjKhxML+Bm5/xBWfbuOgCf5CtMdZclIxZKeQiQUo6Oxg1hUDIsLQjIiWBIGNgmGTCgi1FRJXLeVqhJpd5CRW0Jj5c4VBhWEPUFn0GEw6fDVbUQJd/2dyjo9hrQMIm4nEa8bSZIZM72YQEBl9YINu5TM1ZhqZPSMIcjRAIR8oDFQOmoULfWd1Kxq7KvbEoRBRQRLwm6TmpZC8cg8DGYdqqLSuLGN1tr2Pu1lsmfbUANuko3vxVytFAzPpfLnmj57TEHoSyariVjQ1x0ogURKYQm++mqUyBZDbH4vhhQLI6eVU/FD5U49lkanYdyBw4i0VMdnAHd1kJWVQ3RYDg2ikK4g9CBSBwi7RcnofEZNLcQQaUVprUbtqKOoxMTEWaOQNX33Z2dMMUAseb4mJRTClNJLvidB2MssaSko0SimnHwMmdno09IJuzrjA6VfxHweUq16dIbkSSl7kz8km5i7Ja6m3CaRjmbyh2SBlOBAQdjHiWBJ6HOWjFSyC1IJN9eihH8JZFSFqLMNvC0Mm1LaZ48V8AZBa0jaLhsM+L1iCE7on+w5VopG5KCEQ4ScHcSCAfRWO7I++d+0GnBhz7Ht1ONlFqQR87qTnzvkJ9WeslPnFoTBTAzDCbtEkiCzKIP8IdloNBKhQASdXkekoz7h/krAhzUvB1kj90meJVeLGyaXgNS6OZfTFrT2bBq+2bkhC0HYnQwpBoZPKiJQu657GFkJBfF6PRgysjGkZxHqaO3Tx5S20WukKgqSLLqWBGFromdJ2GmSLDH+0FGUlJqhs5ZYSxX6YDMmo4IhLXn2biUc6Bo+6yPrF9dgyCtF1m0ebpNkDfqsAhx1LkJ+UVZF6H9KRuUT7WhKON8u1N6CzmpPeJxksuFs2bk8Za52HxqTOWm7bEzB50xepFcQ9lUiWBJ22pCJxWiC7UScbaB09eookQj+hhokWUabkrh+laTREo3E+uw6XK1uVi6oImzKRp9Xji6vDNKLqFzdRu1qsbpH6J9S00zEgoGk7bFQoMdwnCbFgtcdJhLcudxKdWua0KblJuxi0qTa6Gz1isz6gpCAGIYTdookS6RlWwg3tiRsD7Y2Y84vIurzbH0gClrCgb7t7fG7/Kz6VpR5EAaQbSwKlWQNWlsGMa8TSZKRUtMIBGHN9zv/dx4OhFn/cz3DJg8h5m4jFvAha7XIlnQCIZnKn9ft9LkFYTATwZKwU/RGHWo4+cRpNRZFkrfquJQkDLklrPs58XymgSw9z07BsBy0Og0Bb5Daiib87uS9BoLg7vBhNZmJBRIPe0UULRtWNpGeayUWVWhZVdsnSSk7m10snreK3PJsbBlZRMIxGhc3iuE3QeiFCJaEnRKLKkibCr4loWp06HNLUcJ+JI0eVaNn3dI6nC3JV+MMNJIsMe6gEeilIFFXA0oshslgZMz+xTjq3NRVNO3tSxT6qdqKJiYeOpxYU1X3MPYmuvQcGitbcTa7cDb3fR3FWFShYZ2Dhj4/syAMTiJYEnZKNBwlEpOQNBrUWM/5R9pUCw3rW2hY34zJYiQajvZpqYb+onxCMdpIJ5EtlmMroSBhRy25BUW4Wr242zy9nKF/0Bt16M0GwoHQoK81JmtkDCY90UiMSGjv3Ws4EKZiUTUj9ytH8btQQ34krRY5NZ3WRjeNG0RySEHoL0SwJOy0DUtrGT2tlJCjOu6bsaw3IFlyqP9xNbFIDE+7d+9d5G4kyRLpOcnnbUU6HJSMymPFN/03WDKmGhmxXyl6rYoSCSHrDESiMmt/rOrKYTWIyBqZYVNKsaaZUMJBZK2WiKJhw8+1eDt2/m9UkiW0Oi3RcHSHs9N72r38+MkKMgvSSU0zEw5GaPlxHdFwz6SRgiDsPSJYEnaat8NHxY81DJtUgqxGUKMRZIOJYDDGyq/WEOvDFW/9kcGkRw0nDyjUaAS9sf++xAxmPeMPGka4uYZwdHMPi6TVMu6gYSz7eh0h/+BI6CnJEhMOG4nkaSHc1LTFdg2jp5VQ8WPNDgf1epOeYZNLMKfqUWMRJI0OjzPAhp9rdyzYUaGtvoO2+o4denxBEPac/vtOLgwInnYvSz5bjTHFgM6gI+gLEgntG9+KY1EFtjVvaw9dy84oG19EpK0eNRo/FKVGo0Ta6ikfX7jTNcj6Ump6Cpa0VKLhKO2NnTu1tD2nNBM55CIa8MVtV5UYIUc1QyeV8PNnq7f7fHqTjgmHjiDaVke4aXNAaTKamHDYSJZ+UTHovywIwr5kwORZ6ujo4KyzzsJqtWK327nwwgvxepN/E6yurkaSpIT/Xn/99e79ErW/+uqre+KWBpWgL4Snw7vPBEoAkVCEmKqBrVf9/UKbYqG9se8n5/YVi82IEkrcM6aEgqTajHv4iuIZUwxMnj2GkRNzKciVKCk1MmX2KIpH5e/wufLKMom6OxM3KgoaouhN219DsHxCMbG2epRQfM+bEgygupt36hr3Fp1RR9n4IiYdPpqJs0aRNyS7T+s3CsJgMGBeEWeddRarVq1i3rx5vP/++8yfP5+5c+cm3b+oqIimpqa4f3feeSepqakcffTRcfs+99xzcfudcMIJu/luhMGiank9hpziHkn+ZJ0e2ZZDwzrHXrqy7bGtfq+91y+m0WoYd9BwlI46Im2NRDwuIq4Owk0byc41UDAsZ4fOJ0lSwkzZm6jRCDrD9ne0W+xGYqEgsk6PPi0DvT0DSdtV3Dbm85KRZ92h69tb7Dk2Jh02gjRzEKW1CrW9hvw8LZOPGLNDwaMgDHYDYhiuoqKCjz/+mB9//JGpU6cC8Pe//51jjjmGhx9+mPz8nt/iNBoNubm5cdvefvttTj31VFJTU+O22+32HvsKwvZwtrhZv0xiyPhyiAYgGgG9iUhUYtnXa/s0U3lfU1SJrhLziYMIRd1736XyhmajuFt7DBECRNod5JcPoWFD83bHc5FQFI1Wl/B8ALLeRNC3ffOzZI0MqkpKYSmqqhBxu0CSMOcWoCox/E31vQZm/YVGp2H45GJCjZWbr1dVibo7kQM+Rs8YytIvtn9oUhAGswHRs7RgwQLsdnt3oAQwe/ZsZFlm4cKF23WOxYsXs3TpUi688MIebZdffjmZmZlMmzaNZ599dpsrWkKhEG63O+6fsO/qdLj46dOVVCxpYsNaFyu+r2LZV2v6PEt5X3NUtaG1ZyRs09oycFS37eEr2iyrII2oN/kQphr2k2pP2e7z1VQ0oUvLTtgmG4z4fJHtnmOkxBQMKSaC7a34G2q7er3cTnz11UQ8bsx5Rajq9hejlTUy+cNymDRrFJMOH82wKaUYzD1rJ2r1WrKKM8kpzcKYuutDpHnl2cRcrQkDOyUSRitFMNuS15EThH3JgOhZcjgcZGfHv9FptVrS09NxOLZvmOOZZ55h1KhRzJw5M277XXfdxaxZszCbzXz66adcdtlleL1errrqqqTnuu+++7jzzjt3/EaEPmMwG8gbkoXBqMPd6ae5qnWv17QaaBmQG9Y3Y8saijkjh6izDTUWQ9Jo0NoyCUR0NKzrv+VjVEVBkrc/IHG3eWhvSyMjq4BIRzNqrGtundZih5QM1n21ZrvPZclIJeJxE9tqsjhAxONCb0+nrX775qrpTTrGHzwS1ddGtK0GULGYzEw4eCgbVzTS1tA1z2ro5BLSMlNQA05QVYqHFhOKSKxeULnTaQbsWanEfL0kTQ15saSl4HcNrL9rQdgd9mqwdPPNN/PAAw/0uk9FRcUuP04gEODll1/m9ttv79G25bZJkybh8/l46KGHeg2WbrnlFq677rrun91uN0VFRbt8ncL2GTalFHu6AcXTgRL1kppromjYGDYsraOjydlnj6Mz6NAZtIT8YWLR/juctitWf7+BtDw7RcML0OpkohGFjWub+/T3uDPcHT5svZQCkY0p+Jy1O3TOjctqac+2UjwqH51eAyo013XQ+MPqHQq0c0oyiLjak7aHne3EwtsXyI2eMZRYey1KeHMvZCzgJxbYSPn4IbjaPJSMKcBmjhJ2VG0+0O1EYzAy7uAR/PzZqu2+9i1FIzF0Gm3CpLIAyBpi0cGROkIQdtVeDZauv/56zjvvvF73KS8vJzc3l5aW+MR/0WiUjo6O7Zpr9MYbb+D3+znnnHO2ue/06dO5++67CYVCGAw9u8IBDAZD0jZh9yoamYc1RSHcvPmDUgmHiHpcDJ1QxgpPcJeTKabYzQybXIJOo6BEI8g6I35fhLU/VvX4Fm9MNVI8Ko8UqxFVheaa9q5eLqX/z1nZpLPJSedeDo62VremiYxDhxFrrOoxTKRJseFs8+1UT6Krxc2KXSy3I8sSqpL8sVVF7VkXMQGTxYhWihEJJx6ujTlbKBqZR1pWCuGmqh7tSiiINuIjLddOp8O53de/SWNlK8PHZaG0J+5dksw2Opoad/i8gjAY7dVgKSsri6ysrG3uN2PGDJxOJ4sXL2bKlCkAfPHFFyiKwvTp07d5/DPPPMPxxx+/XY+1dOlS0tLSRDDUT+WUZhBpTJD7R1WJtjdSMiafNQs37vT5zTYzY2aUE3ZUE97iG7feYGTiYSP5+fOK7l6m7JJMSkdmE+10EGt1gCSRn5dG/pAxLP96DZFQFINZT+HwXMw2E2F/mLp1zWJYYzuEA2HW/1zPsElDiLlbifl9SFotGksGwbDMhiXr9tq1tTW6sA6zoYQTZ26XzFY6KrYdZKTaUyCcPP1J1O8lPTcH1Z88WWXM20FeWfZOBUvuNg9hpQCtOZWYP/46dOk5tNTtXE4rQRiMBsScpVGjRjFnzhwuuuginnzySSKRCFdccQWnn35690q4hoYGDj/8cP79738zbdq07mM3bNjA/Pnz+fDDD3uc97333qO5uZn9998fo9HIvHnzuPfee7nhhhv22L0J28+YYoBeMmbHQkFScnZt4uuwScWEm2t6DE0ooSCyp5WC4TnUrm7EYNZTOiqHUOMWgZmqEnV3IIf8jJw+hM5mF3mlacScLcSc7Zj1ekZPLcDjjrL2x66eAkmCzKIMMnJtKIqCo6odd/veL48iyRJqkt4xk9WE2WIiEors1rp3nQ4Xi+etJm9IFtaMHKKRGI0/N+Ht7DlXaE/qaOykfFwBkrazx+o62WAkquq2a/5aNBIDOflbsKTVoigqstpLwKIoSJrtn7u1tRXfrGXEfmVY8rJQAl4kWUYyptJU1Ub92v6c9kIQ9qwBESwBvPTSS1xxxRUcfvjhyLLMySefzGOPPdbdHolEWLt2LX5//JvUs88+S2FhIUceeWSPc+p0Oh5//HGuvfZaVFVl6NChPPLII1x00UW7/X6EHaeq9Mhn1GOfXTi/Riuj10uEo4knzEa9brIKyqld3UjRiDyinYk/TJRQEFO6jKnUTqipOm670lpPqj2LguG5dDhcjJ05FMXvRPG1gywzfHwWEQpZ+c3argzhe5AkQfHoArKK0pCUGMgyfk+YyqW1BH0hjKlGRk0vR0MEwn5UjRXJUELVygbaG5IkfNxFsWisX35or/x2PWMPHA4BJzGfC5DQWOwoulRWzl+7XedwtriQJhZCZ2vCdq01g5p1DkqGpYPbmXAfTYqV5qadT3yqKiprFm5Eo9NgSUtFURQ87TU7XONOEAY7SRWvil3mdrux2Wy899RHpJi2fzmzsOOmHjWWSFPiEhwaUwruSAobltTs1Ln1Jh3jZ5YSaa1Puo82p4zF81YzcdYo1Lbq5OdKz0KNRogk+ZDT5Q9BQiLSXN29MmsTjclMWJfBym/34FCTBOMOGoE+5iHq2Rz4yHo9uqxiKhZVM3Jaadf1xgWTEoa8YtYtbcLZvOeylWt0GjRaDZFgeK+lNJIkyChMJ6sgDVVVaa7p2OHhsIJhOeQXpxJujR+205hTiRkzWfZVBeMOHoE20NIj27oka9DllbH4k5UDao6cIPQnvoCP4+YejcvlwmpNnkx2wPQsCQJA/bpmCsvyiLRtNSlVltGm51L39c4HGJFgFEmbPGuxJGuIRbs+lGIxBY2sQVUSrySSdToiweRDMRoUon5Pj0AJulZDmayZGMx6Qv49k6spqzADvRSMC5QAlHCYcHM1o2eUE2lr2CpQAlAJOeooH1fCkj0QLFnSUxkysQidFtRYFElnoN3hpmpZ3R7vDVFVaKvroK1u5wvgNqxvRokpFAwfAiE/qhJDNqbg7gyybn5XOoPVCzYw/pAR6KJ+Yp5OVEVBk2JFtqSzekGlCJQEYQ8QwZIwoDiqWtEZtOSWlqN4OyEaQTKYkUxWVi+s2qXgQlVVnG0+LAkmvAJo07Ko+qV8SVNlKyVDMoh2Jp7kq0u1EWxOPslX1mmIeZ3JryXgxpZlo6Um8RBNXysYmk20M/FSfDUaRUOMUCjJMnJVQSPF0Oq1O53zZ3tYMy2MnFpM2FFDeIsg1Z5qY9zBI1j+9fbnSupPmja20rSxlRR7ChqtjM9ZH5eqIhaJ8fNnq0nLtZNbmomskXE0umipFj1KgrCniGBJGHDq1jTRuKGFrKJ09CYdnk4XnU07N/S2tcqltUw4dCRanb6r8KqqImm0aO1ZuD1q99yctoYOSkbnY8iAWMBH1L950rE+uwiPK4DGaCIWDPR8EFn+ZWJvL/OvJGmP9pTIGolYL8vhY+EQsk5HLEFPGICqxNBoZaK7sSNs2OTirjlgW014jnld6NMMpOen0dG4e+ZO7Qk+Z+8T1zsdzp1a9SYIwq4TwZIwIMWiMRxVfd/rosQUln5ZQU5pJnllpUhS16qlyorm7g8qvVHHqP2HICkRVEVBa7FhyiskEggRQ0NNRROuVi8TDhmG0lS91VCdhCG7iPr1zWRnpSXtmZJMdjode27OUjQaQ9ZokiYo1OiNhHsJ3iSdcbeWdzFZTUjRUI9AaZOou53CYQUDOlgSBKH/EsGSIGxFVVQcG1txbOwZjGm0MuMPGUmsvTYumWCwGQy5xdRUNNP6yxyWVQs2MnJaGVI0AJEgaPRIxlQqVzTQ3thJduFYZIMLZavhLY3FjqszsFuHtLZWv7aZ8pGZRNqbe7TJOj2BQAytPTsuGegmWksarQ3O3TrRWm/QQSxxEVwANRZDo9XsvgsQBGGfJoIlQdgBeUNyUL2tceUpNgk56igZVd4dLPmcfhZ/ugpLeiomi5Fw0Ad0YrYYychPY8X8dYyeOQS9NYoScIMkIaek4XaGWP/TzifW3BkdTU5ySjMwp2URdbZ1Z83WmMxo0vJZ9vVasovTyS8tJepsIRbwI+l0XXXkwlqqV+7eXrCAN4ikT55DS9btucnwgiDse0SwJAg7IKsojWhLz9ITXVSIBjFZjAQ8m5d5ezq8IMGIqaUQ8v7Sy5SKNK6A6tVN+F0B0nKsxGIqbQ0biAST96DsThULKsktzyK/vBRZBiQJV5uP6i/XEAlFqF/roK2+k6KReaRkZhMORti4vHm3JqbcJBwIEwpLaHR6lEjPoEiblsP6xQ27/ToEQdg3iWBJEHbAtnIlq0rX3J8tGVONjJpWQqipCracRO1so3RkCRuWNVG/rn8kXkw2/LhJ0Bdi/eLqPXdBW1izaCMTDhlBzOUg5utarShptOjSc2hr9vdZZm97jg1bZiqRcJTW2nYioT03HCoIQv8kgiVB2AF+bwiTwdgjQeAmssFMwBO/Aq50bAGRtsb4QOkX4ZY6SseW0LkHEzoOVOFAmJ+/qKBoRB7pedmgqkQiMdavdPRJQkyz1cToGUMg5EUNekGjpaBsGB2tfip/7pvVloIgDEwiWBKEHVC7upExM0oJb1HGZBON2YKrw9+j+Giq1UjEkaSmnaKg1ajIGlkULd0O0XCUqhV1VK1I3C5JkFmYQW5ZBpIs09HkpKmyNS5vUSJanYYxM4cSaa6KXxHocWG3Z1IyJp+aVdsujisIwuAk7+0LEISBxO8OULe+DUN+GZpfSttsGgqKGTOSDFH1vkxMVVUkeeeLoQpddAYdk48YS3GZGY2nEamzlpxMmHLEaFLTei9DlD8sh5irJWHqhKizjeyidKRt1CUUBGHwEsGSIOwgx8ZWls3fgDucgpRZSjQ1j8o1TpZ9VYGaIKNyKBRD0ibrxJVA1hKL9N7zIWzbmJlDUTrqiTrbuoIeVSXqcRFq2sjo/cuRNcnf7jLy7MR87qTtatC7zYBLEITBSwzDCcJOCPnDbNjOeSw1qxoZMSk/cY6itGwaNiROTClsP7PVhFaOEgknKMmiKMQ87eSUZtJUmeR3vR2dRqJnSRD2XaJnSRB2M3ebh/qNnRjyy9GkpCJptGhMZvQ5xTjdavIPcGG7WTMtEEyewiDm85CWk7yiuLPZg8acmrRdMqTg7exZL1AQhH2D6FkShD2gqbKFtvpOCoZlY7baCPnDNHxfTdCXpDitsENiUQWk5G9nkkbTtU8S9escZM0aQSzo77FqUWtNo73ZI4rWCsI+TARLgrCHREIRqleKxIm7Q2dTJ6WjRoA7cW04jSUNx8q2pMdHQhEqFlYzcno5ircTJehF1uiQUtPwemJsXFq5uy5dEIQBQARLgiAMeNFIjI4WH3ZbBlFXe1ybxpRCFCOu1uQTuKEr0/pPn6wkqygDW6adaDBG08pagt4kaR8EQdhniGBJEIRBofLnGsonFJGRV44acIOqIJms+H1RKuav3a5zqIpKS00bLTXJe6EEQdj3iGBJEIRBY+OyOqpXythzbMiyhLuthfBeqrUnCMLgIYIlQdhH2bKt5JZkIEkSbY1O2ho6tpU/c0BQYgodjYnnLgmCIOwMESwJwj5Gq9cy7qDhaBQ/MU8nKCol5TZKx+Sz8tv1YoWeIAjCVkSeJUHYx4yZORTcTUTam1HCIZRImKizlWhrLWMPHLZdCRoFQRD2JSJYEoR9iNlmRqeJoYR6rvBSoxHUgJuM/LS9cGWCIAj9lwiWBGEfkp5r7VoploTid5GZb99zFyQIgjAAiGBJEPYh28xCLcko6iCY5S0IgtCHRLAkCPuQ9oZO5BR70nZNqp3m6vak7YIgCPsiESwJwj4k5A/j88bQpPQsKisbTcRkE+625AVpBUEQ9kUiWBKEfUzFwkoCpKLPKUFrsaGzWNFlFxEzZbNiOzNdC4Ig7EtEniVB2MeoikrFgkr0Jj2Z+WkgS3Q66gh4Anv70gRBEPolESwJwj4qHAjTWNm8ty9DEASh3xPDcIIgCIIgCL0QwZIgCIIgCEIvRLAkCIIgCILQCxEsCYIgCIIg9GLABEt//vOfmTlzJmazGbvdvl3HqKrKHXfcQV5eHiaTidmzZ7N+/fq4fTo6OjjrrLOwWq3Y7XYuvPBCvF7vbrgDQRAEQRAGogETLIXDYU455RQuvfTS7T7mwQcf5LHHHuPJJ59k4cKFpKSkcNRRRxEMbi4ietZZZ7Fq1SrmzZvH+++/z/z585k7d+7uuAVBEARBEAYgSVUHViGo559/nmuuuQan09nrfqqqkp+fz/XXX88NN9wAgMvlIicnh+eff57TTz+diooKRo8ezY8//sjUqVMB+PjjjznmmGOor68nPz9/u67J7XZjs9l476mPSDGl7NL9CYIgCIKwZ/gCPo6bezQulwurtWdlg00GTM/SjqqqqsLhcDB79uzubTabjenTp7NgwQIAFixYgN1u7w6UAGbPno0syyxcuDDpuUOhEG63O+6fIAiCIAiD06ANlhwOBwA5OTlx23NycrrbHA4H2dnZce1arZb09PTufRK57777sNls3f+Kior6+OoFQRAEQegv9mqwdPPNNyNJUq//1qxZszcvMaFbbrkFl8vV/a+urm5vX5IgCIIgCLvJXi13cv3113Peeef1uk95eflOnTs3NxeA5uZm8vLyurc3NzczceLE7n1aWlrijotGo3R0dHQfn4jBYMBgMHT/vGnalz/g26lrFQRBEARhz9v0ub2t6dt7NVjKysoiKytrt5y7rKyM3NxcPv/88+7gyO12s3Dhwu4VdTNmzMDpdLJ48WKmTJkCwBdffIGiKEyfPn27H8vj8QBw2tW/6dubEARBEARht/N4PNhstqTtA6aQbm1tLR0dHdTW1hKLxVi6dCkAQ4cOJTU1FYCRI0dy3333ceKJJyJJEtdccw333HMPw4YNo6ysjNtvv538/HxOOOEEAEaNGsWcOXO46KKLePLJJ4lEIlxxxRWcfvrp270SDiA/P5+6ujosFguSJPX1rfcLbreboqIi6urqel0xMFiI+x3c9rX7hX3vnsX9Dm59db+qquLxeLb5mT9ggqU77riDF154ofvnSZMmAfDll19y6KGHArB27VpcLlf3PjfeeCM+n4+5c+fidDo58MAD+fjjjzEajd37vPTSS1xxxRUcfvjhyLLMySefzGOPPbZD1ybLMoWFhbtwdwOH1WrdJ16Im4j7Hdz2tfuFfe+exf0Obn1xv731KG0y4PIsCXvHplxS28pFMViI+x3c9rX7hX3vnsX9Dm57+n4HbeoAQRAEQRCEviCCJWG7GAwG/vjHP8atAhzMxP0Obvva/cK+d8/ifge3PX2/YhhOEARBEAShF6JnSRAEQRAEoRciWBIEQRAEQeiFCJYEQRAEQRB6IYIlQRAEQRCEXohgSQCgo6ODs846C6vVit1u58ILL8Tr9Sbdv7q6Omnx49dff717v0Ttr7766p64pV7t6P0CHHrooT3u5ZJLLonbp7a2lmOPPRaz2Ux2dja///3viUaju/NWtsuO3m9HRwdXXnklI0aMwGQyUVxczFVXXRWX9BX61/P7+OOPU1paitFoZPr06SxatKjX/V9//XVGjhyJ0Whk3LhxfPjhh3Htqqpyxx13kJeXh8lkYvbs2axfv3533sIO2ZH7/b//+z8OOugg0tLSSEtLY/bs2T32P++883o8l3PmzNndt7HdduR+n3/++R73smUyYuj/zy/s2D0nen+SJIljjz22e5/+/BzPnz+f4447jvz8fCRJ4p133tnmMV999RWTJ0/GYDAwdOhQnn/++R777Oj7QlKqIKiqOmfOHHXChAnqDz/8oH7zzTfq0KFD1TPOOCPp/tFoVG1qaor7d+edd6qpqamqx+Pp3g9Qn3vuubj9AoHAnrilXu3o/aqqqh5yyCHqRRddFHcvLperuz0ajapjx45VZ8+erf7888/qhx9+qGZmZqq33HLL7r6dbdrR+12xYoV60kknqe+++666YcMG9fPPP1eHDRumnnzyyXH79Zfn99VXX1X1er367LPPqqtWrVIvuugi1W63q83NzQn3/+6771SNRqM++OCD6urVq9XbbrtN1el06ooVK7r3uf/++1Wbzaa+88476rJly9Tjjz9eLSsr6xd/vzt6v2eeeab6+OOPqz///LNaUVGhnnfeearNZlPr6+u79zn33HPVOXPmxD2XHR0de+qWerWj9/vcc8+pVqs17l4cDkfcPv35+VXVHb/n9vb2uPtduXKlqtFo1Oeee657n/78HH/44YfqH/7wB/Wtt95SAfXtt9/udf+NGzeqZrNZve6669TVq1erf//731WNRqN+/PHH3fvs6O+wNyJYEtTVq1ergPrjjz92b/voo49USZLUhoaG7T7PxIkT1QsuuCBu2/b80e9pO3u/hxxyiHr11Vcnbf/www9VWZbj3pSfeOIJ1Wq1qqFQqE+ufWf01fP72muvqXq9Xo1EIt3b+svzO23aNPXyyy/v/jkWi6n5+fnqfffdl3D/U089VT322GPjtk2fPl29+OKLVVVVVUVR1NzcXPWhhx7qbnc6narBYFBfeeWV3XAHO2ZH73dr0WhUtVgs6gsvvNC97dxzz1V//etf9/Wl9okdvd/nnntOtdlsSc/X359fVd315/ivf/2rarFYVK/X272tPz/HW9qe95Ubb7xRHTNmTNy20047TT3qqKO6f97V3+GWxDCcwIIFC7DbwmgxfgAACfNJREFU7UydOrV72+zZs5FlmYULF27XORYvXszSpUu58MILe7RdfvnlZGZmMm3aNJ599lnUvZzaa1fu96WXXiIzM5OxY8dyyy234Pf74847btw4cnJyurcdddRRuN1uVq1a1fc3sp364vkFussKaLXxJSX39vMbDodZvHgxs2fP7t4myzKzZ89mwYIFCY9ZsGBB3P7Q9Vxt2r+qqgqHwxG3j81mY/r06UnPuafszP1uze/3E4lESE9Pj9v+1VdfkZ2dzYgRI7j00ktpb2/v02vfGTt7v16vl5KSEoqKivj1r38d9xrsz88v9M1z/Mwzz3D66aeTkpISt70/Psc7Y1uv4b74HW5pwBTSFXYfh8NBdnZ23DatVkt6ejoOh2O7zvHMM88watQoZs6cGbf9rrvuYtasWZjNZj799FMuu+wyvF4vV111VZ9d/47a2fs988wzKSkpIT8/n+XLl3PTTTexdu1a3nrrre7zbhkoAd0/b+/vcXfoi+e3ra2Nu+++m7lz58Zt7w/Pb1tbG7FYLOHvfs2aNQmPSfZcbfp9bPpvb/vsLTtzv1u76aabyM/Pj/sgmTNnDieddBJlZWVUVlZy6623cvTRR7NgwQI0Gk2f3sOO2Jn7HTFiBM8++yzjx4/H5XLx8MMPM3PmTFatWkVhYWG/fn5h15/jRYsWsXLlSp555pm47f31Od4ZyV7DbrebQCBAZ2fnLr9OtiSCpUHs5ptv5oEHHuh1n4qKil1+nEAgwMsvv8ztt9/eo23LbZMmTcLn8/HQQw/tlg/T3X2/WwYK48aNIy8v7//bu7uQJtswDuB/M59lmlm4XAdlzdQy/KpQFqIHo2EGb51p5UdFBkEHgglGH5ZCjZCgoijC6CSQFEKp7EPTg76EzNEoCyezkMKkSC3J0F3vwcuGQ318Z7mN+v9gOJ9dz7P72rWPC/fctzAajeju7kZ0dPSMjztT3qrv4OAgtmzZgvj4eBw/ftztNm/Wl34Ps9mMmpoatLa2up30nJub67qekJCAxMREREdHo7W1FUaj0RdDnTGDwQCDweD6fePGjVizZg0uX76MyspKH47MO6qrq5GQkIDU1FS37X9Sjb2NzdIfrKSkBLt27VKN0ev10Ol0+PTpk9v20dFRfPnyBTqdbtr7qaurw/DwMAoKCqaNTUtLQ2VlJUZGRn77//TxVr5OaWlpAACbzYbo6GjodLoJMy36+voAwKPj/l/eyHdoaAhZWVlYsGABbt68iaCgINX42azvVCIiIhAYGOh6rJ36+vqmzE+n06nGO3/29fVh6dKlbjHJycm/cfSem0m+TlVVVTCbzWhqakJiYqJqrF6vR0REBGw2m08/SH8lX6egoCCkpKTAZrMB8O/6Ar+W8/fv31FTU4OKiopp78dfajwTU72Gw8LCEBwcjMDAwF9+3ozHc5b+YFqtFqtXr1a9KIoCg8GAr1+/or293bXvw4cP4XA4XA2Bmurqavzzzz/QarXTxlosFixatGhWPki9le/4XAC43mwNBgOsVqtbY/LgwQOEhYUhPj7+9yQ5zmznOzg4CJPJBEVR0NDQMGHq9WRms75TURQF69evR3Nzs2ubw+FAc3Oz218XxjMYDG7xwH+1csavXLkSOp3OLWZwcBBtbW1THtNbZpIvAJw+fRqVlZW4e/eu2/lrU+nt7cXnz5/dmglfmGm+442NjcFqtbpy8ef6Ar+Wc21tLUZGRpCXlzft/fhLjWdiutfw73jeuPH4lHD6I2VlZUlKSoq0tbXJo0ePJCYmxm1qeW9vr8TFxUlbW5vbfl1dXRIQECCNjY0TjtnQ0CBXrlwRq9UqXV1dcvHiRZk/f74cO3Zs1vOZjqf52mw2qaiokOfPn4vdbpf6+nrR6/WSkZHh2se5dIDJZBKLxSJ3794VrVbrN0sHeJLvwMCApKWlSUJCgthsNrepxqOjoyLiX/WtqakRjUYj165dk9evX8u+ffskPDzcNTMxPz9fysrKXPGPHz+WuXPnSlVVlXR2dkp5efmkSweEh4dLfX29vHz5UrZu3eo3U8s9zddsNouiKFJXV+dWS+cyH0NDQ3Lw4EF5+vSp2O12aWpqknXr1klMTIz8+PHDJzmO52m+J06ckHv37kl3d7e0t7dLbm6uzJs3T169euWK8ef6inies1N6errk5ORM2O7vNR4aGpKOjg7p6OgQAHLmzBnp6OiQd+/eiYhIWVmZ5Ofnu+KdSweUlpZKZ2enXLhwYdKlA9QeQ0+wWSIR+W+Nju3bt0toaKiEhYXJ7t273dZLstvtAkBaWlrc9jt06JAsW7ZMxsbGJhyzsbFRkpOTJTQ0VEJCQiQpKUkuXbo0aay3eZrv+/fvJSMjQxYvXiwajUZWrVolpaWlbussiYj09PTI5s2bJTg4WCIiIqSkpMRtqr2veJpvS0uLAJj0YrfbRcT/6nv+/HlZvny5KIoiqamp8uzZM9dtmZmZUlhY6BZ/48YNiY2NFUVRZO3atXL79m232x0Ohxw9elQiIyNFo9GI0WiUt2/feiOV/8WTfKOioiatZXl5uYiIDA8Pi8lkEq1WK0FBQRIVFSVFRUUz+lCZLZ7kW1xc7IqNjIyU7OxsefHihdvx/L2+Ip4/p9+8eSMA5P79+xOO5e81nuo9x5ljYWGhZGZmTtgnOTlZFEURvV7vtqaUk9pj6IkAER/P4yYiIiLyYzxniYiIiEgFmyUiIiIiFWyWiIiIiFSwWSIiIiJSwWaJiIiISAWbJSIiIiIVbJaIiIiIVLBZIiIiIlLBZomIiIhIBZslIiIVHz9+xI4dOxAbG4s5c+aguLjY10MiIi9js0REpGJkZARarRZHjhxBUlKSr4dDRD7AZomI/mr9/f3Q6XQ4efKka9uTJ0+gKAqam5uxYsUKnD17FgUFBVi4cKEPR0pEvjLX1wMgIvIlrVaLq1evYtu2bTCZTIiLi0N+fj4OHDgAo9Ho6+ERkR9gs0REf73s7GwUFRVh586d2LBhA0JCQnDq1ClfD4uI/AS/hiMiAlBVVYXR0VHU1tbi+vXr0Gg0vh4SEfkJNktERAC6u7vx4cMHOBwO9PT0+Ho4RORH+DUcEf31fv78iby8POTk5CAuLg579+6F1WrFkiVLfD00IvIDbJaI6K93+PBhDAwM4Ny5cwgNDcWdO3ewZ88e3Lp1CwBgsVgAAN++fUN/fz8sFgsURUF8fLwPR01E3hIgIuLrQRAR+Uprays2bdqElpYWpKenAwB6enqQlJQEs9mM/fv3IyAgYMJ+UVFR/LqO6C/BZomIiIhIBU/wJiIiIlLBZomIiIhIBZslIiIiIhVsloiIiIhUsFkiIiIiUsFmiYiIiEgFmyUiIiIiFWyWiIiIiFSwWSIiIiJSwWaJiIiISAWbJSIiIiIV/wLH9fo3W+53yAAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "N = 100 # number of points per class \n",
    "D = 2 # dimensionality\n",
    "K = 3 # number of classes\n",
    "\n",
    "X = np.zeros((N*K,D))\n",
    "y = np.zeros(N*K, dtype='uint8')\n",
    "for j in range(K):\n",
    "  ix = range(N*j,N*(j+1))\n",
    "  r = np.linspace(0.0,1,N) # radius\n",
    "  t = np.linspace(j*4,(j+1)*4,N) + np.random.randn(N)*0.2 # theta\n",
    "  X[ix] = np.c_[r*np.sin(t), r*np.cos(t)]\n",
    "  y[ix] = j\n",
    "\n",
    "## visualize data\n",
    "data = {\n",
    "    'x1': X[:,0],\n",
    "    'x2': X[:,1],\n",
    "    'y': y\n",
    "}\n",
    "plt.rcParams['axes.facecolor'] = '#bea3c2'\n",
    "sns.scatterplot(x='x1', y='x2', hue='y', data=data)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><b>TRAINING A SOFTMAX LINEAR CLASSIFIER</b></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize weights and biases\n",
    "W = np.random.randn(K,D)*0.01\n",
    "b = np.zeros((K,1))\n",
    "\n",
    "# score calculation\n",
    "scores = np.dot(W, X.T) + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 300)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Applying softmax\n",
    "n_examples = X.shape[0]\n",
    "exp_scores = np.exp(scores)\n",
    "probs = exp_scores/np.sum(exp_scores, axis=0, keepdims=True)\n",
    "probs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyperparameters\n",
    "reg = 0.5\n",
    "alpha = 0.01\n",
    "step_size = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss calculation\n",
    "data_loss = -np.sum(np.log(probs[y, range(n_examples)]))/n_examples\n",
    "reg_loss = 0.5*reg*np.sum(W*W)\n",
    "loss = data_loss + reg_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><b>BACKPROPAGATION STEP (GRADIENT CALCULATION)</b></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "dscores = probs \n",
    "## writing these is the derivatives is the toughest part\n",
    "dscores[y, range(n_examples)]-=1\n",
    "db = np.sum(dscores, axis=1, keepdims=True)\n",
    "dW = np.dot(dscores, X) + reg*W    # reg*W comes from the regularization term\n",
    "\n",
    "W = W - step_size*dW\n",
    "b = b - step_size*db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><b>TRAINING A LINEAR SOFTMAX LOGISTIC REGRESSION CLASSIFIER FOR MULTICLASS CLASSIFICATION</b><center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration no 1: Loss: 1.0906241034875648, accuracy: 0.5166666666666667\n",
      "iteration no 2: Loss: 1.06075292161153, accuracy: 0.5166666666666667\n",
      "iteration no 3: Loss: 1.0341741746100637, accuracy: 0.52\n",
      "iteration no 4: Loss: 1.0104883356437766, accuracy: 0.5233333333333333\n",
      "iteration no 5: Loss: 0.989342280144734, accuracy: 0.52\n",
      "iteration no 6: Loss: 0.9704252715911619, accuracy: 0.5166666666666667\n",
      "iteration no 7: Loss: 0.9534651367983092, accuracy: 0.5166666666666667\n",
      "iteration no 8: Loss: 0.9382243793950965, accuracy: 0.5166666666666667\n",
      "iteration no 9: Loss: 0.9244963054774655, accuracy: 0.5166666666666667\n",
      "iteration no 10: Loss: 0.9121012995888186, accuracy: 0.5166666666666667\n",
      "iteration no 11: Loss: 0.9008833621014147, accuracy: 0.5166666666666667\n",
      "iteration no 12: Loss: 0.890706972303566, accuracy: 0.5166666666666667\n",
      "iteration no 13: Loss: 0.8814543008083056, accuracy: 0.5166666666666667\n",
      "iteration no 14: Loss: 0.8730227665386333, accuracy: 0.5166666666666667\n",
      "iteration no 15: Loss: 0.8653229165273787, accuracy: 0.5166666666666667\n",
      "iteration no 16: Loss: 0.858276598172384, accuracy: 0.5133333333333333\n",
      "iteration no 17: Loss: 0.8518153905559317, accuracy: 0.5133333333333333\n",
      "iteration no 18: Loss: 0.8458792617679387, accuracy: 0.5133333333333333\n",
      "iteration no 19: Loss: 0.840415421344367, accuracy: 0.5133333333333333\n",
      "iteration no 20: Loss: 0.8353773399683637, accuracy: 0.5133333333333333\n",
      "iteration no 21: Loss: 0.8307239118902549, accuracy: 0.5133333333333333\n",
      "iteration no 22: Loss: 0.8264187387677868, accuracy: 0.5133333333333333\n",
      "iteration no 23: Loss: 0.8224295166354372, accuracy: 0.5133333333333333\n",
      "iteration no 24: Loss: 0.8187275104045898, accuracy: 0.5166666666666667\n",
      "iteration no 25: Loss: 0.8152871026550818, accuracy: 0.5166666666666667\n",
      "iteration no 26: Loss: 0.8120854055143036, accuracy: 0.5166666666666667\n",
      "iteration no 27: Loss: 0.8091019261594317, accuracy: 0.5166666666666667\n",
      "iteration no 28: Loss: 0.8063182779544823, accuracy: 0.5166666666666667\n",
      "iteration no 29: Loss: 0.8037179304808351, accuracy: 0.5166666666666667\n",
      "iteration no 30: Loss: 0.8012859927700855, accuracy: 0.5166666666666667\n",
      "iteration no 31: Loss: 0.7990090249310245, accuracy: 0.52\n",
      "iteration no 32: Loss: 0.7968748741041182, accuracy: 0.5166666666666667\n",
      "iteration no 33: Loss: 0.7948725312995839, accuracy: 0.5166666666666667\n",
      "iteration no 34: Loss: 0.7929920061981591, accuracy: 0.5166666666666667\n",
      "iteration no 35: Loss: 0.7912242174332228, accuracy: 0.5166666666666667\n",
      "iteration no 36: Loss: 0.7895608962426869, accuracy: 0.5166666666666667\n",
      "iteration no 37: Loss: 0.7879945016904895, accuracy: 0.5166666666666667\n",
      "iteration no 38: Loss: 0.7865181459201559, accuracy: 0.5166666666666667\n",
      "iteration no 39: Loss: 0.7851255281247072, accuracy: 0.5166666666666667\n",
      "iteration no 40: Loss: 0.7838108761048532, accuracy: 0.5166666666666667\n",
      "iteration no 41: Loss: 0.7825688944464223, accuracy: 0.5133333333333333\n",
      "iteration no 42: Loss: 0.7813947184829667, accuracy: 0.5133333333333333\n",
      "iteration no 43: Loss: 0.7802838733242702, accuracy: 0.5133333333333333\n",
      "iteration no 44: Loss: 0.7792322373292949, accuracy: 0.5133333333333333\n",
      "iteration no 45: Loss: 0.7782360094855661, accuracy: 0.5133333333333333\n",
      "iteration no 46: Loss: 0.7772916802283845, accuracy: 0.5133333333333333\n",
      "iteration no 47: Loss: 0.7763960052944036, accuracy: 0.5133333333333333\n",
      "iteration no 48: Loss: 0.7755459822566022, accuracy: 0.5133333333333333\n",
      "iteration no 49: Loss: 0.7747388294328066, accuracy: 0.5133333333333333\n",
      "iteration no 50: Loss: 0.7739719668988073, accuracy: 0.5133333333333333\n",
      "iteration no 51: Loss: 0.7732429993706652, accuracy: 0.5133333333333333\n",
      "iteration no 52: Loss: 0.7725497007498148, accuracy: 0.5133333333333333\n",
      "iteration no 53: Loss: 0.7718900001496994, accuracy: 0.5133333333333333\n",
      "iteration no 54: Loss: 0.7712619692444763, accuracy: 0.5133333333333333\n",
      "iteration no 55: Loss: 0.7706638107992742, accuracy: 0.5133333333333333\n",
      "iteration no 56: Loss: 0.7700938482579909, accuracy: 0.5133333333333333\n",
      "iteration no 57: Loss: 0.7695505162789987, accuracy: 0.5133333333333333\n",
      "iteration no 58: Loss: 0.7690323521216966, accuracy: 0.5133333333333333\n",
      "iteration no 59: Loss: 0.7685379877978452, accuracy: 0.5133333333333333\n",
      "iteration no 60: Loss: 0.7680661429112543, accuracy: 0.5166666666666667\n",
      "iteration no 61: Loss: 0.7676156181178562, accuracy: 0.5166666666666667\n",
      "iteration no 62: Loss: 0.7671852891456273, accuracy: 0.5166666666666667\n",
      "iteration no 63: Loss: 0.7667741013203753, accuracy: 0.5166666666666667\n",
      "iteration no 64: Loss: 0.7663810645491704, accuracy: 0.5166666666666667\n",
      "iteration no 65: Loss: 0.7660052487183127, accuracy: 0.5166666666666667\n",
      "iteration no 66: Loss: 0.7656457794672142, accuracy: 0.5166666666666667\n",
      "iteration no 67: Loss: 0.7653018343035876, accuracy: 0.5166666666666667\n",
      "iteration no 68: Loss: 0.7649726390288537, accuracy: 0.5166666666666667\n",
      "iteration no 69: Loss: 0.7646574644458393, accuracy: 0.5166666666666667\n",
      "iteration no 70: Loss: 0.7643556233236167, accuracy: 0.5166666666666667\n",
      "iteration no 71: Loss: 0.7640664675968367, accuracy: 0.5166666666666667\n",
      "iteration no 72: Loss: 0.7637893857791176, accuracy: 0.5166666666666667\n",
      "iteration no 73: Loss: 0.7635238005720334, accuracy: 0.5166666666666667\n",
      "iteration no 74: Loss: 0.7632691666530173, accuracy: 0.5166666666666667\n",
      "iteration no 75: Loss: 0.763024968627074, accuracy: 0.5166666666666667\n",
      "iteration no 76: Loss: 0.7627907191286175, accuracy: 0.5166666666666667\n",
      "iteration no 77: Loss: 0.7625659570610217, accuracy: 0.5166666666666667\n",
      "iteration no 78: Loss: 0.7623502459626076, accuracy: 0.5133333333333333\n",
      "iteration no 79: Loss: 0.7621431724888296, accuracy: 0.5133333333333333\n",
      "iteration no 80: Loss: 0.761944345001333, accuracy: 0.5133333333333333\n",
      "iteration no 81: Loss: 0.7617533922553976, accuracy: 0.5133333333333333\n",
      "iteration no 82: Loss: 0.7615699621780284, accuracy: 0.5133333333333333\n",
      "iteration no 83: Loss: 0.7613937207296316, accuracy: 0.5133333333333333\n",
      "iteration no 84: Loss: 0.7612243508428291, accuracy: 0.5133333333333333\n",
      "iteration no 85: Loss: 0.761061551432515, accuracy: 0.5133333333333333\n",
      "iteration no 86: Loss: 0.760905036471762, accuracy: 0.5133333333333333\n",
      "iteration no 87: Loss: 0.7607545341286414, accuracy: 0.5133333333333333\n",
      "iteration no 88: Loss: 0.7606097859594244, accuracy: 0.5133333333333333\n",
      "iteration no 89: Loss: 0.7604705461540215, accuracy: 0.5133333333333333\n",
      "iteration no 90: Loss: 0.7603365808298413, accuracy: 0.5133333333333333\n",
      "iteration no 91: Loss: 0.7602076673705722, accuracy: 0.5133333333333333\n",
      "iteration no 92: Loss: 0.7600835938066656, accuracy: 0.5133333333333333\n",
      "iteration no 93: Loss: 0.7599641582345562, accuracy: 0.5133333333333333\n",
      "iteration no 94: Loss: 0.7598491682718964, accuracy: 0.5133333333333333\n",
      "iteration no 95: Loss: 0.7597384405462838, accuracy: 0.5133333333333333\n",
      "iteration no 96: Loss: 0.7596318002151684, accuracy: 0.5133333333333333\n",
      "iteration no 97: Loss: 0.7595290805147973, accuracy: 0.5133333333333333\n",
      "iteration no 98: Loss: 0.7594301223362186, accuracy: 0.5133333333333333\n",
      "iteration no 99: Loss: 0.7593347738265233, accuracy: 0.5133333333333333\n",
      "iteration no 100: Loss: 0.7592428900136281, accuracy: 0.5133333333333333\n",
      "iteration no 101: Loss: 0.7591543324530428, accuracy: 0.5133333333333333\n",
      "iteration no 102: Loss: 0.7590689688951691, accuracy: 0.5133333333333333\n",
      "iteration no 103: Loss: 0.7589866729717922, accuracy: 0.5166666666666667\n",
      "iteration no 104: Loss: 0.7589073239005201, accuracy: 0.5166666666666667\n",
      "iteration no 105: Loss: 0.7588308062060141, accuracy: 0.5166666666666667\n",
      "iteration no 106: Loss: 0.7587570094569435, accuracy: 0.5166666666666667\n",
      "iteration no 107: Loss: 0.7586858280176625, accuracy: 0.5166666666666667\n",
      "iteration no 108: Loss: 0.7586171608136908, accuracy: 0.5166666666666667\n",
      "iteration no 109: Loss: 0.75855091111013, accuracy: 0.5166666666666667\n",
      "iteration no 110: Loss: 0.7584869863022201, accuracy: 0.5166666666666667\n",
      "iteration no 111: Loss: 0.7584252977172885, accuracy: 0.5166666666666667\n",
      "iteration no 112: Loss: 0.7583657604273947, accuracy: 0.5166666666666667\n",
      "iteration no 113: Loss: 0.7583082930720257, accuracy: 0.5166666666666667\n",
      "iteration no 114: Loss: 0.7582528176902368, accuracy: 0.5166666666666667\n",
      "iteration no 115: Loss: 0.7581992595616721, accuracy: 0.5166666666666667\n",
      "iteration no 116: Loss: 0.7581475470559433, accuracy: 0.5166666666666667\n",
      "iteration no 117: Loss: 0.7580976114898688, accuracy: 0.5166666666666667\n",
      "iteration no 118: Loss: 0.7580493869921174, accuracy: 0.5166666666666667\n",
      "iteration no 119: Loss: 0.7580028103748274, accuracy: 0.5166666666666667\n",
      "iteration no 120: Loss: 0.7579578210117961, accuracy: 0.5166666666666667\n",
      "iteration no 121: Loss: 0.7579143607228662, accuracy: 0.5166666666666667\n",
      "iteration no 122: Loss: 0.7578723736641542, accuracy: 0.5166666666666667\n",
      "iteration no 123: Loss: 0.7578318062237941, accuracy: 0.5166666666666667\n",
      "iteration no 124: Loss: 0.7577926069228834, accuracy: 0.5166666666666667\n",
      "iteration no 125: Loss: 0.7577547263213433, accuracy: 0.5166666666666667\n",
      "iteration no 126: Loss: 0.7577181169284214, accuracy: 0.5166666666666667\n",
      "iteration no 127: Loss: 0.7576827331175784, accuracy: 0.5166666666666667\n",
      "iteration no 128: Loss: 0.7576485310455241, accuracy: 0.5166666666666667\n",
      "iteration no 129: Loss: 0.7576154685751716, accuracy: 0.5166666666666667\n",
      "iteration no 130: Loss: 0.7575835052023033, accuracy: 0.5166666666666667\n",
      "iteration no 131: Loss: 0.7575526019857444, accuracy: 0.5133333333333333\n",
      "iteration no 132: Loss: 0.7575227214808625, accuracy: 0.5133333333333333\n",
      "iteration no 133: Loss: 0.7574938276762117, accuracy: 0.5133333333333333\n",
      "iteration no 134: Loss: 0.7574658859331584, accuracy: 0.5133333333333333\n",
      "iteration no 135: Loss: 0.757438862928334, accuracy: 0.5133333333333333\n",
      "iteration no 136: Loss: 0.7574127265987621, accuracy: 0.5133333333333333\n",
      "iteration no 137: Loss: 0.7573874460895288, accuracy: 0.5133333333333333\n",
      "iteration no 138: Loss: 0.7573629917038583, accuracy: 0.5133333333333333\n",
      "iteration no 139: Loss: 0.7573393348554771, accuracy: 0.5133333333333333\n",
      "iteration no 140: Loss: 0.7573164480231457, accuracy: 0.5133333333333333\n",
      "iteration no 141: Loss: 0.7572943047072502, accuracy: 0.5133333333333333\n",
      "iteration no 142: Loss: 0.7572728793883505, accuracy: 0.5133333333333333\n",
      "iteration no 143: Loss: 0.7572521474875873, accuracy: 0.5133333333333333\n",
      "iteration no 144: Loss: 0.7572320853288527, accuracy: 0.5133333333333333\n",
      "iteration no 145: Loss: 0.7572126701026426, accuracy: 0.5133333333333333\n",
      "iteration no 146: Loss: 0.7571938798315039, accuracy: 0.5133333333333333\n",
      "iteration no 147: Loss: 0.7571756933369986, accuracy: 0.5133333333333333\n",
      "iteration no 148: Loss: 0.757158090208114, accuracy: 0.5133333333333333\n",
      "iteration no 149: Loss: 0.7571410507710459, accuracy: 0.5133333333333333\n",
      "iteration no 150: Loss: 0.7571245560602918, accuracy: 0.5133333333333333\n",
      "iteration no 151: Loss: 0.7571085877909876, accuracy: 0.5133333333333333\n",
      "iteration no 152: Loss: 0.7570931283324319, accuracy: 0.5133333333333333\n",
      "iteration no 153: Loss: 0.7570781606827408, accuracy: 0.5133333333333333\n",
      "iteration no 154: Loss: 0.7570636684445784, accuracy: 0.5133333333333333\n",
      "iteration no 155: Loss: 0.757049635801915, accuracy: 0.5133333333333333\n",
      "iteration no 156: Loss: 0.7570360474977624, accuracy: 0.5133333333333333\n",
      "iteration no 157: Loss: 0.7570228888128446, accuracy: 0.5133333333333333\n",
      "iteration no 158: Loss: 0.7570101455451551, accuracy: 0.5133333333333333\n",
      "iteration no 159: Loss: 0.7569978039903674, accuracy: 0.5133333333333333\n",
      "iteration no 160: Loss: 0.756985850923052, accuracy: 0.5133333333333333\n",
      "iteration no 161: Loss: 0.7569742735786701, accuracy: 0.5133333333333333\n",
      "iteration no 162: Loss: 0.7569630596363028, accuracy: 0.5133333333333333\n",
      "iteration no 163: Loss: 0.7569521972020883, accuracy: 0.5133333333333333\n",
      "iteration no 164: Loss: 0.7569416747933301, accuracy: 0.5133333333333333\n",
      "iteration no 165: Loss: 0.7569314813232499, accuracy: 0.5133333333333333\n",
      "iteration no 166: Loss: 0.756921606086356, accuracy: 0.5133333333333333\n",
      "iteration no 167: Loss: 0.7569120387443973, accuracy: 0.5133333333333333\n",
      "iteration no 168: Loss: 0.7569027693128814, accuracy: 0.5133333333333333\n",
      "iteration no 169: Loss: 0.7568937881481282, accuracy: 0.5133333333333333\n",
      "iteration no 170: Loss: 0.7568850859348378, accuracy: 0.5133333333333333\n",
      "iteration no 171: Loss: 0.7568766536741507, accuracy: 0.5133333333333333\n",
      "iteration no 172: Loss: 0.7568684826721758, accuracy: 0.5133333333333333\n",
      "iteration no 173: Loss: 0.7568605645289712, accuracy: 0.5133333333333333\n",
      "iteration no 174: Loss: 0.7568528911279542, accuracy: 0.5133333333333333\n",
      "iteration no 175: Loss: 0.7568454546257226, accuracy: 0.5133333333333333\n",
      "iteration no 176: Loss: 0.7568382474422729, accuracy: 0.5133333333333333\n",
      "iteration no 177: Loss: 0.7568312622515948, accuracy: 0.5133333333333333\n",
      "iteration no 178: Loss: 0.7568244919726292, accuracy: 0.5133333333333333\n",
      "iteration no 179: Loss: 0.7568179297605719, accuracy: 0.5133333333333333\n",
      "iteration no 180: Loss: 0.7568115689985097, accuracy: 0.5133333333333333\n",
      "iteration no 181: Loss: 0.7568054032893767, accuracy: 0.5133333333333333\n",
      "iteration no 182: Loss: 0.7567994264482149, accuracy: 0.5133333333333333\n",
      "iteration no 183: Loss: 0.7567936324947262, accuracy: 0.5133333333333333\n",
      "iteration no 184: Loss: 0.7567880156461082, accuracy: 0.5133333333333333\n",
      "iteration no 185: Loss: 0.7567825703101566, accuracy: 0.5133333333333333\n",
      "iteration no 186: Loss: 0.7567772910786277, accuracy: 0.5133333333333333\n",
      "iteration no 187: Loss: 0.7567721727208467, accuracy: 0.5133333333333333\n",
      "iteration no 188: Loss: 0.7567672101775565, accuracy: 0.5133333333333333\n",
      "iteration no 189: Loss: 0.7567623985549918, accuracy: 0.5133333333333333\n",
      "iteration no 190: Loss: 0.7567577331191742, accuracy: 0.5133333333333333\n",
      "iteration no 191: Loss: 0.7567532092904162, accuracy: 0.5133333333333333\n",
      "iteration no 192: Loss: 0.7567488226380292, accuracy: 0.5133333333333333\n",
      "iteration no 193: Loss: 0.756744568875222, accuracy: 0.5133333333333333\n",
      "iteration no 194: Loss: 0.7567404438541886, accuracy: 0.5133333333333333\n",
      "iteration no 195: Loss: 0.7567364435613735, accuracy: 0.5133333333333333\n",
      "iteration no 196: Loss: 0.7567325641129083, accuracy: 0.5133333333333333\n",
      "iteration no 197: Loss: 0.7567288017502151, accuracy: 0.5133333333333333\n",
      "iteration no 198: Loss: 0.7567251528357662, accuracy: 0.5133333333333333\n",
      "iteration no 199: Loss: 0.756721613848999, accuracy: 0.5133333333333333\n",
      "iteration no 200: Loss: 0.7567181813823749, accuracy: 0.5166666666666667\n",
      "iteration no 1: Loss: 1.0846443225442366, accuracy: 0.49666666666666665\n",
      "iteration no 2: Loss: 1.0556148174250704, accuracy: 0.49\n",
      "iteration no 3: Loss: 1.0297599272295608, accuracy: 0.5033333333333333\n",
      "iteration no 4: Loss: 1.006695365151529, accuracy: 0.5066666666666667\n",
      "iteration no 5: Loss: 0.9860811423332747, accuracy: 0.5066666666666667\n",
      "iteration no 6: Loss: 0.967618416121604, accuracy: 0.5066666666666667\n",
      "iteration no 7: Loss: 0.951045806108126, accuracy: 0.5066666666666667\n",
      "iteration no 8: Loss: 0.9361354720241776, accuracy: 0.5066666666666667\n",
      "iteration no 9: Loss: 0.9226892156128176, accuracy: 0.51\n",
      "iteration no 10: Loss: 0.9105347875035461, accuracy: 0.5133333333333333\n",
      "iteration no 11: Loss: 0.899522502520199, accuracy: 0.5133333333333333\n",
      "iteration no 12: Loss: 0.8895222084176962, accuracy: 0.5133333333333333\n",
      "iteration no 13: Loss: 0.8804206144123476, accuracy: 0.5133333333333333\n",
      "iteration no 14: Loss: 0.8721189629000868, accuracy: 0.5133333333333333\n",
      "iteration no 15: Loss: 0.8645310157281736, accuracy: 0.5133333333333333\n",
      "iteration no 16: Loss: 0.8575813214136532, accuracy: 0.5133333333333333\n",
      "iteration no 17: Loss: 0.8512037289829281, accuracy: 0.5133333333333333\n",
      "iteration no 18: Loss: 0.8453401157643651, accuracy: 0.5133333333333333\n",
      "iteration no 19: Loss: 0.8399392993076684, accuracy: 0.5133333333333333\n",
      "iteration no 20: Loss: 0.8349561069060488, accuracy: 0.5133333333333333\n",
      "iteration no 21: Loss: 0.8303505795418809, accuracy: 0.5133333333333333\n",
      "iteration no 22: Loss: 0.8260872902378843, accuracy: 0.5133333333333333\n",
      "iteration no 23: Loss: 0.8221347596656514, accuracy: 0.5133333333333333\n",
      "iteration no 24: Loss: 0.8184649544026039, accuracy: 0.5133333333333333\n",
      "iteration no 25: Loss: 0.815052855437399, accuracy: 0.5133333333333333\n",
      "iteration no 26: Loss: 0.8118760864232208, accuracy: 0.5133333333333333\n",
      "iteration no 27: Loss: 0.8089145927985976, accuracy: 0.5133333333333333\n",
      "iteration no 28: Loss: 0.806150364269865, accuracy: 0.5133333333333333\n",
      "iteration no 29: Loss: 0.8035671943109828, accuracy: 0.5133333333333333\n",
      "iteration no 30: Loss: 0.8011504713157893, accuracy: 0.5133333333333333\n",
      "iteration no 31: Loss: 0.7988869968622315, accuracy: 0.5133333333333333\n",
      "iteration no 32: Loss: 0.7967648272417334, accuracy: 0.5133333333333333\n",
      "iteration no 33: Loss: 0.7947731349902869, accuracy: 0.51\n",
      "iteration no 34: Loss: 0.7929020876487534, accuracy: 0.51\n",
      "iteration no 35: Loss: 0.791142741393188, accuracy: 0.5133333333333333\n",
      "iteration no 36: Loss: 0.789486947524331, accuracy: 0.5133333333333333\n",
      "iteration no 37: Loss: 0.7879272700992938, accuracy: 0.5133333333333333\n",
      "iteration no 38: Loss: 0.7864569132367591, accuracy: 0.5133333333333333\n",
      "iteration no 39: Loss: 0.7850696568370806, accuracy: 0.5133333333333333\n",
      "iteration no 40: Loss: 0.783759799636662, accuracy: 0.5133333333333333\n",
      "iteration no 41: Loss: 0.7825221086670879, accuracy: 0.5133333333333333\n",
      "iteration no 42: Loss: 0.7813517743179154, accuracy: 0.5133333333333333\n",
      "iteration no 43: Loss: 0.7802443703114389, accuracy: 0.5133333333333333\n",
      "iteration no 44: Loss: 0.7791958179910742, accuracy: 0.5133333333333333\n",
      "iteration no 45: Loss: 0.7782023544047807, accuracy: 0.5133333333333333\n",
      "iteration no 46: Loss: 0.7772605037332524, accuracy: 0.5133333333333333\n",
      "iteration no 47: Loss: 0.7763670516711978, accuracy: 0.5133333333333333\n",
      "iteration no 48: Loss: 0.7755190224203897, accuracy: 0.5133333333333333\n",
      "iteration no 49: Loss: 0.7747136579965078, accuracy: 0.5133333333333333\n",
      "iteration no 50: Loss: 0.7739483995891937, accuracy: 0.5133333333333333\n",
      "iteration no 51: Loss: 0.7732208707470303, accuracy: 0.5133333333333333\n",
      "iteration no 52: Loss: 0.7725288621871219, accuracy: 0.5133333333333333\n",
      "iteration no 53: Loss: 0.7718703180531853, accuracy: 0.5133333333333333\n",
      "iteration no 54: Loss: 0.7712433234671191, accuracy: 0.5133333333333333\n",
      "iteration no 55: Loss: 0.7706460932373259, accuracy: 0.5133333333333333\n",
      "iteration no 56: Loss: 0.7700769616030312, accuracy: 0.5133333333333333\n",
      "iteration no 57: Loss: 0.7695343729077615, accuracy: 0.5133333333333333\n",
      "iteration no 58: Loss: 0.7690168731073346, accuracy: 0.5166666666666667\n",
      "iteration no 59: Loss: 0.7685231020283758, accuracy: 0.5166666666666667\n",
      "iteration no 60: Loss: 0.7680517863027236, accuracy: 0.5166666666666667\n",
      "iteration no 61: Loss: 0.7676017329113136, accuracy: 0.5166666666666667\n",
      "iteration no 62: Loss: 0.7671718232783514, accuracy: 0.5166666666666667\n",
      "iteration no 63: Loss: 0.7667610078629566, accuracy: 0.5166666666666667\n",
      "iteration no 64: Loss: 0.7663683012010811, accuracy: 0.5166666666666667\n",
      "iteration no 65: Loss: 0.7659927773554654, accuracy: 0.5166666666666667\n",
      "iteration no 66: Loss: 0.7656335657357991, accuracy: 0.5166666666666667\n",
      "iteration no 67: Loss: 0.7652898472551355, accuracy: 0.5166666666666667\n",
      "iteration no 68: Loss: 0.7649608507920732, accuracy: 0.5166666666666667\n",
      "iteration no 69: Loss: 0.7646458499312835, accuracy: 0.5166666666666667\n",
      "iteration no 70: Loss: 0.7643441599576909, accuracy: 0.5166666666666667\n",
      "iteration no 71: Loss: 0.7640551350820544, accuracy: 0.5166666666666667\n",
      "iteration no 72: Loss: 0.7637781658778581, accuracy: 0.5166666666666667\n",
      "iteration no 73: Loss: 0.7635126769113652, accuracy: 0.5166666666666667\n",
      "iteration no 74: Loss: 0.7632581245484155, accuracy: 0.5166666666666667\n",
      "iteration no 75: Loss: 0.7630139949231042, accuracy: 0.5166666666666667\n",
      "iteration no 76: Loss: 0.7627798020548648, accuracy: 0.5166666666666667\n",
      "iteration no 77: Loss: 0.7625550861017296, accuracy: 0.5166666666666667\n",
      "iteration no 78: Loss: 0.7623394117386585, accuracy: 0.5133333333333333\n",
      "iteration no 79: Loss: 0.7621323666508403, accuracy: 0.5133333333333333\n",
      "iteration no 80: Loss: 0.7619335601327719, accuracy: 0.5133333333333333\n",
      "iteration no 81: Loss: 0.7617426217847421, accuracy: 0.5133333333333333\n",
      "iteration no 82: Loss: 0.761559200299083, accuracy: 0.5133333333333333\n",
      "iteration no 83: Loss: 0.761382962329219, accuracy: 0.5133333333333333\n",
      "iteration no 84: Loss: 0.7612135914351476, accuracy: 0.5133333333333333\n",
      "iteration no 85: Loss: 0.7610507870995236, accuracy: 0.5133333333333333\n",
      "iteration no 86: Loss: 0.7608942638090257, accuracy: 0.5133333333333333\n",
      "iteration no 87: Loss: 0.760743750196118, accuracy: 0.5133333333333333\n",
      "iteration no 88: Loss: 0.7605989882367328, accuracy: 0.5133333333333333\n",
      "iteration no 89: Loss: 0.7604597324997714, accuracy: 0.5133333333333333\n",
      "iteration no 90: Loss: 0.7603257494446499, accuracy: 0.5133333333333333\n",
      "iteration no 91: Loss: 0.7601968167634271, accuracy: 0.5133333333333333\n",
      "iteration no 92: Loss: 0.7600727227643256, accuracy: 0.5133333333333333\n",
      "iteration no 93: Loss: 0.7599532657937146, accuracy: 0.5133333333333333\n",
      "iteration no 94: Loss: 0.7598382536938547, accuracy: 0.5133333333333333\n",
      "iteration no 95: Loss: 0.7597275032939087, accuracy: 0.5133333333333333\n",
      "iteration no 96: Loss: 0.7596208399319263, accuracy: 0.5133333333333333\n",
      "iteration no 97: Loss: 0.75951809700568, accuracy: 0.5133333333333333\n",
      "iteration no 98: Loss: 0.7594191155503924, accuracy: 0.5133333333333333\n",
      "iteration no 99: Loss: 0.759323743841545, accuracy: 0.5133333333333333\n",
      "iteration no 100: Loss: 0.7592318370210915, accuracy: 0.5133333333333333\n",
      "iteration no 101: Loss: 0.7591432567455292, accuracy: 0.5133333333333333\n",
      "iteration no 102: Loss: 0.7590578708543847, accuracy: 0.5166666666666667\n",
      "iteration no 103: Loss: 0.7589755530577897, accuracy: 0.5166666666666667\n",
      "iteration no 104: Loss: 0.7588961826419092, accuracy: 0.5166666666666667\n",
      "iteration no 105: Loss: 0.7588196441910738, accuracy: 0.5166666666666667\n",
      "iteration no 106: Loss: 0.7587458273255574, accuracy: 0.5166666666666667\n",
      "iteration no 107: Loss: 0.758674626454007, accuracy: 0.5166666666666667\n",
      "iteration no 108: Loss: 0.7586059405396073, accuracy: 0.5166666666666667\n",
      "iteration no 109: Loss: 0.758539672879125, accuracy: 0.5166666666666667\n",
      "iteration no 110: Loss: 0.7584757308940373, accuracy: 0.5166666666666667\n",
      "iteration no 111: Loss: 0.7584140259330023, accuracy: 0.5166666666666667\n",
      "iteration no 112: Loss: 0.7583544730849813, accuracy: 0.5166666666666667\n",
      "iteration no 113: Loss: 0.7582969910023704, accuracy: 0.5166666666666667\n",
      "iteration no 114: Loss: 0.7582415017335367, accuracy: 0.5166666666666667\n",
      "iteration no 115: Loss: 0.7581879305642057, accuracy: 0.5166666666666667\n",
      "iteration no 116: Loss: 0.7581362058671679, accuracy: 0.5166666666666667\n",
      "iteration no 117: Loss: 0.7580862589598226, accuracy: 0.5166666666666667\n",
      "iteration no 118: Loss: 0.7580380239690984, accuracy: 0.5166666666666667\n",
      "iteration no 119: Loss: 0.7579914377033224, accuracy: 0.5166666666666667\n",
      "iteration no 120: Loss: 0.7579464395306411, accuracy: 0.5166666666666667\n",
      "iteration no 121: Loss: 0.7579029712636158, accuracy: 0.5166666666666667\n",
      "iteration no 122: Loss: 0.7578609770496432, accuracy: 0.5166666666666667\n",
      "iteration no 123: Loss: 0.7578204032668714, accuracy: 0.5166666666666667\n",
      "iteration no 124: Loss: 0.7577811984253062, accuracy: 0.5166666666666667\n",
      "iteration no 125: Loss: 0.757743313072815, accuracy: 0.5166666666666667\n",
      "iteration no 126: Loss: 0.7577066997057605, accuracy: 0.5166666666666667\n",
      "iteration no 127: Loss: 0.7576713126840081, accuracy: 0.5166666666666667\n",
      "iteration no 128: Loss: 0.7576371081500686, accuracy: 0.5166666666666667\n",
      "iteration no 129: Loss: 0.7576040439521522, accuracy: 0.5166666666666667\n",
      "iteration no 130: Loss: 0.7575720795709229, accuracy: 0.5166666666666667\n",
      "iteration no 131: Loss: 0.7575411760497536, accuracy: 0.5166666666666667\n",
      "iteration no 132: Loss: 0.7575112959282979, accuracy: 0.5166666666666667\n",
      "iteration no 133: Loss: 0.757482403179201, accuracy: 0.5166666666666667\n",
      "iteration no 134: Loss: 0.7574544631477852, accuracy: 0.5133333333333333\n",
      "iteration no 135: Loss: 0.7574274424945553, accuracy: 0.5133333333333333\n",
      "iteration no 136: Loss: 0.7574013091403757, accuracy: 0.5133333333333333\n",
      "iteration no 137: Loss: 0.7573760322141815, accuracy: 0.5133333333333333\n",
      "iteration no 138: Loss: 0.7573515820030959, accuracy: 0.5133333333333333\n",
      "iteration no 139: Loss: 0.7573279299048268, accuracy: 0.5133333333333333\n",
      "iteration no 140: Loss: 0.7573050483822297, accuracy: 0.5133333333333333\n",
      "iteration no 141: Loss: 0.7572829109199273, accuracy: 0.5133333333333333\n",
      "iteration no 142: Loss: 0.7572614919828816, accuracy: 0.5133333333333333\n",
      "iteration no 143: Loss: 0.7572407669768213, accuracy: 0.5133333333333333\n",
      "iteration no 144: Loss: 0.7572207122104325, accuracy: 0.5133333333333333\n",
      "iteration no 145: Loss: 0.7572013048592265, accuracy: 0.5133333333333333\n",
      "iteration no 146: Loss: 0.7571825229310006, accuracy: 0.5133333333333333\n",
      "iteration no 147: Loss: 0.7571643452328145, accuracy: 0.5133333333333333\n",
      "iteration no 148: Loss: 0.757146751339411, accuracy: 0.5133333333333333\n",
      "iteration no 149: Loss: 0.7571297215630077, accuracy: 0.5133333333333333\n",
      "iteration no 150: Loss: 0.7571132369243964, accuracy: 0.5133333333333333\n",
      "iteration no 151: Loss: 0.7570972791252863, accuracy: 0.5133333333333333\n",
      "iteration no 152: Loss: 0.7570818305218321, accuracy: 0.5133333333333333\n",
      "iteration no 153: Loss: 0.7570668740992927, accuracy: 0.5133333333333333\n",
      "iteration no 154: Loss: 0.757052393447763, accuracy: 0.5133333333333333\n",
      "iteration no 155: Loss: 0.7570383727389339, accuracy: 0.5133333333333333\n",
      "iteration no 156: Loss: 0.7570247967038289, accuracy: 0.5133333333333333\n",
      "iteration no 157: Loss: 0.7570116506114725, accuracy: 0.5133333333333333\n",
      "iteration no 158: Loss: 0.7569989202484493, accuracy: 0.5133333333333333\n",
      "iteration no 159: Loss: 0.7569865918993103, accuracy: 0.5133333333333333\n",
      "iteration no 160: Loss: 0.7569746523277897, accuracy: 0.5133333333333333\n",
      "iteration no 161: Loss: 0.756963088758795, accuracy: 0.5133333333333333\n",
      "iteration no 162: Loss: 0.7569518888611347, accuracy: 0.5133333333333333\n",
      "iteration no 163: Loss: 0.75694104073095, accuracy: 0.5133333333333333\n",
      "iteration no 164: Loss: 0.7569305328758208, accuracy: 0.5133333333333333\n",
      "iteration no 165: Loss: 0.7569203541995159, accuracy: 0.5133333333333333\n",
      "iteration no 166: Loss: 0.7569104939873533, accuracy: 0.5133333333333333\n",
      "iteration no 167: Loss: 0.7569009418921545, accuracy: 0.5133333333333333\n",
      "iteration no 168: Loss: 0.756891687920755, accuracy: 0.5133333333333333\n",
      "iteration no 169: Loss: 0.7568827224210538, accuracy: 0.5133333333333333\n",
      "iteration no 170: Loss: 0.7568740360695776, accuracy: 0.5133333333333333\n",
      "iteration no 171: Loss: 0.756865619859534, accuracy: 0.5133333333333333\n",
      "iteration no 172: Loss: 0.7568574650893372, accuracy: 0.5133333333333333\n",
      "iteration no 173: Loss: 0.756849563351581, accuracy: 0.5133333333333333\n",
      "iteration no 174: Loss: 0.7568419065224454, accuracy: 0.5133333333333333\n",
      "iteration no 175: Loss: 0.7568344867515129, accuracy: 0.5133333333333333\n",
      "iteration no 176: Loss: 0.7568272964519799, accuracy: 0.5133333333333333\n",
      "iteration no 177: Loss: 0.7568203282912481, accuracy: 0.5133333333333333\n",
      "iteration no 178: Loss: 0.756813575181876, accuracy: 0.5133333333333333\n",
      "iteration no 179: Loss: 0.7568070302728775, accuracy: 0.5133333333333333\n",
      "iteration no 180: Loss: 0.7568006869413543, accuracy: 0.5133333333333333\n",
      "iteration no 181: Loss: 0.7567945387844458, accuracy: 0.5133333333333333\n",
      "iteration no 182: Loss: 0.7567885796115846, accuracy: 0.5133333333333333\n",
      "iteration no 183: Loss: 0.7567828034370451, accuracy: 0.5133333333333333\n",
      "iteration no 184: Loss: 0.7567772044727729, accuracy: 0.5133333333333333\n",
      "iteration no 185: Loss: 0.7567717771214829, accuracy: 0.5133333333333333\n",
      "iteration no 186: Loss: 0.7567665159700178, accuracy: 0.5133333333333333\n",
      "iteration no 187: Loss: 0.7567614157829508, accuracy: 0.5133333333333333\n",
      "iteration no 188: Loss: 0.7567564714964298, accuracy: 0.5133333333333333\n",
      "iteration no 189: Loss: 0.7567516782122479, accuracy: 0.5133333333333333\n",
      "iteration no 190: Loss: 0.7567470311921333, accuracy: 0.5133333333333333\n",
      "iteration no 191: Loss: 0.75674252585225, accuracy: 0.5133333333333333\n",
      "iteration no 192: Loss: 0.7567381577578995, accuracy: 0.5133333333333333\n",
      "iteration no 193: Loss: 0.7567339226184179, accuracy: 0.5133333333333333\n",
      "iteration no 194: Loss: 0.7567298162822577, accuracy: 0.5133333333333333\n",
      "iteration no 195: Loss: 0.7567258347322495, accuracy: 0.5133333333333333\n",
      "iteration no 196: Loss: 0.7567219740810357, accuracy: 0.5133333333333333\n",
      "iteration no 197: Loss: 0.7567182305666685, accuracy: 0.5133333333333333\n",
      "iteration no 198: Loss: 0.7567146005483677, accuracy: 0.5133333333333333\n",
      "iteration no 199: Loss: 0.7567110805024302, accuracy: 0.5133333333333333\n",
      "iteration no 200: Loss: 0.7567076670182871, accuracy: 0.5166666666666667\n",
      "iteration no 1: Loss: 1.0800376694668052, accuracy: 0.5166666666666667\n",
      "iteration no 2: Loss: 1.0513490439581967, accuracy: 0.53\n",
      "iteration no 3: Loss: 1.0258039809840531, accuracy: 0.5233333333333333\n",
      "iteration no 4: Loss: 1.0030189528366216, accuracy: 0.5233333333333333\n",
      "iteration no 5: Loss: 0.9826582366645931, accuracy: 0.5166666666666667\n",
      "iteration no 6: Loss: 0.9644273702797349, accuracy: 0.5133333333333333\n",
      "iteration no 7: Loss: 0.9480684885971049, accuracy: 0.5166666666666667\n",
      "iteration no 8: Loss: 0.9333563283358863, accuracy: 0.52\n",
      "iteration no 9: Loss: 0.9200945284501725, accuracy: 0.5233333333333333\n",
      "iteration no 10: Loss: 0.9081121654876017, accuracy: 0.5233333333333333\n",
      "iteration no 11: Loss: 0.8972605521812007, accuracy: 0.5233333333333333\n",
      "iteration no 12: Loss: 0.8874103334673425, accuracy: 0.5233333333333333\n",
      "iteration no 13: Loss: 0.8784488961291246, accuracy: 0.52\n",
      "iteration no 14: Loss: 0.8702780888971763, accuracy: 0.5166666666666667\n",
      "iteration no 15: Loss: 0.8628122357929083, accuracy: 0.5166666666666667\n",
      "iteration no 16: Loss: 0.8559764173757376, accuracy: 0.52\n",
      "iteration no 17: Loss: 0.8497049910695447, accuracy: 0.52\n",
      "iteration no 18: Loss: 0.8439403213584186, accuracy: 0.5166666666666667\n",
      "iteration no 19: Loss: 0.8386316921004244, accuracy: 0.5166666666666667\n",
      "iteration no 20: Loss: 0.8337343756231517, accuracy: 0.5166666666666667\n",
      "iteration no 21: Loss: 0.8292088360626625, accuracy: 0.5166666666666667\n",
      "iteration no 22: Loss: 0.8250200472437237, accuracy: 0.5166666666666667\n",
      "iteration no 23: Loss: 0.8211369080830111, accuracy: 0.5166666666666667\n",
      "iteration no 24: Loss: 0.8175317409350472, accuracy: 0.5166666666666667\n",
      "iteration no 25: Loss: 0.8141798604585896, accuracy: 0.5166666666666667\n",
      "iteration no 26: Loss: 0.8110592024584518, accuracy: 0.5166666666666667\n",
      "iteration no 27: Loss: 0.8081500037716968, accuracy: 0.5166666666666667\n",
      "iteration no 28: Loss: 0.8054345256435506, accuracy: 0.5166666666666667\n",
      "iteration no 29: Loss: 0.8028968142057319, accuracy: 0.5166666666666667\n",
      "iteration no 30: Loss: 0.8005224926563007, accuracy: 0.5166666666666667\n",
      "iteration no 31: Loss: 0.7982985805716564, accuracy: 0.5166666666666667\n",
      "iteration no 32: Loss: 0.796213336481363, accuracy: 0.5166666666666667\n",
      "iteration no 33: Loss: 0.7942561204254428, accuracy: 0.5166666666666667\n",
      "iteration no 34: Loss: 0.7924172737093068, accuracy: 0.5166666666666667\n",
      "iteration no 35: Loss: 0.7906880134885483, accuracy: 0.5166666666666667\n",
      "iteration no 36: Loss: 0.7890603401671243, accuracy: 0.5166666666666667\n",
      "iteration no 37: Loss: 0.7875269558886417, accuracy: 0.5166666666666667\n",
      "iteration no 38: Loss: 0.7860811926505311, accuracy: 0.5166666666666667\n",
      "iteration no 39: Loss: 0.7847169487822663, accuracy: 0.5166666666666667\n",
      "iteration no 40: Loss: 0.7834286327077731, accuracy: 0.5166666666666667\n",
      "iteration no 41: Loss: 0.7822111130639401, accuracy: 0.5166666666666667\n",
      "iteration no 42: Loss: 0.7810596743760733, accuracy: 0.5133333333333333\n",
      "iteration no 43: Loss: 0.7799699776008402, accuracy: 0.5133333333333333\n",
      "iteration no 44: Loss: 0.7789380249407706, accuracy: 0.5133333333333333\n",
      "iteration no 45: Loss: 0.7779601284142396, accuracy: 0.5133333333333333\n",
      "iteration no 46: Loss: 0.7770328817331872, accuracy: 0.5133333333333333\n",
      "iteration no 47: Loss: 0.7761531350993843, accuracy: 0.5133333333333333\n",
      "iteration no 48: Loss: 0.7753179725803411, accuracy: 0.5133333333333333\n",
      "iteration no 49: Loss: 0.7745246917692046, accuracy: 0.5133333333333333\n",
      "iteration no 50: Loss: 0.7737707854702699, accuracy: 0.5133333333333333\n",
      "iteration no 51: Loss: 0.7730539251839053, accuracy: 0.5133333333333333\n",
      "iteration no 52: Loss: 0.7723719461925279, accuracy: 0.5133333333333333\n",
      "iteration no 53: Loss: 0.7717228340733716, accuracy: 0.5133333333333333\n",
      "iteration no 54: Loss: 0.7711047124847221, accuracy: 0.5166666666666667\n",
      "iteration no 55: Loss: 0.7705158320904809, accuracy: 0.5166666666666667\n",
      "iteration no 56: Loss: 0.769954560503768, accuracy: 0.5166666666666667\n",
      "iteration no 57: Loss: 0.7694193731440888, accuracy: 0.5166666666666667\n",
      "iteration no 58: Loss: 0.7689088449146659, accuracy: 0.5166666666666667\n",
      "iteration no 59: Loss: 0.7684216426171092, accuracy: 0.5166666666666667\n",
      "iteration no 60: Loss: 0.7679565180298488, accuracy: 0.5166666666666667\n",
      "iteration no 61: Loss: 0.767512301584899, accuracy: 0.5166666666666667\n",
      "iteration no 62: Loss: 0.7670878965846628, accuracy: 0.5166666666666667\n",
      "iteration no 63: Loss: 0.7666822739067835, accuracy: 0.5166666666666667\n",
      "iteration no 64: Loss: 0.7662944671506049, accuracy: 0.5166666666666667\n",
      "iteration no 65: Loss: 0.7659235681837051, accuracy: 0.5166666666666667\n",
      "iteration no 66: Loss: 0.765568723051297, accuracy: 0.5166666666666667\n",
      "iteration no 67: Loss: 0.7652291282151433, accuracy: 0.5166666666666667\n",
      "iteration no 68: Loss: 0.7649040270920299, accuracy: 0.5166666666666667\n",
      "iteration no 69: Loss: 0.7645927068648685, accuracy: 0.5166666666666667\n",
      "iteration no 70: Loss: 0.7642944955421944, accuracy: 0.5166666666666667\n",
      "iteration no 71: Loss: 0.7640087592442149, accuracy: 0.5166666666666667\n",
      "iteration no 72: Loss: 0.7637348996957016, accuracy: 0.5166666666666667\n",
      "iteration no 73: Loss: 0.763472351907931, accuracy: 0.5166666666666667\n",
      "iteration no 74: Loss: 0.7632205820335712, accuracy: 0.5166666666666667\n",
      "iteration no 75: Loss: 0.7629790853799504, accuracy: 0.5166666666666667\n",
      "iteration no 76: Loss: 0.7627473845674981, accuracy: 0.5166666666666667\n",
      "iteration no 77: Loss: 0.7625250278213823, accuracy: 0.5166666666666667\n",
      "iteration no 78: Loss: 0.7623115873854628, accuracy: 0.5166666666666667\n",
      "iteration no 79: Loss: 0.7621066580486726, accuracy: 0.5166666666666667\n",
      "iteration no 80: Loss: 0.7619098557748275, accuracy: 0.5133333333333333\n",
      "iteration no 81: Loss: 0.7617208164276676, accuracy: 0.5133333333333333\n",
      "iteration no 82: Loss: 0.7615391945836583, accuracy: 0.5133333333333333\n",
      "iteration no 83: Loss: 0.7613646624257291, accuracy: 0.5133333333333333\n",
      "iteration no 84: Loss: 0.7611969087117241, accuracy: 0.5133333333333333\n",
      "iteration no 85: Loss: 0.761035637811866, accuracy: 0.5133333333333333\n",
      "iteration no 86: Loss: 0.7608805688100241, accuracy: 0.5133333333333333\n",
      "iteration no 87: Loss: 0.7607314346640125, accuracy: 0.5133333333333333\n",
      "iteration no 88: Loss: 0.7605879814205437, accuracy: 0.5133333333333333\n",
      "iteration no 89: Loss: 0.760449967480826, accuracy: 0.5133333333333333\n",
      "iteration no 90: Loss: 0.7603171629131139, accuracy: 0.5133333333333333\n",
      "iteration no 91: Loss: 0.7601893488088333, accuracy: 0.5133333333333333\n",
      "iteration no 92: Loss: 0.7600663166791607, accuracy: 0.5133333333333333\n",
      "iteration no 93: Loss: 0.7599478678891929, accuracy: 0.5133333333333333\n",
      "iteration no 94: Loss: 0.7598338131270681, accuracy: 0.5133333333333333\n",
      "iteration no 95: Loss: 0.7597239719056044, accuracy: 0.5133333333333333\n",
      "iteration no 96: Loss: 0.7596181720942098, accuracy: 0.5133333333333333\n",
      "iteration no 97: Loss: 0.7595162494789964, accuracy: 0.5133333333333333\n",
      "iteration no 98: Loss: 0.7594180473491792, accuracy: 0.5133333333333333\n",
      "iteration no 99: Loss: 0.7593234161079947, accuracy: 0.5133333333333333\n",
      "iteration no 100: Loss: 0.7592322129065018, accuracy: 0.5133333333333333\n",
      "iteration no 101: Loss: 0.7591443012987489, accuracy: 0.5166666666666667\n",
      "iteration no 102: Loss: 0.7590595509169072, accuracy: 0.5166666666666667\n",
      "iteration no 103: Loss: 0.758977837165066, accuracy: 0.5166666666666667\n",
      "iteration no 104: Loss: 0.7588990409304862, accuracy: 0.5166666666666667\n",
      "iteration no 105: Loss: 0.7588230483111944, accuracy: 0.5166666666666667\n",
      "iteration no 106: Loss: 0.7587497503588738, accuracy: 0.5166666666666667\n",
      "iteration no 107: Loss: 0.7586790428360907, accuracy: 0.5166666666666667\n",
      "iteration no 108: Loss: 0.7586108259869547, accuracy: 0.5166666666666667\n",
      "iteration no 109: Loss: 0.7585450043203834, accuracy: 0.5166666666666667\n",
      "iteration no 110: Loss: 0.7584814864051848, accuracy: 0.5166666666666667\n",
      "iteration no 111: Loss: 0.7584201846762443, accuracy: 0.5166666666666667\n",
      "iteration no 112: Loss: 0.7583610152511329, accuracy: 0.5166666666666667\n",
      "iteration no 113: Loss: 0.7583038977565117, accuracy: 0.5166666666666667\n",
      "iteration no 114: Loss: 0.7582487551637457, accuracy: 0.5166666666666667\n",
      "iteration no 115: Loss: 0.758195513633178, accuracy: 0.5166666666666667\n",
      "iteration no 116: Loss: 0.7581441023665534, accuracy: 0.5166666666666667\n",
      "iteration no 117: Loss: 0.7580944534671145, accuracy: 0.5166666666666667\n",
      "iteration no 118: Loss: 0.7580465018069213, accuracy: 0.5166666666666667\n",
      "iteration no 119: Loss: 0.7580001849009794, accuracy: 0.5166666666666667\n",
      "iteration no 120: Loss: 0.7579554427877845, accuracy: 0.5166666666666667\n",
      "iteration no 121: Loss: 0.7579122179159151, accuracy: 0.5166666666666667\n",
      "iteration no 122: Loss: 0.7578704550363352, accuracy: 0.5166666666666667\n",
      "iteration no 123: Loss: 0.7578301011000822, accuracy: 0.5166666666666667\n",
      "iteration no 124: Loss: 0.7577911051610395, accuracy: 0.5166666666666667\n",
      "iteration no 125: Loss: 0.7577534182835127, accuracy: 0.5166666666666667\n",
      "iteration no 126: Loss: 0.7577169934543436, accuracy: 0.5166666666666667\n",
      "iteration no 127: Loss: 0.7576817854993123, accuracy: 0.5166666666666667\n",
      "iteration no 128: Loss: 0.7576477510035949, accuracy: 0.5133333333333333\n",
      "iteration no 129: Loss: 0.7576148482360586, accuracy: 0.5133333333333333\n",
      "iteration no 130: Loss: 0.757583037077184, accuracy: 0.5133333333333333\n",
      "iteration no 131: Loss: 0.7575522789504248, accuracy: 0.5133333333333333\n",
      "iteration no 132: Loss: 0.7575225367568192, accuracy: 0.5133333333333333\n",
      "iteration no 133: Loss: 0.757493774812685, accuracy: 0.5133333333333333\n",
      "iteration no 134: Loss: 0.7574659587902297, accuracy: 0.5133333333333333\n",
      "iteration no 135: Loss: 0.7574390556609341, accuracy: 0.5133333333333333\n",
      "iteration no 136: Loss: 0.7574130336415541, accuracy: 0.5133333333333333\n",
      "iteration no 137: Loss: 0.757387862142615, accuracy: 0.5133333333333333\n",
      "iteration no 138: Loss: 0.7573635117192646, accuracy: 0.5133333333333333\n",
      "iteration no 139: Loss: 0.7573399540243686, accuracy: 0.5133333333333333\n",
      "iteration no 140: Loss: 0.7573171617637326, accuracy: 0.5133333333333333\n",
      "iteration no 141: Loss: 0.757295108653344, accuracy: 0.5133333333333333\n",
      "iteration no 142: Loss: 0.7572737693785341, accuracy: 0.5133333333333333\n",
      "iteration no 143: Loss: 0.7572531195549622, accuracy: 0.5133333333333333\n",
      "iteration no 144: Loss: 0.757233135691335, accuracy: 0.5133333333333333\n",
      "iteration no 145: Loss: 0.7572137951537712, accuracy: 0.5133333333333333\n",
      "iteration no 146: Loss: 0.7571950761317352, accuracy: 0.5133333333333333\n",
      "iteration no 147: Loss: 0.7571769576054624, accuracy: 0.5133333333333333\n",
      "iteration no 148: Loss: 0.7571594193147995, accuracy: 0.5133333333333333\n",
      "iteration no 149: Loss: 0.7571424417293995, accuracy: 0.5133333333333333\n",
      "iteration no 150: Loss: 0.7571260060201993, accuracy: 0.5133333333333333\n",
      "iteration no 151: Loss: 0.7571100940321233, accuracy: 0.5133333333333333\n",
      "iteration no 152: Loss: 0.7570946882579515, accuracy: 0.5133333333333333\n",
      "iteration no 153: Loss: 0.7570797718133011, accuracy: 0.5133333333333333\n",
      "iteration no 154: Loss: 0.7570653284126645, accuracy: 0.5133333333333333\n",
      "iteration no 155: Loss: 0.7570513423464593, accuracy: 0.5133333333333333\n",
      "iteration no 156: Loss: 0.7570377984590401, accuracy: 0.5133333333333333\n",
      "iteration no 157: Loss: 0.7570246821276272, accuracy: 0.5133333333333333\n",
      "iteration no 158: Loss: 0.7570119792421138, accuracy: 0.5133333333333333\n",
      "iteration no 159: Loss: 0.756999676185707, accuracy: 0.5133333333333333\n",
      "iteration no 160: Loss: 0.7569877598163668, accuracy: 0.5133333333333333\n",
      "iteration no 161: Loss: 0.7569762174490091, accuracy: 0.5133333333333333\n",
      "iteration no 162: Loss: 0.7569650368384315, accuracy: 0.5133333333333333\n",
      "iteration no 163: Loss: 0.7569542061629384, accuracy: 0.5133333333333333\n",
      "iteration no 164: Loss: 0.7569437140086268, accuracy: 0.5133333333333333\n",
      "iteration no 165: Loss: 0.7569335493543066, accuracy: 0.5133333333333333\n",
      "iteration no 166: Loss: 0.7569237015570278, accuracy: 0.5133333333333333\n",
      "iteration no 167: Loss: 0.7569141603381867, accuracy: 0.5133333333333333\n",
      "iteration no 168: Loss: 0.7569049157701851, accuracy: 0.5133333333333333\n",
      "iteration no 169: Loss: 0.7568959582636204, accuracy: 0.5133333333333333\n",
      "iteration no 170: Loss: 0.7568872785549812, accuracy: 0.5133333333333333\n",
      "iteration no 171: Loss: 0.7568788676948288, accuracy: 0.5133333333333333\n",
      "iteration no 172: Loss: 0.7568707170364425, accuracy: 0.5133333333333333\n",
      "iteration no 173: Loss: 0.7568628182249074, accuracy: 0.5133333333333333\n",
      "iteration no 174: Loss: 0.7568551631866292, accuracy: 0.5133333333333333\n",
      "iteration no 175: Loss: 0.7568477441192543, accuracy: 0.5133333333333333\n",
      "iteration no 176: Loss: 0.7568405534819805, accuracy: 0.5133333333333333\n",
      "iteration no 177: Loss: 0.7568335839862416, accuracy: 0.5133333333333333\n",
      "iteration no 178: Loss: 0.7568268285867475, accuracy: 0.5133333333333333\n",
      "iteration no 179: Loss: 0.7568202804728695, accuracy: 0.5133333333333333\n",
      "iteration no 180: Loss: 0.7568139330603515, accuracy: 0.5133333333333333\n",
      "iteration no 181: Loss: 0.7568077799833383, accuracy: 0.5133333333333333\n",
      "iteration no 182: Loss: 0.7568018150867043, accuracy: 0.5133333333333333\n",
      "iteration no 183: Loss: 0.7567960324186711, accuracy: 0.5133333333333333\n",
      "iteration no 184: Loss: 0.756790426223705, accuracy: 0.5133333333333333\n",
      "iteration no 185: Loss: 0.7567849909356772, accuracy: 0.5133333333333333\n",
      "iteration no 186: Loss: 0.7567797211712824, accuracy: 0.5133333333333333\n",
      "iteration no 187: Loss: 0.7567746117237013, accuracy: 0.5133333333333333\n",
      "iteration no 188: Loss: 0.7567696575564969, accuracy: 0.5133333333333333\n",
      "iteration no 189: Loss: 0.7567648537977395, accuracy: 0.5133333333333333\n",
      "iteration no 190: Loss: 0.7567601957343452, accuracy: 0.5133333333333333\n",
      "iteration no 191: Loss: 0.7567556788066242, accuracy: 0.5133333333333333\n",
      "iteration no 192: Loss: 0.7567512986030294, accuracy: 0.5133333333333333\n",
      "iteration no 193: Loss: 0.756747050855095, accuracy: 0.5133333333333333\n",
      "iteration no 194: Loss: 0.7567429314325601, accuracy: 0.5133333333333333\n",
      "iteration no 195: Loss: 0.7567389363386703, accuracy: 0.5133333333333333\n",
      "iteration no 196: Loss: 0.7567350617056486, accuracy: 0.5133333333333333\n",
      "iteration no 197: Loss: 0.756731303790329, accuracy: 0.5133333333333333\n",
      "iteration no 198: Loss: 0.7567276589699495, accuracy: 0.5133333333333333\n",
      "iteration no 199: Loss: 0.7567241237380932, accuracy: 0.5133333333333333\n",
      "iteration no 200: Loss: 0.7567206947007767, accuracy: 0.5133333333333333\n",
      "iteration no 1: Loss: 1.1082344265528707, accuracy: 0.5\n",
      "iteration no 2: Loss: 1.076231037452579, accuracy: 0.5166666666666667\n",
      "iteration no 3: Loss: 1.0477824316039968, accuracy: 0.5466666666666666\n",
      "iteration no 4: Loss: 1.0224578453960094, accuracy: 0.5366666666666666\n",
      "iteration no 5: Loss: 0.9998786023684763, accuracy: 0.5366666666666666\n",
      "iteration no 6: Loss: 0.9797107996715949, accuracy: 0.5266666666666666\n",
      "iteration no 7: Loss: 0.9616606838680186, accuracy: 0.53\n",
      "iteration no 8: Loss: 0.9454707898930396, accuracy: 0.52\n",
      "iteration no 9: Loss: 0.9309162470678487, accuracy: 0.5166666666666667\n",
      "iteration no 10: Loss: 0.9178011667906812, accuracy: 0.52\n",
      "iteration no 11: Loss: 0.9059551810804473, accuracy: 0.5233333333333333\n",
      "iteration no 12: Loss: 0.8952302168315074, accuracy: 0.5233333333333333\n",
      "iteration no 13: Loss: 0.8854975631011145, accuracy: 0.5233333333333333\n",
      "iteration no 14: Loss: 0.8766452560080598, accuracy: 0.5233333333333333\n",
      "iteration no 15: Loss: 0.868575780228624, accuracy: 0.52\n",
      "iteration no 16: Loss: 0.8612040696029949, accuracy: 0.52\n",
      "iteration no 17: Loss: 0.8544557804579934, accuracy: 0.52\n",
      "iteration no 18: Loss: 0.8482658076559502, accuracy: 0.52\n",
      "iteration no 19: Loss: 0.842577013132795, accuracy: 0.52\n",
      "iteration no 20: Loss: 0.8373391383520745, accuracy: 0.52\n",
      "iteration no 21: Loss: 0.8325078747154427, accuracy: 0.52\n",
      "iteration no 22: Loss: 0.8280440689359454, accuracy: 0.52\n",
      "iteration no 23: Loss: 0.8239130433490606, accuracy: 0.52\n",
      "iteration no 24: Loss: 0.820084013920483, accuracy: 0.52\n",
      "iteration no 25: Loss: 0.8165295912214287, accuracy: 0.52\n",
      "iteration no 26: Loss: 0.8132253518532633, accuracy: 0.52\n",
      "iteration no 27: Loss: 0.8101494697179722, accuracy: 0.52\n",
      "iteration no 28: Loss: 0.8072823981709109, accuracy: 0.52\n",
      "iteration no 29: Loss: 0.8046065954862797, accuracy: 0.52\n",
      "iteration no 30: Loss: 0.8021062872447684, accuracy: 0.52\n",
      "iteration no 31: Loss: 0.7997672602466688, accuracy: 0.52\n",
      "iteration no 32: Loss: 0.7975766833898034, accuracy: 0.52\n",
      "iteration no 33: Loss: 0.7955229516541945, accuracy: 0.5166666666666667\n",
      "iteration no 34: Loss: 0.7935955499255353, accuracy: 0.5166666666666667\n",
      "iteration no 35: Loss: 0.7917849338853442, accuracy: 0.5166666666666667\n",
      "iteration no 36: Loss: 0.7900824256124751, accuracy: 0.5166666666666667\n",
      "iteration no 37: Loss: 0.7884801218913285, accuracy: 0.5166666666666667\n",
      "iteration no 38: Loss: 0.7869708135175139, accuracy: 0.5166666666666667\n",
      "iteration no 39: Loss: 0.7855479141408661, accuracy: 0.5166666666666667\n",
      "iteration no 40: Loss: 0.7842053973961965, accuracy: 0.5166666666666667\n",
      "iteration no 41: Loss: 0.7829377412502314, accuracy: 0.5166666666666667\n",
      "iteration no 42: Loss: 0.7817398786441192, accuracy: 0.5166666666666667\n",
      "iteration no 43: Loss: 0.7806071536390156, accuracy: 0.5166666666666667\n",
      "iteration no 44: Loss: 0.7795352823812353, accuracy: 0.5133333333333333\n",
      "iteration no 45: Loss: 0.778520318296319, accuracy: 0.5133333333333333\n",
      "iteration no 46: Loss: 0.7775586210006263, accuracy: 0.5133333333333333\n",
      "iteration no 47: Loss: 0.7766468284868651, accuracy: 0.5133333333333333\n",
      "iteration no 48: Loss: 0.7757818321980415, accuracy: 0.5133333333333333\n",
      "iteration no 49: Loss: 0.774960754654188, accuracy: 0.5133333333333333\n",
      "iteration no 50: Loss: 0.7741809293390991, accuracy: 0.5133333333333333\n",
      "iteration no 51: Loss: 0.7734398825912486, accuracy: 0.5133333333333333\n",
      "iteration no 52: Loss: 0.7727353172749583, accuracy: 0.5133333333333333\n",
      "iteration no 53: Loss: 0.7720650980354503, accuracy: 0.5133333333333333\n",
      "iteration no 54: Loss: 0.7714272379653105, accuracy: 0.5133333333333333\n",
      "iteration no 55: Loss: 0.7708198865306094, accuracy: 0.5133333333333333\n",
      "iteration no 56: Loss: 0.7702413186229472, accuracy: 0.5133333333333333\n",
      "iteration no 57: Loss: 0.7696899246193766, accuracy: 0.5133333333333333\n",
      "iteration no 58: Loss: 0.7691642013458379, accuracy: 0.5133333333333333\n",
      "iteration no 59: Loss: 0.7686627438516953, accuracy: 0.5133333333333333\n",
      "iteration no 60: Loss: 0.7681842379134254, accuracy: 0.5166666666666667\n",
      "iteration no 61: Loss: 0.7677274531946732, accuracy: 0.5166666666666667\n",
      "iteration no 62: Loss: 0.767291236997941, accuracy: 0.5166666666666667\n",
      "iteration no 63: Loss: 0.7668745085502507, accuracy: 0.5166666666666667\n",
      "iteration no 64: Loss: 0.7664762537713498, accuracy: 0.5166666666666667\n",
      "iteration no 65: Loss: 0.7660955204785257, accuracy: 0.5166666666666667\n",
      "iteration no 66: Loss: 0.7657314139869439, accuracy: 0.5166666666666667\n",
      "iteration no 67: Loss: 0.765383093068718, accuracy: 0.5166666666666667\n",
      "iteration no 68: Loss: 0.7650497662377167, accuracy: 0.5166666666666667\n",
      "iteration no 69: Loss: 0.7647306883304872, accuracy: 0.5166666666666667\n",
      "iteration no 70: Loss: 0.7644251573566632, accuracy: 0.5166666666666667\n",
      "iteration no 71: Loss: 0.7641325115948895, accuracy: 0.5166666666666667\n",
      "iteration no 72: Loss: 0.763852126912664, accuracy: 0.5166666666666667\n",
      "iteration no 73: Loss: 0.7635834142906107, accuracy: 0.5166666666666667\n",
      "iteration no 74: Loss: 0.7633258175335824, accuracy: 0.5166666666666667\n",
      "iteration no 75: Loss: 0.7630788111526811, accuracy: 0.5166666666666667\n",
      "iteration no 76: Loss: 0.7628418984037829, accuracy: 0.5166666666666667\n",
      "iteration no 77: Loss: 0.7626146094695205, accuracy: 0.5166666666666667\n",
      "iteration no 78: Loss: 0.762396499772872, accuracy: 0.5133333333333333\n",
      "iteration no 79: Loss: 0.7621871484116071, accuracy: 0.5133333333333333\n",
      "iteration no 80: Loss: 0.7619861567038073, accuracy: 0.5133333333333333\n",
      "iteration no 81: Loss: 0.7617931468355736, accuracy: 0.5133333333333333\n",
      "iteration no 82: Loss: 0.7616077606028082, accuracy: 0.5133333333333333\n",
      "iteration no 83: Loss: 0.7614296582396912, accuracy: 0.5133333333333333\n",
      "iteration no 84: Loss: 0.7612585173271083, accuracy: 0.5133333333333333\n",
      "iteration no 85: Loss: 0.7610940317748704, accuracy: 0.5133333333333333\n",
      "iteration no 86: Loss: 0.7609359108721006, accuracy: 0.5133333333333333\n",
      "iteration no 87: Loss: 0.7607838784006324, accuracy: 0.5133333333333333\n",
      "iteration no 88: Loss: 0.7606376718067074, accuracy: 0.5133333333333333\n",
      "iteration no 89: Loss: 0.7604970414266439, accuracy: 0.5133333333333333\n",
      "iteration no 90: Loss: 0.7603617497625139, accuracy: 0.5133333333333333\n",
      "iteration no 91: Loss: 0.7602315708041857, accuracy: 0.5133333333333333\n",
      "iteration no 92: Loss: 0.7601062893943819, accuracy: 0.5133333333333333\n",
      "iteration no 93: Loss: 0.7599857006336843, accuracy: 0.5133333333333333\n",
      "iteration no 94: Loss: 0.7598696093226451, accuracy: 0.5133333333333333\n",
      "iteration no 95: Loss: 0.7597578294384004, accuracy: 0.5133333333333333\n",
      "iteration no 96: Loss: 0.7596501836433832, accuracy: 0.5133333333333333\n",
      "iteration no 97: Loss: 0.7595465028239149, accuracy: 0.5133333333333333\n",
      "iteration no 98: Loss: 0.7594466256566308, accuracy: 0.5133333333333333\n",
      "iteration no 99: Loss: 0.7593503982008509, accuracy: 0.5133333333333333\n",
      "iteration no 100: Loss: 0.7592576735151433, accuracy: 0.5133333333333333\n",
      "iteration no 101: Loss: 0.7591683112964696, accuracy: 0.5133333333333333\n",
      "iteration no 102: Loss: 0.759082177540412, accuracy: 0.5133333333333333\n",
      "iteration no 103: Loss: 0.7589991442210993, accuracy: 0.5133333333333333\n",
      "iteration no 104: Loss: 0.7589190889895442, accuracy: 0.5166666666666667\n",
      "iteration no 105: Loss: 0.7588418948892053, accuracy: 0.5166666666666667\n",
      "iteration no 106: Loss: 0.7587674500876647, accuracy: 0.5166666666666667\n",
      "iteration no 107: Loss: 0.7586956476233959, accuracy: 0.5166666666666667\n",
      "iteration no 108: Loss: 0.7586263851666712, accuracy: 0.5166666666666667\n",
      "iteration no 109: Loss: 0.7585595647937168, accuracy: 0.5166666666666667\n",
      "iteration no 110: Loss: 0.7584950927732961, accuracy: 0.5166666666666667\n",
      "iteration no 111: Loss: 0.7584328793649517, accuracy: 0.5166666666666667\n",
      "iteration no 112: Loss: 0.7583728386281904, accuracy: 0.5166666666666667\n",
      "iteration no 113: Loss: 0.7583148882419465, accuracy: 0.5166666666666667\n",
      "iteration no 114: Loss: 0.7582589493337022, accuracy: 0.5166666666666667\n",
      "iteration no 115: Loss: 0.7582049463176863, accuracy: 0.5166666666666667\n",
      "iteration no 116: Loss: 0.758152806741609, accuracy: 0.5166666666666667\n",
      "iteration no 117: Loss: 0.7581024611414311, accuracy: 0.5166666666666667\n",
      "iteration no 118: Loss: 0.7580538429036916, accuracy: 0.5166666666666667\n",
      "iteration no 119: Loss: 0.7580068881349585, accuracy: 0.5166666666666667\n",
      "iteration no 120: Loss: 0.7579615355379853, accuracy: 0.5166666666666667\n",
      "iteration no 121: Loss: 0.75791772629419, accuracy: 0.5166666666666667\n",
      "iteration no 122: Loss: 0.757875403952097, accuracy: 0.5166666666666667\n",
      "iteration no 123: Loss: 0.7578345143213989, accuracy: 0.5166666666666667\n",
      "iteration no 124: Loss: 0.7577950053723282, accuracy: 0.5166666666666667\n",
      "iteration no 125: Loss: 0.7577568271400349, accuracy: 0.5166666666666667\n",
      "iteration no 126: Loss: 0.7577199316336963, accuracy: 0.5166666666666667\n",
      "iteration no 127: Loss: 0.7576842727500956, accuracy: 0.5166666666666667\n",
      "iteration no 128: Loss: 0.7576498061914256, accuracy: 0.5133333333333333\n",
      "iteration no 129: Loss: 0.7576164893870827, accuracy: 0.5133333333333333\n",
      "iteration no 130: Loss: 0.7575842814192421, accuracy: 0.5133333333333333\n",
      "iteration no 131: Loss: 0.757553142952003, accuracy: 0.5133333333333333\n",
      "iteration no 132: Loss: 0.7575230361639196, accuracy: 0.5133333333333333\n",
      "iteration no 133: Loss: 0.7574939246837301, accuracy: 0.5133333333333333\n",
      "iteration no 134: Loss: 0.7574657735291241, accuracy: 0.5133333333333333\n",
      "iteration no 135: Loss: 0.7574385490483796, accuracy: 0.5133333333333333\n",
      "iteration no 136: Loss: 0.7574122188647253, accuracy: 0.5133333333333333\n",
      "iteration no 137: Loss: 0.7573867518232846, accuracy: 0.5133333333333333\n",
      "iteration no 138: Loss: 0.7573621179404704, accuracy: 0.5133333333333333\n",
      "iteration no 139: Loss: 0.7573382883556995, accuracy: 0.5133333333333333\n",
      "iteration no 140: Loss: 0.7573152352853161, accuracy: 0.5133333333333333\n",
      "iteration no 141: Loss: 0.7572929319786044, accuracy: 0.5133333333333333\n",
      "iteration no 142: Loss: 0.757271352675791, accuracy: 0.5133333333333333\n",
      "iteration no 143: Loss: 0.7572504725679351, accuracy: 0.5133333333333333\n",
      "iteration no 144: Loss: 0.75723026775861, accuracy: 0.5133333333333333\n",
      "iteration no 145: Loss: 0.7572107152272918, accuracy: 0.5133333333333333\n",
      "iteration no 146: Loss: 0.7571917927943677, accuracy: 0.5133333333333333\n",
      "iteration no 147: Loss: 0.7571734790876853, accuracy: 0.5133333333333333\n",
      "iteration no 148: Loss: 0.7571557535105677, accuracy: 0.5133333333333333\n",
      "iteration no 149: Loss: 0.7571385962112256, accuracy: 0.5133333333333333\n",
      "iteration no 150: Loss: 0.7571219880534923, accuracy: 0.5133333333333333\n",
      "iteration no 151: Loss: 0.7571059105888278, accuracy: 0.5133333333333333\n",
      "iteration no 152: Loss: 0.7570903460295233, accuracy: 0.5133333333333333\n",
      "iteration no 153: Loss: 0.7570752772230526, accuracy: 0.5133333333333333\n",
      "iteration no 154: Loss: 0.7570606876275162, accuracy: 0.5133333333333333\n",
      "iteration no 155: Loss: 0.7570465612881258, accuracy: 0.5133333333333333\n",
      "iteration no 156: Loss: 0.757032882814681, accuracy: 0.5133333333333333\n",
      "iteration no 157: Loss: 0.7570196373599924, accuracy: 0.5133333333333333\n",
      "iteration no 158: Loss: 0.7570068105992067, accuracy: 0.5133333333333333\n",
      "iteration no 159: Loss: 0.7569943887099916, accuracy: 0.5133333333333333\n",
      "iteration no 160: Loss: 0.7569823583535438, accuracy: 0.5133333333333333\n",
      "iteration no 161: Loss: 0.756970706656379, accuracy: 0.5133333333333333\n",
      "iteration no 162: Loss: 0.7569594211928706, accuracy: 0.5133333333333333\n",
      "iteration no 163: Loss: 0.7569484899685036, accuracy: 0.5133333333333333\n",
      "iteration no 164: Loss: 0.7569379014038088, accuracy: 0.5133333333333333\n",
      "iteration no 165: Loss: 0.7569276443189512, accuracy: 0.5133333333333333\n",
      "iteration no 166: Loss: 0.7569177079189393, accuracy: 0.5133333333333333\n",
      "iteration no 167: Loss: 0.7569080817794297, accuracy: 0.5133333333333333\n",
      "iteration no 168: Loss: 0.7568987558331012, accuracy: 0.5133333333333333\n",
      "iteration no 169: Loss: 0.7568897203565725, accuracy: 0.5133333333333333\n",
      "iteration no 170: Loss: 0.75688096595784, accuracy: 0.5133333333333333\n",
      "iteration no 171: Loss: 0.7568724835642123, accuracy: 0.5133333333333333\n",
      "iteration no 172: Loss: 0.7568642644107217, accuracy: 0.5133333333333333\n",
      "iteration no 173: Loss: 0.7568563000289908, accuracy: 0.5133333333333333\n",
      "iteration no 174: Loss: 0.7568485822365333, accuracy: 0.5133333333333333\n",
      "iteration no 175: Loss: 0.756841103126475, accuracy: 0.5133333333333333\n",
      "iteration no 176: Loss: 0.7568338550576713, accuracy: 0.5133333333333333\n",
      "iteration no 177: Loss: 0.756826830645209, accuracy: 0.5133333333333333\n",
      "iteration no 178: Loss: 0.7568200227512734, accuracy: 0.5133333333333333\n",
      "iteration no 179: Loss: 0.7568134244763661, accuracy: 0.5133333333333333\n",
      "iteration no 180: Loss: 0.7568070291508608, accuracy: 0.5133333333333333\n",
      "iteration no 181: Loss: 0.7568008303268796, accuracy: 0.5133333333333333\n",
      "iteration no 182: Loss: 0.7567948217704777, accuracy: 0.5133333333333333\n",
      "iteration no 183: Loss: 0.7567889974541268, accuracy: 0.5133333333333333\n",
      "iteration no 184: Loss: 0.7567833515494783, accuracy: 0.5133333333333333\n",
      "iteration no 185: Loss: 0.7567778784204034, accuracy: 0.5133333333333333\n",
      "iteration no 186: Loss: 0.7567725726162908, accuracy: 0.5133333333333333\n",
      "iteration no 187: Loss: 0.7567674288655954, accuracy: 0.5133333333333333\n",
      "iteration no 188: Loss: 0.7567624420696298, accuracy: 0.5133333333333333\n",
      "iteration no 189: Loss: 0.7567576072965826, accuracy: 0.5133333333333333\n",
      "iteration no 190: Loss: 0.7567529197757603, accuracy: 0.5133333333333333\n",
      "iteration no 191: Loss: 0.7567483748920432, accuracy: 0.5133333333333333\n",
      "iteration no 192: Loss: 0.7567439681805407, accuracy: 0.5133333333333333\n",
      "iteration no 193: Loss: 0.7567396953214471, accuracy: 0.5133333333333333\n",
      "iteration no 194: Loss: 0.7567355521350834, accuracy: 0.5133333333333333\n",
      "iteration no 195: Loss: 0.7567315345771186, accuracy: 0.5133333333333333\n",
      "iteration no 196: Loss: 0.7567276387339686, accuracy: 0.5133333333333333\n",
      "iteration no 197: Loss: 0.756723860818356, accuracy: 0.5133333333333333\n",
      "iteration no 198: Loss: 0.7567201971650358, accuracy: 0.5133333333333333\n",
      "iteration no 199: Loss: 0.7567166442266721, accuracy: 0.5133333333333333\n",
      "iteration no 200: Loss: 0.7567131985698633, accuracy: 0.5166666666666667\n",
      "iteration no 1: Loss: 1.0997691569030492, accuracy: 0.53\n",
      "iteration no 2: Loss: 1.0689935641906079, accuracy: 0.5233333333333333\n",
      "iteration no 3: Loss: 1.0416106474533127, accuracy: 0.5166666666666667\n",
      "iteration no 4: Loss: 1.017206415903098, accuracy: 0.5133333333333333\n",
      "iteration no 5: Loss: 0.9954184753076096, accuracy: 0.5166666666666667\n",
      "iteration no 6: Loss: 0.9759286648507275, accuracy: 0.5166666666666667\n",
      "iteration no 7: Loss: 0.9584580547859014, accuracy: 0.5166666666666667\n",
      "iteration no 8: Loss: 0.942762753819113, accuracy: 0.5166666666666667\n",
      "iteration no 9: Loss: 0.9286300304204647, accuracy: 0.52\n",
      "iteration no 10: Loss: 0.9158746550993918, accuracy: 0.5166666666666667\n",
      "iteration no 11: Loss: 0.904335493281416, accuracy: 0.52\n",
      "iteration no 12: Loss: 0.8938723931414058, accuracy: 0.5166666666666667\n",
      "iteration no 13: Loss: 0.8843633943190398, accuracy: 0.5133333333333333\n",
      "iteration no 14: Loss: 0.8757022610417101, accuracy: 0.51\n",
      "iteration no 15: Loss: 0.8677963260628498, accuracy: 0.51\n",
      "iteration no 16: Loss: 0.8605646213603473, accuracy: 0.51\n",
      "iteration no 17: Loss: 0.8539362665097846, accuracy: 0.5133333333333333\n",
      "iteration no 18: Loss: 0.8478490843577878, accuracy: 0.51\n",
      "iteration no 19: Loss: 0.8422484146253075, accuracy: 0.51\n",
      "iteration no 20: Loss: 0.8370860983228875, accuracy: 0.51\n",
      "iteration no 21: Loss: 0.8323196086706377, accuracy: 0.5133333333333333\n",
      "iteration no 22: Loss: 0.8279113071618316, accuracy: 0.5133333333333333\n",
      "iteration no 23: Loss: 0.8238278062497499, accuracy: 0.5133333333333333\n",
      "iteration no 24: Loss: 0.82003942274853, accuracy: 0.5133333333333333\n",
      "iteration no 25: Loss: 0.8165197083682068, accuracy: 0.5133333333333333\n",
      "iteration no 26: Loss: 0.8132450458416752, accuracy: 0.5133333333333333\n",
      "iteration no 27: Loss: 0.8101943008598678, accuracy: 0.5133333333333333\n",
      "iteration no 28: Loss: 0.8073485215353448, accuracy: 0.5133333333333333\n",
      "iteration no 29: Loss: 0.8046906783925056, accuracy: 0.5166666666666667\n",
      "iteration no 30: Loss: 0.802205438964017, accuracy: 0.5166666666666667\n",
      "iteration no 31: Loss: 0.7998789719855176, accuracy: 0.5166666666666667\n",
      "iteration no 32: Loss: 0.7976987769492703, accuracy: 0.5166666666666667\n",
      "iteration no 33: Loss: 0.7956535354242811, accuracy: 0.5166666666666667\n",
      "iteration no 34: Loss: 0.7937329810946707, accuracy: 0.5166666666666667\n",
      "iteration no 35: Loss: 0.7919277859261227, accuracy: 0.5166666666666667\n",
      "iteration no 36: Loss: 0.7902294602559529, accuracy: 0.5166666666666667\n",
      "iteration no 37: Loss: 0.7886302649274551, accuracy: 0.5166666666666667\n",
      "iteration no 38: Loss: 0.7871231338635144, accuracy: 0.5166666666666667\n",
      "iteration no 39: Loss: 0.7857016057062794, accuracy: 0.5166666666666667\n",
      "iteration no 40: Loss: 0.784359763345823, accuracy: 0.5166666666666667\n",
      "iteration no 41: Loss: 0.7830921803269546, accuracy: 0.5166666666666667\n",
      "iteration no 42: Loss: 0.7818938732644451, accuracy: 0.5166666666666667\n",
      "iteration no 43: Loss: 0.7807602595169312, accuracy: 0.5133333333333333\n",
      "iteration no 44: Loss: 0.779687119471958, accuracy: 0.5133333333333333\n",
      "iteration no 45: Loss: 0.7786705628818626, accuracy: 0.5133333333333333\n",
      "iteration no 46: Loss: 0.777706998764745, accuracy: 0.5133333333333333\n",
      "iteration no 47: Loss: 0.7767931084486456, accuracy: 0.5133333333333333\n",
      "iteration no 48: Loss: 0.7759258213918423, accuracy: 0.5133333333333333\n",
      "iteration no 49: Loss: 0.7751022934592668, accuracy: 0.5133333333333333\n",
      "iteration no 50: Loss: 0.7743198873756063, accuracy: 0.5133333333333333\n",
      "iteration no 51: Loss: 0.773576155110638, accuracy: 0.5133333333333333\n",
      "iteration no 52: Loss: 0.7728688219825802, accuracy: 0.5133333333333333\n",
      "iteration no 53: Loss: 0.7721957722914193, accuracy: 0.5133333333333333\n",
      "iteration no 54: Loss: 0.7715550363168714, accuracy: 0.5133333333333333\n",
      "iteration no 55: Loss: 0.7709447785353631, accuracy: 0.5166666666666667\n",
      "iteration no 56: Loss: 0.7703632869275674, accuracy: 0.5166666666666667\n",
      "iteration no 57: Loss: 0.7698089632630002, accuracy: 0.5166666666666667\n",
      "iteration no 58: Loss: 0.7692803142612358, accuracy: 0.5166666666666667\n",
      "iteration no 59: Loss: 0.768775943540729, accuracy: 0.5166666666666667\n",
      "iteration no 60: Loss: 0.7682945442762268, accuracy: 0.5166666666666667\n",
      "iteration no 61: Loss: 0.7678348924945413, accuracy: 0.5166666666666667\n",
      "iteration no 62: Loss: 0.7673958409461606, accuracy: 0.5166666666666667\n",
      "iteration no 63: Loss: 0.7669763134969616, accuracy: 0.5166666666666667\n",
      "iteration no 64: Loss: 0.7665752999902754, accuracy: 0.5166666666666667\n",
      "iteration no 65: Loss: 0.7661918515348287, accuracy: 0.5166666666666667\n",
      "iteration no 66: Loss: 0.7658250761787555, accuracy: 0.5166666666666667\n",
      "iteration no 67: Loss: 0.7654741349340033, accuracy: 0.5166666666666667\n",
      "iteration no 68: Loss: 0.7651382381191152, accuracy: 0.5166666666666667\n",
      "iteration no 69: Loss: 0.7648166419916241, accuracy: 0.5166666666666667\n",
      "iteration no 70: Loss: 0.7645086456441784, accuracy: 0.5166666666666667\n",
      "iteration no 71: Loss: 0.7642135881410916, accuracy: 0.5166666666666667\n",
      "iteration no 72: Loss: 0.7639308458742968, accuracy: 0.5166666666666667\n",
      "iteration no 73: Loss: 0.7636598301197307, accuracy: 0.5166666666666667\n",
      "iteration no 74: Loss: 0.7633999847769966, accuracy: 0.5166666666666667\n",
      "iteration no 75: Loss: 0.7631507842767902, accuracy: 0.5166666666666667\n",
      "iteration no 76: Loss: 0.7629117316420297, accuracy: 0.5166666666666667\n",
      "iteration no 77: Loss: 0.7626823566899463, accuracy: 0.5166666666666667\n",
      "iteration no 78: Loss: 0.7624622143635665, accuracy: 0.5166666666666667\n",
      "iteration no 79: Loss: 0.7622508831820737, accuracy: 0.5166666666666667\n",
      "iteration no 80: Loss: 0.7620479638004869, accuracy: 0.5166666666666667\n",
      "iteration no 81: Loss: 0.7618530776699522, accuracy: 0.5133333333333333\n",
      "iteration no 82: Loss: 0.7616658657907149, accuracy: 0.5133333333333333\n",
      "iteration no 83: Loss: 0.7614859875505339, accuracy: 0.5133333333333333\n",
      "iteration no 84: Loss: 0.7613131196419328, accuracy: 0.5133333333333333\n",
      "iteration no 85: Loss: 0.761146955052249, accuracy: 0.5133333333333333\n",
      "iteration no 86: Loss: 0.760987202120958, accuracy: 0.5133333333333333\n",
      "iteration no 87: Loss: 0.7608335836592168, accuracy: 0.5133333333333333\n",
      "iteration no 88: Loss: 0.7606858361269948, accuracy: 0.5133333333333333\n",
      "iteration no 89: Loss: 0.7605437088635407, accuracy: 0.5133333333333333\n",
      "iteration no 90: Loss: 0.760406963367291, accuracy: 0.5133333333333333\n",
      "iteration no 91: Loss: 0.7602753726216324, accuracy: 0.5133333333333333\n",
      "iteration no 92: Loss: 0.7601487204632329, accuracy: 0.5133333333333333\n",
      "iteration no 93: Loss: 0.7600268009899044, accuracy: 0.5133333333333333\n",
      "iteration no 94: Loss: 0.7599094180052166, accuracy: 0.5133333333333333\n",
      "iteration no 95: Loss: 0.7597963844972825, accuracy: 0.5133333333333333\n",
      "iteration no 96: Loss: 0.7596875221493566, accuracy: 0.5133333333333333\n",
      "iteration no 97: Loss: 0.7595826608800509, accuracy: 0.5133333333333333\n",
      "iteration no 98: Loss: 0.7594816384111539, accuracy: 0.5133333333333333\n",
      "iteration no 99: Loss: 0.7593842998611855, accuracy: 0.5133333333333333\n",
      "iteration no 100: Loss: 0.759290497362964, accuracy: 0.5133333333333333\n",
      "iteration no 101: Loss: 0.759200089703586, accuracy: 0.5133333333333333\n",
      "iteration no 102: Loss: 0.7591129419853445, accuracy: 0.5166666666666667\n",
      "iteration no 103: Loss: 0.7590289253062128, accuracy: 0.5166666666666667\n",
      "iteration no 104: Loss: 0.7589479164586252, accuracy: 0.5166666666666667\n",
      "iteration no 105: Loss: 0.7588697976453781, accuracy: 0.5166666666666667\n",
      "iteration no 106: Loss: 0.758794456211554, accuracy: 0.5166666666666667\n",
      "iteration no 107: Loss: 0.7587217843914579, accuracy: 0.5166666666666667\n",
      "iteration no 108: Loss: 0.7586516790696166, accuracy: 0.5166666666666667\n",
      "iteration no 109: Loss: 0.7585840415549678, accuracy: 0.5166666666666667\n",
      "iteration no 110: Loss: 0.7585187773674162, accuracy: 0.5166666666666667\n",
      "iteration no 111: Loss: 0.7584557960360022, accuracy: 0.5166666666666667\n",
      "iteration no 112: Loss: 0.7583950109079712, accuracy: 0.5166666666666667\n",
      "iteration no 113: Loss: 0.7583363389680821, accuracy: 0.5166666666666667\n",
      "iteration no 114: Loss: 0.7582797006675406, accuracy: 0.5166666666666667\n",
      "iteration no 115: Loss: 0.7582250197619842, accuracy: 0.5166666666666667\n",
      "iteration no 116: Loss: 0.7581722231579769, accuracy: 0.5166666666666667\n",
      "iteration no 117: Loss: 0.7581212407675199, accuracy: 0.5166666666666667\n",
      "iteration no 118: Loss: 0.7580720053701019, accuracy: 0.5166666666666667\n",
      "iteration no 119: Loss: 0.7580244524818582, accuracy: 0.5166666666666667\n",
      "iteration no 120: Loss: 0.7579785202314236, accuracy: 0.5166666666666667\n",
      "iteration no 121: Loss: 0.7579341492420987, accuracy: 0.5166666666666667\n",
      "iteration no 122: Loss: 0.7578912825199693, accuracy: 0.5166666666666667\n",
      "iteration no 123: Loss: 0.7578498653476433, accuracy: 0.5166666666666667\n",
      "iteration no 124: Loss: 0.7578098451832888, accuracy: 0.5166666666666667\n",
      "iteration no 125: Loss: 0.7577711715646789, accuracy: 0.5166666666666667\n",
      "iteration no 126: Loss: 0.7577337960179652, accuracy: 0.5166666666666667\n",
      "iteration no 127: Loss: 0.7576976719709199, accuracy: 0.5166666666666667\n",
      "iteration no 128: Loss: 0.7576627546704023, accuracy: 0.5166666666666667\n",
      "iteration no 129: Loss: 0.7576290011038211, accuracy: 0.5166666666666667\n",
      "iteration no 130: Loss: 0.7575963699243741, accuracy: 0.5166666666666667\n",
      "iteration no 131: Loss: 0.757564821379865, accuracy: 0.5133333333333333\n",
      "iteration no 132: Loss: 0.7575343172449074, accuracy: 0.5133333333333333\n",
      "iteration no 133: Loss: 0.7575048207563323, accuracy: 0.5133333333333333\n",
      "iteration no 134: Loss: 0.7574762965516363, accuracy: 0.5133333333333333\n",
      "iteration no 135: Loss: 0.7574487106103045, accuracy: 0.5133333333333333\n",
      "iteration no 136: Loss: 0.7574220301978657, accuracy: 0.5133333333333333\n",
      "iteration no 137: Loss: 0.7573962238125327, accuracy: 0.5133333333333333\n",
      "iteration no 138: Loss: 0.7573712611342976, accuracy: 0.5133333333333333\n",
      "iteration no 139: Loss: 0.7573471129763564, accuracy: 0.5133333333333333\n",
      "iteration no 140: Loss: 0.7573237512387442, accuracy: 0.5133333333333333\n",
      "iteration no 141: Loss: 0.7573011488640683, accuracy: 0.5133333333333333\n",
      "iteration no 142: Loss: 0.7572792797952362, accuracy: 0.5133333333333333\n",
      "iteration no 143: Loss: 0.7572581189350766, accuracy: 0.5133333333333333\n",
      "iteration no 144: Loss: 0.7572376421077586, accuracy: 0.5133333333333333\n",
      "iteration no 145: Loss: 0.7572178260219258, accuracy: 0.5133333333333333\n",
      "iteration no 146: Loss: 0.7571986482354522, accuracy: 0.5133333333333333\n",
      "iteration no 147: Loss: 0.7571800871217487, accuracy: 0.5133333333333333\n",
      "iteration no 148: Loss: 0.7571621218375406, accuracy: 0.5133333333333333\n",
      "iteration no 149: Loss: 0.7571447322920458, accuracy: 0.5133333333333333\n",
      "iteration no 150: Loss: 0.7571278991174858, accuracy: 0.5133333333333333\n",
      "iteration no 151: Loss: 0.757111603640869, accuracy: 0.5133333333333333\n",
      "iteration no 152: Loss: 0.7570958278569799, accuracy: 0.5133333333333333\n",
      "iteration no 153: Loss: 0.7570805544025225, accuracy: 0.5133333333333333\n",
      "iteration no 154: Loss: 0.7570657665313619, accuracy: 0.5133333333333333\n",
      "iteration no 155: Loss: 0.7570514480908102, accuracy: 0.5133333333333333\n",
      "iteration no 156: Loss: 0.7570375834989116, accuracy: 0.5133333333333333\n",
      "iteration no 157: Loss: 0.7570241577226788, accuracy: 0.5133333333333333\n",
      "iteration no 158: Loss: 0.7570111562572364, accuracy: 0.5133333333333333\n",
      "iteration no 159: Loss: 0.7569985651058301, accuracy: 0.5133333333333333\n",
      "iteration no 160: Loss: 0.7569863707606617, accuracy: 0.5133333333333333\n",
      "iteration no 161: Loss: 0.7569745601845145, accuracy: 0.5133333333333333\n",
      "iteration no 162: Loss: 0.7569631207931292, accuracy: 0.5133333333333333\n",
      "iteration no 163: Loss: 0.7569520404383009, accuracy: 0.5133333333333333\n",
      "iteration no 164: Loss: 0.7569413073916613, accuracy: 0.5133333333333333\n",
      "iteration no 165: Loss: 0.7569309103291193, accuracy: 0.5133333333333333\n",
      "iteration no 166: Loss: 0.7569208383159256, accuracy: 0.5133333333333333\n",
      "iteration no 167: Loss: 0.7569110807923395, accuracy: 0.5133333333333333\n",
      "iteration no 168: Loss: 0.7569016275598668, accuracy: 0.5133333333333333\n",
      "iteration no 169: Loss: 0.7568924687680467, accuracy: 0.5133333333333333\n",
      "iteration no 170: Loss: 0.756883594901762, accuracy: 0.5133333333333333\n",
      "iteration no 171: Loss: 0.756874996769051, accuracy: 0.5133333333333333\n",
      "iteration no 172: Loss: 0.7568666654893986, accuracy: 0.5133333333333333\n",
      "iteration no 173: Loss: 0.7568585924824871, accuracy: 0.5133333333333333\n",
      "iteration no 174: Loss: 0.7568507694573849, accuracy: 0.5133333333333333\n",
      "iteration no 175: Loss: 0.7568431884021578, accuracy: 0.5133333333333333\n",
      "iteration no 176: Loss: 0.7568358415738807, accuracy: 0.5133333333333333\n",
      "iteration no 177: Loss: 0.7568287214890376, accuracy: 0.5133333333333333\n",
      "iteration no 178: Loss: 0.7568218209142881, accuracy: 0.5133333333333333\n",
      "iteration no 179: Loss: 0.756815132857591, accuracy: 0.5133333333333333\n",
      "iteration no 180: Loss: 0.7568086505596643, accuracy: 0.5133333333333333\n",
      "iteration no 181: Loss: 0.7568023674857719, accuracy: 0.5133333333333333\n",
      "iteration no 182: Loss: 0.756796277317822, accuracy: 0.5133333333333333\n",
      "iteration no 183: Loss: 0.7567903739467629, accuracy: 0.5133333333333333\n",
      "iteration no 184: Loss: 0.7567846514652672, accuracy: 0.5133333333333333\n",
      "iteration no 185: Loss: 0.756779104160689, accuracy: 0.5133333333333333\n",
      "iteration no 186: Loss: 0.7567737265082857, accuracy: 0.5133333333333333\n",
      "iteration no 187: Loss: 0.7567685131646926, accuracy: 0.5133333333333333\n",
      "iteration no 188: Loss: 0.7567634589616398, accuracy: 0.5133333333333333\n",
      "iteration no 189: Loss: 0.7567585588999021, accuracy: 0.5133333333333333\n",
      "iteration no 190: Loss: 0.7567538081434728, accuracy: 0.5133333333333333\n",
      "iteration no 191: Loss: 0.7567492020139518, accuracy: 0.5133333333333333\n",
      "iteration no 192: Loss: 0.7567447359851394, accuracy: 0.5133333333333333\n",
      "iteration no 193: Loss: 0.7567404056778283, accuracy: 0.5133333333333333\n",
      "iteration no 194: Loss: 0.7567362068547863, accuracy: 0.5133333333333333\n",
      "iteration no 195: Loss: 0.7567321354159204, accuracy: 0.5133333333333333\n",
      "iteration no 196: Loss: 0.756728187393617, accuracy: 0.5133333333333333\n",
      "iteration no 197: Loss: 0.7567243589482504, accuracy: 0.5133333333333333\n",
      "iteration no 198: Loss: 0.7567206463638537, accuracy: 0.5133333333333333\n",
      "iteration no 199: Loss: 0.7567170460439444, accuracy: 0.5133333333333333\n",
      "iteration no 200: Loss: 0.756713554507501, accuracy: 0.5133333333333333\n",
      "iteration no 1: Loss: 1.123072847163536, accuracy: 0.26\n",
      "iteration no 2: Loss: 1.0897154600661527, accuracy: 0.41\n",
      "iteration no 3: Loss: 1.0600964338104513, accuracy: 0.4633333333333333\n",
      "iteration no 4: Loss: 1.0337515192813644, accuracy: 0.4666666666666667\n",
      "iteration no 5: Loss: 1.0102756644714916, accuracy: 0.4866666666666667\n",
      "iteration no 6: Loss: 0.989314090606265, accuracy: 0.49333333333333335\n",
      "iteration no 7: Loss: 0.9705564434899545, accuracy: 0.49666666666666665\n",
      "iteration no 8: Loss: 0.9537319863390957, accuracy: 0.5033333333333333\n",
      "iteration no 9: Loss: 0.9386051709800854, accuracy: 0.5066666666666667\n",
      "iteration no 10: Loss: 0.9249714555456771, accuracy: 0.5066666666666667\n",
      "iteration no 11: Loss: 0.912653402361447, accuracy: 0.5066666666666667\n",
      "iteration no 12: Loss: 0.9014971125038813, accuracy: 0.5066666666666667\n",
      "iteration no 13: Loss: 0.8913690315236555, accuracy: 0.5133333333333333\n",
      "iteration no 14: Loss: 0.8821531327834701, accuracy: 0.51\n",
      "iteration no 15: Loss: 0.8737484632395145, accuracy: 0.5066666666666667\n",
      "iteration no 16: Loss: 0.8660670232816879, accuracy: 0.5066666666666667\n",
      "iteration no 17: Loss: 0.8590319459319706, accuracy: 0.5133333333333333\n",
      "iteration no 18: Loss: 0.8525759391154958, accuracy: 0.5166666666666667\n",
      "iteration no 19: Loss: 0.8466399560072977, accuracy: 0.5166666666666667\n",
      "iteration no 20: Loss: 0.8411720612817746, accuracy: 0.5166666666666667\n",
      "iteration no 21: Loss: 0.8361264645786126, accuracy: 0.5133333333333333\n",
      "iteration no 22: Loss: 0.8314626961202831, accuracy: 0.5133333333333333\n",
      "iteration no 23: Loss: 0.8271449028776358, accuracy: 0.5166666666666667\n",
      "iteration no 24: Loss: 0.8231412468351917, accuracy: 0.5166666666666667\n",
      "iteration no 25: Loss: 0.8194233896997145, accuracy: 0.5166666666666667\n",
      "iteration no 26: Loss: 0.8159660508188354, accuracy: 0.5166666666666667\n",
      "iteration no 27: Loss: 0.81274662715243, accuracy: 0.5166666666666667\n",
      "iteration no 28: Loss: 0.8097448659022144, accuracy: 0.5133333333333333\n",
      "iteration no 29: Loss: 0.8069425818930164, accuracy: 0.5166666666666667\n",
      "iteration no 30: Loss: 0.8043234130502898, accuracy: 0.5133333333333333\n",
      "iteration no 31: Loss: 0.8018726083678476, accuracy: 0.5133333333333333\n",
      "iteration no 32: Loss: 0.7995768436388072, accuracy: 0.5133333333333333\n",
      "iteration no 33: Loss: 0.7974240609586638, accuracy: 0.5133333333333333\n",
      "iteration no 34: Loss: 0.7954033286255697, accuracy: 0.5166666666666667\n",
      "iteration no 35: Loss: 0.7935047185790921, accuracy: 0.5166666666666667\n",
      "iteration no 36: Loss: 0.7917191989515471, accuracy: 0.5166666666666667\n",
      "iteration no 37: Loss: 0.7900385396693895, accuracy: 0.5166666666666667\n",
      "iteration no 38: Loss: 0.7884552293476277, accuracy: 0.5166666666666667\n",
      "iteration no 39: Loss: 0.7869624019774614, accuracy: 0.52\n",
      "iteration no 40: Loss: 0.7855537721243215, accuracy: 0.52\n",
      "iteration no 41: Loss: 0.7842235775368149, accuracy: 0.52\n",
      "iteration no 42: Loss: 0.7829665282222871, accuracy: 0.5166666666666667\n",
      "iteration no 43: Loss: 0.7817777611763452, accuracy: 0.5166666666666667\n",
      "iteration no 44: Loss: 0.7806528000655608, accuracy: 0.5166666666666667\n",
      "iteration no 45: Loss: 0.7795875192578142, accuracy: 0.5166666666666667\n",
      "iteration no 46: Loss: 0.7785781116760133, accuracy: 0.5166666666666667\n",
      "iteration no 47: Loss: 0.777621060020389, accuracy: 0.5166666666666667\n",
      "iteration no 48: Loss: 0.7767131109640579, accuracy: 0.5166666666666667\n",
      "iteration no 49: Loss: 0.7758512519776243, accuracy: 0.5133333333333333\n",
      "iteration no 50: Loss: 0.7750326904824736, accuracy: 0.5133333333333333\n",
      "iteration no 51: Loss: 0.7742548350702565, accuracy: 0.5133333333333333\n",
      "iteration no 52: Loss: 0.7735152785586905, accuracy: 0.5133333333333333\n",
      "iteration no 53: Loss: 0.772811782682041, accuracy: 0.5133333333333333\n",
      "iteration no 54: Loss: 0.7721422642391065, accuracy: 0.5133333333333333\n",
      "iteration no 55: Loss: 0.7715047825427401, accuracy: 0.5133333333333333\n",
      "iteration no 56: Loss: 0.7708975280334106, accuracy: 0.5133333333333333\n",
      "iteration no 57: Loss: 0.7703188119353633, accuracy: 0.5133333333333333\n",
      "iteration no 58: Loss: 0.7697670568479696, accuracy: 0.5133333333333333\n",
      "iteration no 59: Loss: 0.7692407881771075, accuracy: 0.5133333333333333\n",
      "iteration no 60: Loss: 0.7687386263221355, accuracy: 0.5133333333333333\n",
      "iteration no 61: Loss: 0.7682592795434371, accuracy: 0.5133333333333333\n",
      "iteration no 62: Loss: 0.7678015374437609, accuracy: 0.5133333333333333\n",
      "iteration no 63: Loss: 0.7673642650038558, accuracy: 0.5133333333333333\n",
      "iteration no 64: Loss: 0.7669463971192926, accuracy: 0.5133333333333333\n",
      "iteration no 65: Loss: 0.7665469335910143, accuracy: 0.5166666666666667\n",
      "iteration no 66: Loss: 0.766164934527143, accuracy: 0.5166666666666667\n",
      "iteration no 67: Loss: 0.7657995161179859, accuracy: 0.5166666666666667\n",
      "iteration no 68: Loss: 0.7654498467500951, accuracy: 0.5166666666666667\n",
      "iteration no 69: Loss: 0.7651151434287038, accuracy: 0.5166666666666667\n",
      "iteration no 70: Loss: 0.764794668480949, accuracy: 0.5166666666666667\n",
      "iteration no 71: Loss: 0.7644877265150278, accuracy: 0.5166666666666667\n",
      "iteration no 72: Loss: 0.7641936616128889, accuracy: 0.5166666666666667\n",
      "iteration no 73: Loss: 0.7639118547362289, accuracy: 0.5166666666666667\n",
      "iteration no 74: Loss: 0.7636417213275193, accuracy: 0.5166666666666667\n",
      "iteration no 75: Loss: 0.7633827090895291, accuracy: 0.5166666666666667\n",
      "iteration no 76: Loss: 0.7631342959283663, accuracy: 0.5166666666666667\n",
      "iteration no 77: Loss: 0.7628959880464574, accuracy: 0.5166666666666667\n",
      "iteration no 78: Loss: 0.7626673181731436, accuracy: 0.5166666666666667\n",
      "iteration no 79: Loss: 0.7624478439216942, accuracy: 0.5166666666666667\n",
      "iteration no 80: Loss: 0.762237146262555, accuracy: 0.5166666666666667\n",
      "iteration no 81: Loss: 0.7620348281035589, accuracy: 0.5166666666666667\n",
      "iteration no 82: Loss: 0.7618405129686523, accuracy: 0.5166666666666667\n",
      "iteration no 83: Loss: 0.7616538437674308, accuracy: 0.5133333333333333\n",
      "iteration no 84: Loss: 0.7614744816484509, accuracy: 0.5133333333333333\n",
      "iteration no 85: Loss: 0.7613021049298849, accuracy: 0.5133333333333333\n",
      "iteration no 86: Loss: 0.7611364081016475, accuracy: 0.5133333333333333\n",
      "iteration no 87: Loss: 0.7609771008936046, accuracy: 0.5133333333333333\n",
      "iteration no 88: Loss: 0.7608239074049366, accuracy: 0.5133333333333333\n",
      "iteration no 89: Loss: 0.7606765652901363, accuracy: 0.5133333333333333\n",
      "iteration no 90: Loss: 0.7605348249974895, accuracy: 0.5133333333333333\n",
      "iteration no 91: Loss: 0.7603984490562288, accuracy: 0.5133333333333333\n",
      "iteration no 92: Loss: 0.7602672114088577, accuracy: 0.5133333333333333\n",
      "iteration no 93: Loss: 0.7601408967854213, accuracy: 0.5133333333333333\n",
      "iteration no 94: Loss: 0.7600193001167597, accuracy: 0.5133333333333333\n",
      "iteration no 95: Loss: 0.7599022259840075, accuracy: 0.5133333333333333\n",
      "iteration no 96: Loss: 0.7597894881018239, accuracy: 0.5133333333333333\n",
      "iteration no 97: Loss: 0.7596809088330242, accuracy: 0.5133333333333333\n",
      "iteration no 98: Loss: 0.7595763187324698, accuracy: 0.5133333333333333\n",
      "iteration no 99: Loss: 0.75947555611823, accuracy: 0.5133333333333333\n",
      "iteration no 100: Loss: 0.7593784666681868, accuracy: 0.5133333333333333\n",
      "iteration no 101: Loss: 0.7592849030403798, accuracy: 0.5133333333333333\n",
      "iteration no 102: Loss: 0.7591947245155258, accuracy: 0.5133333333333333\n",
      "iteration no 103: Loss: 0.7591077966602572, accuracy: 0.5133333333333333\n",
      "iteration no 104: Loss: 0.7590239910097266, accuracy: 0.5133333333333333\n",
      "iteration no 105: Loss: 0.7589431847683302, accuracy: 0.5166666666666667\n",
      "iteration no 106: Loss: 0.7588652605273907, accuracy: 0.5166666666666667\n",
      "iteration no 107: Loss: 0.7587901059987118, accuracy: 0.5166666666666667\n",
      "iteration no 108: Loss: 0.7587176137630154, accuracy: 0.5166666666666667\n",
      "iteration no 109: Loss: 0.7586476810323174, accuracy: 0.5166666666666667\n",
      "iteration no 110: Loss: 0.7585802094253835, accuracy: 0.5166666666666667\n",
      "iteration no 111: Loss: 0.7585151047554528, accuracy: 0.5166666666666667\n",
      "iteration no 112: Loss: 0.7584522768294812, accuracy: 0.5166666666666667\n",
      "iteration no 113: Loss: 0.7583916392582011, accuracy: 0.5166666666666667\n",
      "iteration no 114: Loss: 0.7583331092763457, accuracy: 0.5166666666666667\n",
      "iteration no 115: Loss: 0.7582766075724295, accuracy: 0.5166666666666667\n",
      "iteration no 116: Loss: 0.7582220581275135, accuracy: 0.5166666666666667\n",
      "iteration no 117: Loss: 0.7581693880624267, accuracy: 0.5166666666666667\n",
      "iteration no 118: Loss: 0.7581185274929458, accuracy: 0.5166666666666667\n",
      "iteration no 119: Loss: 0.7580694093924707, accuracy: 0.5166666666666667\n",
      "iteration no 120: Loss: 0.7580219694617595, accuracy: 0.5166666666666667\n",
      "iteration no 121: Loss: 0.7579761460053205, accuracy: 0.5166666666666667\n",
      "iteration no 122: Loss: 0.7579318798140777, accuracy: 0.5166666666666667\n",
      "iteration no 123: Loss: 0.7578891140539558, accuracy: 0.5166666666666667\n",
      "iteration no 124: Loss: 0.7578477941600497, accuracy: 0.5166666666666667\n",
      "iteration no 125: Loss: 0.7578078677360691, accuracy: 0.5166666666666667\n",
      "iteration no 126: Loss: 0.7577692844587588, accuracy: 0.5166666666666667\n",
      "iteration no 127: Loss: 0.7577319959870263, accuracy: 0.5166666666666667\n",
      "iteration no 128: Loss: 0.7576959558755135, accuracy: 0.5166666666666667\n",
      "iteration no 129: Loss: 0.7576611194923726, accuracy: 0.5166666666666667\n",
      "iteration no 130: Loss: 0.7576274439410158, accuracy: 0.5166666666666667\n",
      "iteration no 131: Loss: 0.7575948879856285, accuracy: 0.5166666666666667\n",
      "iteration no 132: Loss: 0.7575634119802395, accuracy: 0.5133333333333333\n",
      "iteration no 133: Loss: 0.7575329778011637, accuracy: 0.5133333333333333\n",
      "iteration no 134: Loss: 0.7575035487826354, accuracy: 0.5133333333333333\n",
      "iteration no 135: Loss: 0.7574750896554655, accuracy: 0.5133333333333333\n",
      "iteration no 136: Loss: 0.7574475664885661, accuracy: 0.5133333333333333\n",
      "iteration no 137: Loss: 0.7574209466331908, accuracy: 0.5133333333333333\n",
      "iteration no 138: Loss: 0.757395198669752, accuracy: 0.5133333333333333\n",
      "iteration no 139: Loss: 0.757370292357083, accuracy: 0.5133333333333333\n",
      "iteration no 140: Loss: 0.7573461985840204, accuracy: 0.5133333333333333\n",
      "iteration no 141: Loss: 0.757322889323186, accuracy: 0.5133333333333333\n",
      "iteration no 142: Loss: 0.7573003375868623, accuracy: 0.5133333333333333\n",
      "iteration no 143: Loss: 0.7572785173848515, accuracy: 0.5133333333333333\n",
      "iteration no 144: Loss: 0.757257403684223, accuracy: 0.5133333333333333\n",
      "iteration no 145: Loss: 0.7572369723708525, accuracy: 0.5133333333333333\n",
      "iteration no 146: Loss: 0.7572172002126659, accuracy: 0.5133333333333333\n",
      "iteration no 147: Loss: 0.7571980648245057, accuracy: 0.5133333333333333\n",
      "iteration no 148: Loss: 0.7571795446345361, accuracy: 0.5133333333333333\n",
      "iteration no 149: Loss: 0.7571616188521177, accuracy: 0.5133333333333333\n",
      "iteration no 150: Loss: 0.7571442674370777, accuracy: 0.5133333333333333\n",
      "iteration no 151: Loss: 0.7571274710703073, accuracy: 0.5133333333333333\n",
      "iteration no 152: Loss: 0.7571112111256267, accuracy: 0.5133333333333333\n",
      "iteration no 153: Loss: 0.757095469642854, accuracy: 0.5133333333333333\n",
      "iteration no 154: Loss: 0.7570802293020235, accuracy: 0.5133333333333333\n",
      "iteration no 155: Loss: 0.7570654733986962, accuracy: 0.5133333333333333\n",
      "iteration no 156: Loss: 0.7570511858203165, accuracy: 0.5133333333333333\n",
      "iteration no 157: Loss: 0.7570373510235595, accuracy: 0.5133333333333333\n",
      "iteration no 158: Loss: 0.7570239540126308, accuracy: 0.5133333333333333\n",
      "iteration no 159: Loss: 0.7570109803184664, accuracy: 0.5133333333333333\n",
      "iteration no 160: Loss: 0.756998415978799, accuracy: 0.5133333333333333\n",
      "iteration no 161: Loss: 0.756986247519048, accuracy: 0.5133333333333333\n",
      "iteration no 162: Loss: 0.7569744619339929, accuracy: 0.5133333333333333\n",
      "iteration no 163: Loss: 0.7569630466701994, accuracy: 0.5133333333333333\n",
      "iteration no 164: Loss: 0.7569519896091637, accuracy: 0.5133333333333333\n",
      "iteration no 165: Loss: 0.7569412790511377, accuracy: 0.5133333333333333\n",
      "iteration no 166: Loss: 0.7569309036996128, accuracy: 0.5133333333333333\n",
      "iteration no 167: Loss: 0.7569208526464251, accuracy: 0.5133333333333333\n",
      "iteration no 168: Loss: 0.7569111153574613, accuracy: 0.5133333333333333\n",
      "iteration no 169: Loss: 0.7569016816589331, accuracy: 0.5133333333333333\n",
      "iteration no 170: Loss: 0.7568925417242007, accuracy: 0.5133333333333333\n",
      "iteration no 171: Loss: 0.7568836860611162, accuracy: 0.5133333333333333\n",
      "iteration no 172: Loss: 0.7568751054998685, accuracy: 0.5133333333333333\n",
      "iteration no 173: Loss: 0.7568667911813062, accuracy: 0.5133333333333333\n",
      "iteration no 174: Loss: 0.7568587345457171, accuracy: 0.5133333333333333\n",
      "iteration no 175: Loss: 0.7568509273220467, accuracy: 0.5133333333333333\n",
      "iteration no 176: Loss: 0.7568433615175367, accuracy: 0.5133333333333333\n",
      "iteration no 177: Loss: 0.7568360294077635, accuracy: 0.5133333333333333\n",
      "iteration no 178: Loss: 0.756828923527062, accuracy: 0.5133333333333333\n",
      "iteration no 179: Loss: 0.7568220366593187, accuracy: 0.5133333333333333\n",
      "iteration no 180: Loss: 0.7568153618291162, accuracy: 0.5133333333333333\n",
      "iteration no 181: Loss: 0.7568088922932176, accuracy: 0.5133333333333333\n",
      "iteration no 182: Loss: 0.7568026215323737, accuracy: 0.5133333333333333\n",
      "iteration no 183: Loss: 0.7567965432434413, accuracy: 0.5133333333333333\n",
      "iteration no 184: Loss: 0.7567906513318007, accuracy: 0.5133333333333333\n",
      "iteration no 185: Loss: 0.7567849399040562, accuracy: 0.5133333333333333\n",
      "iteration no 186: Loss: 0.7567794032610144, accuracy: 0.5133333333333333\n",
      "iteration no 187: Loss: 0.7567740358909217, accuracy: 0.5133333333333333\n",
      "iteration no 188: Loss: 0.7567688324629559, accuracy: 0.5133333333333333\n",
      "iteration no 189: Loss: 0.7567637878209603, accuracy: 0.5133333333333333\n",
      "iteration no 190: Loss: 0.7567588969774077, accuracy: 0.5133333333333333\n",
      "iteration no 191: Loss: 0.7567541551075905, accuracy: 0.5133333333333333\n",
      "iteration no 192: Loss: 0.7567495575440221, accuracy: 0.5133333333333333\n",
      "iteration no 193: Loss: 0.7567450997710449, accuracy: 0.5133333333333333\n",
      "iteration no 194: Loss: 0.7567407774196364, accuracy: 0.5133333333333333\n",
      "iteration no 195: Loss: 0.7567365862624031, accuracy: 0.5133333333333333\n",
      "iteration no 196: Loss: 0.7567325222087584, accuracy: 0.5133333333333333\n",
      "iteration no 197: Loss: 0.7567285813002735, accuracy: 0.5133333333333333\n",
      "iteration no 198: Loss: 0.7567247597061982, accuracy: 0.5133333333333333\n",
      "iteration no 199: Loss: 0.7567210537191417, accuracy: 0.5133333333333333\n",
      "iteration no 200: Loss: 0.7567174597509102, accuracy: 0.5133333333333333\n",
      "iteration no 1: Loss: 1.118652373259734, accuracy: 0.51\n",
      "iteration no 2: Loss: 1.0856888439396923, accuracy: 0.5533333333333333\n",
      "iteration no 3: Loss: 1.0563774156077244, accuracy: 0.5366666666666666\n",
      "iteration no 4: Loss: 1.0302756245660636, accuracy: 0.5333333333333333\n",
      "iteration no 5: Loss: 1.0069960604826764, accuracy: 0.5366666666666666\n",
      "iteration no 6: Loss: 0.9861974992875283, accuracy: 0.52\n",
      "iteration no 7: Loss: 0.9675796190431711, accuracy: 0.5133333333333333\n",
      "iteration no 8: Loss: 0.9508788909756748, accuracy: 0.5133333333333333\n",
      "iteration no 9: Loss: 0.9358648114669368, accuracy: 0.5133333333333333\n",
      "iteration no 10: Loss: 0.9223362783815692, accuracy: 0.5166666666666667\n",
      "iteration no 11: Loss: 0.9101181328073327, accuracy: 0.52\n",
      "iteration no 12: Loss: 0.8990579328164383, accuracy: 0.52\n",
      "iteration no 13: Loss: 0.8890230115591216, accuracy: 0.52\n",
      "iteration no 14: Loss: 0.8798978446052609, accuracy: 0.52\n",
      "iteration no 15: Loss: 0.8715817276310239, accuracy: 0.52\n",
      "iteration no 16: Loss: 0.8639867493304848, accuracy: 0.52\n",
      "iteration no 17: Loss: 0.8570360352066192, accuracy: 0.5166666666666667\n",
      "iteration no 18: Loss: 0.8506622338237539, accuracy: 0.5166666666666667\n",
      "iteration no 19: Loss: 0.8448062163941767, accuracy: 0.5166666666666667\n",
      "iteration no 20: Loss: 0.8394159618475693, accuracy: 0.52\n",
      "iteration no 21: Loss: 0.8344456018455968, accuracy: 0.52\n",
      "iteration no 22: Loss: 0.8298546029498635, accuracy: 0.52\n",
      "iteration no 23: Loss: 0.8256070659665525, accuracy: 0.5166666666666667\n",
      "iteration no 24: Loss: 0.8216711251731462, accuracy: 0.5166666666666667\n",
      "iteration no 25: Loss: 0.8180184325809216, accuracy: 0.5166666666666667\n",
      "iteration no 26: Loss: 0.814623714562095, accuracy: 0.5166666666666667\n",
      "iteration no 27: Loss: 0.811464390068614, accuracy: 0.5166666666666667\n",
      "iteration no 28: Loss: 0.8085202413058045, accuracy: 0.5166666666666667\n",
      "iteration no 29: Loss: 0.805773129122702, accuracy: 0.5166666666666667\n",
      "iteration no 30: Loss: 0.8032067465694931, accuracy: 0.5166666666666667\n",
      "iteration no 31: Loss: 0.8008064050786599, accuracy: 0.5166666666666667\n",
      "iteration no 32: Loss: 0.7985588485759921, accuracy: 0.5166666666666667\n",
      "iteration no 33: Loss: 0.7964520915438575, accuracy: 0.5166666666666667\n",
      "iteration no 34: Loss: 0.7944752776624889, accuracy: 0.5166666666666667\n",
      "iteration no 35: Loss: 0.7926185561632425, accuracy: 0.5166666666666667\n",
      "iteration no 36: Loss: 0.7908729734559354, accuracy: 0.5166666666666667\n",
      "iteration no 37: Loss: 0.7892303779533212, accuracy: 0.5166666666666667\n",
      "iteration no 38: Loss: 0.787683336320334, accuracy: 0.5166666666666667\n",
      "iteration no 39: Loss: 0.7862250596330214, accuracy: 0.5166666666666667\n",
      "iteration no 40: Loss: 0.78484933814972, accuracy: 0.5166666666666667\n",
      "iteration no 41: Loss: 0.7835504835813665, accuracy: 0.5133333333333333\n",
      "iteration no 42: Loss: 0.7823232779042356, accuracy: 0.5133333333333333\n",
      "iteration no 43: Loss: 0.7811629278912932, accuracy: 0.5133333333333333\n",
      "iteration no 44: Loss: 0.7800650246514544, accuracy: 0.5133333333333333\n",
      "iteration no 45: Loss: 0.7790255075624893, accuracy: 0.5133333333333333\n",
      "iteration no 46: Loss: 0.7780406320656859, accuracy: 0.5133333333333333\n",
      "iteration no 47: Loss: 0.777106940860854, accuracy: 0.5133333333333333\n",
      "iteration no 48: Loss: 0.7762212381006792, accuracy: 0.5133333333333333\n",
      "iteration no 49: Loss: 0.7753805662352856, accuracy: 0.5133333333333333\n",
      "iteration no 50: Loss: 0.7745821852025083, accuracy: 0.5133333333333333\n",
      "iteration no 51: Loss: 0.7738235536978133, accuracy: 0.5133333333333333\n",
      "iteration no 52: Loss: 0.7731023122910015, accuracy: 0.5133333333333333\n",
      "iteration no 53: Loss: 0.7724162681855342, accuracy: 0.5133333333333333\n",
      "iteration no 54: Loss: 0.7717633814411851, accuracy: 0.5166666666666667\n",
      "iteration no 55: Loss: 0.7711417525022938, accuracy: 0.5166666666666667\n",
      "iteration no 56: Loss: 0.7705496108926617, accuracy: 0.5166666666666667\n",
      "iteration no 57: Loss: 0.7699853049544461, accuracy: 0.5166666666666667\n",
      "iteration no 58: Loss: 0.7694472925226613, accuracy: 0.5166666666666667\n",
      "iteration no 59: Loss: 0.7689341324393286, accuracy: 0.5166666666666667\n",
      "iteration no 60: Loss: 0.7684444768222026, accuracy: 0.5166666666666667\n",
      "iteration no 61: Loss: 0.7679770640125365, accuracy: 0.5166666666666667\n",
      "iteration no 62: Loss: 0.7675307121347301, accuracy: 0.5166666666666667\n",
      "iteration no 63: Loss: 0.7671043132080435, accuracy: 0.5166666666666667\n",
      "iteration no 64: Loss: 0.7666968277570585, accuracy: 0.5166666666666667\n",
      "iteration no 65: Loss: 0.7663072798732623, accuracy: 0.5166666666666667\n",
      "iteration no 66: Loss: 0.7659347526851858, accuracy: 0.5166666666666667\n",
      "iteration no 67: Loss: 0.7655783841989707, accuracy: 0.5166666666666667\n",
      "iteration no 68: Loss: 0.7652373634752029, accuracy: 0.5166666666666667\n",
      "iteration no 69: Loss: 0.7649109271113371, accuracy: 0.5166666666666667\n",
      "iteration no 70: Loss: 0.7645983560021504, accuracy: 0.5166666666666667\n",
      "iteration no 71: Loss: 0.7642989723534251, accuracy: 0.5166666666666667\n",
      "iteration no 72: Loss: 0.7640121369265153, accuracy: 0.5166666666666667\n",
      "iteration no 73: Loss: 0.7637372464936478, accuracy: 0.5166666666666667\n",
      "iteration no 74: Loss: 0.7634737314857625, accuracy: 0.5166666666666667\n",
      "iteration no 75: Loss: 0.763221053816442, accuracy: 0.5166666666666667\n",
      "iteration no 76: Loss: 0.7629787048670476, accuracy: 0.5166666666666667\n",
      "iteration no 77: Loss: 0.7627462036195736, accuracy: 0.5166666666666667\n",
      "iteration no 78: Loss: 0.7625230949249956, accuracy: 0.5166666666666667\n",
      "iteration no 79: Loss: 0.7623089478960035, accuracy: 0.5166666666666667\n",
      "iteration no 80: Loss: 0.7621033544140361, accuracy: 0.5133333333333333\n",
      "iteration no 81: Loss: 0.761905927741433, accuracy: 0.5133333333333333\n",
      "iteration no 82: Loss: 0.761716301230352, accuracy: 0.5133333333333333\n",
      "iteration no 83: Loss: 0.7615341271208298, accuracy: 0.5133333333333333\n",
      "iteration no 84: Loss: 0.7613590754210376, accuracy: 0.5133333333333333\n",
      "iteration no 85: Loss: 0.7611908328633894, accuracy: 0.5133333333333333\n",
      "iteration no 86: Loss: 0.7610291019306964, accuracy: 0.5133333333333333\n",
      "iteration no 87: Loss: 0.7608735999470678, accuracy: 0.5133333333333333\n",
      "iteration no 88: Loss: 0.7607240582286955, accuracy: 0.5133333333333333\n",
      "iteration no 89: Loss: 0.7605802212900734, accuracy: 0.5133333333333333\n",
      "iteration no 90: Loss: 0.7604418461015678, accuracy: 0.5133333333333333\n",
      "iteration no 91: Loss: 0.7603087013945897, accuracy: 0.5133333333333333\n",
      "iteration no 92: Loss: 0.7601805670109285, accuracy: 0.5133333333333333\n",
      "iteration no 93: Loss: 0.7600572332930787, accuracy: 0.5133333333333333\n",
      "iteration no 94: Loss: 0.75993850051265, accuracy: 0.5133333333333333\n",
      "iteration no 95: Loss: 0.7598241783341763, accuracy: 0.5133333333333333\n",
      "iteration no 96: Loss: 0.7597140853118538, accuracy: 0.5133333333333333\n",
      "iteration no 97: Loss: 0.759608048416929, accuracy: 0.5133333333333333\n",
      "iteration no 98: Loss: 0.7595059025936322, accuracy: 0.5133333333333333\n",
      "iteration no 99: Loss: 0.7594074903417152, accuracy: 0.5133333333333333\n",
      "iteration no 100: Loss: 0.7593126613237975, accuracy: 0.5133333333333333\n",
      "iteration no 101: Loss: 0.759221271995861, accuracy: 0.5133333333333333\n",
      "iteration no 102: Loss: 0.7591331852593568, accuracy: 0.5166666666666667\n",
      "iteration no 103: Loss: 0.7590482701335007, accuracy: 0.5166666666666667\n",
      "iteration no 104: Loss: 0.7589664014464408, accuracy: 0.5166666666666667\n",
      "iteration no 105: Loss: 0.7588874595440721, accuracy: 0.5166666666666667\n",
      "iteration no 106: Loss: 0.7588113300153664, accuracy: 0.5166666666666667\n",
      "iteration no 107: Loss: 0.758737903433163, accuracy: 0.5166666666666667\n",
      "iteration no 108: Loss: 0.7586670751094412, accuracy: 0.5166666666666667\n",
      "iteration no 109: Loss: 0.7585987448641677, accuracy: 0.5166666666666667\n",
      "iteration no 110: Loss: 0.7585328168068706, accuracy: 0.5166666666666667\n",
      "iteration no 111: Loss: 0.7584691991301546, accuracy: 0.5166666666666667\n",
      "iteration no 112: Loss: 0.7584078039144238, accuracy: 0.5166666666666667\n",
      "iteration no 113: Loss: 0.7583485469431287, accuracy: 0.5166666666666667\n",
      "iteration no 114: Loss: 0.7582913475279026, accuracy: 0.5166666666666667\n",
      "iteration no 115: Loss: 0.7582361283429916, accuracy: 0.5166666666666667\n",
      "iteration no 116: Loss: 0.758182815268426, accuracy: 0.5166666666666667\n",
      "iteration no 117: Loss: 0.7581313372414142, accuracy: 0.5166666666666667\n",
      "iteration no 118: Loss: 0.7580816261154787, accuracy: 0.5166666666666667\n",
      "iteration no 119: Loss: 0.7580336165268766, accuracy: 0.5166666666666667\n",
      "iteration no 120: Loss: 0.757987245767891, accuracy: 0.5166666666666667\n",
      "iteration no 121: Loss: 0.7579424536665897, accuracy: 0.5166666666666667\n",
      "iteration no 122: Loss: 0.7578991824726874, accuracy: 0.5166666666666667\n",
      "iteration no 123: Loss: 0.7578573767491605, accuracy: 0.5166666666666667\n",
      "iteration no 124: Loss: 0.7578169832692945, accuracy: 0.5166666666666667\n",
      "iteration no 125: Loss: 0.7577779509188556, accuracy: 0.5166666666666667\n",
      "iteration no 126: Loss: 0.7577402306031044, accuracy: 0.5166666666666667\n",
      "iteration no 127: Loss: 0.7577037751583818, accuracy: 0.5166666666666667\n",
      "iteration no 128: Loss: 0.7576685392680188, accuracy: 0.5166666666666667\n",
      "iteration no 129: Loss: 0.7576344793823295, accuracy: 0.5166666666666667\n",
      "iteration no 130: Loss: 0.7576015536424717, accuracy: 0.5133333333333333\n",
      "iteration no 131: Loss: 0.7575697218079624, accuracy: 0.5133333333333333\n",
      "iteration no 132: Loss: 0.7575389451876542, accuracy: 0.5133333333333333\n",
      "iteration no 133: Loss: 0.7575091865739875, accuracy: 0.5133333333333333\n",
      "iteration no 134: Loss: 0.7574804101803473, accuracy: 0.5133333333333333\n",
      "iteration no 135: Loss: 0.7574525815813585, accuracy: 0.5133333333333333\n",
      "iteration no 136: Loss: 0.757425667655969, accuracy: 0.5133333333333333\n",
      "iteration no 137: Loss: 0.7573996365331725, accuracy: 0.5133333333333333\n",
      "iteration no 138: Loss: 0.7573744575402397, accuracy: 0.5133333333333333\n",
      "iteration no 139: Loss: 0.7573501011533229, accuracy: 0.5133333333333333\n",
      "iteration no 140: Loss: 0.757326538950318, accuracy: 0.5133333333333333\n",
      "iteration no 141: Loss: 0.7573037435658678, accuracy: 0.5133333333333333\n",
      "iteration no 142: Loss: 0.7572816886483962, accuracy: 0.5133333333333333\n",
      "iteration no 143: Loss: 0.7572603488190769, accuracy: 0.5133333333333333\n",
      "iteration no 144: Loss: 0.7572396996326327, accuracy: 0.5133333333333333\n",
      "iteration no 145: Loss: 0.757219717539883, accuracy: 0.5133333333333333\n",
      "iteration no 146: Loss: 0.7572003798519454, accuracy: 0.5133333333333333\n",
      "iteration no 147: Loss: 0.7571816647060156, accuracy: 0.5133333333333333\n",
      "iteration no 148: Loss: 0.7571635510326469, accuracy: 0.5133333333333333\n",
      "iteration no 149: Loss: 0.7571460185244563, accuracy: 0.5133333333333333\n",
      "iteration no 150: Loss: 0.7571290476061886, accuracy: 0.5133333333333333\n",
      "iteration no 151: Loss: 0.7571126194060728, accuracy: 0.5133333333333333\n",
      "iteration no 152: Loss: 0.7570967157284102, accuracy: 0.5133333333333333\n",
      "iteration no 153: Loss: 0.7570813190273361, accuracy: 0.5133333333333333\n",
      "iteration no 154: Loss: 0.757066412381696, accuracy: 0.5133333333333333\n",
      "iteration no 155: Loss: 0.7570519794709891, accuracy: 0.5133333333333333\n",
      "iteration no 156: Loss: 0.7570380045523264, accuracy: 0.5133333333333333\n",
      "iteration no 157: Loss: 0.7570244724383544, accuracy: 0.5133333333333333\n",
      "iteration no 158: Loss: 0.7570113684761048, accuracy: 0.5133333333333333\n",
      "iteration no 159: Loss: 0.7569986785267224, accuracy: 0.5133333333333333\n",
      "iteration no 160: Loss: 0.7569863889460332, accuracy: 0.5133333333333333\n",
      "iteration no 161: Loss: 0.756974486565916, accuracy: 0.5133333333333333\n",
      "iteration no 162: Loss: 0.7569629586764364, accuracy: 0.5133333333333333\n",
      "iteration no 163: Loss: 0.7569517930087154, accuracy: 0.5133333333333333\n",
      "iteration no 164: Loss: 0.7569409777184918, accuracy: 0.5133333333333333\n",
      "iteration no 165: Loss: 0.7569305013703547, accuracy: 0.5133333333333333\n",
      "iteration no 166: Loss: 0.7569203529226107, accuracy: 0.5133333333333333\n",
      "iteration no 167: Loss: 0.7569105217127616, accuracy: 0.5133333333333333\n",
      "iteration no 168: Loss: 0.7569009974435627, accuracy: 0.5133333333333333\n",
      "iteration no 169: Loss: 0.7568917701696378, accuracy: 0.5133333333333333\n",
      "iteration no 170: Loss: 0.7568828302846272, accuracy: 0.5133333333333333\n",
      "iteration no 171: Loss: 0.7568741685088434, accuracy: 0.5133333333333333\n",
      "iteration no 172: Loss: 0.7568657758774138, accuracy: 0.5133333333333333\n",
      "iteration no 173: Loss: 0.7568576437288894, accuracy: 0.5133333333333333\n",
      "iteration no 174: Loss: 0.7568497636943006, accuracy: 0.5133333333333333\n",
      "iteration no 175: Loss: 0.7568421276866371, accuracy: 0.5133333333333333\n",
      "iteration no 176: Loss: 0.7568347278907388, accuracy: 0.5133333333333333\n",
      "iteration no 177: Loss: 0.756827556753577, accuracy: 0.5133333333333333\n",
      "iteration no 178: Loss: 0.7568206069749106, accuracy: 0.5133333333333333\n",
      "iteration no 179: Loss: 0.7568138714983002, accuracy: 0.5133333333333333\n",
      "iteration no 180: Loss: 0.7568073435024687, accuracy: 0.5133333333333333\n",
      "iteration no 181: Loss: 0.7568010163929888, accuracy: 0.5133333333333333\n",
      "iteration no 182: Loss: 0.7567948837942886, accuracy: 0.5133333333333333\n",
      "iteration no 183: Loss: 0.756788939541958, accuracy: 0.5133333333333333\n",
      "iteration no 184: Loss: 0.7567831776753484, accuracy: 0.5133333333333333\n",
      "iteration no 185: Loss: 0.7567775924304473, accuracy: 0.5133333333333333\n",
      "iteration no 186: Loss: 0.7567721782330236, accuracy: 0.5133333333333333\n",
      "iteration no 187: Loss: 0.7567669296920271, accuracy: 0.5133333333333333\n",
      "iteration no 188: Loss: 0.7567618415932336, accuracy: 0.5133333333333333\n",
      "iteration no 189: Loss: 0.7567569088931276, accuracy: 0.5133333333333333\n",
      "iteration no 190: Loss: 0.75675212671301, accuracy: 0.5133333333333333\n",
      "iteration no 191: Loss: 0.7567474903333243, accuracy: 0.5133333333333333\n",
      "iteration no 192: Loss: 0.7567429951881904, accuracy: 0.5133333333333333\n",
      "iteration no 193: Loss: 0.7567386368601395, accuracy: 0.5133333333333333\n",
      "iteration no 194: Loss: 0.7567344110750416, accuracy: 0.5133333333333333\n",
      "iteration no 195: Loss: 0.7567303136972178, accuracy: 0.5133333333333333\n",
      "iteration no 196: Loss: 0.7567263407247289, accuracy: 0.5133333333333333\n",
      "iteration no 197: Loss: 0.7567224882848373, accuracy: 0.5133333333333333\n",
      "iteration no 198: Loss: 0.7567187526296305, accuracy: 0.5133333333333333\n",
      "iteration no 199: Loss: 0.756715130131803, accuracy: 0.5133333333333333\n",
      "iteration no 200: Loss: 0.7567116172805901, accuracy: 0.5133333333333333\n",
      "iteration no 1: Loss: 1.09084815948843, accuracy: 0.5133333333333333\n",
      "iteration no 2: Loss: 1.0606996886740196, accuracy: 0.5033333333333333\n",
      "iteration no 3: Loss: 1.0338961430937714, accuracy: 0.53\n",
      "iteration no 4: Loss: 1.010026958490366, accuracy: 0.5266666666666666\n",
      "iteration no 5: Loss: 0.9887338668628741, accuracy: 0.5366666666666666\n",
      "iteration no 6: Loss: 0.9697023699827146, accuracy: 0.5366666666666666\n",
      "iteration no 7: Loss: 0.9526565556063138, accuracy: 0.5266666666666666\n",
      "iteration no 8: Loss: 0.9373550324263921, accuracy: 0.5233333333333333\n",
      "iteration no 9: Loss: 0.9235872245783162, accuracy: 0.52\n",
      "iteration no 10: Loss: 0.9111698483155452, accuracy: 0.52\n",
      "iteration no 11: Loss: 0.8999435875920292, accuracy: 0.52\n",
      "iteration no 12: Loss: 0.8897700235980298, accuracy: 0.5233333333333333\n",
      "iteration no 13: Loss: 0.8805288588892222, accuracy: 0.5233333333333333\n",
      "iteration no 14: Loss: 0.8721154517020573, accuracy: 0.5233333333333333\n",
      "iteration no 15: Loss: 0.8644386549865403, accuracy: 0.5233333333333333\n",
      "iteration no 16: Loss: 0.8574189408709143, accuracy: 0.5233333333333333\n",
      "iteration no 17: Loss: 0.8509867838965108, accuracy: 0.5233333333333333\n",
      "iteration no 18: Loss: 0.8450812736147988, accuracy: 0.5233333333333333\n",
      "iteration no 19: Loss: 0.8396489273285692, accuracy: 0.5233333333333333\n",
      "iteration no 20: Loss: 0.8346426756109184, accuracy: 0.52\n",
      "iteration no 21: Loss: 0.8300209958908531, accuracy: 0.52\n",
      "iteration no 22: Loss: 0.8257471723180126, accuracy: 0.52\n",
      "iteration no 23: Loss: 0.8217886630013009, accuracy: 0.52\n",
      "iteration no 24: Loss: 0.8181165583937061, accuracy: 0.52\n",
      "iteration no 25: Loss: 0.8147051169951034, accuracy: 0.52\n",
      "iteration no 26: Loss: 0.8115313666462023, accuracy: 0.52\n",
      "iteration no 27: Loss: 0.8085747614990859, accuracy: 0.52\n",
      "iteration no 28: Loss: 0.8058168862966335, accuracy: 0.52\n",
      "iteration no 29: Loss: 0.8032412009042201, accuracy: 0.52\n",
      "iteration no 30: Loss: 0.8008328191431804, accuracy: 0.52\n",
      "iteration no 31: Loss: 0.7985783169059156, accuracy: 0.52\n",
      "iteration no 32: Loss: 0.7964655653137404, accuracy: 0.52\n",
      "iteration no 33: Loss: 0.7944835853339695, accuracy: 0.52\n",
      "iteration no 34: Loss: 0.7926224208225319, accuracy: 0.52\n",
      "iteration no 35: Loss: 0.7908730274197274, accuracy: 0.52\n",
      "iteration no 36: Loss: 0.7892271751141576, accuracy: 0.52\n",
      "iteration no 37: Loss: 0.7876773626155527, accuracy: 0.5166666666666667\n",
      "iteration no 38: Loss: 0.7862167419513475, accuracy: 0.5166666666666667\n",
      "iteration no 39: Loss: 0.784839051932969, accuracy: 0.5166666666666667\n",
      "iteration no 40: Loss: 0.7835385593329093, accuracy: 0.5166666666666667\n",
      "iteration no 41: Loss: 0.7823100067787057, accuracy: 0.5166666666666667\n",
      "iteration no 42: Loss: 0.7811485665097639, accuracy: 0.5166666666666667\n",
      "iteration no 43: Loss: 0.780049799261666, accuracy: 0.5166666666666667\n",
      "iteration no 44: Loss: 0.7790096176435417, accuracy: 0.5166666666666667\n",
      "iteration no 45: Loss: 0.7780242534600919, accuracy: 0.5166666666666667\n",
      "iteration no 46: Loss: 0.777090228503274, accuracy: 0.5166666666666667\n",
      "iteration no 47: Loss: 0.7762043284014662, accuracy: 0.5166666666666667\n",
      "iteration no 48: Loss: 0.7753635791677357, accuracy: 0.5133333333333333\n",
      "iteration no 49: Loss: 0.7745652261350542, accuracy: 0.5133333333333333\n",
      "iteration no 50: Loss: 0.7738067150060426, accuracy: 0.5133333333333333\n",
      "iteration no 51: Loss: 0.7730856747790876, accuracy: 0.5133333333333333\n",
      "iteration no 52: Loss: 0.7723999023422503, accuracy: 0.5133333333333333\n",
      "iteration no 53: Loss: 0.7717473485519681, accuracy: 0.5133333333333333\n",
      "iteration no 54: Loss: 0.7711261056357258, accuracy: 0.5133333333333333\n",
      "iteration no 55: Loss: 0.7705343957771132, accuracy: 0.5133333333333333\n",
      "iteration no 56: Loss: 0.7699705607584312, accuracy: 0.5133333333333333\n",
      "iteration no 57: Loss: 0.769433052550579, accuracy: 0.5133333333333333\n",
      "iteration no 58: Loss: 0.7689204247526872, accuracy: 0.5133333333333333\n",
      "iteration no 59: Loss: 0.7684313247950757, accuracy: 0.5133333333333333\n",
      "iteration no 60: Loss: 0.767964486828855, accuracy: 0.5133333333333333\n",
      "iteration no 61: Loss: 0.7675187252340248, accuracy: 0.5166666666666667\n",
      "iteration no 62: Loss: 0.7670929286854249, accuracy: 0.5166666666666667\n",
      "iteration no 63: Loss: 0.7666860547224865, accuracy: 0.5166666666666667\n",
      "iteration no 64: Loss: 0.7662971247745444, accuracy: 0.5166666666666667\n",
      "iteration no 65: Loss: 0.7659252195985985, accuracy: 0.5166666666666667\n",
      "iteration no 66: Loss: 0.7655694750909436, accuracy: 0.5166666666666667\n",
      "iteration no 67: Loss: 0.7652290784380916, accuracy: 0.5166666666666667\n",
      "iteration no 68: Loss: 0.7649032645759682, accuracy: 0.5166666666666667\n",
      "iteration no 69: Loss: 0.7645913129295148, accuracy: 0.5166666666666667\n",
      "iteration no 70: Loss: 0.7642925444076243, accuracy: 0.5166666666666667\n",
      "iteration no 71: Loss: 0.7640063186308426, accuracy: 0.5166666666666667\n",
      "iteration no 72: Loss: 0.7637320313714702, accuracy: 0.5166666666666667\n",
      "iteration no 73: Loss: 0.7634691121876976, accuracy: 0.5166666666666667\n",
      "iteration no 74: Loss: 0.7632170222351612, accuracy: 0.5166666666666667\n",
      "iteration no 75: Loss: 0.7629752522408999, accuracy: 0.5166666666666667\n",
      "iteration no 76: Loss: 0.7627433206261002, accuracy: 0.5166666666666667\n",
      "iteration no 77: Loss: 0.7625207717652933, accuracy: 0.5166666666666667\n",
      "iteration no 78: Loss: 0.762307174370805, accuracy: 0.5166666666666667\n",
      "iteration no 79: Loss: 0.7621021199922776, accuracy: 0.5133333333333333\n",
      "iteration no 80: Loss: 0.7619052216220178, accuracy: 0.5133333333333333\n",
      "iteration no 81: Loss: 0.7617161123977338, accuracy: 0.5133333333333333\n",
      "iteration no 82: Loss: 0.7615344443949917, accuracy: 0.5133333333333333\n",
      "iteration no 83: Loss: 0.7613598875023824, accuracy: 0.5133333333333333\n",
      "iteration no 84: Loss: 0.7611921283730049, accuracy: 0.5133333333333333\n",
      "iteration no 85: Loss: 0.7610308694464232, accuracy: 0.5133333333333333\n",
      "iteration no 86: Loss: 0.7608758280357494, accuracy: 0.5133333333333333\n",
      "iteration no 87: Loss: 0.7607267354749614, accuracy: 0.5133333333333333\n",
      "iteration no 88: Loss: 0.7605833363219697, accuracy: 0.5133333333333333\n",
      "iteration no 89: Loss: 0.7604453876133231, accuracy: 0.5133333333333333\n",
      "iteration no 90: Loss: 0.7603126581667767, accuracy: 0.5133333333333333\n",
      "iteration no 91: Loss: 0.7601849279282604, accuracy: 0.5133333333333333\n",
      "iteration no 92: Loss: 0.7600619873600555, accuracy: 0.5133333333333333\n",
      "iteration no 93: Loss: 0.7599436368672507, accuracy: 0.5133333333333333\n",
      "iteration no 94: Loss: 0.7598296862597799, accuracy: 0.5133333333333333\n",
      "iteration no 95: Loss: 0.7597199542475495, accuracy: 0.5133333333333333\n",
      "iteration no 96: Loss: 0.7596142679663654, accuracy: 0.5133333333333333\n",
      "iteration no 97: Loss: 0.7595124625325442, accuracy: 0.5133333333333333\n",
      "iteration no 98: Loss: 0.7594143806242488, accuracy: 0.5133333333333333\n",
      "iteration no 99: Loss: 0.7593198720877505, accuracy: 0.5133333333333333\n",
      "iteration no 100: Loss: 0.7592287935669385, accuracy: 0.5133333333333333\n",
      "iteration no 101: Loss: 0.75914100815454, accuracy: 0.5133333333333333\n",
      "iteration no 102: Loss: 0.7590563850636133, accuracy: 0.5133333333333333\n",
      "iteration no 103: Loss: 0.7589747993179927, accuracy: 0.5133333333333333\n",
      "iteration no 104: Loss: 0.758896131460452, accuracy: 0.5166666666666667\n",
      "iteration no 105: Loss: 0.758820267277449, accuracy: 0.5166666666666667\n",
      "iteration no 106: Loss: 0.7587470975393916, accuracy: 0.5166666666666667\n",
      "iteration no 107: Loss: 0.7586765177554412, accuracy: 0.5166666666666667\n",
      "iteration no 108: Loss: 0.7586084279419381, accuracy: 0.5166666666666667\n",
      "iteration no 109: Loss: 0.7585427324036037, accuracy: 0.5166666666666667\n",
      "iteration no 110: Loss: 0.758479339526722, accuracy: 0.5166666666666667\n",
      "iteration no 111: Loss: 0.7584181615835686, accuracy: 0.5166666666666667\n",
      "iteration no 112: Loss: 0.7583591145473989, accuracy: 0.5166666666666667\n",
      "iteration no 113: Loss: 0.758302117917356, accuracy: 0.5166666666666667\n",
      "iteration no 114: Loss: 0.7582470945527029, accuracy: 0.5166666666666667\n",
      "iteration no 115: Loss: 0.7581939705158203, accuracy: 0.5166666666666667\n",
      "iteration no 116: Loss: 0.7581426749234544, accuracy: 0.5166666666666667\n",
      "iteration no 117: Loss: 0.7580931398057228, accuracy: 0.5166666666666667\n",
      "iteration no 118: Loss: 0.7580452999724323, accuracy: 0.5166666666666667\n",
      "iteration no 119: Loss: 0.7579990928862781, accuracy: 0.5166666666666667\n",
      "iteration no 120: Loss: 0.7579544585425321, accuracy: 0.5166666666666667\n",
      "iteration no 121: Loss: 0.7579113393548469, accuracy: 0.5166666666666667\n",
      "iteration no 122: Loss: 0.7578696800468261, accuracy: 0.5166666666666667\n",
      "iteration no 123: Loss: 0.7578294275490408, accuracy: 0.5166666666666667\n",
      "iteration no 124: Loss: 0.7577905309011785, accuracy: 0.5166666666666667\n",
      "iteration no 125: Loss: 0.7577529411590465, accuracy: 0.5133333333333333\n",
      "iteration no 126: Loss: 0.7577166113061556, accuracy: 0.5133333333333333\n",
      "iteration no 127: Loss: 0.7576814961696349, accuracy: 0.5133333333333333\n",
      "iteration no 128: Loss: 0.7576475523402377, accuracy: 0.5133333333333333\n",
      "iteration no 129: Loss: 0.7576147380962219, accuracy: 0.5133333333333333\n",
      "iteration no 130: Loss: 0.7575830133308893, accuracy: 0.5133333333333333\n",
      "iteration no 131: Loss: 0.7575523394835916, accuracy: 0.5133333333333333\n",
      "iteration no 132: Loss: 0.7575226794740189, accuracy: 0.5133333333333333\n",
      "iteration no 133: Loss: 0.7574939976395926, accuracy: 0.5133333333333333\n",
      "iteration no 134: Loss: 0.7574662596758033, accuracy: 0.5133333333333333\n",
      "iteration no 135: Loss: 0.757439432579338, accuracy: 0.5133333333333333\n",
      "iteration no 136: Loss: 0.7574134845938517, accuracy: 0.5133333333333333\n",
      "iteration no 137: Loss: 0.7573883851582459, accuracy: 0.5133333333333333\n",
      "iteration no 138: Loss: 0.7573641048573259, accuracy: 0.5133333333333333\n",
      "iteration no 139: Loss: 0.7573406153747161, accuracy: 0.5133333333333333\n",
      "iteration no 140: Loss: 0.7573178894479157, accuracy: 0.5133333333333333\n",
      "iteration no 141: Loss: 0.7572959008253906, accuracy: 0.5133333333333333\n",
      "iteration no 142: Loss: 0.7572746242255961, accuracy: 0.5133333333333333\n",
      "iteration no 143: Loss: 0.7572540352978349, accuracy: 0.5133333333333333\n",
      "iteration no 144: Loss: 0.7572341105848589, accuracy: 0.5133333333333333\n",
      "iteration no 145: Loss: 0.7572148274871309, accuracy: 0.5133333333333333\n",
      "iteration no 146: Loss: 0.7571961642286612, accuracy: 0.5133333333333333\n",
      "iteration no 147: Loss: 0.7571780998243435, accuracy: 0.5133333333333333\n",
      "iteration no 148: Loss: 0.7571606140487191, accuracy: 0.5133333333333333\n",
      "iteration no 149: Loss: 0.7571436874060977, accuracy: 0.5133333333333333\n",
      "iteration no 150: Loss: 0.7571273011019706, accuracy: 0.5133333333333333\n",
      "iteration no 151: Loss: 0.7571114370156546, accuracy: 0.5133333333333333\n",
      "iteration no 152: Loss: 0.7570960776741092, accuracy: 0.5133333333333333\n",
      "iteration no 153: Loss: 0.757081206226868, accuracy: 0.5133333333333333\n",
      "iteration no 154: Loss: 0.7570668064220364, accuracy: 0.5133333333333333\n",
      "iteration no 155: Loss: 0.7570528625833027, accuracy: 0.5133333333333333\n",
      "iteration no 156: Loss: 0.7570393595879152, accuracy: 0.5133333333333333\n",
      "iteration no 157: Loss: 0.7570262828455836, accuracy: 0.5133333333333333\n",
      "iteration no 158: Loss: 0.757013618278257, accuracy: 0.5133333333333333\n",
      "iteration no 159: Loss: 0.7570013523007443, accuracy: 0.5133333333333333\n",
      "iteration no 160: Loss: 0.7569894718021322, accuracy: 0.5133333333333333\n",
      "iteration no 161: Loss: 0.7569779641279706, accuracy: 0.5133333333333333\n",
      "iteration no 162: Loss: 0.7569668170631855, accuracy: 0.5133333333333333\n",
      "iteration no 163: Loss: 0.7569560188156906, accuracy: 0.5133333333333333\n",
      "iteration no 164: Loss: 0.7569455580006632, accuracy: 0.5133333333333333\n",
      "iteration no 165: Loss: 0.7569354236254567, accuracy: 0.5133333333333333\n",
      "iteration no 166: Loss: 0.7569256050751221, accuracy: 0.5133333333333333\n",
      "iteration no 167: Loss: 0.7569160920985081, accuracy: 0.5133333333333333\n",
      "iteration no 168: Loss: 0.7569068747949186, accuracy: 0.5133333333333333\n",
      "iteration no 169: Loss: 0.7568979436012998, accuracy: 0.5133333333333333\n",
      "iteration no 170: Loss: 0.7568892892799373, accuracy: 0.5133333333333333\n",
      "iteration no 171: Loss: 0.756880902906636, accuracy: 0.5133333333333333\n",
      "iteration no 172: Loss: 0.7568727758593681, accuracy: 0.5133333333333333\n",
      "iteration no 173: Loss: 0.7568648998073635, accuracy: 0.5133333333333333\n",
      "iteration no 174: Loss: 0.7568572667006279, accuracy: 0.5133333333333333\n",
      "iteration no 175: Loss: 0.7568498687598672, accuracy: 0.5133333333333333\n",
      "iteration no 176: Loss: 0.7568426984668026, accuracy: 0.5133333333333333\n",
      "iteration no 177: Loss: 0.7568357485548611, accuracy: 0.5133333333333333\n",
      "iteration no 178: Loss: 0.7568290120002211, accuracy: 0.5133333333333333\n",
      "iteration no 179: Loss: 0.7568224820132046, accuracy: 0.5133333333333333\n",
      "iteration no 180: Loss: 0.7568161520299951, accuracy: 0.5133333333333333\n",
      "iteration no 181: Loss: 0.7568100157046727, accuracy: 0.5133333333333333\n",
      "iteration no 182: Loss: 0.7568040669015512, accuracy: 0.5133333333333333\n",
      "iteration no 183: Loss: 0.7567982996878029, accuracy: 0.5133333333333333\n",
      "iteration no 184: Loss: 0.7567927083263638, accuracy: 0.5133333333333333\n",
      "iteration no 185: Loss: 0.756787287269103, accuracy: 0.5133333333333333\n",
      "iteration no 186: Loss: 0.756782031150249, accuracy: 0.5133333333333333\n",
      "iteration no 187: Loss: 0.7567769347800604, accuracy: 0.5133333333333333\n",
      "iteration no 188: Loss: 0.7567719931387319, accuracy: 0.5133333333333333\n",
      "iteration no 189: Loss: 0.7567672013705273, accuracy: 0.5133333333333333\n",
      "iteration no 190: Loss: 0.7567625547781268, accuracy: 0.5133333333333333\n",
      "iteration no 191: Loss: 0.7567580488171843, accuracy: 0.5133333333333333\n",
      "iteration no 192: Loss: 0.7567536790910842, accuracy: 0.5133333333333333\n",
      "iteration no 193: Loss: 0.7567494413458893, accuracy: 0.5133333333333333\n",
      "iteration no 194: Loss: 0.7567453314654736, accuracy: 0.5133333333333333\n",
      "iteration no 195: Loss: 0.7567413454668319, accuracy: 0.5133333333333333\n",
      "iteration no 196: Loss: 0.7567374794955587, accuracy: 0.5133333333333333\n",
      "iteration no 197: Loss: 0.7567337298214929, accuracy: 0.5133333333333333\n",
      "iteration no 198: Loss: 0.756730092834516, accuracy: 0.5133333333333333\n",
      "iteration no 199: Loss: 0.7567265650405042, accuracy: 0.5133333333333333\n",
      "iteration no 200: Loss: 0.7567231430574247, accuracy: 0.5166666666666667\n",
      "iteration no 1: Loss: 1.084183273044056, accuracy: 0.5233333333333333\n",
      "iteration no 2: Loss: 1.0551539635800649, accuracy: 0.53\n",
      "iteration no 3: Loss: 1.029293076386116, accuracy: 0.5133333333333333\n",
      "iteration no 4: Loss: 1.0062194898762717, accuracy: 0.5166666666666667\n",
      "iteration no 5: Loss: 0.9855955985225979, accuracy: 0.52\n",
      "iteration no 6: Loss: 0.9671241688852525, accuracy: 0.5066666666666667\n",
      "iteration no 7: Loss: 0.9505447973866064, accuracy: 0.51\n",
      "iteration no 8: Loss: 0.9356301513726479, accuracy: 0.51\n",
      "iteration no 9: Loss: 0.9221822137252157, accuracy: 0.5133333333333333\n",
      "iteration no 10: Loss: 0.910028701358796, accuracy: 0.51\n",
      "iteration no 11: Loss: 0.8990197626141441, accuracy: 0.51\n",
      "iteration no 12: Loss: 0.8890250039461967, accuracy: 0.51\n",
      "iteration no 13: Loss: 0.8799308580419334, accuracy: 0.5133333333333333\n",
      "iteration no 14: Loss: 0.8716382817092336, accuracy: 0.51\n",
      "iteration no 15: Loss: 0.8640607587737169, accuracy: 0.51\n",
      "iteration no 16: Loss: 0.8571225772688723, accuracy: 0.51\n",
      "iteration no 17: Loss: 0.8507573486969917, accuracy: 0.51\n",
      "iteration no 18: Loss: 0.8449067381960985, accuracy: 0.51\n",
      "iteration no 19: Loss: 0.8395193768472728, accuracy: 0.51\n",
      "iteration no 20: Loss: 0.8345499303381975, accuracy: 0.51\n",
      "iteration no 21: Loss: 0.8299583013133052, accuracy: 0.51\n",
      "iteration no 22: Loss: 0.8257089457385798, accuracy: 0.51\n",
      "iteration no 23: Loss: 0.8217702863632502, accuracy: 0.51\n",
      "iteration no 24: Loss: 0.8181142088188599, accuracy: 0.5133333333333333\n",
      "iteration no 25: Loss: 0.8147156280488327, accuracy: 0.5133333333333333\n",
      "iteration no 26: Loss: 0.8115521146224037, accuracy: 0.5133333333333333\n",
      "iteration no 27: Loss: 0.8086035720807954, accuracy: 0.5133333333333333\n",
      "iteration no 28: Loss: 0.8058519578206068, accuracy: 0.5133333333333333\n",
      "iteration no 29: Loss: 0.8032810411697673, accuracy: 0.5133333333333333\n",
      "iteration no 30: Loss: 0.8008761932837964, accuracy: 0.5133333333333333\n",
      "iteration no 31: Loss: 0.798624204310584, accuracy: 0.5166666666666667\n",
      "iteration no 32: Loss: 0.796513123963461, accuracy: 0.5133333333333333\n",
      "iteration no 33: Loss: 0.7945321222250389, accuracy: 0.5133333333333333\n",
      "iteration no 34: Loss: 0.7926713673953125, accuracy: 0.5133333333333333\n",
      "iteration no 35: Loss: 0.7909219191114797, accuracy: 0.5133333333333333\n",
      "iteration no 36: Loss: 0.7892756343161825, accuracy: 0.5133333333333333\n",
      "iteration no 37: Loss: 0.7877250844458504, accuracy: 0.5133333333333333\n",
      "iteration no 38: Loss: 0.7862634823602285, accuracy: 0.5133333333333333\n",
      "iteration no 39: Loss: 0.7848846177453505, accuracy: 0.5133333333333333\n",
      "iteration no 40: Loss: 0.7835827999012752, accuracy: 0.5133333333333333\n",
      "iteration no 41: Loss: 0.7823528069779592, accuracy: 0.5133333333333333\n",
      "iteration no 42: Loss: 0.7811898408519847, accuracy: 0.5133333333333333\n",
      "iteration no 43: Loss: 0.7800894869470629, accuracy: 0.5133333333333333\n",
      "iteration no 44: Loss: 0.7790476783952759, accuracy: 0.5133333333333333\n",
      "iteration no 45: Loss: 0.7780606640164418, accuracy: 0.5133333333333333\n",
      "iteration no 46: Loss: 0.7771249796618385, accuracy: 0.5133333333333333\n",
      "iteration no 47: Loss: 0.7762374225276163, accuracy: 0.5133333333333333\n",
      "iteration no 48: Loss: 0.7753950280939961, accuracy: 0.5133333333333333\n",
      "iteration no 49: Loss: 0.7745950493900755, accuracy: 0.5133333333333333\n",
      "iteration no 50: Loss: 0.7738349383217694, accuracy: 0.5133333333333333\n",
      "iteration no 51: Loss: 0.7731123288329834, accuracy: 0.5133333333333333\n",
      "iteration no 52: Loss: 0.7724250216983213, accuracy: 0.5133333333333333\n",
      "iteration no 53: Loss: 0.7717709707700631, accuracy: 0.5133333333333333\n",
      "iteration no 54: Loss: 0.7711482705233812, accuracy: 0.5133333333333333\n",
      "iteration no 55: Loss: 0.7705551447622249, accuracy: 0.5133333333333333\n",
      "iteration no 56: Loss: 0.7699899363643952, accuracy: 0.5166666666666667\n",
      "iteration no 57: Loss: 0.7694510979583684, accuracy: 0.5166666666666667\n",
      "iteration no 58: Loss: 0.7689371834367033, accuracy: 0.5166666666666667\n",
      "iteration no 59: Loss: 0.7684468402216129, accuracy: 0.5166666666666667\n",
      "iteration no 60: Loss: 0.7679788022077056, accuracy: 0.5166666666666667\n",
      "iteration no 61: Loss: 0.7675318833151706, accuracy: 0.5166666666666667\n",
      "iteration no 62: Loss: 0.7671049715939686, accuracy: 0.5166666666666667\n",
      "iteration no 63: Loss: 0.7666970238259945, accuracy: 0.5166666666666667\n",
      "iteration no 64: Loss: 0.7663070605778404, accuracy: 0.5166666666666667\n",
      "iteration no 65: Loss: 0.7659341616617745, accuracy: 0.5166666666666667\n",
      "iteration no 66: Loss: 0.765577461966984, accuracy: 0.5166666666666667\n",
      "iteration no 67: Loss: 0.7652361476270337, accuracy: 0.5166666666666667\n",
      "iteration no 68: Loss: 0.7649094524929761, accuracy: 0.5166666666666667\n",
      "iteration no 69: Loss: 0.7645966548846225, accuracy: 0.5166666666666667\n",
      "iteration no 70: Loss: 0.7642970745952435, accuracy: 0.5166666666666667\n",
      "iteration no 71: Loss: 0.7640100701273972, accuracy: 0.5166666666666667\n",
      "iteration no 72: Loss: 0.7637350361397715, accuracy: 0.5166666666666667\n",
      "iteration no 73: Loss: 0.7634714010868686, accuracy: 0.5166666666666667\n",
      "iteration no 74: Loss: 0.763218625035103, accuracy: 0.5166666666666667\n",
      "iteration no 75: Loss: 0.762976197640434, accuracy: 0.5166666666666667\n",
      "iteration no 76: Loss: 0.7627436362740571, accuracy: 0.5166666666666667\n",
      "iteration no 77: Loss: 0.7625204842839224, accuracy: 0.5133333333333333\n",
      "iteration no 78: Loss: 0.7623063093809751, accuracy: 0.5133333333333333\n",
      "iteration no 79: Loss: 0.7621007021400253, accuracy: 0.5133333333333333\n",
      "iteration no 80: Loss: 0.7619032746060587, accuracy: 0.5133333333333333\n",
      "iteration no 81: Loss: 0.7617136589976213, accuracy: 0.5133333333333333\n",
      "iteration no 82: Loss: 0.7615315064996535, accuracy: 0.5133333333333333\n",
      "iteration no 83: Loss: 0.7613564861388104, accuracy: 0.5133333333333333\n",
      "iteration no 84: Loss: 0.7611882837349102, accuracy: 0.5133333333333333\n",
      "iteration no 85: Loss: 0.7610266009227029, accuracy: 0.5133333333333333\n",
      "iteration no 86: Loss: 0.7608711542386353, accuracy: 0.5133333333333333\n",
      "iteration no 87: Loss: 0.7607216742677466, accuracy: 0.5133333333333333\n",
      "iteration no 88: Loss: 0.7605779048462308, accuracy: 0.5133333333333333\n",
      "iteration no 89: Loss: 0.7604396023155677, accuracy: 0.5133333333333333\n",
      "iteration no 90: Loss: 0.7603065348244662, accuracy: 0.5133333333333333\n",
      "iteration no 91: Loss: 0.7601784816751658, accuracy: 0.5133333333333333\n",
      "iteration no 92: Loss: 0.7600552327109179, accuracy: 0.5133333333333333\n",
      "iteration no 93: Loss: 0.7599365877417258, accuracy: 0.5133333333333333\n",
      "iteration no 94: Loss: 0.7598223560056514, accuracy: 0.5133333333333333\n",
      "iteration no 95: Loss: 0.7597123556632059, accuracy: 0.5133333333333333\n",
      "iteration no 96: Loss: 0.7596064133225404, accuracy: 0.5133333333333333\n",
      "iteration no 97: Loss: 0.7595043635933179, accuracy: 0.5133333333333333\n",
      "iteration no 98: Loss: 0.7594060486673228, accuracy: 0.5133333333333333\n",
      "iteration no 99: Loss: 0.7593113179240008, accuracy: 0.5133333333333333\n",
      "iteration no 100: Loss: 0.7592200275592625, accuracy: 0.5133333333333333\n",
      "iteration no 101: Loss: 0.7591320402360086, accuracy: 0.5133333333333333\n",
      "iteration no 102: Loss: 0.7590472247549438, accuracy: 0.5166666666666667\n",
      "iteration no 103: Loss: 0.7589654557443607, accuracy: 0.5166666666666667\n",
      "iteration no 104: Loss: 0.7588866133676581, accuracy: 0.5166666666666667\n",
      "iteration no 105: Loss: 0.7588105830474606, accuracy: 0.5166666666666667\n",
      "iteration no 106: Loss: 0.7587372552052764, accuracy: 0.5166666666666667\n",
      "iteration no 107: Loss: 0.7586665250157114, accuracy: 0.5166666666666667\n",
      "iteration no 108: Loss: 0.7585982921743274, accuracy: 0.5166666666666667\n",
      "iteration no 109: Loss: 0.7585324606782924, accuracy: 0.5166666666666667\n",
      "iteration no 110: Loss: 0.7584689386190312, accuracy: 0.5166666666666667\n",
      "iteration no 111: Loss: 0.7584076379861421, accuracy: 0.5166666666666667\n",
      "iteration no 112: Loss: 0.7583484744818907, accuracy: 0.5166666666666667\n",
      "iteration no 113: Loss: 0.7582913673456423, accuracy: 0.5166666666666667\n",
      "iteration no 114: Loss: 0.7582362391876356, accuracy: 0.5166666666666667\n",
      "iteration no 115: Loss: 0.7581830158315408, accuracy: 0.5166666666666667\n",
      "iteration no 116: Loss: 0.7581316261652815, accuracy: 0.5166666666666667\n",
      "iteration no 117: Loss: 0.7580820019996359, accuracy: 0.5166666666666667\n",
      "iteration no 118: Loss: 0.7580340779341617, accuracy: 0.5166666666666667\n",
      "iteration no 119: Loss: 0.7579877912300195, accuracy: 0.5166666666666667\n",
      "iteration no 120: Loss: 0.7579430816893006, accuracy: 0.5166666666666667\n",
      "iteration no 121: Loss: 0.7578998915404848, accuracy: 0.5166666666666667\n",
      "iteration no 122: Loss: 0.7578581653296783, accuracy: 0.5166666666666667\n",
      "iteration no 123: Loss: 0.7578178498173103, accuracy: 0.5166666666666667\n",
      "iteration no 124: Loss: 0.7577788938799779, accuracy: 0.5166666666666667\n",
      "iteration no 125: Loss: 0.7577412484171554, accuracy: 0.5166666666666667\n",
      "iteration no 126: Loss: 0.7577048662624962, accuracy: 0.5166666666666667\n",
      "iteration no 127: Loss: 0.7576697020994783, accuracy: 0.5166666666666667\n",
      "iteration no 128: Loss: 0.7576357123811529, accuracy: 0.5166666666666667\n",
      "iteration no 129: Loss: 0.7576028552537756, accuracy: 0.5166666666666667\n",
      "iteration no 130: Loss: 0.757571090484109, accuracy: 0.5166666666666667\n",
      "iteration no 131: Loss: 0.7575403793902021, accuracy: 0.5166666666666667\n",
      "iteration no 132: Loss: 0.7575106847754586, accuracy: 0.5166666666666667\n",
      "iteration no 133: Loss: 0.7574819708658233, accuracy: 0.5133333333333333\n",
      "iteration no 134: Loss: 0.7574542032499183, accuracy: 0.5133333333333333\n",
      "iteration no 135: Loss: 0.7574273488219799, accuracy: 0.5133333333333333\n",
      "iteration no 136: Loss: 0.7574013757274448, accuracy: 0.5133333333333333\n",
      "iteration no 137: Loss: 0.7573762533110536, accuracy: 0.5133333333333333\n",
      "iteration no 138: Loss: 0.7573519520673401, accuracy: 0.5133333333333333\n",
      "iteration no 139: Loss: 0.7573284435933836, accuracy: 0.5133333333333333\n",
      "iteration no 140: Loss: 0.7573057005437116, accuracy: 0.5133333333333333\n",
      "iteration no 141: Loss: 0.7572836965872417, accuracy: 0.5133333333333333\n",
      "iteration no 142: Loss: 0.7572624063661635, accuracy: 0.5133333333333333\n",
      "iteration no 143: Loss: 0.75724180545666, accuracy: 0.5133333333333333\n",
      "iteration no 144: Loss: 0.7572218703313801, accuracy: 0.5133333333333333\n",
      "iteration no 145: Loss: 0.7572025783235736, accuracy: 0.5133333333333333\n",
      "iteration no 146: Loss: 0.7571839075928096, accuracy: 0.5133333333333333\n",
      "iteration no 147: Loss: 0.7571658370921973, accuracy: 0.5133333333333333\n",
      "iteration no 148: Loss: 0.7571483465370408, accuracy: 0.5133333333333333\n",
      "iteration no 149: Loss: 0.7571314163748538, accuracy: 0.5133333333333333\n",
      "iteration no 150: Loss: 0.7571150277566726, accuracy: 0.5133333333333333\n",
      "iteration no 151: Loss: 0.7570991625096041, accuracy: 0.5133333333333333\n",
      "iteration no 152: Loss: 0.7570838031105483, accuracy: 0.5133333333333333\n",
      "iteration no 153: Loss: 0.7570689326610439, accuracy: 0.5133333333333333\n",
      "iteration no 154: Loss: 0.757054534863178, accuracy: 0.5133333333333333\n",
      "iteration no 155: Loss: 0.7570405939965181, accuracy: 0.5133333333333333\n",
      "iteration no 156: Loss: 0.7570270948960108, accuracy: 0.5133333333333333\n",
      "iteration no 157: Loss: 0.7570140229308066, accuracy: 0.5133333333333333\n",
      "iteration no 158: Loss: 0.7570013639839681, accuracy: 0.5133333333333333\n",
      "iteration no 159: Loss: 0.7569891044330211, accuracy: 0.5133333333333333\n",
      "iteration no 160: Loss: 0.7569772311313068, accuracy: 0.5133333333333333\n",
      "iteration no 161: Loss: 0.7569657313901036, accuracy: 0.5133333333333333\n",
      "iteration no 162: Loss: 0.7569545929614804, accuracy: 0.5133333333333333\n",
      "iteration no 163: Loss: 0.756943804021849, accuracy: 0.5133333333333333\n",
      "iteration no 164: Loss: 0.7569333531561855, accuracy: 0.5133333333333333\n",
      "iteration no 165: Loss: 0.7569232293428894, accuracy: 0.5133333333333333\n",
      "iteration no 166: Loss: 0.7569134219392526, accuracy: 0.5133333333333333\n",
      "iteration no 167: Loss: 0.7569039206675126, accuracy: 0.5133333333333333\n",
      "iteration no 168: Loss: 0.7568947156014614, accuracy: 0.5133333333333333\n",
      "iteration no 169: Loss: 0.7568857971535887, accuracy: 0.5133333333333333\n",
      "iteration no 170: Loss: 0.7568771560627344, accuracy: 0.5133333333333333\n",
      "iteration no 171: Loss: 0.7568687833822294, accuracy: 0.5133333333333333\n",
      "iteration no 172: Loss: 0.7568606704685013, accuracy: 0.5133333333333333\n",
      "iteration no 173: Loss: 0.7568528089701285, accuracy: 0.5133333333333333\n",
      "iteration no 174: Loss: 0.7568451908173208, accuracy: 0.5133333333333333\n",
      "iteration no 175: Loss: 0.7568378082118098, accuracy: 0.5133333333333333\n",
      "iteration no 176: Loss: 0.7568306536171298, accuracy: 0.5133333333333333\n",
      "iteration no 177: Loss: 0.7568237197492758, accuracy: 0.5133333333333333\n",
      "iteration no 178: Loss: 0.756816999567719, accuracy: 0.5133333333333333\n",
      "iteration no 179: Loss: 0.7568104862667686, accuracy: 0.5133333333333333\n",
      "iteration no 180: Loss: 0.7568041732672625, accuracy: 0.5133333333333333\n",
      "iteration no 181: Loss: 0.7567980542085738, accuracy: 0.5133333333333333\n",
      "iteration no 182: Loss: 0.7567921229409224, accuracy: 0.5133333333333333\n",
      "iteration no 183: Loss: 0.7567863735179751, accuracy: 0.5133333333333333\n",
      "iteration no 184: Loss: 0.756780800189726, accuracy: 0.5133333333333333\n",
      "iteration no 185: Loss: 0.7567753973956433, accuracy: 0.5133333333333333\n",
      "iteration no 186: Loss: 0.7567701597580735, accuracy: 0.5133333333333333\n",
      "iteration no 187: Loss: 0.7567650820758909, accuracy: 0.5133333333333333\n",
      "iteration no 188: Loss: 0.7567601593183838, accuracy: 0.5133333333333333\n",
      "iteration no 189: Loss: 0.756755386619366, accuracy: 0.5133333333333333\n",
      "iteration no 190: Loss: 0.7567507592715078, accuracy: 0.5133333333333333\n",
      "iteration no 191: Loss: 0.7567462727208741, accuracy: 0.5133333333333333\n",
      "iteration no 192: Loss: 0.7567419225616641, accuracy: 0.5133333333333333\n",
      "iteration no 193: Loss: 0.7567377045311428, accuracy: 0.5133333333333333\n",
      "iteration no 194: Loss: 0.7567336145047578, accuracy: 0.5133333333333333\n",
      "iteration no 195: Loss: 0.7567296484914338, accuracy: 0.5133333333333333\n",
      "iteration no 196: Loss: 0.7567258026290371, accuracy: 0.5133333333333333\n",
      "iteration no 197: Loss: 0.7567220731800058, accuracy: 0.5133333333333333\n",
      "iteration no 198: Loss: 0.7567184565271354, accuracy: 0.5133333333333333\n",
      "iteration no 199: Loss: 0.7567149491695171, accuracy: 0.5133333333333333\n",
      "iteration no 200: Loss: 0.756711547718622, accuracy: 0.5166666666666667\n",
      "iteration no 1: Loss: 1.098728997485846, accuracy: 0.44666666666666666\n",
      "iteration no 2: Loss: 1.0683707121703232, accuracy: 0.5\n",
      "iteration no 3: Loss: 1.041299479720588, accuracy: 0.5233333333333333\n",
      "iteration no 4: Loss: 1.0171280777277911, accuracy: 0.52\n",
      "iteration no 5: Loss: 0.995512309712334, accuracy: 0.5166666666666667\n",
      "iteration no 6: Loss: 0.9761475633820649, accuracy: 0.5066666666666667\n",
      "iteration no 7: Loss: 0.9587654984623629, accuracy: 0.51\n",
      "iteration no 8: Loss: 0.9431306442687105, accuracy: 0.5133333333333333\n",
      "iteration no 9: Loss: 0.9290369664234359, accuracy: 0.51\n",
      "iteration no 10: Loss: 0.9163045234278159, accuracy: 0.51\n",
      "iteration no 11: Loss: 0.9047763159707081, accuracy: 0.51\n",
      "iteration no 12: Loss: 0.8943153944102529, accuracy: 0.51\n",
      "iteration no 13: Loss: 0.8848022551951057, accuracy: 0.5133333333333333\n",
      "iteration no 14: Loss: 0.8761325313512044, accuracy: 0.51\n",
      "iteration no 15: Loss: 0.868214965552601, accuracy: 0.51\n",
      "iteration no 16: Loss: 0.8609696448046548, accuracy: 0.5066666666666667\n",
      "iteration no 17: Loss: 0.8543264712785776, accuracy: 0.5066666666666667\n",
      "iteration no 18: Loss: 0.8482238425970521, accuracy: 0.5133333333333333\n",
      "iteration no 19: Loss: 0.8426075155966184, accuracy: 0.5133333333333333\n",
      "iteration no 20: Loss: 0.8374296294043291, accuracy: 0.5133333333333333\n",
      "iteration no 21: Loss: 0.8326478659915638, accuracy: 0.5133333333333333\n",
      "iteration no 22: Loss: 0.8282247288516166, accuracy: 0.51\n",
      "iteration no 23: Loss: 0.8241269228806553, accuracy: 0.51\n",
      "iteration no 24: Loss: 0.8203248208101402, accuracy: 0.51\n",
      "iteration no 25: Loss: 0.8167920035890568, accuracy: 0.5133333333333333\n",
      "iteration no 26: Loss: 0.8135048639292174, accuracy: 0.5133333333333333\n",
      "iteration no 27: Loss: 0.8104422638104762, accuracy: 0.5133333333333333\n",
      "iteration no 28: Loss: 0.8075852381105145, accuracy: 0.5133333333333333\n",
      "iteration no 29: Loss: 0.8049167376967677, accuracy: 0.5133333333333333\n",
      "iteration no 30: Loss: 0.8024214063187528, accuracy: 0.5133333333333333\n",
      "iteration no 31: Loss: 0.8000853864897249, accuracy: 0.5133333333333333\n",
      "iteration no 32: Loss: 0.7978961502680283, accuracy: 0.5166666666666667\n",
      "iteration no 33: Loss: 0.7958423514594075, accuracy: 0.52\n",
      "iteration no 34: Loss: 0.7939136962784138, accuracy: 0.52\n",
      "iteration no 35: Loss: 0.7921008299442489, accuracy: 0.52\n",
      "iteration no 36: Loss: 0.7903952370562491, accuracy: 0.52\n",
      "iteration no 37: Loss: 0.7887891539072599, accuracy: 0.52\n",
      "iteration no 38: Loss: 0.7872754911583177, accuracy: 0.52\n",
      "iteration no 39: Loss: 0.7858477655228768, accuracy: 0.52\n",
      "iteration no 40: Loss: 0.7845000392996501, accuracy: 0.52\n",
      "iteration no 41: Loss: 0.783226866755329, accuracy: 0.52\n",
      "iteration no 42: Loss: 0.7820232464964982, accuracy: 0.52\n",
      "iteration no 43: Loss: 0.7808845790877124, accuracy: 0.52\n",
      "iteration no 44: Loss: 0.7798066292731627, accuracy: 0.5166666666666667\n",
      "iteration no 45: Loss: 0.778785492245238, accuracy: 0.5166666666666667\n",
      "iteration no 46: Loss: 0.7778175634768477, accuracy: 0.5166666666666667\n",
      "iteration no 47: Loss: 0.7768995116974702, accuracy: 0.5166666666666667\n",
      "iteration no 48: Loss: 0.7760282546471183, accuracy: 0.5166666666666667\n",
      "iteration no 49: Loss: 0.775200937289081, accuracy: 0.5166666666666667\n",
      "iteration no 50: Loss: 0.7744149122025427, accuracy: 0.5166666666666667\n",
      "iteration no 51: Loss: 0.7736677219109379, accuracy: 0.5166666666666667\n",
      "iteration no 52: Loss: 0.772957082931964, accuracy: 0.5166666666666667\n",
      "iteration no 53: Loss: 0.7722808713612279, accuracy: 0.5166666666666667\n",
      "iteration no 54: Loss: 0.7716371098241221, accuracy: 0.5166666666666667\n",
      "iteration no 55: Loss: 0.7710239556501864, accuracy: 0.5166666666666667\n",
      "iteration no 56: Loss: 0.7704396901413325, accuracy: 0.5166666666666667\n",
      "iteration no 57: Loss: 0.7698827088202519, accuracy: 0.5166666666666667\n",
      "iteration no 58: Loss: 0.7693515125583682, accuracy: 0.5166666666666667\n",
      "iteration no 59: Loss: 0.7688446994941188, accuracy: 0.5166666666666667\n",
      "iteration no 60: Loss: 0.768360957662357, accuracy: 0.5166666666666667\n",
      "iteration no 61: Loss: 0.767899058264444, accuracy: 0.5166666666666667\n",
      "iteration no 62: Loss: 0.7674578495163288, accuracy: 0.5166666666666667\n",
      "iteration no 63: Loss: 0.767036251018702, accuracy: 0.5166666666666667\n",
      "iteration no 64: Loss: 0.7666332485993098, accuracy: 0.5166666666666667\n",
      "iteration no 65: Loss: 0.766247889582797, accuracy: 0.5166666666666667\n",
      "iteration no 66: Loss: 0.7658792784481324, accuracy: 0.5166666666666667\n",
      "iteration no 67: Loss: 0.7655265728378088, accuracy: 0.5166666666666667\n",
      "iteration no 68: Loss: 0.7651889798866757, accuracy: 0.5166666666666667\n",
      "iteration no 69: Loss: 0.7648657528415337, accuracy: 0.5166666666666667\n",
      "iteration no 70: Loss: 0.7645561879455036, accuracy: 0.5166666666666667\n",
      "iteration no 71: Loss: 0.7642596215637752, accuracy: 0.5166666666666667\n",
      "iteration no 72: Loss: 0.7639754275296265, accuracy: 0.5166666666666667\n",
      "iteration no 73: Loss: 0.7637030146916658, accuracy: 0.5166666666666667\n",
      "iteration no 74: Loss: 0.7634418246450709, accuracy: 0.5166666666666667\n",
      "iteration no 75: Loss: 0.763191329631252, accuracy: 0.5166666666666667\n",
      "iteration no 76: Loss: 0.7629510305918146, accuracy: 0.5166666666666667\n",
      "iteration no 77: Loss: 0.7627204553640322, accuracy: 0.5166666666666667\n",
      "iteration no 78: Loss: 0.7624991570062091, accuracy: 0.5166666666666667\n",
      "iteration no 79: Loss: 0.7622867122423772, accuracy: 0.5166666666666667\n",
      "iteration no 80: Loss: 0.762082720016727, accuracy: 0.5166666666666667\n",
      "iteration no 81: Loss: 0.7618868001490339, accuracy: 0.5133333333333333\n",
      "iteration no 82: Loss: 0.7616985920831101, accuracy: 0.5133333333333333\n",
      "iteration no 83: Loss: 0.761517753721022, accuracy: 0.5133333333333333\n",
      "iteration no 84: Loss: 0.7613439603364334, accuracy: 0.5133333333333333\n",
      "iteration no 85: Loss: 0.7611769035610167, accuracy: 0.5133333333333333\n",
      "iteration no 86: Loss: 0.761016290438386, accuracy: 0.5133333333333333\n",
      "iteration no 87: Loss: 0.7608618425404745, accuracy: 0.5133333333333333\n",
      "iteration no 88: Loss: 0.7607132951417079, accuracy: 0.5133333333333333\n",
      "iteration no 89: Loss: 0.7605703964467061, accuracy: 0.5133333333333333\n",
      "iteration no 90: Loss: 0.7604329068676023, accuracy: 0.5133333333333333\n",
      "iteration no 91: Loss: 0.7603005983473803, accuracy: 0.5133333333333333\n",
      "iteration no 92: Loss: 0.7601732537259286, accuracy: 0.5133333333333333\n",
      "iteration no 93: Loss: 0.7600506661457678, accuracy: 0.5133333333333333\n",
      "iteration no 94: Loss: 0.7599326384946542, accuracy: 0.5133333333333333\n",
      "iteration no 95: Loss: 0.7598189828824801, accuracy: 0.5133333333333333\n",
      "iteration no 96: Loss: 0.7597095201500904, accuracy: 0.5133333333333333\n",
      "iteration no 97: Loss: 0.7596040794078243, accuracy: 0.5133333333333333\n",
      "iteration no 98: Loss: 0.7595024976017528, accuracy: 0.5133333333333333\n",
      "iteration no 99: Loss: 0.759404619105743, accuracy: 0.5166666666666667\n",
      "iteration no 100: Loss: 0.7593102953376137, accuracy: 0.5166666666666667\n",
      "iteration no 101: Loss: 0.7592193843977832, accuracy: 0.5166666666666667\n",
      "iteration no 102: Loss: 0.7591317507289269, accuracy: 0.5166666666666667\n",
      "iteration no 103: Loss: 0.7590472647952659, accuracy: 0.5166666666666667\n",
      "iteration no 104: Loss: 0.7589658027802167, accuracy: 0.5166666666666667\n",
      "iteration no 105: Loss: 0.7588872463012191, accuracy: 0.5166666666666667\n",
      "iteration no 106: Loss: 0.7588114821406416, accuracy: 0.5166666666666667\n",
      "iteration no 107: Loss: 0.7587384019917512, accuracy: 0.5166666666666667\n",
      "iteration no 108: Loss: 0.7586679022187939, accuracy: 0.5166666666666667\n",
      "iteration no 109: Loss: 0.7585998836303085, accuracy: 0.5166666666666667\n",
      "iteration no 110: Loss: 0.7585342512648551, accuracy: 0.5166666666666667\n",
      "iteration no 111: Loss: 0.7584709141883906, accuracy: 0.5166666666666667\n",
      "iteration no 112: Loss: 0.7584097853025852, accuracy: 0.5166666666666667\n",
      "iteration no 113: Loss: 0.7583507811634126, accuracy: 0.5166666666666667\n",
      "iteration no 114: Loss: 0.7582938218094013, accuracy: 0.5166666666666667\n",
      "iteration no 115: Loss: 0.7582388305989642, accuracy: 0.5166666666666667\n",
      "iteration no 116: Loss: 0.7581857340562728, accuracy: 0.5166666666666667\n",
      "iteration no 117: Loss: 0.758134461725172, accuracy: 0.5166666666666667\n",
      "iteration no 118: Loss: 0.7580849460306638, accuracy: 0.5166666666666667\n",
      "iteration no 119: Loss: 0.7580371221475237, accuracy: 0.5166666666666667\n",
      "iteration no 120: Loss: 0.757990927875634, accuracy: 0.5166666666666667\n",
      "iteration no 121: Loss: 0.7579463035216571, accuracy: 0.5166666666666667\n",
      "iteration no 122: Loss: 0.7579031917866778, accuracy: 0.5166666666666667\n",
      "iteration no 123: Loss: 0.7578615376594885, accuracy: 0.5166666666666667\n",
      "iteration no 124: Loss: 0.7578212883151922, accuracy: 0.5166666666666667\n",
      "iteration no 125: Loss: 0.757782393018832, accuracy: 0.5166666666666667\n",
      "iteration no 126: Loss: 0.7577448030337659, accuracy: 0.5166666666666667\n",
      "iteration no 127: Loss: 0.7577084715345282, accuracy: 0.5166666666666667\n",
      "iteration no 128: Loss: 0.7576733535239303, accuracy: 0.5166666666666667\n",
      "iteration no 129: Loss: 0.7576394057541715, accuracy: 0.5166666666666667\n",
      "iteration no 130: Loss: 0.7576065866517443, accuracy: 0.5166666666666667\n",
      "iteration no 131: Loss: 0.7575748562459286, accuracy: 0.5166666666666667\n",
      "iteration no 132: Loss: 0.7575441761006859, accuracy: 0.5166666666666667\n",
      "iteration no 133: Loss: 0.7575145092497716, accuracy: 0.5166666666666667\n",
      "iteration no 134: Loss: 0.7574858201348972, accuracy: 0.5166666666666667\n",
      "iteration no 135: Loss: 0.7574580745467814, accuracy: 0.5133333333333333\n",
      "iteration no 136: Loss: 0.7574312395689426, accuracy: 0.5133333333333333\n",
      "iteration no 137: Loss: 0.757405283524087, accuracy: 0.5133333333333333\n",
      "iteration no 138: Loss: 0.7573801759229629, accuracy: 0.5133333333333333\n",
      "iteration no 139: Loss: 0.7573558874155528, accuracy: 0.5133333333333333\n",
      "iteration no 140: Loss: 0.7573323897444846, accuracy: 0.5133333333333333\n",
      "iteration no 141: Loss: 0.7573096557005521, accuracy: 0.5133333333333333\n",
      "iteration no 142: Loss: 0.7572876590802349, accuracy: 0.5133333333333333\n",
      "iteration no 143: Loss: 0.7572663746451228, accuracy: 0.5133333333333333\n",
      "iteration no 144: Loss: 0.7572457780831451, accuracy: 0.5133333333333333\n",
      "iteration no 145: Loss: 0.7572258459715189, accuracy: 0.5133333333333333\n",
      "iteration no 146: Loss: 0.7572065557413321, accuracy: 0.5133333333333333\n",
      "iteration no 147: Loss: 0.7571878856436798, accuracy: 0.5133333333333333\n",
      "iteration no 148: Loss: 0.7571698147172773, accuracy: 0.5133333333333333\n",
      "iteration no 149: Loss: 0.7571523227574839, accuracy: 0.5133333333333333\n",
      "iteration no 150: Loss: 0.7571353902866624, accuracy: 0.5133333333333333\n",
      "iteration no 151: Loss: 0.7571189985258171, accuracy: 0.5133333333333333\n",
      "iteration no 152: Loss: 0.7571031293674443, accuracy: 0.5133333333333333\n",
      "iteration no 153: Loss: 0.7570877653495414, accuracy: 0.5133333333333333\n",
      "iteration no 154: Loss: 0.7570728896307178, accuracy: 0.5133333333333333\n",
      "iteration no 155: Loss: 0.7570584859663592, accuracy: 0.5133333333333333\n",
      "iteration no 156: Loss: 0.7570445386857907, accuracy: 0.5133333333333333\n",
      "iteration no 157: Loss: 0.7570310326703996, accuracy: 0.5133333333333333\n",
      "iteration no 158: Loss: 0.7570179533326663, accuracy: 0.5133333333333333\n",
      "iteration no 159: Loss: 0.7570052865960681, accuracy: 0.5133333333333333\n",
      "iteration no 160: Loss: 0.7569930188758114, accuracy: 0.5133333333333333\n",
      "iteration no 161: Loss: 0.7569811370603562, accuracy: 0.5133333333333333\n",
      "iteration no 162: Loss: 0.7569696284936992, accuracy: 0.5133333333333333\n",
      "iteration no 163: Loss: 0.7569584809583775, accuracy: 0.5133333333333333\n",
      "iteration no 164: Loss: 0.7569476826591626, accuracy: 0.5133333333333333\n",
      "iteration no 165: Loss: 0.7569372222074151, accuracy: 0.5133333333333333\n",
      "iteration no 166: Loss: 0.7569270886060687, accuracy: 0.5133333333333333\n",
      "iteration no 167: Loss: 0.7569172712352168, accuracy: 0.5133333333333333\n",
      "iteration no 168: Loss: 0.7569077598382747, accuracy: 0.5133333333333333\n",
      "iteration no 169: Loss: 0.7568985445086927, accuracy: 0.5133333333333333\n",
      "iteration no 170: Loss: 0.7568896156771957, accuracy: 0.5133333333333333\n",
      "iteration no 171: Loss: 0.7568809640995272, accuracy: 0.5133333333333333\n",
      "iteration no 172: Loss: 0.7568725808446745, accuracy: 0.5133333333333333\n",
      "iteration no 173: Loss: 0.7568644572835558, accuracy: 0.5133333333333333\n",
      "iteration no 174: Loss: 0.7568565850781491, accuracy: 0.5133333333333333\n",
      "iteration no 175: Loss: 0.7568489561710429, accuracy: 0.5133333333333333\n",
      "iteration no 176: Loss: 0.7568415627753933, accuracy: 0.5133333333333333\n",
      "iteration no 177: Loss: 0.7568343973652668, accuracy: 0.5133333333333333\n",
      "iteration no 178: Loss: 0.7568274526663575, accuracy: 0.5133333333333333\n",
      "iteration no 179: Loss: 0.7568207216470564, accuracy: 0.5133333333333333\n",
      "iteration no 180: Loss: 0.7568141975098648, accuracy: 0.5133333333333333\n",
      "iteration no 181: Loss: 0.7568078736831328, accuracy: 0.5133333333333333\n",
      "iteration no 182: Loss: 0.756801743813113, accuracy: 0.5133333333333333\n",
      "iteration no 183: Loss: 0.7567958017563112, accuracy: 0.5133333333333333\n",
      "iteration no 184: Loss: 0.7567900415721299, accuracy: 0.5133333333333333\n",
      "iteration no 185: Loss: 0.7567844575157827, accuracy: 0.5133333333333333\n",
      "iteration no 186: Loss: 0.7567790440314789, accuracy: 0.5133333333333333\n",
      "iteration no 187: Loss: 0.7567737957458582, accuracy: 0.5133333333333333\n",
      "iteration no 188: Loss: 0.7567687074616731, accuracy: 0.5133333333333333\n",
      "iteration no 189: Loss: 0.7567637741517015, accuracy: 0.5133333333333333\n",
      "iteration no 190: Loss: 0.7567589909528879, accuracy: 0.5133333333333333\n",
      "iteration no 191: Loss: 0.7567543531606974, accuracy: 0.5133333333333333\n",
      "iteration no 192: Loss: 0.7567498562236793, accuracy: 0.5133333333333333\n",
      "iteration no 193: Loss: 0.7567454957382278, accuracy: 0.5133333333333333\n",
      "iteration no 194: Loss: 0.7567412674435349, accuracy: 0.5133333333333333\n",
      "iteration no 195: Loss: 0.7567371672167268, accuracy: 0.5133333333333333\n",
      "iteration no 196: Loss: 0.7567331910681764, accuracy: 0.5133333333333333\n",
      "iteration no 197: Loss: 0.7567293351369849, accuracy: 0.5133333333333333\n",
      "iteration no 198: Loss: 0.7567255956866279, accuracy: 0.5133333333333333\n",
      "iteration no 199: Loss: 0.7567219691007564, accuracy: 0.5133333333333333\n",
      "iteration no 200: Loss: 0.7567184518791488, accuracy: 0.5133333333333333\n",
      "iteration no 1: Loss: 1.102860430246852, accuracy: 0.5133333333333333\n",
      "iteration no 2: Loss: 1.0716622224428418, accuracy: 0.5066666666666667\n",
      "iteration no 3: Loss: 1.043920865275639, accuracy: 0.5166666666666667\n",
      "iteration no 4: Loss: 1.0192153480378552, accuracy: 0.52\n",
      "iteration no 5: Loss: 0.9971744431631092, accuracy: 0.52\n",
      "iteration no 6: Loss: 0.9774715416027101, accuracy: 0.5166666666666667\n",
      "iteration no 7: Loss: 0.9598204137263914, accuracy: 0.5166666666666667\n",
      "iteration no 8: Loss: 0.943971151466987, accuracy: 0.5166666666666667\n",
      "iteration no 9: Loss: 0.9297061769146695, accuracy: 0.5133333333333333\n",
      "iteration no 10: Loss: 0.9168363967748491, accuracy: 0.5133333333333333\n",
      "iteration no 11: Loss: 0.9051976048892113, accuracy: 0.5133333333333333\n",
      "iteration no 12: Loss: 0.8946472036356113, accuracy: 0.5166666666666667\n",
      "iteration no 13: Loss: 0.8850612764003308, accuracy: 0.5166666666666667\n",
      "iteration no 14: Loss: 0.8763320128451618, accuracy: 0.5166666666666667\n",
      "iteration no 15: Loss: 0.8683654689494178, accuracy: 0.5166666666666667\n",
      "iteration no 16: Loss: 0.8610796330147523, accuracy: 0.5133333333333333\n",
      "iteration no 17: Loss: 0.8544027643199162, accuracy: 0.5133333333333333\n",
      "iteration no 18: Loss: 0.8482719705826262, accuracy: 0.5133333333333333\n",
      "iteration no 19: Loss: 0.8426319921268237, accuracy: 0.5133333333333333\n",
      "iteration no 20: Loss: 0.8374341635333666, accuracy: 0.5133333333333333\n",
      "iteration no 21: Loss: 0.8326355268660695, accuracy: 0.5133333333333333\n",
      "iteration no 22: Loss: 0.8281980739019227, accuracy: 0.5133333333333333\n",
      "iteration no 23: Loss: 0.8240880979328671, accuracy: 0.5133333333333333\n",
      "iteration no 24: Loss: 0.8202756385424186, accuracy: 0.5133333333333333\n",
      "iteration no 25: Loss: 0.8167340052585293, accuracy: 0.5133333333333333\n",
      "iteration no 26: Loss: 0.8134393681479113, accuracy: 0.5166666666666667\n",
      "iteration no 27: Loss: 0.8103704052701532, accuracy: 0.5166666666666667\n",
      "iteration no 28: Loss: 0.8075079984847033, accuracy: 0.5166666666666667\n",
      "iteration no 29: Loss: 0.8048349704350084, accuracy: 0.5166666666666667\n",
      "iteration no 30: Loss: 0.8023358566555807, accuracy: 0.5166666666666667\n",
      "iteration no 31: Loss: 0.7999967076905229, accuracy: 0.5166666666666667\n",
      "iteration no 32: Loss: 0.7978049169036492, accuracy: 0.5166666666666667\n",
      "iteration no 33: Loss: 0.7957490703247253, accuracy: 0.52\n",
      "iteration no 34: Loss: 0.7938188154340394, accuracy: 0.52\n",
      "iteration no 35: Loss: 0.7920047462558797, accuracy: 0.52\n",
      "iteration no 36: Loss: 0.790298302525204, accuracy: 0.5166666666666667\n",
      "iteration no 37: Loss: 0.7886916810231064, accuracy: 0.5166666666666667\n",
      "iteration no 38: Loss: 0.787177757455865, accuracy: 0.5166666666666667\n",
      "iteration no 39: Loss: 0.7857500174879783, accuracy: 0.5166666666666667\n",
      "iteration no 40: Loss: 0.7844024957387291, accuracy: 0.5166666666666667\n",
      "iteration no 41: Loss: 0.783129721720446, accuracy: 0.5133333333333333\n",
      "iteration no 42: Loss: 0.7819266718396465, accuracy: 0.5133333333333333\n",
      "iteration no 43: Loss: 0.7807887267037659, accuracy: 0.5133333333333333\n",
      "iteration no 44: Loss: 0.7797116330796257, accuracy: 0.5133333333333333\n",
      "iteration no 45: Loss: 0.778691469938031, accuracy: 0.5133333333333333\n",
      "iteration no 46: Loss: 0.7777246180942567, accuracy: 0.5133333333333333\n",
      "iteration no 47: Loss: 0.7768077330187367, accuracy: 0.5133333333333333\n",
      "iteration no 48: Loss: 0.775937720447598, accuracy: 0.5133333333333333\n",
      "iteration no 49: Loss: 0.7751117144702657, accuracy: 0.5133333333333333\n",
      "iteration no 50: Loss: 0.774327057812285, accuracy: 0.5133333333333333\n",
      "iteration no 51: Loss: 0.7735812840668285, accuracy: 0.5133333333333333\n",
      "iteration no 52: Loss: 0.7728721016588663, accuracy: 0.5133333333333333\n",
      "iteration no 53: Loss: 0.7721973793523743, accuracy: 0.5133333333333333\n",
      "iteration no 54: Loss: 0.7715551331338638, accuracy: 0.5133333333333333\n",
      "iteration no 55: Loss: 0.7709435143253962, accuracy: 0.5133333333333333\n",
      "iteration no 56: Loss: 0.7703607987975585, accuracy: 0.5133333333333333\n",
      "iteration no 57: Loss: 0.7698053771679542, accuracy: 0.5133333333333333\n",
      "iteration no 58: Loss: 0.7692757458839358, accuracy: 0.5133333333333333\n",
      "iteration no 59: Loss: 0.7687704990998194, accuracy: 0.5133333333333333\n",
      "iteration no 60: Loss: 0.7682883212689093, accuracy: 0.5166666666666667\n",
      "iteration no 61: Loss: 0.767827980379508, accuracy: 0.5166666666666667\n",
      "iteration no 62: Loss: 0.7673883217718637, accuracy: 0.5166666666666667\n",
      "iteration no 63: Loss: 0.7669682624798483, accuracy: 0.5166666666666667\n",
      "iteration no 64: Loss: 0.7665667860471896, accuracy: 0.5166666666666667\n",
      "iteration no 65: Loss: 0.7661829377734022, accuracy: 0.5166666666666667\n",
      "iteration no 66: Loss: 0.765815820349267, accuracy: 0.5166666666666667\n",
      "iteration no 67: Loss: 0.7654645898458771, accuracy: 0.5166666666666667\n",
      "iteration no 68: Loss: 0.7651284520249464, accuracy: 0.5166666666666667\n",
      "iteration no 69: Loss: 0.7648066589413726, accuracy: 0.5166666666666667\n",
      "iteration no 70: Loss: 0.7644985058119386, accuracy: 0.5166666666666667\n",
      "iteration no 71: Loss: 0.7642033281266416, accuracy: 0.5166666666666667\n",
      "iteration no 72: Loss: 0.7639204989814427, accuracy: 0.5166666666666667\n",
      "iteration no 73: Loss: 0.7636494266132889, accuracy: 0.5166666666666667\n",
      "iteration no 74: Loss: 0.763389552120106, accuracy: 0.5166666666666667\n",
      "iteration no 75: Loss: 0.7631403473501001, accuracy: 0.5166666666666667\n",
      "iteration no 76: Loss: 0.7629013129461848, accuracy: 0.5166666666666667\n",
      "iteration no 77: Loss: 0.7626719765326739, accuracy: 0.5166666666666667\n",
      "iteration no 78: Loss: 0.7624518910325601, accuracy: 0.5166666666666667\n",
      "iteration no 79: Loss: 0.7622406331047709, accuracy: 0.5166666666666667\n",
      "iteration no 80: Loss: 0.7620378016917556, accuracy: 0.5133333333333333\n",
      "iteration no 81: Loss: 0.7618430166686113, accuracy: 0.5133333333333333\n",
      "iteration no 82: Loss: 0.7616559175857437, accuracy: 0.5133333333333333\n",
      "iteration no 83: Loss: 0.761476162497759, accuracy: 0.5133333333333333\n",
      "iteration no 84: Loss: 0.7613034268719132, accuracy: 0.5133333333333333\n",
      "iteration no 85: Loss: 0.7611374025700296, accuracy: 0.5133333333333333\n",
      "iteration no 86: Loss: 0.760977796898304, accuracy: 0.5133333333333333\n",
      "iteration no 87: Loss: 0.7608243317198968, accuracy: 0.5133333333333333\n",
      "iteration no 88: Loss: 0.7606767426256338, accuracy: 0.5133333333333333\n",
      "iteration no 89: Loss: 0.760534778158526, accuracy: 0.5133333333333333\n",
      "iteration no 90: Loss: 0.7603981990881755, accuracy: 0.5133333333333333\n",
      "iteration no 91: Loss: 0.7602667777314474, accuracy: 0.5133333333333333\n",
      "iteration no 92: Loss: 0.7601402973160848, accuracy: 0.5133333333333333\n",
      "iteration no 93: Loss: 0.7600185513842129, accuracy: 0.5133333333333333\n",
      "iteration no 94: Loss: 0.7599013432329106, accuracy: 0.5133333333333333\n",
      "iteration no 95: Loss: 0.7597884853892586, accuracy: 0.5133333333333333\n",
      "iteration no 96: Loss: 0.7596797991174729, accuracy: 0.5133333333333333\n",
      "iteration no 97: Loss: 0.7595751139559144, accuracy: 0.5133333333333333\n",
      "iteration no 98: Loss: 0.7594742672819365, accuracy: 0.5133333333333333\n",
      "iteration no 99: Loss: 0.7593771039026885, accuracy: 0.5133333333333333\n",
      "iteration no 100: Loss: 0.7592834756701322, accuracy: 0.5133333333333333\n",
      "iteration no 101: Loss: 0.759193241118661, accuracy: 0.5133333333333333\n",
      "iteration no 102: Loss: 0.7591062651238293, accuracy: 0.5133333333333333\n",
      "iteration no 103: Loss: 0.7590224185808088, accuracy: 0.5133333333333333\n",
      "iteration no 104: Loss: 0.7589415781012903, accuracy: 0.5166666666666667\n",
      "iteration no 105: Loss: 0.7588636257276448, accuracy: 0.5166666666666667\n",
      "iteration no 106: Loss: 0.7587884486632348, accuracy: 0.5166666666666667\n",
      "iteration no 107: Loss: 0.7587159390178562, accuracy: 0.5166666666666667\n",
      "iteration no 108: Loss: 0.7586459935673521, accuracy: 0.5166666666666667\n",
      "iteration no 109: Loss: 0.7585785135265204, accuracy: 0.5166666666666667\n",
      "iteration no 110: Loss: 0.7585134043344817, accuracy: 0.5166666666666667\n",
      "iteration no 111: Loss: 0.7584505754517471, accuracy: 0.5166666666666667\n",
      "iteration no 112: Loss: 0.7583899401682674, accuracy: 0.5166666666666667\n",
      "iteration no 113: Loss: 0.7583314154217977, accuracy: 0.5166666666666667\n",
      "iteration no 114: Loss: 0.7582749216259564, accuracy: 0.5166666666666667\n",
      "iteration no 115: Loss: 0.7582203825074003, accuracy: 0.5166666666666667\n",
      "iteration no 116: Loss: 0.7581677249515714, accuracy: 0.5166666666666667\n",
      "iteration no 117: Loss: 0.7581168788565109, accuracy: 0.5166666666666667\n",
      "iteration no 118: Loss: 0.7580677769942709, accuracy: 0.5166666666666667\n",
      "iteration no 119: Loss: 0.7580203548794772, accuracy: 0.5166666666666667\n",
      "iteration no 120: Loss: 0.7579745506446346, accuracy: 0.5166666666666667\n",
      "iteration no 121: Loss: 0.7579303049217849, accuracy: 0.5166666666666667\n",
      "iteration no 122: Loss: 0.7578875607301572, accuracy: 0.5166666666666667\n",
      "iteration no 123: Loss: 0.7578462633694694, accuracy: 0.5166666666666667\n",
      "iteration no 124: Loss: 0.7578063603185691, accuracy: 0.5166666666666667\n",
      "iteration no 125: Loss: 0.7577678011391044, accuracy: 0.5166666666666667\n",
      "iteration no 126: Loss: 0.7577305373839595, accuracy: 0.5166666666666667\n",
      "iteration no 127: Loss: 0.7576945225101799, accuracy: 0.5166666666666667\n",
      "iteration no 128: Loss: 0.7576597117961495, accuracy: 0.5166666666666667\n",
      "iteration no 129: Loss: 0.7576260622627845, accuracy: 0.5166666666666667\n",
      "iteration no 130: Loss: 0.7575935325985272, accuracy: 0.5166666666666667\n",
      "iteration no 131: Loss: 0.7575620830879372, accuracy: 0.5133333333333333\n",
      "iteration no 132: Loss: 0.7575316755436841, accuracy: 0.5133333333333333\n",
      "iteration no 133: Loss: 0.7575022732417666, accuracy: 0.5133333333333333\n",
      "iteration no 134: Loss: 0.7574738408597819, accuracy: 0.5133333333333333\n",
      "iteration no 135: Loss: 0.7574463444180897, accuracy: 0.5133333333333333\n",
      "iteration no 136: Loss: 0.7574197512237174, accuracy: 0.5133333333333333\n",
      "iteration no 137: Loss: 0.7573940298168661, accuracy: 0.5133333333333333\n",
      "iteration no 138: Loss: 0.7573691499198822, accuracy: 0.5133333333333333\n",
      "iteration no 139: Loss: 0.7573450823885687, accuracy: 0.5133333333333333\n",
      "iteration no 140: Loss: 0.7573217991657163, accuracy: 0.5133333333333333\n",
      "iteration no 141: Loss: 0.7572992732367447, accuracy: 0.5133333333333333\n",
      "iteration no 142: Loss: 0.7572774785873435, accuracy: 0.5133333333333333\n",
      "iteration no 143: Loss: 0.7572563901630158, accuracy: 0.5133333333333333\n",
      "iteration no 144: Loss: 0.7572359838304299, accuracy: 0.5133333333333333\n",
      "iteration no 145: Loss: 0.7572162363404857, accuracy: 0.5133333333333333\n",
      "iteration no 146: Loss: 0.757197125293017, accuracy: 0.5133333333333333\n",
      "iteration no 147: Loss: 0.7571786291030458, accuracy: 0.5133333333333333\n",
      "iteration no 148: Loss: 0.7571607269685134, accuracy: 0.5133333333333333\n",
      "iteration no 149: Loss: 0.7571433988394177, accuracy: 0.5133333333333333\n",
      "iteration no 150: Loss: 0.757126625388289, accuracy: 0.5133333333333333\n",
      "iteration no 151: Loss: 0.757110387981939, accuracy: 0.5133333333333333\n",
      "iteration no 152: Loss: 0.7570946686544234, accuracy: 0.5133333333333333\n",
      "iteration no 153: Loss: 0.7570794500811606, accuracy: 0.5133333333333333\n",
      "iteration no 154: Loss: 0.7570647155541504, accuracy: 0.5133333333333333\n",
      "iteration no 155: Loss: 0.7570504489582429, accuracy: 0.5133333333333333\n",
      "iteration no 156: Loss: 0.7570366347484077, accuracy: 0.5133333333333333\n",
      "iteration no 157: Loss: 0.7570232579279569, accuracy: 0.5133333333333333\n",
      "iteration no 158: Loss: 0.7570103040276781, accuracy: 0.5133333333333333\n",
      "iteration no 159: Loss: 0.7569977590858348, accuracy: 0.5133333333333333\n",
      "iteration no 160: Loss: 0.7569856096289956, accuracy: 0.5133333333333333\n",
      "iteration no 161: Loss: 0.7569738426536541, accuracy: 0.5133333333333333\n",
      "iteration no 162: Loss: 0.7569624456086018, accuracy: 0.5133333333333333\n",
      "iteration no 163: Loss: 0.7569514063780233, accuracy: 0.5133333333333333\n",
      "iteration no 164: Loss: 0.75694071326528, accuracy: 0.5133333333333333\n",
      "iteration no 165: Loss: 0.7569303549773487, accuracy: 0.5133333333333333\n",
      "iteration no 166: Loss: 0.7569203206098915, accuracy: 0.5133333333333333\n",
      "iteration no 167: Loss: 0.7569105996329231, accuracy: 0.5133333333333333\n",
      "iteration no 168: Loss: 0.756901181877055, accuracy: 0.5133333333333333\n",
      "iteration no 169: Loss: 0.7568920575202858, accuracy: 0.5133333333333333\n",
      "iteration no 170: Loss: 0.7568832170753182, accuracy: 0.5133333333333333\n",
      "iteration no 171: Loss: 0.7568746513773765, accuracy: 0.5133333333333333\n",
      "iteration no 172: Loss: 0.7568663515725048, accuracy: 0.5133333333333333\n",
      "iteration no 173: Loss: 0.7568583091063255, accuracy: 0.5133333333333333\n",
      "iteration no 174: Loss: 0.7568505157132359, accuracy: 0.5133333333333333\n",
      "iteration no 175: Loss: 0.7568429634060276, accuracy: 0.5133333333333333\n",
      "iteration no 176: Loss: 0.7568356444659079, accuracy: 0.5133333333333333\n",
      "iteration no 177: Loss: 0.7568285514329072, accuracy: 0.5133333333333333\n",
      "iteration no 178: Loss: 0.7568216770966562, accuracy: 0.5133333333333333\n",
      "iteration no 179: Loss: 0.7568150144875181, accuracy: 0.5133333333333333\n",
      "iteration no 180: Loss: 0.7568085568680591, accuracy: 0.5133333333333333\n",
      "iteration no 181: Loss: 0.756802297724845, accuracy: 0.5133333333333333\n",
      "iteration no 182: Loss: 0.756796230760548, accuracy: 0.5133333333333333\n",
      "iteration no 183: Loss: 0.756790349886355, accuracy: 0.5133333333333333\n",
      "iteration no 184: Loss: 0.7567846492146597, accuracy: 0.5133333333333333\n",
      "iteration no 185: Loss: 0.7567791230520313, accuracy: 0.5133333333333333\n",
      "iteration no 186: Loss: 0.7567737658924464, accuracy: 0.5133333333333333\n",
      "iteration no 187: Loss: 0.756768572410773, accuracy: 0.5133333333333333\n",
      "iteration no 188: Loss: 0.7567635374564986, accuracy: 0.5133333333333333\n",
      "iteration no 189: Loss: 0.7567586560476897, accuracy: 0.5133333333333333\n",
      "iteration no 190: Loss: 0.756753923365175, accuracy: 0.5133333333333333\n",
      "iteration no 191: Loss: 0.7567493347469447, accuracy: 0.5133333333333333\n",
      "iteration no 192: Loss: 0.7567448856827536, accuracy: 0.5133333333333333\n",
      "iteration no 193: Loss: 0.7567405718089234, accuracy: 0.5133333333333333\n",
      "iteration no 194: Loss: 0.7567363889033344, accuracy: 0.5133333333333333\n",
      "iteration no 195: Loss: 0.7567323328805992, accuracy: 0.5133333333333333\n",
      "iteration no 196: Loss: 0.7567283997874137, accuracy: 0.5133333333333333\n",
      "iteration no 197: Loss: 0.7567245857980736, accuracy: 0.5133333333333333\n",
      "iteration no 198: Loss: 0.7567208872101545, accuracy: 0.5133333333333333\n",
      "iteration no 199: Loss: 0.756717300440348, accuracy: 0.5133333333333333\n",
      "iteration no 200: Loss: 0.7567138220204463, accuracy: 0.5133333333333333\n",
      "iteration no 1: Loss: 1.0902173047088672, accuracy: 0.46\n",
      "iteration no 2: Loss: 1.060796595747014, accuracy: 0.49333333333333335\n",
      "iteration no 3: Loss: 1.0345529538599616, accuracy: 0.5133333333333333\n",
      "iteration no 4: Loss: 1.0111112819535673, accuracy: 0.5166666666666667\n",
      "iteration no 5: Loss: 0.9901378318308169, accuracy: 0.5133333333333333\n",
      "iteration no 6: Loss: 0.9713376531726632, accuracy: 0.5033333333333333\n",
      "iteration no 7: Loss: 0.954451440434687, accuracy: 0.5066666666666667\n",
      "iteration no 8: Loss: 0.9392520740141753, accuracy: 0.5066666666666667\n",
      "iteration no 9: Loss: 0.9255411109868602, accuracy: 0.5066666666666667\n",
      "iteration no 10: Loss: 0.91314540476956, accuracy: 0.51\n",
      "iteration no 11: Loss: 0.9019139621361402, accuracy: 0.51\n",
      "iteration no 12: Loss: 0.8917150919173809, accuracy: 0.51\n",
      "iteration no 13: Loss: 0.8824338627788864, accuracy: 0.51\n",
      "iteration no 14: Loss: 0.8739698643103098, accuracy: 0.5066666666666667\n",
      "iteration no 15: Loss: 0.8662352524024834, accuracy: 0.5066666666666667\n",
      "iteration no 16: Loss: 0.8591530532830177, accuracy: 0.5166666666666667\n",
      "iteration no 17: Loss: 0.8526556982069023, accuracy: 0.5166666666666667\n",
      "iteration no 18: Loss: 0.8466837609750568, accuracy: 0.5133333333333333\n",
      "iteration no 19: Loss: 0.8411848720621465, accuracy: 0.5133333333333333\n",
      "iteration no 20: Loss: 0.8361127854574641, accuracy: 0.51\n",
      "iteration no 21: Loss: 0.8314265769145163, accuracy: 0.51\n",
      "iteration no 22: Loss: 0.8270899549021797, accuracy: 0.51\n",
      "iteration no 23: Loss: 0.8230706680056645, accuracy: 0.51\n",
      "iteration no 24: Loss: 0.8193399947654602, accuracy: 0.51\n",
      "iteration no 25: Loss: 0.815872303938433, accuracy: 0.51\n",
      "iteration no 26: Loss: 0.8126446749154695, accuracy: 0.51\n",
      "iteration no 27: Loss: 0.8096365695474185, accuracy: 0.5133333333333333\n",
      "iteration no 28: Loss: 0.8068295479359952, accuracy: 0.5133333333333333\n",
      "iteration no 29: Loss: 0.804207021861995, accuracy: 0.5166666666666667\n",
      "iteration no 30: Loss: 0.8017540404731978, accuracy: 0.5166666666666667\n",
      "iteration no 31: Loss: 0.7994571036609952, accuracy: 0.5133333333333333\n",
      "iteration no 32: Loss: 0.7973039992384535, accuracy: 0.5133333333333333\n",
      "iteration no 33: Loss: 0.7952836606112714, accuracy: 0.5133333333333333\n",
      "iteration no 34: Loss: 0.7933860421228321, accuracy: 0.5133333333333333\n",
      "iteration no 35: Loss: 0.7916020096688672, accuracy: 0.5166666666666667\n",
      "iteration no 36: Loss: 0.7899232445279294, accuracy: 0.5166666666666667\n",
      "iteration no 37: Loss: 0.7883421586508437, accuracy: 0.5166666666666667\n",
      "iteration no 38: Loss: 0.7868518199040027, accuracy: 0.5166666666666667\n",
      "iteration no 39: Loss: 0.7854458859749408, accuracy: 0.5166666666666667\n",
      "iteration no 40: Loss: 0.7841185458300309, accuracy: 0.5133333333333333\n",
      "iteration no 41: Loss: 0.782864467768455, accuracy: 0.5133333333333333\n",
      "iteration no 42: Loss: 0.7816787532480405, accuracy: 0.5133333333333333\n",
      "iteration no 43: Loss: 0.7805568957706777, accuracy: 0.5133333333333333\n",
      "iteration no 44: Loss: 0.7794947442108319, accuracy: 0.5133333333333333\n",
      "iteration no 45: Loss: 0.7784884700526365, accuracy: 0.5166666666666667\n",
      "iteration no 46: Loss: 0.7775345380713322, accuracy: 0.5166666666666667\n",
      "iteration no 47: Loss: 0.7766296800551268, accuracy: 0.5166666666666667\n",
      "iteration no 48: Loss: 0.775770871215451, accuracy: 0.5166666666666667\n",
      "iteration no 49: Loss: 0.7749553089782616, accuracy: 0.5166666666666667\n",
      "iteration no 50: Loss: 0.7741803938876166, accuracy: 0.5166666666666667\n",
      "iteration no 51: Loss: 0.7734437123860711, accuracy: 0.5166666666666667\n",
      "iteration no 52: Loss: 0.7727430212653057, accuracy: 0.5166666666666667\n",
      "iteration no 53: Loss: 0.7720762336054188, accuracy: 0.5166666666666667\n",
      "iteration no 54: Loss: 0.7714414060430566, accuracy: 0.5166666666666667\n",
      "iteration no 55: Loss: 0.7708367272274572, accuracy: 0.5166666666666667\n",
      "iteration no 56: Loss: 0.770260507339983, accuracy: 0.5166666666666667\n",
      "iteration no 57: Loss: 0.769711168567079, accuracy: 0.5166666666666667\n",
      "iteration no 58: Loss: 0.769187236429194, accuracy: 0.5166666666666667\n",
      "iteration no 59: Loss: 0.7686873318791926, accuracy: 0.5166666666666667\n",
      "iteration no 60: Loss: 0.7682101640934575, accuracy: 0.5166666666666667\n",
      "iteration no 61: Loss: 0.7677545238873514, accuracy: 0.5166666666666667\n",
      "iteration no 62: Loss: 0.7673192776941735, accuracy: 0.5166666666666667\n",
      "iteration no 63: Loss: 0.7669033620533121, accuracy: 0.5166666666666667\n",
      "iteration no 64: Loss: 0.7665057785590937, accuracy: 0.5166666666666667\n",
      "iteration no 65: Loss: 0.7661255892269448, accuracy: 0.5166666666666667\n",
      "iteration no 66: Loss: 0.7657619122380178, accuracy: 0.5166666666666667\n",
      "iteration no 67: Loss: 0.7654139180274336, accuracy: 0.5166666666666667\n",
      "iteration no 68: Loss: 0.7650808256848691, accuracy: 0.5166666666666667\n",
      "iteration no 69: Loss: 0.7647618996393585, accuracy: 0.5166666666666667\n",
      "iteration no 70: Loss: 0.764456446603005, accuracy: 0.5166666666666667\n",
      "iteration no 71: Loss: 0.7641638127507939, accuracy: 0.5166666666666667\n",
      "iteration no 72: Loss: 0.7638833811159309, accuracy: 0.5166666666666667\n",
      "iteration no 73: Loss: 0.7636145691821247, accuracy: 0.5166666666666667\n",
      "iteration no 74: Loss: 0.7633568266560128, accuracy: 0.5166666666666667\n",
      "iteration no 75: Loss: 0.7631096334045194, accuracy: 0.5166666666666667\n",
      "iteration no 76: Loss: 0.762872497543368, accuracy: 0.5166666666666667\n",
      "iteration no 77: Loss: 0.7626449536642473, accuracy: 0.5166666666666667\n",
      "iteration no 78: Loss: 0.7624265611892816, accuracy: 0.5166666666666667\n",
      "iteration no 79: Loss: 0.7622169028424871, accuracy: 0.5133333333333333\n",
      "iteration no 80: Loss: 0.762015583228833, accuracy: 0.5133333333333333\n",
      "iteration no 81: Loss: 0.7618222275123533, accuracy: 0.5133333333333333\n",
      "iteration no 82: Loss: 0.7616364801855233, accuracy: 0.5133333333333333\n",
      "iteration no 83: Loss: 0.7614580039227888, accuracy: 0.5133333333333333\n",
      "iteration no 84: Loss: 0.761286478511752, accuracy: 0.5133333333333333\n",
      "iteration no 85: Loss: 0.7611215998560871, accuracy: 0.5133333333333333\n",
      "iteration no 86: Loss: 0.7609630790447459, accuracy: 0.5133333333333333\n",
      "iteration no 87: Loss: 0.76081064148249, accuracy: 0.5133333333333333\n",
      "iteration no 88: Loss: 0.7606640260771858, accuracy: 0.5133333333333333\n",
      "iteration no 89: Loss: 0.7605229844796884, accuracy: 0.5133333333333333\n",
      "iteration no 90: Loss: 0.7603872803724735, accuracy: 0.5133333333333333\n",
      "iteration no 91: Loss: 0.760256688803494, accuracy: 0.5133333333333333\n",
      "iteration no 92: Loss: 0.7601309955620207, accuracy: 0.5133333333333333\n",
      "iteration no 93: Loss: 0.7600099965934826, accuracy: 0.5133333333333333\n",
      "iteration no 94: Loss: 0.7598934974505638, accuracy: 0.5133333333333333\n",
      "iteration no 95: Loss: 0.7597813127780229, accuracy: 0.5133333333333333\n",
      "iteration no 96: Loss: 0.7596732658289028, accuracy: 0.5133333333333333\n",
      "iteration no 97: Loss: 0.7595691880099763, accuracy: 0.5133333333333333\n",
      "iteration no 98: Loss: 0.7594689184544371, accuracy: 0.5133333333333333\n",
      "iteration no 99: Loss: 0.7593723036199989, accuracy: 0.5166666666666667\n",
      "iteration no 100: Loss: 0.7592791969107021, accuracy: 0.5166666666666667\n",
      "iteration no 101: Loss: 0.7591894583208522, accuracy: 0.5166666666666667\n",
      "iteration no 102: Loss: 0.7591029540996371, accuracy: 0.5166666666666667\n",
      "iteration no 103: Loss: 0.7590195564350658, accuracy: 0.5166666666666667\n",
      "iteration no 104: Loss: 0.7589391431559845, accuracy: 0.5166666666666667\n",
      "iteration no 105: Loss: 0.7588615974510015, accuracy: 0.5166666666666667\n",
      "iteration no 106: Loss: 0.7587868076032445, accuracy: 0.5166666666666667\n",
      "iteration no 107: Loss: 0.7587146667399509, accuracy: 0.5166666666666667\n",
      "iteration no 108: Loss: 0.7586450725959544, accuracy: 0.5166666666666667\n",
      "iteration no 109: Loss: 0.7585779272902047, accuracy: 0.5166666666666667\n",
      "iteration no 110: Loss: 0.7585131371145147, accuracy: 0.5166666666666667\n",
      "iteration no 111: Loss: 0.7584506123337814, accuracy: 0.5166666666666667\n",
      "iteration no 112: Loss: 0.758390266996985, accuracy: 0.5166666666666667\n",
      "iteration no 113: Loss: 0.7583320187583089, accuracy: 0.5166666666666667\n",
      "iteration no 114: Loss: 0.7582757887077785, accuracy: 0.5166666666666667\n",
      "iteration no 115: Loss: 0.7582215012108464, accuracy: 0.5166666666666667\n",
      "iteration no 116: Loss: 0.7581690837563975, accuracy: 0.5166666666666667\n",
      "iteration no 117: Loss: 0.7581184668126755, accuracy: 0.5166666666666667\n",
      "iteration no 118: Loss: 0.7580695836906725, accuracy: 0.5166666666666667\n",
      "iteration no 119: Loss: 0.7580223704145448, accuracy: 0.5166666666666667\n",
      "iteration no 120: Loss: 0.7579767655986537, accuracy: 0.5166666666666667\n",
      "iteration no 121: Loss: 0.7579327103308512, accuracy: 0.5166666666666667\n",
      "iteration no 122: Loss: 0.7578901480616532, accuracy: 0.5166666666666667\n",
      "iteration no 123: Loss: 0.7578490244989736, accuracy: 0.5166666666666667\n",
      "iteration no 124: Loss: 0.7578092875081021, accuracy: 0.5166666666666667\n",
      "iteration no 125: Loss: 0.7577708870166375, accuracy: 0.5166666666666667\n",
      "iteration no 126: Loss: 0.7577337749240993, accuracy: 0.5166666666666667\n",
      "iteration no 127: Loss: 0.7576979050159642, accuracy: 0.5166666666666667\n",
      "iteration no 128: Loss: 0.757663232881881, accuracy: 0.5166666666666667\n",
      "iteration no 129: Loss: 0.7576297158378428, accuracy: 0.5166666666666667\n",
      "iteration no 130: Loss: 0.7575973128520975, accuracy: 0.5166666666666667\n",
      "iteration no 131: Loss: 0.7575659844746004, accuracy: 0.5166666666666667\n",
      "iteration no 132: Loss: 0.7575356927698188, accuracy: 0.5166666666666667\n",
      "iteration no 133: Loss: 0.7575064012527098, accuracy: 0.5166666666666667\n",
      "iteration no 134: Loss: 0.7574780748277086, accuracy: 0.5166666666666667\n",
      "iteration no 135: Loss: 0.7574506797305626, accuracy: 0.5166666666666667\n",
      "iteration no 136: Loss: 0.757424183472872, accuracy: 0.5133333333333333\n",
      "iteration no 137: Loss: 0.757398554789191, accuracy: 0.5133333333333333\n",
      "iteration no 138: Loss: 0.7573737635865596, accuracy: 0.5133333333333333\n",
      "iteration no 139: Loss: 0.7573497808963426, accuracy: 0.5133333333333333\n",
      "iteration no 140: Loss: 0.7573265788282584, accuracy: 0.5133333333333333\n",
      "iteration no 141: Loss: 0.7573041305264872, accuracy: 0.5133333333333333\n",
      "iteration no 142: Loss: 0.7572824101277521, accuracy: 0.5133333333333333\n",
      "iteration no 143: Loss: 0.7572613927212797, accuracy: 0.5133333333333333\n",
      "iteration no 144: Loss: 0.7572410543105409, accuracy: 0.5133333333333333\n",
      "iteration no 145: Loss: 0.757221371776687, accuracy: 0.5133333333333333\n",
      "iteration no 146: Loss: 0.7572023228435999, accuracy: 0.5133333333333333\n",
      "iteration no 147: Loss: 0.757183886044473, accuracy: 0.5133333333333333\n",
      "iteration no 148: Loss: 0.7571660406898517, accuracy: 0.5133333333333333\n",
      "iteration no 149: Loss: 0.7571487668370631, accuracy: 0.5133333333333333\n",
      "iteration no 150: Loss: 0.7571320452609657, accuracy: 0.5133333333333333\n",
      "iteration no 151: Loss: 0.7571158574259585, accuracy: 0.5133333333333333\n",
      "iteration no 152: Loss: 0.7571001854591887, accuracy: 0.5133333333333333\n",
      "iteration no 153: Loss: 0.7570850121249014, accuracy: 0.5133333333333333\n",
      "iteration no 154: Loss: 0.7570703207998766, accuracy: 0.5133333333333333\n",
      "iteration no 155: Loss: 0.7570560954499049, accuracy: 0.5133333333333333\n",
      "iteration no 156: Loss: 0.7570423206072515, accuracy: 0.5133333333333333\n",
      "iteration no 157: Loss: 0.7570289813490626, accuracy: 0.5133333333333333\n",
      "iteration no 158: Loss: 0.7570160632766735, accuracy: 0.5133333333333333\n",
      "iteration no 159: Loss: 0.7570035524957724, accuracy: 0.5133333333333333\n",
      "iteration no 160: Loss: 0.7569914355973864, accuracy: 0.5133333333333333\n",
      "iteration no 161: Loss: 0.756979699639647, accuracy: 0.5133333333333333\n",
      "iteration no 162: Loss: 0.7569683321303031, accuracy: 0.5133333333333333\n",
      "iteration no 163: Loss: 0.7569573210099488, accuracy: 0.5133333333333333\n",
      "iteration no 164: Loss: 0.756946654635929, accuracy: 0.5133333333333333\n",
      "iteration no 165: Loss: 0.7569363217668977, accuracy: 0.5133333333333333\n",
      "iteration no 166: Loss: 0.7569263115479993, accuracy: 0.5133333333333333\n",
      "iteration no 167: Loss: 0.7569166134966417, accuracy: 0.5133333333333333\n",
      "iteration no 168: Loss: 0.7569072174888403, accuracy: 0.5133333333333333\n",
      "iteration no 169: Loss: 0.7568981137461024, accuracy: 0.5133333333333333\n",
      "iteration no 170: Loss: 0.7568892928228357, accuracy: 0.5133333333333333\n",
      "iteration no 171: Loss: 0.7568807455942487, accuracy: 0.5133333333333333\n",
      "iteration no 172: Loss: 0.7568724632447323, accuracy: 0.5133333333333333\n",
      "iteration no 173: Loss: 0.7568644372566925, accuracy: 0.5133333333333333\n",
      "iteration no 174: Loss: 0.7568566593998206, accuracy: 0.5133333333333333\n",
      "iteration no 175: Loss: 0.7568491217207807, accuracy: 0.5133333333333333\n",
      "iteration no 176: Loss: 0.7568418165332976, accuracy: 0.5133333333333333\n",
      "iteration no 177: Loss: 0.7568347364086252, accuracy: 0.5133333333333333\n",
      "iteration no 178: Loss: 0.7568278741663824, accuracy: 0.5133333333333333\n",
      "iteration no 179: Loss: 0.7568212228657422, accuracy: 0.5133333333333333\n",
      "iteration no 180: Loss: 0.7568147757969536, accuracy: 0.5133333333333333\n",
      "iteration no 181: Loss: 0.7568085264731896, accuracy: 0.5133333333333333\n",
      "iteration no 182: Loss: 0.7568024686227024, accuracy: 0.5133333333333333\n",
      "iteration no 183: Loss: 0.7567965961812749, accuracy: 0.5133333333333333\n",
      "iteration no 184: Loss: 0.756790903284959, accuracy: 0.5133333333333333\n",
      "iteration no 185: Loss: 0.7567853842630832, accuracy: 0.5133333333333333\n",
      "iteration no 186: Loss: 0.756780033631524, accuracy: 0.5133333333333333\n",
      "iteration no 187: Loss: 0.7567748460862271, accuracy: 0.5133333333333333\n",
      "iteration no 188: Loss: 0.7567698164969715, accuracy: 0.5133333333333333\n",
      "iteration no 189: Loss: 0.7567649399013614, accuracy: 0.5133333333333333\n",
      "iteration no 190: Loss: 0.756760211499043, accuracy: 0.5133333333333333\n",
      "iteration no 191: Loss: 0.7567556266461326, accuracy: 0.5133333333333333\n",
      "iteration no 192: Loss: 0.7567511808498493, accuracy: 0.5133333333333333\n",
      "iteration no 193: Loss: 0.7567468697633454, accuracy: 0.5133333333333333\n",
      "iteration no 194: Loss: 0.7567426891807236, accuracy: 0.5133333333333333\n",
      "iteration no 195: Loss: 0.7567386350322374, accuracy: 0.5133333333333333\n",
      "iteration no 196: Loss: 0.7567347033796632, accuracy: 0.5133333333333333\n",
      "iteration no 197: Loss: 0.7567308904118425, accuracy: 0.5133333333333333\n",
      "iteration no 198: Loss: 0.7567271924403821, accuracy: 0.5133333333333333\n",
      "iteration no 199: Loss: 0.7567236058955114, accuracy: 0.5133333333333333\n",
      "iteration no 200: Loss: 0.7567201273220855, accuracy: 0.5133333333333333\n",
      "iteration no 1: Loss: 1.1490217115974666, accuracy: 0.2633333333333333\n",
      "iteration no 2: Loss: 1.1128741631845793, accuracy: 0.36333333333333334\n",
      "iteration no 3: Loss: 1.0807438747645484, accuracy: 0.49666666666666665\n",
      "iteration no 4: Loss: 1.052157508541664, accuracy: 0.49333333333333335\n",
      "iteration no 5: Loss: 1.0266906820691921, accuracy: 0.51\n",
      "iteration no 6: Loss: 1.0039655382923647, accuracy: 0.51\n",
      "iteration no 7: Loss: 0.9836481016848698, accuracy: 0.5066666666666667\n",
      "iteration no 8: Loss: 0.965445012534385, accuracy: 0.5066666666666667\n",
      "iteration no 9: Loss: 0.9490997915775387, accuracy: 0.5066666666666667\n",
      "iteration no 10: Loss: 0.9343889157153387, accuracy: 0.51\n",
      "iteration no 11: Loss: 0.9211179498007246, accuracy: 0.51\n",
      "iteration no 12: Loss: 0.9091179010770724, accuracy: 0.51\n",
      "iteration no 13: Loss: 0.898241889428447, accuracy: 0.51\n",
      "iteration no 14: Loss: 0.8883621719071615, accuracy: 0.51\n",
      "iteration no 15: Loss: 0.8793675241570665, accuracy: 0.51\n",
      "iteration no 16: Loss: 0.871160960252074, accuracy: 0.51\n",
      "iteration no 17: Loss: 0.8636577616088846, accuracy: 0.51\n",
      "iteration no 18: Loss: 0.8567837813412361, accuracy: 0.51\n",
      "iteration no 19: Loss: 0.8504739900682128, accuracy: 0.51\n",
      "iteration no 20: Loss: 0.8446712310158849, accuracy: 0.51\n",
      "iteration no 21: Loss: 0.8393251551479883, accuracy: 0.51\n",
      "iteration no 22: Loss: 0.8343913103548287, accuracy: 0.51\n",
      "iteration no 23: Loss: 0.8298303620331837, accuracy: 0.51\n",
      "iteration no 24: Loss: 0.8256074254963913, accuracy: 0.51\n",
      "iteration no 25: Loss: 0.8216914934652824, accuracy: 0.51\n",
      "iteration no 26: Loss: 0.8180549443737539, accuracy: 0.51\n",
      "iteration no 27: Loss: 0.8146731193804995, accuracy: 0.51\n",
      "iteration no 28: Loss: 0.8115239578325486, accuracy: 0.51\n",
      "iteration no 29: Loss: 0.8085876825072924, accuracy: 0.51\n",
      "iteration no 30: Loss: 0.8058465273006551, accuracy: 0.51\n",
      "iteration no 31: Loss: 0.8032845011623744, accuracy: 0.5133333333333333\n",
      "iteration no 32: Loss: 0.8008871830349028, accuracy: 0.5133333333333333\n",
      "iteration no 33: Loss: 0.7986415433569956, accuracy: 0.5133333333333333\n",
      "iteration no 34: Loss: 0.796535788370046, accuracy: 0.5133333333333333\n",
      "iteration no 35: Loss: 0.7945592240347975, accuracy: 0.5133333333333333\n",
      "iteration no 36: Loss: 0.7927021368454463, accuracy: 0.51\n",
      "iteration no 37: Loss: 0.7909556892318743, accuracy: 0.5133333333333333\n",
      "iteration no 38: Loss: 0.7893118275810954, accuracy: 0.5133333333333333\n",
      "iteration no 39: Loss: 0.7877632011962373, accuracy: 0.5133333333333333\n",
      "iteration no 40: Loss: 0.786303090754122, accuracy: 0.5133333333333333\n",
      "iteration no 41: Loss: 0.7849253450279433, accuracy: 0.5133333333333333\n",
      "iteration no 42: Loss: 0.7836243248156751, accuracy: 0.5133333333333333\n",
      "iteration no 43: Loss: 0.7823948531626834, accuracy: 0.5133333333333333\n",
      "iteration no 44: Loss: 0.7812321710927487, accuracy: 0.5133333333333333\n",
      "iteration no 45: Loss: 0.7801318981688224, accuracy: 0.5133333333333333\n",
      "iteration no 46: Loss: 0.7790899972962588, accuracy: 0.5133333333333333\n",
      "iteration no 47: Loss: 0.778102743259413, accuracy: 0.5133333333333333\n",
      "iteration no 48: Loss: 0.7771666945494511, accuracy: 0.5133333333333333\n",
      "iteration no 49: Loss: 0.7762786680986469, accuracy: 0.5133333333333333\n",
      "iteration no 50: Loss: 0.7754357165858208, accuracy: 0.5133333333333333\n",
      "iteration no 51: Loss: 0.7746351080200914, accuracy: 0.5133333333333333\n",
      "iteration no 52: Loss: 0.773874307346794, accuracy: 0.5133333333333333\n",
      "iteration no 53: Loss: 0.7731509598511119, accuracy: 0.5133333333333333\n",
      "iteration no 54: Loss: 0.7724628761624216, accuracy: 0.5133333333333333\n",
      "iteration no 55: Loss: 0.7718080186861321, accuracy: 0.5133333333333333\n",
      "iteration no 56: Loss: 0.7711844893104929, accuracy: 0.5133333333333333\n",
      "iteration no 57: Loss: 0.7705905182538206, accuracy: 0.5133333333333333\n",
      "iteration no 58: Loss: 0.7700244539332834, accuracy: 0.5133333333333333\n",
      "iteration no 59: Loss: 0.7694847537500659, accuracy: 0.5133333333333333\n",
      "iteration no 60: Loss: 0.7689699756977153, accuracy: 0.5166666666666667\n",
      "iteration no 61: Loss: 0.7684787707109522, accuracy: 0.5166666666666667\n",
      "iteration no 62: Loss: 0.7680098756814265, accuracy: 0.5166666666666667\n",
      "iteration no 63: Loss: 0.7675621070749844, accuracy: 0.5166666666666667\n",
      "iteration no 64: Loss: 0.7671343550921215, accuracy: 0.5166666666666667\n",
      "iteration no 65: Loss: 0.7667255783195639, accuracy: 0.5166666666666667\n",
      "iteration no 66: Loss: 0.76633479882645, accuracy: 0.5166666666666667\n",
      "iteration no 67: Loss: 0.7659610976634744, accuracy: 0.5166666666666667\n",
      "iteration no 68: Loss: 0.7656036107276829, accuracy: 0.5166666666666667\n",
      "iteration no 69: Loss: 0.7652615249594332, accuracy: 0.5166666666666667\n",
      "iteration no 70: Loss: 0.7649340748414529, accuracy: 0.5166666666666667\n",
      "iteration no 71: Loss: 0.7646205391729358, accuracy: 0.5166666666666667\n",
      "iteration no 72: Loss: 0.7643202380943177, accuracy: 0.5166666666666667\n",
      "iteration no 73: Loss: 0.7640325303407645, accuracy: 0.5166666666666667\n",
      "iteration no 74: Loss: 0.7637568107045484, accuracy: 0.5166666666666667\n",
      "iteration no 75: Loss: 0.7634925076883933, accuracy: 0.5166666666666667\n",
      "iteration no 76: Loss: 0.7632390813335838, accuracy: 0.5166666666666667\n",
      "iteration no 77: Loss: 0.7629960212081569, accuracy: 0.5166666666666667\n",
      "iteration no 78: Loss: 0.7627628445418708, accuracy: 0.5166666666666667\n",
      "iteration no 79: Loss: 0.7625390944958702, accuracy: 0.5166666666666667\n",
      "iteration no 80: Loss: 0.7623243385560807, accuracy: 0.5133333333333333\n",
      "iteration no 81: Loss: 0.7621181670403507, accuracy: 0.5133333333333333\n",
      "iteration no 82: Loss: 0.7619201917102604, accuracy: 0.5133333333333333\n",
      "iteration no 83: Loss: 0.7617300444793206, accuracy: 0.5133333333333333\n",
      "iteration no 84: Loss: 0.761547376210018, accuracy: 0.5133333333333333\n",
      "iteration no 85: Loss: 0.7613718555928114, accuracy: 0.5133333333333333\n",
      "iteration no 86: Loss: 0.7612031681007893, accuracy: 0.5133333333333333\n",
      "iteration no 87: Loss: 0.7610410150142286, accuracy: 0.5133333333333333\n",
      "iteration no 88: Loss: 0.760885112509788, accuracy: 0.5133333333333333\n",
      "iteration no 89: Loss: 0.7607351908095048, accuracy: 0.5133333333333333\n",
      "iteration no 90: Loss: 0.7605909933851764, accuracy: 0.5133333333333333\n",
      "iteration no 91: Loss: 0.7604522762140565, accuracy: 0.5133333333333333\n",
      "iteration no 92: Loss: 0.7603188070821455, accuracy: 0.5133333333333333\n",
      "iteration no 93: Loss: 0.7601903649316397, accuracy: 0.5133333333333333\n",
      "iteration no 94: Loss: 0.7600667392493917, accuracy: 0.5133333333333333\n",
      "iteration no 95: Loss: 0.7599477294934772, accuracy: 0.5133333333333333\n",
      "iteration no 96: Loss: 0.7598331445551941, accuracy: 0.5133333333333333\n",
      "iteration no 97: Loss: 0.7597228022540325, accuracy: 0.5133333333333333\n",
      "iteration no 98: Loss: 0.7596165288633359, accuracy: 0.5133333333333333\n",
      "iteration no 99: Loss: 0.7595141586645602, accuracy: 0.5133333333333333\n",
      "iteration no 100: Loss: 0.759415533528188, accuracy: 0.5133333333333333\n",
      "iteration no 101: Loss: 0.7593205025195047, accuracy: 0.5133333333333333\n",
      "iteration no 102: Loss: 0.7592289215275786, accuracy: 0.5133333333333333\n",
      "iteration no 103: Loss: 0.7591406529159107, accuracy: 0.5133333333333333\n",
      "iteration no 104: Loss: 0.7590555651933281, accuracy: 0.5166666666666667\n",
      "iteration no 105: Loss: 0.7589735327038046, accuracy: 0.5166666666666667\n",
      "iteration no 106: Loss: 0.7588944353339886, accuracy: 0.5166666666666667\n",
      "iteration no 107: Loss: 0.758818158237295, accuracy: 0.5166666666666667\n",
      "iteration no 108: Loss: 0.7587445915735154, accuracy: 0.5166666666666667\n",
      "iteration no 109: Loss: 0.7586736302629624, accuracy: 0.5166666666666667\n",
      "iteration no 110: Loss: 0.7586051737542383, accuracy: 0.5166666666666667\n",
      "iteration no 111: Loss: 0.7585391258047814, accuracy: 0.5166666666666667\n",
      "iteration no 112: Loss: 0.758475394273401, accuracy: 0.5166666666666667\n",
      "iteration no 113: Loss: 0.7584138909240653, accuracy: 0.5166666666666667\n",
      "iteration no 114: Loss: 0.7583545312402606, accuracy: 0.5166666666666667\n",
      "iteration no 115: Loss: 0.7582972342492807, accuracy: 0.5166666666666667\n",
      "iteration no 116: Loss: 0.7582419223558525, accuracy: 0.5166666666666667\n",
      "iteration no 117: Loss: 0.758188521184541, accuracy: 0.5166666666666667\n",
      "iteration no 118: Loss: 0.7581369594304184, accuracy: 0.5166666666666667\n",
      "iteration no 119: Loss: 0.7580871687175054, accuracy: 0.5166666666666667\n",
      "iteration no 120: Loss: 0.7580390834645396, accuracy: 0.5166666666666667\n",
      "iteration no 121: Loss: 0.7579926407576414, accuracy: 0.5166666666666667\n",
      "iteration no 122: Loss: 0.7579477802294816, accuracy: 0.5166666666666667\n",
      "iteration no 123: Loss: 0.7579044439445829, accuracy: 0.5166666666666667\n",
      "iteration no 124: Loss: 0.7578625762904025, accuracy: 0.5166666666666667\n",
      "iteration no 125: Loss: 0.7578221238738739, accuracy: 0.5166666666666667\n",
      "iteration no 126: Loss: 0.7577830354230999, accuracy: 0.5166666666666667\n",
      "iteration no 127: Loss: 0.757745261693912, accuracy: 0.5166666666666667\n",
      "iteration no 128: Loss: 0.7577087553810266, accuracy: 0.5166666666666667\n",
      "iteration no 129: Loss: 0.7576734710335442, accuracy: 0.5166666666666667\n",
      "iteration no 130: Loss: 0.7576393649745586, accuracy: 0.5166666666666667\n",
      "iteration no 131: Loss: 0.7576063952246495, accuracy: 0.5166666666666667\n",
      "iteration no 132: Loss: 0.7575745214290497, accuracy: 0.5166666666666667\n",
      "iteration no 133: Loss: 0.7575437047882926, accuracy: 0.5166666666666667\n",
      "iteration no 134: Loss: 0.7575139079921537, accuracy: 0.5166666666666667\n",
      "iteration no 135: Loss: 0.7574850951567101, accuracy: 0.5133333333333333\n",
      "iteration no 136: Loss: 0.7574572317643566, accuracy: 0.5133333333333333\n",
      "iteration no 137: Loss: 0.7574302846066223, accuracy: 0.5133333333333333\n",
      "iteration no 138: Loss: 0.7574042217296441, accuracy: 0.5133333333333333\n",
      "iteration no 139: Loss: 0.757379012382157, accuracy: 0.5133333333333333\n",
      "iteration no 140: Loss: 0.7573546269658754, accuracy: 0.5133333333333333\n",
      "iteration no 141: Loss: 0.7573310369881397, accuracy: 0.5133333333333333\n",
      "iteration no 142: Loss: 0.757308215016718, accuracy: 0.5133333333333333\n",
      "iteration no 143: Loss: 0.7572861346366478, accuracy: 0.5133333333333333\n",
      "iteration no 144: Loss: 0.7572647704090233, accuracy: 0.5133333333333333\n",
      "iteration no 145: Loss: 0.7572440978316227, accuracy: 0.5133333333333333\n",
      "iteration no 146: Loss: 0.7572240933012918, accuracy: 0.5133333333333333\n",
      "iteration no 147: Loss: 0.7572047340779913, accuracy: 0.5133333333333333\n",
      "iteration no 148: Loss: 0.7571859982504309, accuracy: 0.5133333333333333\n",
      "iteration no 149: Loss: 0.7571678647032095, accuracy: 0.5133333333333333\n",
      "iteration no 150: Loss: 0.7571503130853894, accuracy: 0.5133333333333333\n",
      "iteration no 151: Loss: 0.7571333237804367, accuracy: 0.5133333333333333\n",
      "iteration no 152: Loss: 0.7571168778774591, accuracy: 0.5133333333333333\n",
      "iteration no 153: Loss: 0.7571009571436822, accuracy: 0.5133333333333333\n",
      "iteration no 154: Loss: 0.7570855439981052, accuracy: 0.5133333333333333\n",
      "iteration no 155: Loss: 0.7570706214862772, accuracy: 0.5133333333333333\n",
      "iteration no 156: Loss: 0.7570561732561448, accuracy: 0.5133333333333333\n",
      "iteration no 157: Loss: 0.7570421835349198, accuracy: 0.5133333333333333\n",
      "iteration no 158: Loss: 0.7570286371069181, accuracy: 0.5133333333333333\n",
      "iteration no 159: Loss: 0.7570155192923282, accuracy: 0.5133333333333333\n",
      "iteration no 160: Loss: 0.7570028159268617, accuracy: 0.5133333333333333\n",
      "iteration no 161: Loss: 0.7569905133422491, accuracy: 0.5133333333333333\n",
      "iteration no 162: Loss: 0.7569785983475421, accuracy: 0.5133333333333333\n",
      "iteration no 163: Loss: 0.7569670582111829, accuracy: 0.5133333333333333\n",
      "iteration no 164: Loss: 0.7569558806438087, accuracy: 0.5133333333333333\n",
      "iteration no 165: Loss: 0.756945053781758, accuracy: 0.5133333333333333\n",
      "iteration no 166: Loss: 0.7569345661712451, accuracy: 0.5133333333333333\n",
      "iteration no 167: Loss: 0.7569244067531756, accuracy: 0.5133333333333333\n",
      "iteration no 168: Loss: 0.7569145648485734, accuracy: 0.5133333333333333\n",
      "iteration no 169: Loss: 0.7569050301445918, accuracy: 0.5133333333333333\n",
      "iteration no 170: Loss: 0.7568957926810829, accuracy: 0.5133333333333333\n",
      "iteration no 171: Loss: 0.7568868428377034, accuracy: 0.5133333333333333\n",
      "iteration no 172: Loss: 0.7568781713215289, accuracy: 0.5133333333333333\n",
      "iteration no 173: Loss: 0.7568697691551591, accuracy: 0.5133333333333333\n",
      "iteration no 174: Loss: 0.7568616276652894, accuracy: 0.5133333333333333\n",
      "iteration no 175: Loss: 0.7568537384717321, accuracy: 0.5133333333333333\n",
      "iteration no 176: Loss: 0.7568460934768632, accuracy: 0.5133333333333333\n",
      "iteration no 177: Loss: 0.7568386848554829, accuracy: 0.5133333333333333\n",
      "iteration no 178: Loss: 0.7568315050450666, accuracy: 0.5133333333333333\n",
      "iteration no 179: Loss: 0.7568245467363927, accuracy: 0.5133333333333333\n",
      "iteration no 180: Loss: 0.7568178028645314, accuracy: 0.5133333333333333\n",
      "iteration no 181: Loss: 0.7568112666001786, accuracy: 0.5133333333333333\n",
      "iteration no 182: Loss: 0.7568049313413204, accuracy: 0.5133333333333333\n",
      "iteration no 183: Loss: 0.756798790705215, accuracy: 0.5133333333333333\n",
      "iteration no 184: Loss: 0.7567928385206794, accuracy: 0.5133333333333333\n",
      "iteration no 185: Loss: 0.7567870688206653, accuracy: 0.5133333333333333\n",
      "iteration no 186: Loss: 0.7567814758351186, accuracy: 0.5133333333333333\n",
      "iteration no 187: Loss: 0.7567760539841025, accuracy: 0.5133333333333333\n",
      "iteration no 188: Loss: 0.7567707978711811, accuracy: 0.5133333333333333\n",
      "iteration no 189: Loss: 0.7567657022770478, accuracy: 0.5133333333333333\n",
      "iteration no 190: Loss: 0.7567607621533928, accuracy: 0.5133333333333333\n",
      "iteration no 191: Loss: 0.7567559726169946, accuracy: 0.5133333333333333\n",
      "iteration no 192: Loss: 0.7567513289440326, accuracy: 0.5133333333333333\n",
      "iteration no 193: Loss: 0.7567468265646077, accuracy: 0.5133333333333333\n",
      "iteration no 194: Loss: 0.7567424610574637, accuracy: 0.5133333333333333\n",
      "iteration no 195: Loss: 0.7567382281449038, accuracy: 0.5133333333333333\n",
      "iteration no 196: Loss: 0.7567341236878894, accuracy: 0.5133333333333333\n",
      "iteration no 197: Loss: 0.7567301436813209, accuracy: 0.5133333333333333\n",
      "iteration no 198: Loss: 0.7567262842494853, accuracy: 0.5133333333333333\n",
      "iteration no 199: Loss: 0.7567225416416732, accuracy: 0.5133333333333333\n",
      "iteration no 200: Loss: 0.7567189122279495, accuracy: 0.5133333333333333\n",
      "iteration no 1: Loss: 1.0956125586598269, accuracy: 0.48333333333333334\n",
      "iteration no 2: Loss: 1.065380977219688, accuracy: 0.49\n",
      "iteration no 3: Loss: 1.038478702641609, accuracy: 0.49\n",
      "iteration no 4: Loss: 1.0144979320376328, accuracy: 0.49333333333333335\n",
      "iteration no 5: Loss: 0.9930813169572673, accuracy: 0.49333333333333335\n",
      "iteration no 6: Loss: 0.9739155691732636, accuracy: 0.5033333333333333\n",
      "iteration no 7: Loss: 0.9567266254661391, accuracy: 0.5033333333333333\n",
      "iteration no 8: Loss: 0.9412753609204249, accuracy: 0.5\n",
      "iteration no 9: Loss: 0.9273535744558747, accuracy: 0.51\n",
      "iteration no 10: Loss: 0.9147802278016064, accuracy: 0.51\n",
      "iteration no 11: Loss: 0.9033979814916124, accuracy: 0.51\n",
      "iteration no 12: Loss: 0.8930700649735456, accuracy: 0.51\n",
      "iteration no 13: Loss: 0.8836774947361832, accuracy: 0.51\n",
      "iteration no 14: Loss: 0.8751166332017849, accuracy: 0.51\n",
      "iteration no 15: Loss: 0.8672970667948318, accuracy: 0.5066666666666667\n",
      "iteration no 16: Loss: 0.8601397737877234, accuracy: 0.51\n",
      "iteration no 17: Loss: 0.8535755495430365, accuracy: 0.51\n",
      "iteration no 18: Loss: 0.8475436569132907, accuracy: 0.51\n",
      "iteration no 19: Loss: 0.8419906715008835, accuracy: 0.51\n",
      "iteration no 20: Loss: 0.8368694943163268, accuracy: 0.51\n",
      "iteration no 21: Loss: 0.8321385075280954, accuracy: 0.51\n",
      "iteration no 22: Loss: 0.8277608521335574, accuracy: 0.51\n",
      "iteration no 23: Loss: 0.8237038093144659, accuracy: 0.51\n",
      "iteration no 24: Loss: 0.8199382698864355, accuracy: 0.51\n",
      "iteration no 25: Loss: 0.8164382785821256, accuracy: 0.51\n",
      "iteration no 26: Loss: 0.8131806419277441, accuracy: 0.51\n",
      "iteration no 27: Loss: 0.8101445902043635, accuracy: 0.51\n",
      "iteration no 28: Loss: 0.8073114854593745, accuracy: 0.5133333333333333\n",
      "iteration no 29: Loss: 0.8046645687812375, accuracy: 0.5133333333333333\n",
      "iteration no 30: Loss: 0.8021887411035833, accuracy: 0.5133333333333333\n",
      "iteration no 31: Loss: 0.7998703726912076, accuracy: 0.5133333333333333\n",
      "iteration no 32: Loss: 0.7976971372059991, accuracy: 0.5133333333333333\n",
      "iteration no 33: Loss: 0.7956578668774635, accuracy: 0.5133333333333333\n",
      "iteration no 34: Loss: 0.7937424258292322, accuracy: 0.5166666666666667\n",
      "iteration no 35: Loss: 0.7919415990559195, accuracy: 0.5166666666666667\n",
      "iteration no 36: Loss: 0.7902469949175489, accuracy: 0.5166666666666667\n",
      "iteration no 37: Loss: 0.7886509593329211, accuracy: 0.5166666666666667\n",
      "iteration no 38: Loss: 0.7871465001183452, accuracy: 0.5166666666666667\n",
      "iteration no 39: Loss: 0.785727220142088, accuracy: 0.5166666666666667\n",
      "iteration no 40: Loss: 0.7843872581543789, accuracy: 0.5166666666666667\n",
      "iteration no 41: Loss: 0.7831212363134186, accuracy: 0.5166666666666667\n",
      "iteration no 42: Loss: 0.7819242135642029, accuracy: 0.5166666666666667\n",
      "iteration no 43: Loss: 0.780791644142948, accuracy: 0.5133333333333333\n",
      "iteration no 44: Loss: 0.7797193405787302, accuracy: 0.5133333333333333\n",
      "iteration no 45: Loss: 0.7787034406483162, accuracy: 0.5133333333333333\n",
      "iteration no 46: Loss: 0.7777403778122995, accuracy: 0.5133333333333333\n",
      "iteration no 47: Loss: 0.7768268547224702, accuracy: 0.5133333333333333\n",
      "iteration no 48: Loss: 0.7759598194434085, accuracy: 0.5133333333333333\n",
      "iteration no 49: Loss: 0.7751364440769161, accuracy: 0.5133333333333333\n",
      "iteration no 50: Loss: 0.7743541055172062, accuracy: 0.5133333333333333\n",
      "iteration no 51: Loss: 0.7736103680987023, accuracy: 0.5133333333333333\n",
      "iteration no 52: Loss: 0.7729029679276324, accuracy: 0.5133333333333333\n",
      "iteration no 53: Loss: 0.7722297987140058, accuracy: 0.5133333333333333\n",
      "iteration no 54: Loss: 0.7715888989426188, accuracy: 0.5133333333333333\n",
      "iteration no 55: Loss: 0.7709784402408839, accuracy: 0.5133333333333333\n",
      "iteration no 56: Loss: 0.7703967168179785, accuracy: 0.5133333333333333\n",
      "iteration no 57: Loss: 0.7698421358643476, accuracy: 0.5133333333333333\n",
      "iteration no 58: Loss: 0.7693132088133131, accuracy: 0.5166666666666667\n",
      "iteration no 59: Loss: 0.7688085433776665, accuracy: 0.5166666666666667\n",
      "iteration no 60: Loss: 0.7683268362838688, accuracy: 0.5166666666666667\n",
      "iteration no 61: Loss: 0.7678668666350386, accuracy: 0.5166666666666667\n",
      "iteration no 62: Loss: 0.7674274898414334, accuracy: 0.5166666666666667\n",
      "iteration no 63: Loss: 0.767007632063754, accuracy: 0.5166666666666667\n",
      "iteration no 64: Loss: 0.7666062851204398, accuracy: 0.5166666666666667\n",
      "iteration no 65: Loss: 0.766222501815287, accuracy: 0.5166666666666667\n",
      "iteration no 66: Loss: 0.7658553916462737, accuracy: 0.5166666666666667\n",
      "iteration no 67: Loss: 0.7655041168605311, accuracy: 0.5166666666666667\n",
      "iteration no 68: Loss: 0.7651678888239613, accuracy: 0.5166666666666667\n",
      "iteration no 69: Loss: 0.7648459646772074, accuracy: 0.5166666666666667\n",
      "iteration no 70: Loss: 0.7645376442524934, accuracy: 0.5166666666666667\n",
      "iteration no 71: Loss: 0.764242267228375, accuracy: 0.5166666666666667\n",
      "iteration no 72: Loss: 0.7639592105016941, accuracy: 0.5166666666666667\n",
      "iteration no 73: Loss: 0.7636878857580262, accuracy: 0.5166666666666667\n",
      "iteration no 74: Loss: 0.763427737223707, accuracy: 0.5166666666666667\n",
      "iteration no 75: Loss: 0.7631782395841262, accuracy: 0.5166666666666667\n",
      "iteration no 76: Loss: 0.7629388960544103, accuracy: 0.5166666666666667\n",
      "iteration no 77: Loss: 0.7627092365899117, accuracy: 0.5166666666666667\n",
      "iteration no 78: Loss: 0.762488816225068, accuracy: 0.5166666666666667\n",
      "iteration no 79: Loss: 0.7622772135302477, accuracy: 0.5166666666666667\n",
      "iteration no 80: Loss: 0.7620740291771194, accuracy: 0.5166666666666667\n",
      "iteration no 81: Loss: 0.7618788846039447, accuracy: 0.5133333333333333\n",
      "iteration no 82: Loss: 0.7616914207729317, accuracy: 0.5133333333333333\n",
      "iteration no 83: Loss: 0.7615112970124973, accuracy: 0.5133333333333333\n",
      "iteration no 84: Loss: 0.7613381899378858, accuracy: 0.5133333333333333\n",
      "iteration no 85: Loss: 0.7611717924441694, accuracy: 0.5133333333333333\n",
      "iteration no 86: Loss: 0.7610118127661508, accuracy: 0.5133333333333333\n",
      "iteration no 87: Loss: 0.7608579736001627, accuracy: 0.5133333333333333\n",
      "iteration no 88: Loss: 0.7607100112831632, accuracy: 0.5133333333333333\n",
      "iteration no 89: Loss: 0.7605676750249177, accuracy: 0.5133333333333333\n",
      "iteration no 90: Loss: 0.7604307261893968, accuracy: 0.5133333333333333\n",
      "iteration no 91: Loss: 0.7602989376218331, accuracy: 0.5133333333333333\n",
      "iteration no 92: Loss: 0.7601720930181707, accuracy: 0.5133333333333333\n",
      "iteration no 93: Loss: 0.7600499863338976, accuracy: 0.5133333333333333\n",
      "iteration no 94: Loss: 0.7599324212294906, accuracy: 0.5133333333333333\n",
      "iteration no 95: Loss: 0.7598192105499177, accuracy: 0.5133333333333333\n",
      "iteration no 96: Loss: 0.7597101758358464, accuracy: 0.5133333333333333\n",
      "iteration no 97: Loss: 0.7596051468643793, accuracy: 0.5133333333333333\n",
      "iteration no 98: Loss: 0.7595039612173131, accuracy: 0.5133333333333333\n",
      "iteration no 99: Loss: 0.7594064638750634, accuracy: 0.5133333333333333\n",
      "iteration no 100: Loss: 0.7593125068345362, accuracy: 0.5133333333333333\n",
      "iteration no 101: Loss: 0.7592219487493649, accuracy: 0.5133333333333333\n",
      "iteration no 102: Loss: 0.7591346545910322, accuracy: 0.5166666666666667\n",
      "iteration no 103: Loss: 0.7590504953295195, accuracy: 0.5166666666666667\n",
      "iteration no 104: Loss: 0.7589693476322169, accuracy: 0.5166666666666667\n",
      "iteration no 105: Loss: 0.7588910935799212, accuracy: 0.5166666666666667\n",
      "iteration no 106: Loss: 0.7588156203988322, accuracy: 0.5166666666666667\n",
      "iteration no 107: Loss: 0.7587428202075336, accuracy: 0.5166666666666667\n",
      "iteration no 108: Loss: 0.7586725897780213, accuracy: 0.5166666666666667\n",
      "iteration no 109: Loss: 0.7586048303099002, accuracy: 0.5166666666666667\n",
      "iteration no 110: Loss: 0.7585394472169357, accuracy: 0.5166666666666667\n",
      "iteration no 111: Loss: 0.7584763499252039, accuracy: 0.5166666666666667\n",
      "iteration no 112: Loss: 0.7584154516821302, accuracy: 0.5166666666666667\n",
      "iteration no 113: Loss: 0.7583566693757611, accuracy: 0.5166666666666667\n",
      "iteration no 114: Loss: 0.7582999233636504, accuracy: 0.5166666666666667\n",
      "iteration no 115: Loss: 0.7582451373107916, accuracy: 0.5166666666666667\n",
      "iteration no 116: Loss: 0.758192238036056, accuracy: 0.5166666666666667\n",
      "iteration no 117: Loss: 0.7581411553666394, accuracy: 0.5166666666666667\n",
      "iteration no 118: Loss: 0.7580918220000501, accuracy: 0.5166666666666667\n",
      "iteration no 119: Loss: 0.7580441733731975, accuracy: 0.5166666666666667\n",
      "iteration no 120: Loss: 0.7579981475381766, accuracy: 0.5166666666666667\n",
      "iteration no 121: Loss: 0.7579536850443636, accuracy: 0.5166666666666667\n",
      "iteration no 122: Loss: 0.7579107288264625, accuracy: 0.5166666666666667\n",
      "iteration no 123: Loss: 0.7578692240981706, accuracy: 0.5166666666666667\n",
      "iteration no 124: Loss: 0.7578291182511441, accuracy: 0.5166666666666667\n",
      "iteration no 125: Loss: 0.75779036075897, accuracy: 0.5166666666666667\n",
      "iteration no 126: Loss: 0.7577529030858696, accuracy: 0.5166666666666667\n",
      "iteration no 127: Loss: 0.7577166985998692, accuracy: 0.5166666666666667\n",
      "iteration no 128: Loss: 0.7576817024901972, accuracy: 0.5166666666666667\n",
      "iteration no 129: Loss: 0.7576478716886771, accuracy: 0.5166666666666667\n",
      "iteration no 130: Loss: 0.7576151647949009, accuracy: 0.5166666666666667\n",
      "iteration no 131: Loss: 0.7575835420049795, accuracy: 0.5166666666666667\n",
      "iteration no 132: Loss: 0.7575529650436825, accuracy: 0.5133333333333333\n",
      "iteration no 133: Loss: 0.7575233970997828, accuracy: 0.5133333333333333\n",
      "iteration no 134: Loss: 0.7574948027644424, accuracy: 0.5133333333333333\n",
      "iteration no 135: Loss: 0.7574671479724775, accuracy: 0.5133333333333333\n",
      "iteration no 136: Loss: 0.757440399946354, accuracy: 0.5133333333333333\n",
      "iteration no 137: Loss: 0.7574145271427718, accuracy: 0.5133333333333333\n",
      "iteration no 138: Loss: 0.7573894992017055, accuracy: 0.5133333333333333\n",
      "iteration no 139: Loss: 0.7573652868977758, accuracy: 0.5133333333333333\n",
      "iteration no 140: Loss: 0.7573418620938331, accuracy: 0.5133333333333333\n",
      "iteration no 141: Loss: 0.7573191976966406, accuracy: 0.5133333333333333\n",
      "iteration no 142: Loss: 0.7572972676145543, accuracy: 0.5133333333333333\n",
      "iteration no 143: Loss: 0.7572760467170968, accuracy: 0.5133333333333333\n",
      "iteration no 144: Loss: 0.7572555107963321, accuracy: 0.5133333333333333\n",
      "iteration no 145: Loss: 0.7572356365299556, accuracy: 0.5133333333333333\n",
      "iteration no 146: Loss: 0.7572164014460075, accuracy: 0.5133333333333333\n",
      "iteration no 147: Loss: 0.7571977838891395, accuracy: 0.5133333333333333\n",
      "iteration no 148: Loss: 0.7571797629883518, accuracy: 0.5133333333333333\n",
      "iteration no 149: Loss: 0.7571623186261345, accuracy: 0.5133333333333333\n",
      "iteration no 150: Loss: 0.7571454314089422, accuracy: 0.5133333333333333\n",
      "iteration no 151: Loss: 0.7571290826389399, accuracy: 0.5133333333333333\n",
      "iteration no 152: Loss: 0.7571132542869615, accuracy: 0.5133333333333333\n",
      "iteration no 153: Loss: 0.7570979289666177, accuracy: 0.5133333333333333\n",
      "iteration no 154: Loss: 0.7570830899095071, accuracy: 0.5133333333333333\n",
      "iteration no 155: Loss: 0.7570687209414715, accuracy: 0.5133333333333333\n",
      "iteration no 156: Loss: 0.7570548064598525, accuracy: 0.5133333333333333\n",
      "iteration no 157: Loss: 0.7570413314116977, accuracy: 0.5133333333333333\n",
      "iteration no 158: Loss: 0.7570282812728791, accuracy: 0.5133333333333333\n",
      "iteration no 159: Loss: 0.7570156420280745, accuracy: 0.5133333333333333\n",
      "iteration no 160: Loss: 0.7570034001515784, accuracy: 0.5133333333333333\n",
      "iteration no 161: Loss: 0.756991542588902, accuracy: 0.5133333333333333\n",
      "iteration no 162: Loss: 0.7569800567391267, accuracy: 0.5133333333333333\n",
      "iteration no 163: Loss: 0.7569689304379782, accuracy: 0.5133333333333333\n",
      "iteration no 164: Loss: 0.756958151941587, accuracy: 0.5133333333333333\n",
      "iteration no 165: Loss: 0.7569477099109077, accuracy: 0.5133333333333333\n",
      "iteration no 166: Loss: 0.7569375933967645, accuracy: 0.5133333333333333\n",
      "iteration no 167: Loss: 0.756927791825498, accuracy: 0.5133333333333333\n",
      "iteration no 168: Loss: 0.7569182949851844, accuracy: 0.5133333333333333\n",
      "iteration no 169: Loss: 0.7569090930124055, accuracy: 0.5133333333333333\n",
      "iteration no 170: Loss: 0.7569001763795399, accuracy: 0.5133333333333333\n",
      "iteration no 171: Loss: 0.7568915358825596, accuracy: 0.5133333333333333\n",
      "iteration no 172: Loss: 0.7568831626293052, accuracy: 0.5133333333333333\n",
      "iteration no 173: Loss: 0.7568750480282203, accuracy: 0.5133333333333333\n",
      "iteration no 174: Loss: 0.756867183777527, accuracy: 0.5133333333333333\n",
      "iteration no 175: Loss: 0.7568595618548212, accuracy: 0.5133333333333333\n",
      "iteration no 176: Loss: 0.7568521745070715, accuracy: 0.5133333333333333\n",
      "iteration no 177: Loss: 0.7568450142410065, accuracy: 0.5133333333333333\n",
      "iteration no 178: Loss: 0.7568380738138673, accuracy: 0.5133333333333333\n",
      "iteration no 179: Loss: 0.7568313462245199, accuracy: 0.5133333333333333\n",
      "iteration no 180: Loss: 0.7568248247049035, accuracy: 0.5133333333333333\n",
      "iteration no 181: Loss: 0.7568185027118051, accuracy: 0.5133333333333333\n",
      "iteration no 182: Loss: 0.7568123739189473, accuracy: 0.5133333333333333\n",
      "iteration no 183: Loss: 0.7568064322093743, accuracy: 0.5133333333333333\n",
      "iteration no 184: Loss: 0.7568006716681247, accuracy: 0.5133333333333333\n",
      "iteration no 185: Loss: 0.7567950865751798, accuracy: 0.5133333333333333\n",
      "iteration no 186: Loss: 0.7567896713986765, accuracy: 0.5133333333333333\n",
      "iteration no 187: Loss: 0.7567844207883717, accuracy: 0.5133333333333333\n",
      "iteration no 188: Loss: 0.7567793295693512, accuracy: 0.5133333333333333\n",
      "iteration no 189: Loss: 0.7567743927359711, accuracy: 0.5133333333333333\n",
      "iteration no 190: Loss: 0.756769605446024, accuracy: 0.5133333333333333\n",
      "iteration no 191: Loss: 0.7567649630151184, accuracy: 0.5133333333333333\n",
      "iteration no 192: Loss: 0.7567604609112656, accuracy: 0.5133333333333333\n",
      "iteration no 193: Loss: 0.7567560947496649, accuracy: 0.5133333333333333\n",
      "iteration no 194: Loss: 0.7567518602876784, accuracy: 0.5133333333333333\n",
      "iteration no 195: Loss: 0.7567477534199893, accuracy: 0.5133333333333333\n",
      "iteration no 196: Loss: 0.7567437701739355, accuracy: 0.5133333333333333\n",
      "iteration no 197: Loss: 0.7567399067050107, accuracy: 0.5133333333333333\n",
      "iteration no 198: Loss: 0.7567361592925317, accuracy: 0.5133333333333333\n",
      "iteration no 199: Loss: 0.7567325243354561, accuracy: 0.5133333333333333\n",
      "iteration no 200: Loss: 0.7567289983483545, accuracy: 0.5133333333333333\n",
      "iteration no 1: Loss: 1.1076733338209976, accuracy: 0.44333333333333336\n",
      "iteration no 2: Loss: 1.075842090088232, accuracy: 0.47333333333333333\n",
      "iteration no 3: Loss: 1.0475540324523844, accuracy: 0.49333333333333335\n",
      "iteration no 4: Loss: 1.0223806397951247, accuracy: 0.5066666666666667\n",
      "iteration no 5: Loss: 0.9999409661307305, accuracy: 0.49666666666666665\n",
      "iteration no 6: Loss: 0.9798985246338636, accuracy: 0.5033333333333333\n",
      "iteration no 7: Loss: 0.9619579644921976, accuracy: 0.5066666666666667\n",
      "iteration no 8: Loss: 0.9458613578143219, accuracy: 0.5066666666666667\n",
      "iteration no 9: Loss: 0.9313842704834433, accuracy: 0.5166666666666667\n",
      "iteration no 10: Loss: 0.9183318463510763, accuracy: 0.52\n",
      "iteration no 11: Loss: 0.906535085032261, accuracy: 0.52\n",
      "iteration no 12: Loss: 0.8958474241570513, accuracy: 0.52\n",
      "iteration no 13: Loss: 0.8861416779171793, accuracy: 0.52\n",
      "iteration no 14: Loss: 0.8773073423015588, accuracy: 0.52\n",
      "iteration no 15: Loss: 0.8692482518494552, accuracy: 0.5233333333333333\n",
      "iteration no 16: Loss: 0.8618805589924227, accuracy: 0.5166666666666667\n",
      "iteration no 17: Loss: 0.8551310011740253, accuracy: 0.5166666666666667\n",
      "iteration no 18: Loss: 0.8489354198885724, accuracy: 0.5166666666666667\n",
      "iteration no 19: Loss: 0.8432374974553248, accuracy: 0.5166666666666667\n",
      "iteration no 20: Loss: 0.8379876803772183, accuracy: 0.5166666666666667\n",
      "iteration no 21: Loss: 0.8331422616890425, accuracy: 0.5166666666666667\n",
      "iteration no 22: Loss: 0.8286625982990861, accuracy: 0.5166666666666667\n",
      "iteration no 23: Loss: 0.8245144427148939, accuracy: 0.5166666666666667\n",
      "iteration no 24: Loss: 0.820667371598884, accuracy: 0.5166666666666667\n",
      "iteration no 25: Loss: 0.8170942962837211, accuracy: 0.5166666666666667\n",
      "iteration no 26: Loss: 0.813771042695121, accuracy: 0.5166666666666667\n",
      "iteration no 27: Loss: 0.8106759901081005, accuracy: 0.5166666666666667\n",
      "iteration no 28: Loss: 0.8077897598381918, accuracy: 0.5133333333333333\n",
      "iteration no 29: Loss: 0.8050949463807708, accuracy: 0.5133333333333333\n",
      "iteration no 30: Loss: 0.8025758846970344, accuracy: 0.5166666666666667\n",
      "iteration no 31: Loss: 0.8002184483385134, accuracy: 0.5166666666666667\n",
      "iteration no 32: Loss: 0.7980098739336233, accuracy: 0.5166666666666667\n",
      "iteration no 33: Loss: 0.7959386082557618, accuracy: 0.5133333333333333\n",
      "iteration no 34: Loss: 0.793994174675136, accuracy: 0.5133333333333333\n",
      "iteration no 35: Loss: 0.7921670562846511, accuracy: 0.5166666666666667\n",
      "iteration no 36: Loss: 0.7904485933995681, accuracy: 0.5166666666666667\n",
      "iteration no 37: Loss: 0.7888308934744012, accuracy: 0.5166666666666667\n",
      "iteration no 38: Loss: 0.7873067517696141, accuracy: 0.5166666666666667\n",
      "iteration no 39: Loss: 0.785869581344181, accuracy: 0.5166666666666667\n",
      "iteration no 40: Loss: 0.7845133511555514, accuracy: 0.5166666666666667\n",
      "iteration no 41: Loss: 0.7832325312222397, accuracy: 0.5166666666666667\n",
      "iteration no 42: Loss: 0.7820220439513641, accuracy: 0.5166666666666667\n",
      "iteration no 43: Loss: 0.7808772208582764, accuracy: 0.5166666666666667\n",
      "iteration no 44: Loss: 0.7797937640115409, accuracy: 0.5166666666666667\n",
      "iteration no 45: Loss: 0.7787677116269182, accuracy: 0.5166666666666667\n",
      "iteration no 46: Loss: 0.7777954073111688, accuracy: 0.5166666666666667\n",
      "iteration no 47: Loss: 0.7768734725224723, accuracy: 0.5166666666666667\n",
      "iteration no 48: Loss: 0.7759987818707977, accuracy: 0.5166666666666667\n",
      "iteration no 49: Loss: 0.7751684409301027, accuracy: 0.5166666666666667\n",
      "iteration no 50: Loss: 0.7743797662759954, accuracy: 0.5166666666666667\n",
      "iteration no 51: Loss: 0.7736302674984712, accuracy: 0.5166666666666667\n",
      "iteration no 52: Loss: 0.7729176309704159, accuracy: 0.5166666666666667\n",
      "iteration no 53: Loss: 0.7722397051794312, accuracy: 0.5166666666666667\n",
      "iteration no 54: Loss: 0.7715944874538475, accuracy: 0.5166666666666667\n",
      "iteration no 55: Loss: 0.7709801119339914, accuracy: 0.5133333333333333\n",
      "iteration no 56: Loss: 0.7703948386573756, accuracy: 0.5133333333333333\n",
      "iteration no 57: Loss: 0.769837043641797, accuracy: 0.5133333333333333\n",
      "iteration no 58: Loss: 0.769305209863695, accuracy: 0.5133333333333333\n",
      "iteration no 59: Loss: 0.7687979190408205, accuracy: 0.5133333333333333\n",
      "iteration no 60: Loss: 0.7683138441384904, accuracy: 0.5133333333333333\n",
      "iteration no 61: Loss: 0.7678517425276858, accuracy: 0.5133333333333333\n",
      "iteration no 62: Loss: 0.7674104497311404, accuracy: 0.5133333333333333\n",
      "iteration no 63: Loss: 0.7669888737004836, accuracy: 0.5133333333333333\n",
      "iteration no 64: Loss: 0.7665859895736455, accuracy: 0.5133333333333333\n",
      "iteration no 65: Loss: 0.7662008348670944, accuracy: 0.5166666666666667\n",
      "iteration no 66: Loss: 0.7658325050622641, accuracy: 0.5166666666666667\n",
      "iteration no 67: Loss: 0.765480149549741, accuracy: 0.5166666666666667\n",
      "iteration no 68: Loss: 0.7651429678985149, accuracy: 0.5166666666666667\n",
      "iteration no 69: Loss: 0.7648202064209262, accuracy: 0.5166666666666667\n",
      "iteration no 70: Loss: 0.7645111550068788, accuracy: 0.5166666666666667\n",
      "iteration no 71: Loss: 0.7642151442035215, accuracy: 0.5166666666666667\n",
      "iteration no 72: Loss: 0.7639315425189355, accuracy: 0.5166666666666667\n",
      "iteration no 73: Loss: 0.76365975393045, accuracy: 0.5166666666666667\n",
      "iteration no 74: Loss: 0.763399215580075, accuracy: 0.5166666666666667\n",
      "iteration no 75: Loss: 0.7631493956412015, accuracy: 0.5166666666666667\n",
      "iteration no 76: Loss: 0.7629097913422224, accuracy: 0.5166666666666667\n",
      "iteration no 77: Loss: 0.7626799271340478, accuracy: 0.5166666666666667\n",
      "iteration no 78: Loss: 0.762459352989712, accuracy: 0.5166666666666667\n",
      "iteration no 79: Loss: 0.7622476428253256, accuracy: 0.5133333333333333\n",
      "iteration no 80: Loss: 0.762044393032616, accuracy: 0.5133333333333333\n",
      "iteration no 81: Loss: 0.7618492211141614, accuracy: 0.5133333333333333\n",
      "iteration no 82: Loss: 0.7616617644132188, accuracy: 0.5133333333333333\n",
      "iteration no 83: Loss: 0.7614816789307524, accuracy: 0.5133333333333333\n",
      "iteration no 84: Loss: 0.7613086382229178, accuracy: 0.5133333333333333\n",
      "iteration no 85: Loss: 0.7611423323728316, accuracy: 0.5133333333333333\n",
      "iteration no 86: Loss: 0.7609824670309878, accuracy: 0.5133333333333333\n",
      "iteration no 87: Loss: 0.7608287625191568, accuracy: 0.5133333333333333\n",
      "iteration no 88: Loss: 0.7606809529930346, accuracy: 0.5133333333333333\n",
      "iteration no 89: Loss: 0.7605387856593033, accuracy: 0.5133333333333333\n",
      "iteration no 90: Loss: 0.7604020200431209, accuracy: 0.5133333333333333\n",
      "iteration no 91: Loss: 0.7602704273023824, accuracy: 0.5133333333333333\n",
      "iteration no 92: Loss: 0.7601437895853891, accuracy: 0.5133333333333333\n",
      "iteration no 93: Loss: 0.7600218994288336, accuracy: 0.5133333333333333\n",
      "iteration no 94: Loss: 0.7599045591932503, accuracy: 0.5133333333333333\n",
      "iteration no 95: Loss: 0.7597915805333104, accuracy: 0.5133333333333333\n",
      "iteration no 96: Loss: 0.7596827839005384, accuracy: 0.5133333333333333\n",
      "iteration no 97: Loss: 0.7595779980762201, accuracy: 0.5133333333333333\n",
      "iteration no 98: Loss: 0.7594770597324382, accuracy: 0.5133333333333333\n",
      "iteration no 99: Loss: 0.7593798130193335, accuracy: 0.5133333333333333\n",
      "iteration no 100: Loss: 0.7592861091768274, accuracy: 0.5133333333333333\n",
      "iteration no 101: Loss: 0.7591958061691765, accuracy: 0.5133333333333333\n",
      "iteration no 102: Loss: 0.7591087683408547, accuracy: 0.5133333333333333\n",
      "iteration no 103: Loss: 0.7590248660923561, accuracy: 0.5133333333333333\n",
      "iteration no 104: Loss: 0.7589439755746313, accuracy: 0.5133333333333333\n",
      "iteration no 105: Loss: 0.7588659784009513, accuracy: 0.5133333333333333\n",
      "iteration no 106: Loss: 0.7587907613750794, accuracy: 0.5166666666666667\n",
      "iteration no 107: Loss: 0.7587182162347238, accuracy: 0.5166666666666667\n",
      "iteration no 108: Loss: 0.7586482394092962, accuracy: 0.5166666666666667\n",
      "iteration no 109: Loss: 0.7585807317910906, accuracy: 0.5166666666666667\n",
      "iteration no 110: Loss: 0.7585155985190428, accuracy: 0.5166666666666667\n",
      "iteration no 111: Loss: 0.7584527487742968, accuracy: 0.5166666666666667\n",
      "iteration no 112: Loss: 0.7583920955868558, accuracy: 0.5166666666666667\n",
      "iteration no 113: Loss: 0.7583335556526435, accuracy: 0.5166666666666667\n",
      "iteration no 114: Loss: 0.7582770491603479, accuracy: 0.5166666666666667\n",
      "iteration no 115: Loss: 0.7582224996274618, accuracy: 0.5166666666666667\n",
      "iteration no 116: Loss: 0.7581698337449722, accuracy: 0.5166666666666667\n",
      "iteration no 117: Loss: 0.7581189812301897, accuracy: 0.5166666666666667\n",
      "iteration no 118: Loss: 0.7580698746872389, accuracy: 0.5166666666666667\n",
      "iteration no 119: Loss: 0.7580224494747628, accuracy: 0.5166666666666667\n",
      "iteration no 120: Loss: 0.7579766435804277, accuracy: 0.5166666666666667\n",
      "iteration no 121: Loss: 0.7579323975018327, accuracy: 0.5166666666666667\n",
      "iteration no 122: Loss: 0.7578896541334619, accuracy: 0.5166666666666667\n",
      "iteration no 123: Loss: 0.7578483586593374, accuracy: 0.5166666666666667\n",
      "iteration no 124: Loss: 0.757808458451047, accuracy: 0.5166666666666667\n",
      "iteration no 125: Loss: 0.7577699029708524, accuracy: 0.5166666666666667\n",
      "iteration no 126: Loss: 0.7577326436795913, accuracy: 0.5166666666666667\n",
      "iteration no 127: Loss: 0.7576966339491085, accuracy: 0.5166666666666667\n",
      "iteration no 128: Loss: 0.7576618289789718, accuracy: 0.5166666666666667\n",
      "iteration no 129: Loss: 0.7576281857172327, accuracy: 0.5166666666666667\n",
      "iteration no 130: Loss: 0.7575956627850177, accuracy: 0.5133333333333333\n",
      "iteration no 131: Loss: 0.7575642204047425, accuracy: 0.5133333333333333\n",
      "iteration no 132: Loss: 0.757533820331754, accuracy: 0.5133333333333333\n",
      "iteration no 133: Loss: 0.7575044257892194, accuracy: 0.5133333333333333\n",
      "iteration no 134: Loss: 0.7574760014060896, accuracy: 0.5133333333333333\n",
      "iteration no 135: Loss: 0.7574485131579758, accuracy: 0.5133333333333333\n",
      "iteration no 136: Loss: 0.7574219283107879, accuracy: 0.5133333333333333\n",
      "iteration no 137: Loss: 0.7573962153669893, accuracy: 0.5133333333333333\n",
      "iteration no 138: Loss: 0.7573713440143355, accuracy: 0.5133333333333333\n",
      "iteration no 139: Loss: 0.7573472850769659, accuracy: 0.5133333333333333\n",
      "iteration no 140: Loss: 0.7573240104687331, accuracy: 0.5133333333333333\n",
      "iteration no 141: Loss: 0.7573014931486517, accuracy: 0.5133333333333333\n",
      "iteration no 142: Loss: 0.7572797070783617, accuracy: 0.5133333333333333\n",
      "iteration no 143: Loss: 0.757258627181507, accuracy: 0.5133333333333333\n",
      "iteration no 144: Loss: 0.7572382293049299, accuracy: 0.5133333333333333\n",
      "iteration no 145: Loss: 0.7572184901815947, accuracy: 0.5133333333333333\n",
      "iteration no 146: Loss: 0.757199387395153, accuracy: 0.5133333333333333\n",
      "iteration no 147: Loss: 0.7571808993460721, accuracy: 0.5133333333333333\n",
      "iteration no 148: Loss: 0.7571630052192482, accuracy: 0.5133333333333333\n",
      "iteration no 149: Loss: 0.757145684953033, accuracy: 0.5133333333333333\n",
      "iteration no 150: Loss: 0.7571289192096075, accuracy: 0.5133333333333333\n",
      "iteration no 151: Loss: 0.7571126893466334, accuracy: 0.5133333333333333\n",
      "iteration no 152: Loss: 0.7570969773901288, accuracy: 0.5133333333333333\n",
      "iteration no 153: Loss: 0.7570817660084994, accuracy: 0.5133333333333333\n",
      "iteration no 154: Loss: 0.7570670384876823, accuracy: 0.5133333333333333\n",
      "iteration no 155: Loss: 0.7570527787073411, accuracy: 0.5133333333333333\n",
      "iteration no 156: Loss: 0.7570389711180667, accuracy: 0.5133333333333333\n",
      "iteration no 157: Loss: 0.7570256007195376, accuracy: 0.5133333333333333\n",
      "iteration no 158: Loss: 0.7570126530395933, accuracy: 0.5133333333333333\n",
      "iteration no 159: Loss: 0.7570001141141804, accuracy: 0.5133333333333333\n",
      "iteration no 160: Loss: 0.7569879704681299, accuracy: 0.5133333333333333\n",
      "iteration no 161: Loss: 0.7569762090967288, accuracy: 0.5133333333333333\n",
      "iteration no 162: Loss: 0.7569648174480504, accuracy: 0.5133333333333333\n",
      "iteration no 163: Loss: 0.7569537834060063, accuracy: 0.5133333333333333\n",
      "iteration no 164: Loss: 0.7569430952740914, accuracy: 0.5133333333333333\n",
      "iteration no 165: Loss: 0.7569327417597879, accuracy: 0.5133333333333333\n",
      "iteration no 166: Loss: 0.7569227119596008, accuracy: 0.5133333333333333\n",
      "iteration no 167: Loss: 0.7569129953446947, accuracy: 0.5133333333333333\n",
      "iteration no 168: Loss: 0.7569035817471085, accuracy: 0.5133333333333333\n",
      "iteration no 169: Loss: 0.7568944613465198, accuracy: 0.5133333333333333\n",
      "iteration no 170: Loss: 0.7568856246575358, accuracy: 0.5133333333333333\n",
      "iteration no 171: Loss: 0.756877062517489, accuracy: 0.5133333333333333\n",
      "iteration no 172: Loss: 0.7568687660747124, accuracy: 0.5133333333333333\n",
      "iteration no 173: Loss: 0.75686072677728, accuracy: 0.5133333333333333\n",
      "iteration no 174: Loss: 0.7568529363621829, accuracy: 0.5133333333333333\n",
      "iteration no 175: Loss: 0.7568453868449335, accuracy: 0.5133333333333333\n",
      "iteration no 176: Loss: 0.7568380705095696, accuracy: 0.5133333333333333\n",
      "iteration no 177: Loss: 0.7568309798990482, accuracy: 0.5133333333333333\n",
      "iteration no 178: Loss: 0.7568241078060091, accuracy: 0.5133333333333333\n",
      "iteration no 179: Loss: 0.7568174472638937, accuracy: 0.5133333333333333\n",
      "iteration no 180: Loss: 0.7568109915384053, accuracy: 0.5133333333333333\n",
      "iteration no 181: Loss: 0.7568047341192937, accuracy: 0.5133333333333333\n",
      "iteration no 182: Loss: 0.7567986687124542, accuracy: 0.5133333333333333\n",
      "iteration no 183: Loss: 0.7567927892323244, accuracy: 0.5133333333333333\n",
      "iteration no 184: Loss: 0.7567870897945704, accuracy: 0.5133333333333333\n",
      "iteration no 185: Loss: 0.7567815647090466, accuracy: 0.5133333333333333\n",
      "iteration no 186: Loss: 0.7567762084730207, accuracy: 0.5133333333333333\n",
      "iteration no 187: Loss: 0.7567710157646524, accuracy: 0.5133333333333333\n",
      "iteration no 188: Loss: 0.7567659814367143, accuracy: 0.5133333333333333\n",
      "iteration no 189: Loss: 0.7567611005105468, accuracy: 0.5133333333333333\n",
      "iteration no 190: Loss: 0.7567563681702376, accuracy: 0.5133333333333333\n",
      "iteration no 191: Loss: 0.7567517797570145, accuracy: 0.5133333333333333\n",
      "iteration no 192: Loss: 0.7567473307638459, accuracy: 0.5133333333333333\n",
      "iteration no 193: Loss: 0.7567430168302396, accuracy: 0.5133333333333333\n",
      "iteration no 194: Loss: 0.7567388337372306, accuracy: 0.5133333333333333\n",
      "iteration no 195: Loss: 0.7567347774025531, accuracy: 0.5133333333333333\n",
      "iteration no 196: Loss: 0.7567308438759872, accuracy: 0.5133333333333333\n",
      "iteration no 197: Loss: 0.7567270293348748, accuracy: 0.5133333333333333\n",
      "iteration no 198: Loss: 0.7567233300797975, accuracy: 0.5133333333333333\n",
      "iteration no 199: Loss: 0.7567197425304095, accuracy: 0.5133333333333333\n",
      "iteration no 200: Loss: 0.7567162632214218, accuracy: 0.5166666666666667\n",
      "iteration no 1: Loss: 1.1054756149675693, accuracy: 0.4766666666666667\n",
      "iteration no 2: Loss: 1.0740556328063993, accuracy: 0.49666666666666665\n",
      "iteration no 3: Loss: 1.0461250587373605, accuracy: 0.5\n",
      "iteration no 4: Loss: 1.0212530430559792, accuracy: 0.5033333333333333\n",
      "iteration no 5: Loss: 0.999063575047261, accuracy: 0.5033333333333333\n",
      "iteration no 6: Loss: 0.9792273178980349, accuracy: 0.5\n",
      "iteration no 7: Loss: 0.9614561333372377, accuracy: 0.5\n",
      "iteration no 8: Loss: 0.9454985524830273, accuracy: 0.5033333333333333\n",
      "iteration no 9: Loss: 0.9311356245210919, accuracy: 0.5033333333333333\n",
      "iteration no 10: Loss: 0.9181770260447967, accuracy: 0.5066666666666667\n",
      "iteration no 11: Loss: 0.9064574535582317, accuracy: 0.5133333333333333\n",
      "iteration no 12: Loss: 0.8958333409519325, accuracy: 0.5133333333333333\n",
      "iteration no 13: Loss: 0.8861799256953886, accuracy: 0.51\n",
      "iteration no 14: Loss: 0.8773886644964526, accuracy: 0.51\n",
      "iteration no 15: Loss: 0.8693649815964243, accuracy: 0.5133333333333333\n",
      "iteration no 16: Loss: 0.8620263222932524, accuracy: 0.5133333333333333\n",
      "iteration no 17: Loss: 0.8553004794292177, accuracy: 0.5133333333333333\n",
      "iteration no 18: Loss: 0.849124159630557, accuracy: 0.5133333333333333\n",
      "iteration no 19: Loss: 0.8434417574979218, accuracy: 0.5133333333333333\n",
      "iteration no 20: Loss: 0.8382043086068038, accuracy: 0.5133333333333333\n",
      "iteration no 21: Loss: 0.8333685953611807, accuracy: 0.5133333333333333\n",
      "iteration no 22: Loss: 0.8288963830135131, accuracy: 0.5133333333333333\n",
      "iteration no 23: Loss: 0.8247537662756489, accuracy: 0.5133333333333333\n",
      "iteration no 24: Loss: 0.820910609777443, accuracy: 0.5133333333333333\n",
      "iteration no 25: Loss: 0.8173400681369574, accuracy: 0.5133333333333333\n",
      "iteration no 26: Loss: 0.8140181735847404, accuracy: 0.5133333333333333\n",
      "iteration no 27: Loss: 0.8109234809544406, accuracy: 0.5133333333333333\n",
      "iteration no 28: Loss: 0.8080367614430953, accuracy: 0.5166666666666667\n",
      "iteration no 29: Loss: 0.8053407378905688, accuracy: 0.5166666666666667\n",
      "iteration no 30: Loss: 0.8028198554621595, accuracy: 0.5166666666666667\n",
      "iteration no 31: Loss: 0.8004600825722865, accuracy: 0.5166666666666667\n",
      "iteration no 32: Loss: 0.7982487376881009, accuracy: 0.5166666666666667\n",
      "iteration no 33: Loss: 0.7961743383239377, accuracy: 0.5166666666666667\n",
      "iteration no 34: Loss: 0.7942264691015268, accuracy: 0.52\n",
      "iteration no 35: Loss: 0.7923956662243681, accuracy: 0.52\n",
      "iteration no 36: Loss: 0.7906733161125514, accuracy: 0.5166666666666667\n",
      "iteration no 37: Loss: 0.7890515662789718, accuracy: 0.5166666666666667\n",
      "iteration no 38: Loss: 0.7875232468097955, accuracy: 0.5166666666666667\n",
      "iteration no 39: Loss: 0.7860818010498258, accuracy: 0.5166666666666667\n",
      "iteration no 40: Loss: 0.7847212242943338, accuracy: 0.5166666666666667\n",
      "iteration no 41: Loss: 0.78343600945896, accuracy: 0.5166666666666667\n",
      "iteration no 42: Loss: 0.7822210988434657, accuracy: 0.5166666666666667\n",
      "iteration no 43: Loss: 0.7810718412275655, accuracy: 0.5133333333333333\n",
      "iteration no 44: Loss: 0.7799839536412815, accuracy: 0.5133333333333333\n",
      "iteration no 45: Loss: 0.7789534872411025, accuracy: 0.5133333333333333\n",
      "iteration no 46: Loss: 0.77797679679912, accuracy: 0.5133333333333333\n",
      "iteration no 47: Loss: 0.7770505133772619, accuracy: 0.5133333333333333\n",
      "iteration no 48: Loss: 0.7761715198144288, accuracy: 0.5133333333333333\n",
      "iteration no 49: Loss: 0.7753369287021786, accuracy: 0.5133333333333333\n",
      "iteration no 50: Loss: 0.7745440625657761, accuracy: 0.5133333333333333\n",
      "iteration no 51: Loss: 0.773790436002929, accuracy: 0.5133333333333333\n",
      "iteration no 52: Loss: 0.7730737395631918, accuracy: 0.5133333333333333\n",
      "iteration no 53: Loss: 0.7723918251775714, accuracy: 0.5133333333333333\n",
      "iteration no 54: Loss: 0.7717426929708654, accuracy: 0.5133333333333333\n",
      "iteration no 55: Loss: 0.771124479309255, accuracy: 0.5133333333333333\n",
      "iteration no 56: Loss: 0.7705354459530611, accuracy: 0.5133333333333333\n",
      "iteration no 57: Loss: 0.7699739701997169, accuracy: 0.5133333333333333\n",
      "iteration no 58: Loss: 0.7694385359152478, accuracy: 0.5133333333333333\n",
      "iteration no 59: Loss: 0.7689277253641033, accuracy: 0.5133333333333333\n",
      "iteration no 60: Loss: 0.7684402117573277, accuracy: 0.5166666666666667\n",
      "iteration no 61: Loss: 0.7679747524479335, accuracy: 0.5166666666666667\n",
      "iteration no 62: Loss: 0.7675301827101514, accuracy: 0.5166666666666667\n",
      "iteration no 63: Loss: 0.767105410046106, accuracy: 0.5166666666666667\n",
      "iteration no 64: Loss: 0.7666994089695123, accuracy: 0.5166666666666667\n",
      "iteration no 65: Loss: 0.7663112162213416, accuracy: 0.5166666666666667\n",
      "iteration no 66: Loss: 0.765939926377121, accuracy: 0.5166666666666667\n",
      "iteration no 67: Loss: 0.7655846878097191, accuracy: 0.5166666666666667\n",
      "iteration no 68: Loss: 0.7652446989751692, accuracy: 0.5166666666666667\n",
      "iteration no 69: Loss: 0.7649192049923782, accuracy: 0.5166666666666667\n",
      "iteration no 70: Loss: 0.7646074944904881, accuracy: 0.5166666666666667\n",
      "iteration no 71: Loss: 0.7643088967002626, accuracy: 0.5166666666666667\n",
      "iteration no 72: Loss: 0.7640227787681855, accuracy: 0.5166666666666667\n",
      "iteration no 73: Loss: 0.7637485432740357, accuracy: 0.5166666666666667\n",
      "iteration no 74: Loss: 0.7634856259345417, accuracy: 0.5166666666666667\n",
      "iteration no 75: Loss: 0.7632334934773787, accuracy: 0.5166666666666667\n",
      "iteration no 76: Loss: 0.7629916416712523, accuracy: 0.5166666666666667\n",
      "iteration no 77: Loss: 0.7627595934991368, accuracy: 0.5166666666666667\n",
      "iteration no 78: Loss: 0.76253689746293, accuracy: 0.5166666666666667\n",
      "iteration no 79: Loss: 0.7623231260088609, accuracy: 0.5166666666666667\n",
      "iteration no 80: Loss: 0.7621178740639417, accuracy: 0.5166666666666667\n",
      "iteration no 81: Loss: 0.7619207576746357, accuracy: 0.5166666666666667\n",
      "iteration no 82: Loss: 0.7617314127396819, accuracy: 0.5133333333333333\n",
      "iteration no 83: Loss: 0.7615494938297345, accuracy: 0.5133333333333333\n",
      "iteration no 84: Loss: 0.7613746730871079, accuracy: 0.5133333333333333\n",
      "iteration no 85: Loss: 0.7612066391994948, accuracy: 0.5133333333333333\n",
      "iteration no 86: Loss: 0.7610450964420514, accuracy: 0.5133333333333333\n",
      "iteration no 87: Loss: 0.760889763782713, accuracy: 0.5133333333333333\n",
      "iteration no 88: Loss: 0.7607403740460352, accuracy: 0.5133333333333333\n",
      "iteration no 89: Loss: 0.760596673131245, accuracy: 0.5133333333333333\n",
      "iteration no 90: Loss: 0.7604584192805408, accuracy: 0.5133333333333333\n",
      "iteration no 91: Loss: 0.7603253823940028, accuracy: 0.5133333333333333\n",
      "iteration no 92: Loss: 0.7601973433877688, accuracy: 0.5133333333333333\n",
      "iteration no 93: Loss: 0.7600740935923939, accuracy: 0.5133333333333333\n",
      "iteration no 94: Loss: 0.7599554341885678, accuracy: 0.5133333333333333\n",
      "iteration no 95: Loss: 0.7598411756775687, accuracy: 0.5133333333333333\n",
      "iteration no 96: Loss: 0.7597311373840533, accuracy: 0.5133333333333333\n",
      "iteration no 97: Loss: 0.7596251469889558, accuracy: 0.5133333333333333\n",
      "iteration no 98: Loss: 0.7595230400904462, accuracy: 0.5133333333333333\n",
      "iteration no 99: Loss: 0.7594246597910503, accuracy: 0.5133333333333333\n",
      "iteration no 100: Loss: 0.7593298563091775, accuracy: 0.5133333333333333\n",
      "iteration no 101: Loss: 0.7592384866134338, accuracy: 0.5133333333333333\n",
      "iteration no 102: Loss: 0.7591504140782188, accuracy: 0.5133333333333333\n",
      "iteration no 103: Loss: 0.7590655081592099, accuracy: 0.5166666666666667\n",
      "iteration no 104: Loss: 0.7589836440874472, accuracy: 0.5166666666666667\n",
      "iteration no 105: Loss: 0.7589047025808201, accuracy: 0.5166666666666667\n",
      "iteration no 106: Loss: 0.7588285695718404, accuracy: 0.5166666666666667\n",
      "iteration no 107: Loss: 0.758755135950675, accuracy: 0.5166666666666667\n",
      "iteration no 108: Loss: 0.7586842973224712, accuracy: 0.5166666666666667\n",
      "iteration no 109: Loss: 0.7586159537780897, accuracy: 0.5166666666666667\n",
      "iteration no 110: Loss: 0.7585500096774056, accuracy: 0.5166666666666667\n",
      "iteration no 111: Loss: 0.7584863734444126, accuracy: 0.5166666666666667\n",
      "iteration no 112: Loss: 0.7584249573734024, accuracy: 0.5166666666666667\n",
      "iteration no 113: Loss: 0.7583656774455537, accuracy: 0.5166666666666667\n",
      "iteration no 114: Loss: 0.7583084531552997, accuracy: 0.5166666666666667\n",
      "iteration no 115: Loss: 0.7582532073458925, accuracy: 0.5166666666666667\n",
      "iteration no 116: Loss: 0.758199866053618, accuracy: 0.5166666666666667\n",
      "iteration no 117: Loss: 0.7581483583601513, accuracy: 0.5166666666666667\n",
      "iteration no 118: Loss: 0.7580986162525764, accuracy: 0.5166666666666667\n",
      "iteration no 119: Loss: 0.7580505744906255, accuracy: 0.5166666666666667\n",
      "iteration no 120: Loss: 0.7580041704807199, accuracy: 0.5166666666666667\n",
      "iteration no 121: Loss: 0.7579593441564246, accuracy: 0.5166666666666667\n",
      "iteration no 122: Loss: 0.7579160378649501, accuracy: 0.5166666666666667\n",
      "iteration no 123: Loss: 0.7578741962593594, accuracy: 0.5166666666666667\n",
      "iteration no 124: Loss: 0.7578337661961607, accuracy: 0.5166666666666667\n",
      "iteration no 125: Loss: 0.7577946966379848, accuracy: 0.5166666666666667\n",
      "iteration no 126: Loss: 0.7577569385610642, accuracy: 0.5166666666666667\n",
      "iteration no 127: Loss: 0.757720444867251, accuracy: 0.5166666666666667\n",
      "iteration no 128: Loss: 0.7576851703003247, accuracy: 0.5166666666666667\n",
      "iteration no 129: Loss: 0.7576510713663562, accuracy: 0.5166666666666667\n",
      "iteration no 130: Loss: 0.7576181062579086, accuracy: 0.5166666666666667\n",
      "iteration no 131: Loss: 0.7575862347818697, accuracy: 0.5133333333333333\n",
      "iteration no 132: Loss: 0.7575554182907213, accuracy: 0.5133333333333333\n",
      "iteration no 133: Loss: 0.757525619617063, accuracy: 0.5133333333333333\n",
      "iteration no 134: Loss: 0.7574968030112209, accuracy: 0.5133333333333333\n",
      "iteration no 135: Loss: 0.7574689340817757, accuracy: 0.5133333333333333\n",
      "iteration no 136: Loss: 0.7574419797388628, accuracy: 0.5133333333333333\n",
      "iteration no 137: Loss: 0.7574159081400955, accuracy: 0.5133333333333333\n",
      "iteration no 138: Loss: 0.7573906886389821, accuracy: 0.5133333333333333\n",
      "iteration no 139: Loss: 0.7573662917357032, accuracy: 0.5133333333333333\n",
      "iteration no 140: Loss: 0.7573426890301355, accuracy: 0.5133333333333333\n",
      "iteration no 141: Loss: 0.7573198531770011, accuracy: 0.5133333333333333\n",
      "iteration no 142: Loss: 0.7572977578430442, accuracy: 0.5133333333333333\n",
      "iteration no 143: Loss: 0.7572763776661233, accuracy: 0.5133333333333333\n",
      "iteration no 144: Loss: 0.7572556882161343, accuracy: 0.5133333333333333\n",
      "iteration no 145: Loss: 0.7572356659576657, accuracy: 0.5133333333333333\n",
      "iteration no 146: Loss: 0.7572162882143048, accuracy: 0.5133333333333333\n",
      "iteration no 147: Loss: 0.7571975331345127, accuracy: 0.5133333333333333\n",
      "iteration no 148: Loss: 0.7571793796589933, accuracy: 0.5133333333333333\n",
      "iteration no 149: Loss: 0.7571618074894819, accuracy: 0.5133333333333333\n",
      "iteration no 150: Loss: 0.7571447970588855, accuracy: 0.5133333333333333\n",
      "iteration no 151: Loss: 0.7571283295027118, accuracy: 0.5133333333333333\n",
      "iteration no 152: Loss: 0.7571123866317235, accuracy: 0.5133333333333333\n",
      "iteration no 153: Loss: 0.757096950905761, accuracy: 0.5133333333333333\n",
      "iteration no 154: Loss: 0.7570820054086778, accuracy: 0.5133333333333333\n",
      "iteration no 155: Loss: 0.7570675338243359, accuracy: 0.5133333333333333\n",
      "iteration no 156: Loss: 0.7570535204136136, accuracy: 0.5133333333333333\n",
      "iteration no 157: Loss: 0.7570399499923776, accuracy: 0.5133333333333333\n",
      "iteration no 158: Loss: 0.7570268079103724, accuracy: 0.5133333333333333\n",
      "iteration no 159: Loss: 0.7570140800309912, accuracy: 0.5133333333333333\n",
      "iteration no 160: Loss: 0.7570017527118799, accuracy: 0.5133333333333333\n",
      "iteration no 161: Loss: 0.7569898127863425, accuracy: 0.5133333333333333\n",
      "iteration no 162: Loss: 0.7569782475455095, accuracy: 0.5133333333333333\n",
      "iteration no 163: Loss: 0.7569670447212334, accuracy: 0.5133333333333333\n",
      "iteration no 164: Loss: 0.7569561924696823, accuracy: 0.5133333333333333\n",
      "iteration no 165: Loss: 0.756945679355595, accuracy: 0.5133333333333333\n",
      "iteration no 166: Loss: 0.7569354943371744, accuracy: 0.5133333333333333\n",
      "iteration no 167: Loss: 0.7569256267515864, accuracy: 0.5133333333333333\n",
      "iteration no 168: Loss: 0.7569160663010397, accuracy: 0.5133333333333333\n",
      "iteration no 169: Loss: 0.7569068030394195, accuracy: 0.5133333333333333\n",
      "iteration no 170: Loss: 0.7568978273594532, accuracy: 0.5133333333333333\n",
      "iteration no 171: Loss: 0.7568891299803828, accuracy: 0.5133333333333333\n",
      "iteration no 172: Loss: 0.7568807019361238, accuracy: 0.5133333333333333\n",
      "iteration no 173: Loss: 0.7568725345638868, accuracy: 0.5133333333333333\n",
      "iteration no 174: Loss: 0.7568646194932479, accuracy: 0.5133333333333333\n",
      "iteration no 175: Loss: 0.7568569486356393, accuracy: 0.5133333333333333\n",
      "iteration no 176: Loss: 0.7568495141742534, accuracy: 0.5133333333333333\n",
      "iteration no 177: Loss: 0.7568423085543327, accuracy: 0.5133333333333333\n",
      "iteration no 178: Loss: 0.7568353244738365, accuracy: 0.5133333333333333\n",
      "iteration no 179: Loss: 0.756828554874465, accuracy: 0.5133333333333333\n",
      "iteration no 180: Loss: 0.7568219929330262, accuracy: 0.5133333333333333\n",
      "iteration no 181: Loss: 0.7568156320531328, accuracy: 0.5133333333333333\n",
      "iteration no 182: Loss: 0.7568094658572138, accuracy: 0.5133333333333333\n",
      "iteration no 183: Loss: 0.7568034881788283, accuracy: 0.5133333333333333\n",
      "iteration no 184: Loss: 0.7567976930552698, accuracy: 0.5133333333333333\n",
      "iteration no 185: Loss: 0.7567920747204487, accuracy: 0.5133333333333333\n",
      "iteration no 186: Loss: 0.7567866275980406, accuracy: 0.5133333333333333\n",
      "iteration no 187: Loss: 0.7567813462948904, accuracy: 0.5133333333333333\n",
      "iteration no 188: Loss: 0.7567762255946636, accuracy: 0.5133333333333333\n",
      "iteration no 189: Loss: 0.7567712604517316, accuracy: 0.5133333333333333\n",
      "iteration no 190: Loss: 0.7567664459852828, accuracy: 0.5133333333333333\n",
      "iteration no 191: Loss: 0.7567617774736526, accuracy: 0.5133333333333333\n",
      "iteration no 192: Loss: 0.75675725034886, accuracy: 0.5133333333333333\n",
      "iteration no 193: Loss: 0.7567528601913456, accuracy: 0.5133333333333333\n",
      "iteration no 194: Loss: 0.7567486027249009, accuracy: 0.5133333333333333\n",
      "iteration no 195: Loss: 0.7567444738117834, accuracy: 0.5133333333333333\n",
      "iteration no 196: Loss: 0.7567404694480069, accuracy: 0.5133333333333333\n",
      "iteration no 197: Loss: 0.7567365857588054, accuracy: 0.5133333333333333\n",
      "iteration no 198: Loss: 0.7567328189942586, accuracy: 0.5133333333333333\n",
      "iteration no 199: Loss: 0.7567291655250734, accuracy: 0.5133333333333333\n",
      "iteration no 200: Loss: 0.7567256218385223, accuracy: 0.5133333333333333\n",
      "iteration no 1: Loss: 1.0859126549048133, accuracy: 0.44\n",
      "iteration no 2: Loss: 1.0565873792457712, accuracy: 0.4633333333333333\n",
      "iteration no 3: Loss: 1.0305045991832726, accuracy: 0.4866666666666667\n",
      "iteration no 4: Loss: 1.0072662403185835, accuracy: 0.49\n",
      "iteration no 5: Loss: 0.9865210714760881, accuracy: 0.49\n",
      "iteration no 6: Loss: 0.9679609484796022, accuracy: 0.49\n",
      "iteration no 7: Loss: 0.9513168476071807, accuracy: 0.49666666666666665\n",
      "iteration no 8: Loss: 0.936354742098786, accuracy: 0.5\n",
      "iteration no 9: Loss: 0.9228715110280098, accuracy: 0.5033333333333333\n",
      "iteration no 10: Loss: 0.910691051906777, accuracy: 0.51\n",
      "iteration no 11: Loss: 0.8996607088230291, accuracy: 0.5233333333333333\n",
      "iteration no 12: Loss: 0.8896480705136638, accuracy: 0.5233333333333333\n",
      "iteration no 13: Loss: 0.8805381504003007, accuracy: 0.5233333333333333\n",
      "iteration no 14: Loss: 0.8722309337499766, accuracy: 0.5233333333333333\n",
      "iteration no 15: Loss: 0.8646392623682766, accuracy: 0.5233333333333333\n",
      "iteration no 16: Loss: 0.857687020757371, accuracy: 0.52\n",
      "iteration no 17: Loss: 0.8513075863549692, accuracy: 0.52\n",
      "iteration no 18: Loss: 0.8454425080927735, accuracy: 0.5166666666666667\n",
      "iteration no 19: Loss: 0.8400403806193618, accuracy: 0.5133333333333333\n",
      "iteration no 20: Loss: 0.8350558852262464, accuracy: 0.5133333333333333\n",
      "iteration no 21: Loss: 0.8304489722776429, accuracy: 0.5133333333333333\n",
      "iteration no 22: Loss: 0.8261841634956792, accuracy: 0.5133333333333333\n",
      "iteration no 23: Loss: 0.8222299556620755, accuracy: 0.5166666666666667\n",
      "iteration no 24: Loss: 0.8185583101197073, accuracy: 0.5166666666666667\n",
      "iteration no 25: Loss: 0.8151442148957397, accuracy: 0.5166666666666667\n",
      "iteration no 26: Loss: 0.8119653083495081, accuracy: 0.5166666666666667\n",
      "iteration no 27: Loss: 0.8090015550110081, accuracy: 0.5166666666666667\n",
      "iteration no 28: Loss: 0.8062349657606045, accuracy: 0.5166666666666667\n",
      "iteration no 29: Loss: 0.8036493557468629, accuracy: 0.5166666666666667\n",
      "iteration no 30: Loss: 0.8012301344833903, accuracy: 0.5166666666666667\n",
      "iteration no 31: Loss: 0.798964123439117, accuracy: 0.52\n",
      "iteration no 32: Loss: 0.7968393971671787, accuracy: 0.5166666666666667\n",
      "iteration no 33: Loss: 0.7948451446289585, accuracy: 0.5133333333333333\n",
      "iteration no 34: Loss: 0.7929715478817814, accuracy: 0.5133333333333333\n",
      "iteration no 35: Loss: 0.7912096757278219, accuracy: 0.5133333333333333\n",
      "iteration no 36: Loss: 0.7895513902818907, accuracy: 0.5133333333333333\n",
      "iteration no 37: Loss: 0.7879892647184179, accuracy: 0.5166666666666667\n",
      "iteration no 38: Loss: 0.7865165107127456, accuracy: 0.5166666666666667\n",
      "iteration no 39: Loss: 0.7851269143067198, accuracy: 0.5166666666666667\n",
      "iteration no 40: Loss: 0.7838147791101099, accuracy: 0.5166666666666667\n",
      "iteration no 41: Loss: 0.7825748759030603, accuracy: 0.5166666666666667\n",
      "iteration no 42: Loss: 0.7814023978350975, accuracy: 0.5166666666666667\n",
      "iteration no 43: Loss: 0.7802929205269751, accuracy: 0.5166666666666667\n",
      "iteration no 44: Loss: 0.7792423664759297, accuracy: 0.5166666666666667\n",
      "iteration no 45: Loss: 0.7782469732453673, accuracy: 0.5166666666666667\n",
      "iteration no 46: Loss: 0.7773032649887739, accuracy: 0.5166666666666667\n",
      "iteration no 47: Loss: 0.7764080269165371, accuracy: 0.5166666666666667\n",
      "iteration no 48: Loss: 0.7755582823649161, accuracy: 0.5166666666666667\n",
      "iteration no 49: Loss: 0.7747512721698567, accuracy: 0.5166666666666667\n",
      "iteration no 50: Loss: 0.7739844360857936, accuracy: 0.5166666666666667\n",
      "iteration no 51: Loss: 0.7732553960218917, accuracy: 0.5166666666666667\n",
      "iteration no 52: Loss: 0.7725619408961333, accuracy: 0.5166666666666667\n",
      "iteration no 53: Loss: 0.7719020129318546, accuracy: 0.5133333333333333\n",
      "iteration no 54: Loss: 0.771273695242357, accuracy: 0.5133333333333333\n",
      "iteration no 55: Loss: 0.7706752005674788, accuracy: 0.5133333333333333\n",
      "iteration no 56: Loss: 0.7701048610419242, accuracy: 0.5133333333333333\n",
      "iteration no 57: Loss: 0.7695611188890302, accuracy: 0.5133333333333333\n",
      "iteration no 58: Loss: 0.7690425179457764, accuracy: 0.5133333333333333\n",
      "iteration no 59: Loss: 0.7685476959354662, accuracy: 0.5133333333333333\n",
      "iteration no 60: Loss: 0.7680753774138173, accuracy: 0.5133333333333333\n",
      "iteration no 61: Loss: 0.7676243673223713, accuracy: 0.5133333333333333\n",
      "iteration no 62: Loss: 0.7671935450903316, accuracy: 0.5133333333333333\n",
      "iteration no 63: Loss: 0.7667818592322633, accuracy: 0.5133333333333333\n",
      "iteration no 64: Loss: 0.7663883223946902, accuracy: 0.5133333333333333\n",
      "iteration no 65: Loss: 0.7660120068095497, accuracy: 0.5166666666666667\n",
      "iteration no 66: Loss: 0.7656520401168455, accuracy: 0.5166666666666667\n",
      "iteration no 67: Loss: 0.7653076015227027, accuracy: 0.5166666666666667\n",
      "iteration no 68: Loss: 0.7649779182624716, accuracy: 0.5166666666666667\n",
      "iteration no 69: Loss: 0.7646622623415724, accuracy: 0.5166666666666667\n",
      "iteration no 70: Loss: 0.7643599475294933, accuracy: 0.5166666666666667\n",
      "iteration no 71: Loss: 0.7640703265847715, accuracy: 0.5166666666666667\n",
      "iteration no 72: Loss: 0.7637927886909444, accuracy: 0.5166666666666667\n",
      "iteration no 73: Loss: 0.7635267570853901, accuracy: 0.5166666666666667\n",
      "iteration no 74: Loss: 0.7632716868646926, accuracy: 0.5166666666666667\n",
      "iteration no 75: Loss: 0.7630270629517144, accuracy: 0.5166666666666667\n",
      "iteration no 76: Loss: 0.7627923982109459, accuracy: 0.5166666666666667\n",
      "iteration no 77: Loss: 0.7625672316999345, accuracy: 0.5166666666666667\n",
      "iteration no 78: Loss: 0.7623511270457178, accuracy: 0.5166666666666667\n",
      "iteration no 79: Loss: 0.7621436709361871, accuracy: 0.5166666666666667\n",
      "iteration no 80: Loss: 0.7619444717172069, accuracy: 0.5133333333333333\n",
      "iteration no 81: Loss: 0.7617531580871397, accuracy: 0.5133333333333333\n",
      "iteration no 82: Loss: 0.7615693778811459, accuracy: 0.5133333333333333\n",
      "iteration no 83: Loss: 0.7613927969383087, accuracy: 0.5133333333333333\n",
      "iteration no 84: Loss: 0.7612230980452198, accuracy: 0.5133333333333333\n",
      "iteration no 85: Loss: 0.7610599799502137, accuracy: 0.5133333333333333\n",
      "iteration no 86: Loss: 0.7609031564429275, accuracy: 0.5133333333333333\n",
      "iteration no 87: Loss: 0.7607523554943094, accuracy: 0.5133333333333333\n",
      "iteration no 88: Loss: 0.7606073184526031, accuracy: 0.5133333333333333\n",
      "iteration no 89: Loss: 0.7604677992912084, accuracy: 0.5133333333333333\n",
      "iteration no 90: Loss: 0.7603335639046466, accuracy: 0.5133333333333333\n",
      "iteration no 91: Loss: 0.760204389449165, accuracy: 0.5133333333333333\n",
      "iteration no 92: Loss: 0.7600800637248015, accuracy: 0.5133333333333333\n",
      "iteration no 93: Loss: 0.7599603845959663, accuracy: 0.5133333333333333\n",
      "iteration no 94: Loss: 0.7598451594478468, accuracy: 0.5133333333333333\n",
      "iteration no 95: Loss: 0.7597342046761391, accuracy: 0.5133333333333333\n",
      "iteration no 96: Loss: 0.7596273452078113, accuracy: 0.5133333333333333\n",
      "iteration no 97: Loss: 0.7595244140507718, accuracy: 0.5133333333333333\n",
      "iteration no 98: Loss: 0.7594252518704883, accuracy: 0.5133333333333333\n",
      "iteration no 99: Loss: 0.7593297065917382, accuracy: 0.5133333333333333\n",
      "iteration no 100: Loss: 0.7592376330238179, accuracy: 0.5133333333333333\n",
      "iteration no 101: Loss: 0.7591488925076566, accuracy: 0.5133333333333333\n",
      "iteration no 102: Loss: 0.7590633525833965, accuracy: 0.5133333333333333\n",
      "iteration no 103: Loss: 0.7589808866771067, accuracy: 0.5133333333333333\n",
      "iteration no 104: Loss: 0.7589013738053922, accuracy: 0.5133333333333333\n",
      "iteration no 105: Loss: 0.7588246982967544, accuracy: 0.5166666666666667\n",
      "iteration no 106: Loss: 0.7587507495286311, accuracy: 0.5166666666666667\n",
      "iteration no 107: Loss: 0.7586794216791322, accuracy: 0.5166666666666667\n",
      "iteration no 108: Loss: 0.7586106134925447, accuracy: 0.5166666666666667\n",
      "iteration no 109: Loss: 0.7585442280577523, accuracy: 0.5166666666666667\n",
      "iteration no 110: Loss: 0.7584801725987744, accuracy: 0.5166666666666667\n",
      "iteration no 111: Loss: 0.7584183582766764, accuracy: 0.5166666666666667\n",
      "iteration no 112: Loss: 0.7583587000021635, accuracy: 0.5166666666666667\n",
      "iteration no 113: Loss: 0.7583011162582126, accuracy: 0.5166666666666667\n",
      "iteration no 114: Loss: 0.7582455289321361, accuracy: 0.5166666666666667\n",
      "iteration no 115: Loss: 0.75819186315652, accuracy: 0.5166666666666667\n",
      "iteration no 116: Loss: 0.7581400471585102, accuracy: 0.5166666666666667\n",
      "iteration no 117: Loss: 0.7580900121169558, accuracy: 0.5166666666666667\n",
      "iteration no 118: Loss: 0.7580416920269518, accuracy: 0.5166666666666667\n",
      "iteration no 119: Loss: 0.7579950235713516, accuracy: 0.5166666666666667\n",
      "iteration no 120: Loss: 0.7579499459988515, accuracy: 0.5166666666666667\n",
      "iteration no 121: Loss: 0.7579064010082659, accuracy: 0.5166666666666667\n",
      "iteration no 122: Loss: 0.7578643326386482, accuracy: 0.5166666666666667\n",
      "iteration no 123: Loss: 0.7578236871649222, accuracy: 0.5166666666666667\n",
      "iteration no 124: Loss: 0.7577844129987192, accuracy: 0.5166666666666667\n",
      "iteration no 125: Loss: 0.7577464605941286, accuracy: 0.5166666666666667\n",
      "iteration no 126: Loss: 0.7577097823580908, accuracy: 0.5166666666666667\n",
      "iteration no 127: Loss: 0.7576743325651791, accuracy: 0.5166666666666667\n",
      "iteration no 128: Loss: 0.757640067276527, accuracy: 0.5166666666666667\n",
      "iteration no 129: Loss: 0.7576069442626816, accuracy: 0.5166666666666667\n",
      "iteration no 130: Loss: 0.7575749229301648, accuracy: 0.5133333333333333\n",
      "iteration no 131: Loss: 0.7575439642515495, accuracy: 0.5133333333333333\n",
      "iteration no 132: Loss: 0.7575140306988594, accuracy: 0.5133333333333333\n",
      "iteration no 133: Loss: 0.7574850861801189, accuracy: 0.5133333333333333\n",
      "iteration no 134: Loss: 0.7574570959788864, accuracy: 0.5133333333333333\n",
      "iteration no 135: Loss: 0.7574300266966155, accuracy: 0.5133333333333333\n",
      "iteration no 136: Loss: 0.7574038461976949, accuracy: 0.5133333333333333\n",
      "iteration no 137: Loss: 0.7573785235570315, accuracy: 0.5133333333333333\n",
      "iteration no 138: Loss: 0.7573540290100461, accuracy: 0.5133333333333333\n",
      "iteration no 139: Loss: 0.7573303339049537, accuracy: 0.5133333333333333\n",
      "iteration no 140: Loss: 0.7573074106572184, accuracy: 0.5133333333333333\n",
      "iteration no 141: Loss: 0.7572852327060696, accuracy: 0.5133333333333333\n",
      "iteration no 142: Loss: 0.7572637744729762, accuracy: 0.5133333333333333\n",
      "iteration no 143: Loss: 0.7572430113219815, accuracy: 0.5133333333333333\n",
      "iteration no 144: Loss: 0.7572229195218076, accuracy: 0.5133333333333333\n",
      "iteration no 145: Loss: 0.7572034762096399, accuracy: 0.5133333333333333\n",
      "iteration no 146: Loss: 0.7571846593565082, accuracy: 0.5133333333333333\n",
      "iteration no 147: Loss: 0.7571664477341925, accuracy: 0.5133333333333333\n",
      "iteration no 148: Loss: 0.757148820883569, accuracy: 0.5133333333333333\n",
      "iteration no 149: Loss: 0.7571317590843394, accuracy: 0.5133333333333333\n",
      "iteration no 150: Loss: 0.7571152433260643, accuracy: 0.5133333333333333\n",
      "iteration no 151: Loss: 0.757099255280448, accuracy: 0.5133333333333333\n",
      "iteration no 152: Loss: 0.7570837772748106, accuracy: 0.5133333333333333\n",
      "iteration no 153: Loss: 0.7570687922666912, accuracy: 0.5133333333333333\n",
      "iteration no 154: Loss: 0.7570542838195313, accuracy: 0.5133333333333333\n",
      "iteration no 155: Loss: 0.7570402360793844, accuracy: 0.5133333333333333\n",
      "iteration no 156: Loss: 0.7570266337526076, accuracy: 0.5133333333333333\n",
      "iteration no 157: Loss: 0.7570134620844869, accuracy: 0.5133333333333333\n",
      "iteration no 158: Loss: 0.7570007068387544, accuracy: 0.5133333333333333\n",
      "iteration no 159: Loss: 0.7569883542779569, accuracy: 0.5133333333333333\n",
      "iteration no 160: Loss: 0.7569763911446337, accuracy: 0.5133333333333333\n",
      "iteration no 161: Loss: 0.7569648046432726, accuracy: 0.5133333333333333\n",
      "iteration no 162: Loss: 0.7569535824230039, accuracy: 0.5133333333333333\n",
      "iteration no 163: Loss: 0.7569427125610002, accuracy: 0.5133333333333333\n",
      "iteration no 164: Loss: 0.7569321835465525, accuracy: 0.5133333333333333\n",
      "iteration no 165: Loss: 0.7569219842657896, accuracy: 0.5133333333333333\n",
      "iteration no 166: Loss: 0.7569121039870153, accuracy: 0.5133333333333333\n",
      "iteration no 167: Loss: 0.756902532346632, accuracy: 0.5133333333333333\n",
      "iteration no 168: Loss: 0.7568932593356291, accuracy: 0.5133333333333333\n",
      "iteration no 169: Loss: 0.7568842752866088, accuracy: 0.5133333333333333\n",
      "iteration no 170: Loss: 0.7568755708613273, accuracy: 0.5133333333333333\n",
      "iteration no 171: Loss: 0.7568671370387261, accuracy: 0.5133333333333333\n",
      "iteration no 172: Loss: 0.7568589651034389, accuracy: 0.5133333333333333\n",
      "iteration no 173: Loss: 0.756851046634744, accuracy: 0.5133333333333333\n",
      "iteration no 174: Loss: 0.7568433734959525, accuracy: 0.5133333333333333\n",
      "iteration no 175: Loss: 0.7568359378242074, accuracy: 0.5133333333333333\n",
      "iteration no 176: Loss: 0.7568287320206787, accuracy: 0.5133333333333333\n",
      "iteration no 177: Loss: 0.7568217487411372, accuracy: 0.5133333333333333\n",
      "iteration no 178: Loss: 0.7568149808868918, accuracy: 0.5133333333333333\n",
      "iteration no 179: Loss: 0.7568084215960741, accuracy: 0.5133333333333333\n",
      "iteration no 180: Loss: 0.7568020642352551, accuracy: 0.5133333333333333\n",
      "iteration no 181: Loss: 0.7567959023913834, accuracy: 0.5133333333333333\n",
      "iteration no 182: Loss: 0.7567899298640275, accuracy: 0.5133333333333333\n",
      "iteration no 183: Loss: 0.7567841406579118, accuracy: 0.5133333333333333\n",
      "iteration no 184: Loss: 0.7567785289757347, accuracy: 0.5133333333333333\n",
      "iteration no 185: Loss: 0.7567730892112563, accuracy: 0.5133333333333333\n",
      "iteration no 186: Loss: 0.7567678159426442, accuracy: 0.5133333333333333\n",
      "iteration no 187: Loss: 0.7567627039260688, accuracy: 0.5133333333333333\n",
      "iteration no 188: Loss: 0.756757748089536, accuracy: 0.5133333333333333\n",
      "iteration no 189: Loss: 0.7567529435269482, accuracy: 0.5133333333333333\n",
      "iteration no 190: Loss: 0.7567482854923867, accuracy: 0.5133333333333333\n",
      "iteration no 191: Loss: 0.7567437693946024, accuracy: 0.5133333333333333\n",
      "iteration no 192: Loss: 0.7567393907917103, accuracy: 0.5133333333333333\n",
      "iteration no 193: Loss: 0.7567351453860778, accuracy: 0.5133333333333333\n",
      "iteration no 194: Loss: 0.7567310290194004, accuracy: 0.5133333333333333\n",
      "iteration no 195: Loss: 0.7567270376679546, accuracy: 0.5133333333333333\n",
      "iteration no 196: Loss: 0.7567231674380257, accuracy: 0.5133333333333333\n",
      "iteration no 197: Loss: 0.7567194145614985, accuracy: 0.5133333333333333\n",
      "iteration no 198: Loss: 0.7567157753916094, accuracy: 0.5133333333333333\n",
      "iteration no 199: Loss: 0.7567122463988493, accuracy: 0.5133333333333333\n",
      "iteration no 200: Loss: 0.7567088241670146, accuracy: 0.5166666666666667\n",
      "iteration no 1: Loss: 1.0909908406839413, accuracy: 0.46\n",
      "iteration no 2: Loss: 1.0611424084933725, accuracy: 0.5033333333333333\n",
      "iteration no 3: Loss: 1.0345792910046943, accuracy: 0.5066666666666667\n",
      "iteration no 4: Loss: 1.0109042709582283, accuracy: 0.5\n",
      "iteration no 5: Loss: 0.9897647651510249, accuracy: 0.5033333333333333\n",
      "iteration no 6: Loss: 0.9708502231958321, accuracy: 0.5166666666666667\n",
      "iteration no 7: Loss: 0.9538887342335812, accuracy: 0.52\n",
      "iteration no 8: Loss: 0.9386432050137605, accuracy: 0.5233333333333333\n",
      "iteration no 9: Loss: 0.9249074388710765, accuracy: 0.5233333333333333\n",
      "iteration no 10: Loss: 0.912502349837441, accuracy: 0.52\n",
      "iteration no 11: Loss: 0.9012724524883378, accuracy: 0.5166666666666667\n",
      "iteration no 12: Loss: 0.8910826961235809, accuracy: 0.5166666666666667\n",
      "iteration no 13: Loss: 0.881815663280993, accuracy: 0.5166666666666667\n",
      "iteration no 14: Loss: 0.8733691229247769, accuracy: 0.5166666666666667\n",
      "iteration no 15: Loss: 0.8656539124577705, accuracy: 0.5166666666666667\n",
      "iteration no 16: Loss: 0.8585921153250539, accuracy: 0.5166666666666667\n",
      "iteration no 17: Loss: 0.8521154989854972, accuracy: 0.5133333333333333\n",
      "iteration no 18: Loss: 0.8461641791091247, accuracy: 0.5166666666666667\n",
      "iteration no 19: Loss: 0.8406854785251267, accuracy: 0.5166666666666667\n",
      "iteration no 20: Loss: 0.8356329527885238, accuracy: 0.5166666666666667\n",
      "iteration no 21: Loss: 0.8309655577228832, accuracy: 0.5166666666666667\n",
      "iteration no 22: Loss: 0.826646937642095, accuracy: 0.5166666666666667\n",
      "iteration no 23: Loss: 0.8226448160128548, accuracy: 0.5166666666666667\n",
      "iteration no 24: Loss: 0.8189304730347846, accuracy: 0.5133333333333333\n",
      "iteration no 25: Loss: 0.815478296979817, accuracy: 0.51\n",
      "iteration no 26: Loss: 0.8122653981654576, accuracy: 0.51\n",
      "iteration no 27: Loss: 0.8092712761690631, accuracy: 0.51\n",
      "iteration no 28: Loss: 0.8064775323578753, accuracy: 0.51\n",
      "iteration no 29: Loss: 0.803867621047811, accuracy: 0.5133333333333333\n",
      "iteration no 30: Loss: 0.8014266336460177, accuracy: 0.5133333333333333\n",
      "iteration no 31: Loss: 0.7991411110077701, accuracy: 0.5133333333333333\n",
      "iteration no 32: Loss: 0.7969988799734407, accuracy: 0.5133333333333333\n",
      "iteration no 33: Loss: 0.7949889106684864, accuracy: 0.5133333333333333\n",
      "iteration no 34: Loss: 0.7931011916677494, accuracy: 0.5133333333333333\n",
      "iteration no 35: Loss: 0.791326620561023, accuracy: 0.5166666666666667\n",
      "iteration no 36: Loss: 0.7896569078233527, accuracy: 0.5166666666666667\n",
      "iteration no 37: Loss: 0.7880844922022632, accuracy: 0.5166666666666667\n",
      "iteration no 38: Loss: 0.7866024660945029, accuracy: 0.5166666666666667\n",
      "iteration no 39: Loss: 0.7852045096048933, accuracy: 0.5166666666666667\n",
      "iteration no 40: Loss: 0.7838848321660009, accuracy: 0.5166666666666667\n",
      "iteration no 41: Loss: 0.7826381207551374, accuracy: 0.5166666666666667\n",
      "iteration no 42: Loss: 0.781459493879149, accuracy: 0.5166666666666667\n",
      "iteration no 43: Loss: 0.7803444606114208, accuracy: 0.5166666666666667\n",
      "iteration no 44: Loss: 0.7792888840626259, accuracy: 0.5166666666666667\n",
      "iteration no 45: Loss: 0.7782889487496708, accuracy: 0.5133333333333333\n",
      "iteration no 46: Loss: 0.777341131398197, accuracy: 0.5133333333333333\n",
      "iteration no 47: Loss: 0.776442174774781, accuracy: 0.5133333333333333\n",
      "iteration no 48: Loss: 0.7755890641971479, accuracy: 0.5133333333333333\n",
      "iteration no 49: Loss: 0.7747790064155983, accuracy: 0.5133333333333333\n",
      "iteration no 50: Loss: 0.7740094105975116, accuracy: 0.5133333333333333\n",
      "iteration no 51: Loss: 0.7732778711801863, accuracy: 0.5133333333333333\n",
      "iteration no 52: Loss: 0.7725821523861405, accuracy: 0.5133333333333333\n",
      "iteration no 53: Loss: 0.7719201742200169, accuracy: 0.5133333333333333\n",
      "iteration no 54: Loss: 0.7712899997879422, accuracy: 0.5133333333333333\n",
      "iteration no 55: Loss: 0.7706898237990651, accuracy: 0.5133333333333333\n",
      "iteration no 56: Loss: 0.7701179621254394, accuracy: 0.5133333333333333\n",
      "iteration no 57: Loss: 0.7695728423107465, accuracy: 0.5133333333333333\n",
      "iteration no 58: Loss: 0.7690529949308923, accuracy: 0.5133333333333333\n",
      "iteration no 59: Loss: 0.7685570457204673, accuracy: 0.5133333333333333\n",
      "iteration no 60: Loss: 0.7680837083886807, accuracy: 0.5133333333333333\n",
      "iteration no 61: Loss: 0.7676317780568047, accuracy: 0.5166666666666667\n",
      "iteration no 62: Loss: 0.7672001252566025, accuracy: 0.5166666666666667\n",
      "iteration no 63: Loss: 0.766787690435731, accuracy: 0.5166666666666667\n",
      "iteration no 64: Loss: 0.7663934789218837, accuracy: 0.5166666666666667\n",
      "iteration no 65: Loss: 0.7660165563025256, accuracy: 0.5166666666666667\n",
      "iteration no 66: Loss: 0.7656560441815743, accuracy: 0.5166666666666667\n",
      "iteration no 67: Loss: 0.7653111162783688, accuracy: 0.5166666666666667\n",
      "iteration no 68: Loss: 0.7649809948378072, accuracy: 0.5166666666666667\n",
      "iteration no 69: Loss: 0.7646649473236755, accuracy: 0.5166666666666667\n",
      "iteration no 70: Loss: 0.7643622833699778, accuracy: 0.5166666666666667\n",
      "iteration no 71: Loss: 0.7640723519675793, accuracy: 0.5166666666666667\n",
      "iteration no 72: Loss: 0.7637945388656752, accuracy: 0.5166666666666667\n",
      "iteration no 73: Loss: 0.7635282641695961, accuracy: 0.5166666666666667\n",
      "iteration no 74: Loss: 0.7632729801182165, accuracy: 0.5166666666666667\n",
      "iteration no 75: Loss: 0.763028169025828, accuracy: 0.5166666666666667\n",
      "iteration no 76: Loss: 0.7627933413747506, accuracy: 0.5166666666666667\n",
      "iteration no 77: Loss: 0.7625680340462362, accuracy: 0.5166666666666667\n",
      "iteration no 78: Loss: 0.7623518086783536, accuracy: 0.5133333333333333\n",
      "iteration no 79: Loss: 0.762144250140584, accuracy: 0.5133333333333333\n",
      "iteration no 80: Loss: 0.7619449651157649, accuracy: 0.5133333333333333\n",
      "iteration no 81: Loss: 0.761753580780875, accuracy: 0.5133333333333333\n",
      "iteration no 82: Loss: 0.7615697435788844, accuracy: 0.5133333333333333\n",
      "iteration no 83: Loss: 0.7613931180745884, accuracy: 0.5133333333333333\n",
      "iteration no 84: Loss: 0.7612233858879511, accuracy: 0.5133333333333333\n",
      "iteration no 85: Loss: 0.7610602446990358, accuracy: 0.5133333333333333\n",
      "iteration no 86: Loss: 0.760903407319114, accuracy: 0.5133333333333333\n",
      "iteration no 87: Loss: 0.7607526008229899, accuracy: 0.5133333333333333\n",
      "iteration no 88: Loss: 0.760607565737994, accuracy: 0.5133333333333333\n",
      "iteration no 89: Loss: 0.7604680552854786, accuracy: 0.5133333333333333\n",
      "iteration no 90: Loss: 0.7603338346709851, accuracy: 0.5133333333333333\n",
      "iteration no 91: Loss: 0.7602046804195663, accuracy: 0.5133333333333333\n",
      "iteration no 92: Loss: 0.760080379753029, accuracy: 0.5133333333333333\n",
      "iteration no 93: Loss: 0.759960730006119, accuracy: 0.5133333333333333\n",
      "iteration no 94: Loss: 0.7598455380789092, accuracy: 0.5133333333333333\n",
      "iteration no 95: Loss: 0.7597346199228632, accuracy: 0.5133333333333333\n",
      "iteration no 96: Loss: 0.7596278000582429, accuracy: 0.5133333333333333\n",
      "iteration no 97: Loss: 0.7595249111207106, accuracy: 0.5133333333333333\n",
      "iteration no 98: Loss: 0.7594257934351404, accuracy: 0.5133333333333333\n",
      "iteration no 99: Loss: 0.7593302946147973, accuracy: 0.5133333333333333\n",
      "iteration no 100: Loss: 0.7592382691841938, accuracy: 0.5133333333333333\n",
      "iteration no 101: Loss: 0.7591495782240448, accuracy: 0.5133333333333333\n",
      "iteration no 102: Loss: 0.7590640890368692, accuracy: 0.5133333333333333\n",
      "iteration no 103: Loss: 0.7589816748318882, accuracy: 0.5133333333333333\n",
      "iteration no 104: Loss: 0.7589022144279697, accuracy: 0.5166666666666667\n",
      "iteration no 105: Loss: 0.7588255919734577, accuracy: 0.5166666666666667\n",
      "iteration no 106: Loss: 0.758751696681808, accuracy: 0.5166666666666667\n",
      "iteration no 107: Loss: 0.7586804225820323, accuracy: 0.5166666666666667\n",
      "iteration no 108: Loss: 0.7586116682830155, accuracy: 0.5166666666666667\n",
      "iteration no 109: Loss: 0.758545336750845, accuracy: 0.5166666666666667\n",
      "iteration no 110: Loss: 0.7584813350983428, accuracy: 0.5166666666666667\n",
      "iteration no 111: Loss: 0.7584195743860533, accuracy: 0.5166666666666667\n",
      "iteration no 112: Loss: 0.7583599694339846, accuracy: 0.5166666666666667\n",
      "iteration no 113: Loss: 0.7583024386434565, accuracy: 0.5166666666666667\n",
      "iteration no 114: Loss: 0.7582469038284421, accuracy: 0.5166666666666667\n",
      "iteration no 115: Loss: 0.7581932900558405, accuracy: 0.5166666666666667\n",
      "iteration no 116: Loss: 0.7581415254941489, accuracy: 0.5166666666666667\n",
      "iteration no 117: Loss: 0.75809154127004, accuracy: 0.5166666666666667\n",
      "iteration no 118: Loss: 0.7580432713323824, accuracy: 0.5166666666666667\n",
      "iteration no 119: Loss: 0.7579966523232732, accuracy: 0.5166666666666667\n",
      "iteration no 120: Loss: 0.7579516234556753, accuracy: 0.5166666666666667\n",
      "iteration no 121: Loss: 0.7579081263972859, accuracy: 0.5166666666666667\n",
      "iteration no 122: Loss: 0.7578661051602761, accuracy: 0.5166666666666667\n",
      "iteration no 123: Loss: 0.7578255059965742, accuracy: 0.5166666666666667\n",
      "iteration no 124: Loss: 0.7577862772983791, accuracy: 0.5166666666666667\n",
      "iteration no 125: Loss: 0.7577483695036135, accuracy: 0.5166666666666667\n",
      "iteration no 126: Loss: 0.7577117350060413, accuracy: 0.5166666666666667\n",
      "iteration no 127: Loss: 0.7576763280697952, accuracy: 0.5166666666666667\n",
      "iteration no 128: Loss: 0.7576421047480694, accuracy: 0.5166666666666667\n",
      "iteration no 129: Loss: 0.7576090228057548, accuracy: 0.5166666666666667\n",
      "iteration no 130: Loss: 0.7575770416458012, accuracy: 0.5166666666666667\n",
      "iteration no 131: Loss: 0.7575461222391068, accuracy: 0.5166666666666667\n",
      "iteration no 132: Loss: 0.7575162270577481, accuracy: 0.5133333333333333\n",
      "iteration no 133: Loss: 0.7574873200113715, accuracy: 0.5133333333333333\n",
      "iteration no 134: Loss: 0.75745936638658, accuracy: 0.5133333333333333\n",
      "iteration no 135: Loss: 0.7574323327891592, accuracy: 0.5133333333333333\n",
      "iteration no 136: Loss: 0.7574061870889947, accuracy: 0.5133333333333333\n",
      "iteration no 137: Loss: 0.7573808983675377, accuracy: 0.5133333333333333\n",
      "iteration no 138: Loss: 0.7573564368676965, accuracy: 0.5133333333333333\n",
      "iteration no 139: Loss: 0.7573327739460174, accuracy: 0.5133333333333333\n",
      "iteration no 140: Loss: 0.75730988202705, accuracy: 0.5133333333333333\n",
      "iteration no 141: Loss: 0.7572877345597797, accuracy: 0.5133333333333333\n",
      "iteration no 142: Loss: 0.7572663059760253, accuracy: 0.5133333333333333\n",
      "iteration no 143: Loss: 0.7572455716507032, accuracy: 0.5133333333333333\n",
      "iteration no 144: Loss: 0.7572255078638657, accuracy: 0.5133333333333333\n",
      "iteration no 145: Loss: 0.7572060917644259, accuracy: 0.5133333333333333\n",
      "iteration no 146: Loss: 0.7571873013354842, accuracy: 0.5133333333333333\n",
      "iteration no 147: Loss: 0.7571691153611807, accuracy: 0.5133333333333333\n",
      "iteration no 148: Loss: 0.7571515133949986, accuracy: 0.5133333333333333\n",
      "iteration no 149: Loss: 0.757134475729446, accuracy: 0.5133333333333333\n",
      "iteration no 150: Loss: 0.7571179833670532, accuracy: 0.5133333333333333\n",
      "iteration no 151: Loss: 0.7571020179926196, accuracy: 0.5133333333333333\n",
      "iteration no 152: Loss: 0.7570865619466532, accuracy: 0.5133333333333333\n",
      "iteration no 153: Loss: 0.7570715981999437, accuracy: 0.5133333333333333\n",
      "iteration no 154: Loss: 0.7570571103292172, accuracy: 0.5133333333333333\n",
      "iteration no 155: Loss: 0.7570430824938222, accuracy: 0.5133333333333333\n",
      "iteration no 156: Loss: 0.7570294994133965, accuracy: 0.5133333333333333\n",
      "iteration no 157: Loss: 0.7570163463464729, accuracy: 0.5133333333333333\n",
      "iteration no 158: Loss: 0.7570036090699759, accuracy: 0.5133333333333333\n",
      "iteration no 159: Loss: 0.7569912738595735, accuracy: 0.5133333333333333\n",
      "iteration no 160: Loss: 0.756979327470841, accuracy: 0.5133333333333333\n",
      "iteration no 161: Loss: 0.7569677571212005, accuracy: 0.5133333333333333\n",
      "iteration no 162: Loss: 0.7569565504726028, accuracy: 0.5133333333333333\n",
      "iteration no 163: Loss: 0.7569456956149173, accuracy: 0.5133333333333333\n",
      "iteration no 164: Loss: 0.7569351810499958, accuracy: 0.5133333333333333\n",
      "iteration no 165: Loss: 0.7569249956763849, accuracy: 0.5133333333333333\n",
      "iteration no 166: Loss: 0.7569151287746534, accuracy: 0.5133333333333333\n",
      "iteration no 167: Loss: 0.7569055699933096, accuracy: 0.5133333333333333\n",
      "iteration no 168: Loss: 0.7568963093352838, accuracy: 0.5133333333333333\n",
      "iteration no 169: Loss: 0.7568873371449485, accuracy: 0.5133333333333333\n",
      "iteration no 170: Loss: 0.7568786440956535, accuracy: 0.5133333333333333\n",
      "iteration no 171: Loss: 0.7568702211777566, accuracy: 0.5133333333333333\n",
      "iteration no 172: Loss: 0.7568620596871236, accuracy: 0.5133333333333333\n",
      "iteration no 173: Loss: 0.7568541512140805, accuracy: 0.5133333333333333\n",
      "iteration no 174: Loss: 0.7568464876327984, accuracy: 0.5133333333333333\n",
      "iteration no 175: Loss: 0.7568390610910906, accuracy: 0.5133333333333333\n",
      "iteration no 176: Loss: 0.7568318640006072, accuracy: 0.5133333333333333\n",
      "iteration no 177: Loss: 0.7568248890274084, accuracy: 0.5133333333333333\n",
      "iteration no 178: Loss: 0.7568181290829004, accuracy: 0.5133333333333333\n",
      "iteration no 179: Loss: 0.7568115773151199, accuracy: 0.5133333333333333\n",
      "iteration no 180: Loss: 0.7568052271003529, accuracy: 0.5133333333333333\n",
      "iteration no 181: Loss: 0.7567990720350705, accuracy: 0.5133333333333333\n",
      "iteration no 182: Loss: 0.7567931059281736, accuracy: 0.5133333333333333\n",
      "iteration no 183: Loss: 0.756787322793531, accuracy: 0.5133333333333333\n",
      "iteration no 184: Loss: 0.7567817168427969, accuracy: 0.5133333333333333\n",
      "iteration no 185: Loss: 0.7567762824785006, accuracy: 0.5133333333333333\n",
      "iteration no 186: Loss: 0.756771014287394, accuracy: 0.5133333333333333\n",
      "iteration no 187: Loss: 0.7567659070340493, accuracy: 0.5133333333333333\n",
      "iteration no 188: Loss: 0.7567609556546925, accuracy: 0.5133333333333333\n",
      "iteration no 189: Loss: 0.7567561552512676, accuracy: 0.5133333333333333\n",
      "iteration no 190: Loss: 0.7567515010857199, accuracy: 0.5133333333333333\n",
      "iteration no 191: Loss: 0.7567469885744901, accuracy: 0.5133333333333333\n",
      "iteration no 192: Loss: 0.7567426132832108, accuracy: 0.5133333333333333\n",
      "iteration no 193: Loss: 0.7567383709215975, accuracy: 0.5133333333333333\n",
      "iteration no 194: Loss: 0.7567342573385252, accuracy: 0.5133333333333333\n",
      "iteration no 195: Loss: 0.7567302685172863, accuracy: 0.5133333333333333\n",
      "iteration no 196: Loss: 0.7567264005710194, accuracy: 0.5133333333333333\n",
      "iteration no 197: Loss: 0.7567226497383033, accuracy: 0.5133333333333333\n",
      "iteration no 198: Loss: 0.7567190123789114, accuracy: 0.5133333333333333\n",
      "iteration no 199: Loss: 0.7567154849697171, accuracy: 0.5133333333333333\n",
      "iteration no 200: Loss: 0.7567120641007482, accuracy: 0.5166666666666667\n",
      "iteration no 1: Loss: 1.1072133274152072, accuracy: 0.5166666666666667\n",
      "iteration no 2: Loss: 1.0756193668231318, accuracy: 0.5033333333333333\n",
      "iteration no 3: Loss: 1.0475200456965723, accuracy: 0.5066666666666667\n",
      "iteration no 4: Loss: 1.0224906797468443, accuracy: 0.5133333333333333\n",
      "iteration no 5: Loss: 1.000157069352843, accuracy: 0.5133333333333333\n",
      "iteration no 6: Loss: 0.9801899425049765, accuracy: 0.51\n",
      "iteration no 7: Loss: 0.96230056996061, accuracy: 0.51\n",
      "iteration no 8: Loss: 0.9462366687335002, accuracy: 0.5133333333333333\n",
      "iteration no 9: Loss: 0.9317784129418885, accuracy: 0.51\n",
      "iteration no 10: Loss: 0.9187346016193529, accuracy: 0.51\n",
      "iteration no 11: Loss: 0.9069390734156472, accuracy: 0.5066666666666667\n",
      "iteration no 12: Loss: 0.8962474348990302, accuracy: 0.5066666666666667\n",
      "iteration no 13: Loss: 0.8865341341453815, accuracy: 0.51\n",
      "iteration no 14: Loss: 0.877689882238068, accuracy: 0.51\n",
      "iteration no 15: Loss: 0.8696194059563215, accuracy: 0.51\n",
      "iteration no 16: Loss: 0.8622395040924894, accuracy: 0.51\n",
      "iteration no 17: Loss: 0.855477375127046, accuracy: 0.51\n",
      "iteration no 18: Loss: 0.849269183213067, accuracy: 0.51\n",
      "iteration no 19: Loss: 0.843558830935236, accuracy: 0.51\n",
      "iteration no 20: Loss: 0.83829691000014, accuracy: 0.51\n",
      "iteration no 21: Loss: 0.8334398041830741, accuracy: 0.51\n",
      "iteration no 22: Loss: 0.8289489220864774, accuracy: 0.51\n",
      "iteration no 23: Loss: 0.8247900403282045, accuracy: 0.51\n",
      "iteration no 24: Loss: 0.8209327405627838, accuracy: 0.51\n",
      "iteration no 25: Loss: 0.8173499262042668, accuracy: 0.51\n",
      "iteration no 26: Loss: 0.814017406863657, accuracy: 0.5133333333333333\n",
      "iteration no 27: Loss: 0.8109135403568296, accuracy: 0.5133333333333333\n",
      "iteration no 28: Loss: 0.8080189237097088, accuracy: 0.5133333333333333\n",
      "iteration no 29: Loss: 0.805316125918881, accuracy: 0.5133333333333333\n",
      "iteration no 30: Loss: 0.8027894563500838, accuracy: 0.5133333333333333\n",
      "iteration no 31: Loss: 0.800424763604016, accuracy: 0.5133333333333333\n",
      "iteration no 32: Loss: 0.7982092604755026, accuracy: 0.5133333333333333\n",
      "iteration no 33: Loss: 0.7961313713016746, accuracy: 0.5166666666666667\n",
      "iteration no 34: Loss: 0.7941805985576907, accuracy: 0.5166666666666667\n",
      "iteration no 35: Loss: 0.7923474060318089, accuracy: 0.5166666666666667\n",
      "iteration no 36: Loss: 0.7906231163099082, accuracy: 0.5166666666666667\n",
      "iteration no 37: Loss: 0.788999820635029, accuracy: 0.5166666666666667\n",
      "iteration no 38: Loss: 0.7874702994904389, accuracy: 0.5166666666666667\n",
      "iteration no 39: Loss: 0.7860279524936685, accuracy: 0.5133333333333333\n",
      "iteration no 40: Loss: 0.784666736391066, accuracy: 0.5133333333333333\n",
      "iteration no 41: Loss: 0.783381110113617, accuracy: 0.5133333333333333\n",
      "iteration no 42: Loss: 0.7821659860000725, accuracy: 0.5133333333333333\n",
      "iteration no 43: Loss: 0.7810166864169222, accuracy: 0.5133333333333333\n",
      "iteration no 44: Loss: 0.7799289051099251, accuracy: 0.5133333333333333\n",
      "iteration no 45: Loss: 0.7788986727116306, accuracy: 0.5133333333333333\n",
      "iteration no 46: Loss: 0.7779223259060121, accuracy: 0.5133333333333333\n",
      "iteration no 47: Loss: 0.7769964798169903, accuracy: 0.5133333333333333\n",
      "iteration no 48: Loss: 0.7761180032439507, accuracy: 0.5133333333333333\n",
      "iteration no 49: Loss: 0.775283996415769, accuracy: 0.5133333333333333\n",
      "iteration no 50: Loss: 0.7744917709765204, accuracy: 0.5133333333333333\n",
      "iteration no 51: Loss: 0.7737388319520098, accuracy: 0.5133333333333333\n",
      "iteration no 52: Loss: 0.7730228614773017, accuracy: 0.5133333333333333\n",
      "iteration no 53: Loss: 0.7723417040923257, accuracy: 0.5133333333333333\n",
      "iteration no 54: Loss: 0.7716933534359339, accuracy: 0.5133333333333333\n",
      "iteration no 55: Loss: 0.7710759401890462, accuracy: 0.5133333333333333\n",
      "iteration no 56: Loss: 0.770487721135126, accuracy: 0.5133333333333333\n",
      "iteration no 57: Loss: 0.7699270692215919, accuracy: 0.5133333333333333\n",
      "iteration no 58: Loss: 0.7693924645191753, accuracy: 0.5166666666666667\n",
      "iteration no 59: Loss: 0.7688824859879504, accuracy: 0.5166666666666667\n",
      "iteration no 60: Loss: 0.7683958039690391, accuracy: 0.5166666666666667\n",
      "iteration no 61: Loss: 0.7679311733299876, accuracy: 0.5166666666666667\n",
      "iteration no 62: Loss: 0.7674874271997346, accuracy: 0.5166666666666667\n",
      "iteration no 63: Loss: 0.7670634712360463, accuracy: 0.5166666666666667\n",
      "iteration no 64: Loss: 0.7666582783744309, accuracy: 0.5166666666666667\n",
      "iteration no 65: Loss: 0.76627088401296, accuracy: 0.5166666666666667\n",
      "iteration no 66: Loss: 0.7659003815922111, accuracy: 0.5166666666666667\n",
      "iteration no 67: Loss: 0.7655459185337762, accuracy: 0.5166666666666667\n",
      "iteration no 68: Loss: 0.7652066925045413, accuracy: 0.5166666666666667\n",
      "iteration no 69: Loss: 0.7648819479772672, accuracy: 0.5166666666666667\n",
      "iteration no 70: Loss: 0.7645709730609679, accuracy: 0.5166666666666667\n",
      "iteration no 71: Loss: 0.7642730965772132, accuracy: 0.5166666666666667\n",
      "iteration no 72: Loss: 0.76398768536083, accuracy: 0.5166666666666667\n",
      "iteration no 73: Loss: 0.7637141417655751, accuracy: 0.5166666666666667\n",
      "iteration no 74: Loss: 0.7634519013572161, accuracy: 0.5166666666666667\n",
      "iteration no 75: Loss: 0.7632004307781366, accuracy: 0.5166666666666667\n",
      "iteration no 76: Loss: 0.7629592257690728, accuracy: 0.5166666666666667\n",
      "iteration no 77: Loss: 0.7627278093349399, accuracy: 0.5166666666666667\n",
      "iteration no 78: Loss: 0.7625057300429036, accuracy: 0.5166666666666667\n",
      "iteration no 79: Loss: 0.7622925604419396, accuracy: 0.5166666666666667\n",
      "iteration no 80: Loss: 0.7620878955940971, accuracy: 0.5133333333333333\n",
      "iteration no 81: Loss: 0.7618913517085574, accuracy: 0.5133333333333333\n",
      "iteration no 82: Loss: 0.7617025648703759, accuracy: 0.5133333333333333\n",
      "iteration no 83: Loss: 0.7615211898564955, accuracy: 0.5133333333333333\n",
      "iteration no 84: Loss: 0.7613468990322825, accuracy: 0.5133333333333333\n",
      "iteration no 85: Loss: 0.7611793813224027, accuracy: 0.5133333333333333\n",
      "iteration no 86: Loss: 0.7610183412503901, accuracy: 0.5133333333333333\n",
      "iteration no 87: Loss: 0.7608634980417408, accuracy: 0.5133333333333333\n",
      "iteration no 88: Loss: 0.7607145847857913, accuracy: 0.5133333333333333\n",
      "iteration no 89: Loss: 0.7605713476520387, accuracy: 0.5133333333333333\n",
      "iteration no 90: Loss: 0.7604335451569189, accuracy: 0.5133333333333333\n",
      "iteration no 91: Loss: 0.7603009474773753, accuracy: 0.5133333333333333\n",
      "iteration no 92: Loss: 0.7601733358078615, accuracy: 0.5133333333333333\n",
      "iteration no 93: Loss: 0.7600505017576743, accuracy: 0.5133333333333333\n",
      "iteration no 94: Loss: 0.7599322467857701, accuracy: 0.5133333333333333\n",
      "iteration no 95: Loss: 0.7598183816704414, accuracy: 0.5133333333333333\n",
      "iteration no 96: Loss: 0.7597087260114267, accuracy: 0.5133333333333333\n",
      "iteration no 97: Loss: 0.7596031077622262, accuracy: 0.5133333333333333\n",
      "iteration no 98: Loss: 0.7595013627905561, accuracy: 0.5133333333333333\n",
      "iteration no 99: Loss: 0.7594033344650398, accuracy: 0.5133333333333333\n",
      "iteration no 100: Loss: 0.7593088732663702, accuracy: 0.5133333333333333\n",
      "iteration no 101: Loss: 0.7592178364213166, accuracy: 0.5133333333333333\n",
      "iteration no 102: Loss: 0.7591300875580648, accuracy: 0.5133333333333333\n",
      "iteration no 103: Loss: 0.759045496381492, accuracy: 0.5166666666666667\n",
      "iteration no 104: Loss: 0.7589639383670825, accuracy: 0.5166666666666667\n",
      "iteration no 105: Loss: 0.7588852944722781, accuracy: 0.5166666666666667\n",
      "iteration no 106: Loss: 0.7588094508641519, accuracy: 0.5166666666666667\n",
      "iteration no 107: Loss: 0.7587362986623621, accuracy: 0.5166666666666667\n",
      "iteration no 108: Loss: 0.7586657336964319, accuracy: 0.5166666666666667\n",
      "iteration no 109: Loss: 0.7585976562764497, accuracy: 0.5166666666666667\n",
      "iteration no 110: Loss: 0.7585319709763638, accuracy: 0.5166666666666667\n",
      "iteration no 111: Loss: 0.7584685864290911, accuracy: 0.5166666666666667\n",
      "iteration no 112: Loss: 0.7584074151327201, accuracy: 0.5166666666666667\n",
      "iteration no 113: Loss: 0.7583483732671327, accuracy: 0.5166666666666667\n",
      "iteration no 114: Loss: 0.7582913805204169, accuracy: 0.5166666666666667\n",
      "iteration no 115: Loss: 0.7582363599244861, accuracy: 0.5166666666666667\n",
      "iteration no 116: Loss: 0.7581832376993564, accuracy: 0.5166666666666667\n",
      "iteration no 117: Loss: 0.7581319431055704, accuracy: 0.5166666666666667\n",
      "iteration no 118: Loss: 0.7580824083042933, accuracy: 0.5166666666666667\n",
      "iteration no 119: Loss: 0.7580345682246312, accuracy: 0.5166666666666667\n",
      "iteration no 120: Loss: 0.7579883604377571, accuracy: 0.5166666666666667\n",
      "iteration no 121: Loss: 0.7579437250374514, accuracy: 0.5166666666666667\n",
      "iteration no 122: Loss: 0.7579006045266952, accuracy: 0.5166666666666667\n",
      "iteration no 123: Loss: 0.7578589437099675, accuracy: 0.5166666666666667\n",
      "iteration no 124: Loss: 0.757818689590933, accuracy: 0.5166666666666667\n",
      "iteration no 125: Loss: 0.7577797912752119, accuracy: 0.5166666666666667\n",
      "iteration no 126: Loss: 0.7577421998779561, accuracy: 0.5166666666666667\n",
      "iteration no 127: Loss: 0.7577058684359608, accuracy: 0.5166666666666667\n",
      "iteration no 128: Loss: 0.7576707518240688, accuracy: 0.5166666666666667\n",
      "iteration no 129: Loss: 0.7576368066756284, accuracy: 0.5166666666666667\n",
      "iteration no 130: Loss: 0.7576039913067885, accuracy: 0.5166666666666667\n",
      "iteration no 131: Loss: 0.7575722656444261, accuracy: 0.5166666666666667\n",
      "iteration no 132: Loss: 0.7575415911575063, accuracy: 0.5133333333333333\n",
      "iteration no 133: Loss: 0.7575119307916993, accuracy: 0.5133333333333333\n",
      "iteration no 134: Loss: 0.7574832489070783, accuracy: 0.5133333333333333\n",
      "iteration no 135: Loss: 0.7574555112187371, accuracy: 0.5133333333333333\n",
      "iteration no 136: Loss: 0.7574286847401767, accuracy: 0.5133333333333333\n",
      "iteration no 137: Loss: 0.7574027377293162, accuracy: 0.5133333333333333\n",
      "iteration no 138: Loss: 0.7573776396369925, accuracy: 0.5133333333333333\n",
      "iteration no 139: Loss: 0.7573533610578214, accuracy: 0.5133333333333333\n",
      "iteration no 140: Loss: 0.757329873683302, accuracy: 0.5133333333333333\n",
      "iteration no 141: Loss: 0.7573071502570458, accuracy: 0.5133333333333333\n",
      "iteration no 142: Loss: 0.7572851645320295, accuracy: 0.5133333333333333\n",
      "iteration no 143: Loss: 0.7572638912297657, accuracy: 0.5133333333333333\n",
      "iteration no 144: Loss: 0.7572433060012967, accuracy: 0.5133333333333333\n",
      "iteration no 145: Loss: 0.7572233853899253, accuracy: 0.5133333333333333\n",
      "iteration no 146: Loss: 0.7572041067955887, accuracy: 0.5133333333333333\n",
      "iteration no 147: Loss: 0.7571854484408049, accuracy: 0.5133333333333333\n",
      "iteration no 148: Loss: 0.7571673893381066, accuracy: 0.5133333333333333\n",
      "iteration no 149: Loss: 0.757149909258896, accuracy: 0.5133333333333333\n",
      "iteration no 150: Loss: 0.7571329887036491, accuracy: 0.5133333333333333\n",
      "iteration no 151: Loss: 0.7571166088734065, accuracy: 0.5133333333333333\n",
      "iteration no 152: Loss: 0.757100751642487, accuracy: 0.5133333333333333\n",
      "iteration no 153: Loss: 0.757085399532371, accuracy: 0.5133333333333333\n",
      "iteration no 154: Loss: 0.7570705356866905, accuracy: 0.5133333333333333\n",
      "iteration no 155: Loss: 0.757056143847282, accuracy: 0.5133333333333333\n",
      "iteration no 156: Loss: 0.7570422083312477, accuracy: 0.5133333333333333\n",
      "iteration no 157: Loss: 0.7570287140089793, accuracy: 0.5133333333333333\n",
      "iteration no 158: Loss: 0.7570156462830999, accuracy: 0.5133333333333333\n",
      "iteration no 159: Loss: 0.7570029910682822, accuracy: 0.5133333333333333\n",
      "iteration no 160: Loss: 0.7569907347719027, accuracy: 0.5133333333333333\n",
      "iteration no 161: Loss: 0.7569788642754932, accuracy: 0.5133333333333333\n",
      "iteration no 162: Loss: 0.756967366916954, accuracy: 0.5133333333333333\n",
      "iteration no 163: Loss: 0.7569562304734938, accuracy: 0.5133333333333333\n",
      "iteration no 164: Loss: 0.7569454431452664, accuracy: 0.5133333333333333\n",
      "iteration no 165: Loss: 0.7569349935396668, accuracy: 0.5133333333333333\n",
      "iteration no 166: Loss: 0.7569248706562649, accuracy: 0.5133333333333333\n",
      "iteration no 167: Loss: 0.7569150638723448, accuracy: 0.5133333333333333\n",
      "iteration no 168: Loss: 0.7569055629290197, accuracy: 0.5133333333333333\n",
      "iteration no 169: Loss: 0.7568963579179057, accuracy: 0.5133333333333333\n",
      "iteration no 170: Loss: 0.7568874392683206, accuracy: 0.5133333333333333\n",
      "iteration no 171: Loss: 0.756878797734993, accuracy: 0.5133333333333333\n",
      "iteration no 172: Loss: 0.756870424386253, accuracy: 0.5133333333333333\n",
      "iteration no 173: Loss: 0.756862310592688, accuracy: 0.5133333333333333\n",
      "iteration no 174: Loss: 0.7568544480162429, accuracy: 0.5133333333333333\n",
      "iteration no 175: Loss: 0.7568468285997435, accuracy: 0.5133333333333333\n",
      "iteration no 176: Loss: 0.7568394445568284, accuracy: 0.5133333333333333\n",
      "iteration no 177: Loss: 0.7568322883622692, accuracy: 0.5133333333333333\n",
      "iteration no 178: Loss: 0.7568253527426644, accuracy: 0.5133333333333333\n",
      "iteration no 179: Loss: 0.7568186306674901, accuracy: 0.5133333333333333\n",
      "iteration no 180: Loss: 0.7568121153404951, accuracy: 0.5133333333333333\n",
      "iteration no 181: Loss: 0.7568058001914206, accuracy: 0.5133333333333333\n",
      "iteration no 182: Loss: 0.7567996788680387, accuracy: 0.5133333333333333\n",
      "iteration no 183: Loss: 0.7567937452284894, accuracy: 0.5133333333333333\n",
      "iteration no 184: Loss: 0.7567879933339083, accuracy: 0.5133333333333333\n",
      "iteration no 185: Loss: 0.756782417441331, accuracy: 0.5133333333333333\n",
      "iteration no 186: Loss: 0.7567770119968633, accuracy: 0.5133333333333333\n",
      "iteration no 187: Loss: 0.756771771629108, accuracy: 0.5133333333333333\n",
      "iteration no 188: Loss: 0.7567666911428347, accuracy: 0.5133333333333333\n",
      "iteration no 189: Loss: 0.7567617655128854, accuracy: 0.5133333333333333\n",
      "iteration no 190: Loss: 0.7567569898783064, accuracy: 0.5133333333333333\n",
      "iteration no 191: Loss: 0.7567523595366951, accuracy: 0.5133333333333333\n",
      "iteration no 192: Loss: 0.7567478699387555, accuracy: 0.5133333333333333\n",
      "iteration no 193: Loss: 0.7567435166830541, accuracy: 0.5133333333333333\n",
      "iteration no 194: Loss: 0.7567392955109659, accuracy: 0.5133333333333333\n",
      "iteration no 195: Loss: 0.7567352023018049, accuracy: 0.5133333333333333\n",
      "iteration no 196: Loss: 0.7567312330681328, accuracy: 0.5133333333333333\n",
      "iteration no 197: Loss: 0.7567273839512366, accuracy: 0.5133333333333333\n",
      "iteration no 198: Loss: 0.7567236512167685, accuracy: 0.5133333333333333\n",
      "iteration no 199: Loss: 0.756720031250545, accuracy: 0.5133333333333333\n",
      "iteration no 200: Loss: 0.7567165205544952, accuracy: 0.5133333333333333\n",
      "iteration no 1: Loss: 1.0888804062677575, accuracy: 0.47\n",
      "iteration no 2: Loss: 1.0593908870345567, accuracy: 0.49\n",
      "iteration no 3: Loss: 1.0331381481380153, accuracy: 0.49666666666666665\n",
      "iteration no 4: Loss: 1.0097287490177258, accuracy: 0.5133333333333333\n",
      "iteration no 5: Loss: 0.988814852034347, accuracy: 0.5166666666666667\n",
      "iteration no 6: Loss: 0.9700908854926225, accuracy: 0.5133333333333333\n",
      "iteration no 7: Loss: 0.9532897316380486, accuracy: 0.5133333333333333\n",
      "iteration no 8: Loss: 0.9381786889288724, accuracy: 0.5133333333333333\n",
      "iteration no 9: Loss: 0.924555461182769, accuracy: 0.5133333333333333\n",
      "iteration no 10: Loss: 0.9122443563376739, accuracy: 0.51\n",
      "iteration no 11: Loss: 0.9010928018484937, accuracy: 0.5133333333333333\n",
      "iteration no 12: Loss: 0.8909682242331743, accuracy: 0.5133333333333333\n",
      "iteration no 13: Loss: 0.8817553001264412, accuracy: 0.5166666666666667\n",
      "iteration no 14: Loss: 0.8733535620466512, accuracy: 0.5166666666666667\n",
      "iteration no 15: Loss: 0.8656753293360142, accuracy: 0.5166666666666667\n",
      "iteration no 16: Loss: 0.8586439294222666, accuracy: 0.5133333333333333\n",
      "iteration no 17: Loss: 0.8521921737411283, accuracy: 0.5133333333333333\n",
      "iteration no 18: Loss: 0.846261054371721, accuracy: 0.5133333333333333\n",
      "iteration no 19: Loss: 0.8407986304064388, accuracy: 0.5133333333333333\n",
      "iteration no 20: Loss: 0.8357590765333544, accuracy: 0.5133333333333333\n",
      "iteration no 21: Loss: 0.831101869809241, accuracy: 0.5133333333333333\n",
      "iteration no 22: Loss: 0.8267910939057829, accuracy: 0.5133333333333333\n",
      "iteration no 23: Loss: 0.8227948431068051, accuracy: 0.5133333333333333\n",
      "iteration no 24: Loss: 0.8190847109801788, accuracy: 0.5133333333333333\n",
      "iteration no 25: Loss: 0.815635350945641, accuracy: 0.5133333333333333\n",
      "iteration no 26: Loss: 0.8124240979318958, accuracy: 0.5133333333333333\n",
      "iteration no 27: Loss: 0.8094306419956856, accuracy: 0.5133333333333333\n",
      "iteration no 28: Loss: 0.8066367461977031, accuracy: 0.5133333333333333\n",
      "iteration no 29: Loss: 0.8040260022302351, accuracy: 0.5133333333333333\n",
      "iteration no 30: Loss: 0.8015836183015863, accuracy: 0.5133333333333333\n",
      "iteration no 31: Loss: 0.7992962346315311, accuracy: 0.5166666666666667\n",
      "iteration no 32: Loss: 0.7971517626254611, accuracy: 0.5166666666666667\n",
      "iteration no 33: Loss: 0.7951392443942241, accuracy: 0.5133333333333333\n",
      "iteration no 34: Loss: 0.7932487297903321, accuracy: 0.5133333333333333\n",
      "iteration no 35: Loss: 0.7914711685548275, accuracy: 0.5133333333333333\n",
      "iteration no 36: Loss: 0.7897983155257177, accuracy: 0.5133333333333333\n",
      "iteration no 37: Loss: 0.7882226471594979, accuracy: 0.5166666666666667\n",
      "iteration no 38: Loss: 0.7867372878710005, accuracy: 0.5133333333333333\n",
      "iteration no 39: Loss: 0.7853359449113144, accuracy: 0.5133333333333333\n",
      "iteration no 40: Loss: 0.7840128506851156, accuracy: 0.5133333333333333\n",
      "iteration no 41: Loss: 0.782762711562805, accuracy: 0.5133333333333333\n",
      "iteration no 42: Loss: 0.7815806623737064, accuracy: 0.5133333333333333\n",
      "iteration no 43: Loss: 0.7804622258779863, accuracy: 0.5133333333333333\n",
      "iteration no 44: Loss: 0.7794032766099355, accuracy: 0.5133333333333333\n",
      "iteration no 45: Loss: 0.7784000085664058, accuracy: 0.5133333333333333\n",
      "iteration no 46: Loss: 0.7774489062836312, accuracy: 0.5133333333333333\n",
      "iteration no 47: Loss: 0.7765467189052252, accuracy: 0.5133333333333333\n",
      "iteration no 48: Loss: 0.7756904368952774, accuracy: 0.5133333333333333\n",
      "iteration no 49: Loss: 0.7748772710945125, accuracy: 0.5133333333333333\n",
      "iteration no 50: Loss: 0.7741046338554096, accuracy: 0.5133333333333333\n",
      "iteration no 51: Loss: 0.7733701220249695, accuracy: 0.5133333333333333\n",
      "iteration no 52: Loss: 0.7726715015721737, accuracy: 0.5133333333333333\n",
      "iteration no 53: Loss: 0.7720066936817678, accuracy: 0.5133333333333333\n",
      "iteration no 54: Loss: 0.7713737621573401, accuracy: 0.5133333333333333\n",
      "iteration no 55: Loss: 0.7707709019952431, accuracy: 0.5133333333333333\n",
      "iteration no 56: Loss: 0.770196429007067, accuracy: 0.5133333333333333\n",
      "iteration no 57: Loss: 0.7696487703825078, accuracy: 0.5133333333333333\n",
      "iteration no 58: Loss: 0.7691264560967968, accuracy: 0.5133333333333333\n",
      "iteration no 59: Loss: 0.7686281110776706, accuracy: 0.5133333333333333\n",
      "iteration no 60: Loss: 0.7681524480563389, accuracy: 0.5166666666666667\n",
      "iteration no 61: Loss: 0.767698261035214, accuracy: 0.5166666666666667\n",
      "iteration no 62: Loss: 0.7672644193125076, accuracy: 0.5166666666666667\n",
      "iteration no 63: Loss: 0.7668498620102262, accuracy: 0.5166666666666667\n",
      "iteration no 64: Loss: 0.7664535930578069, accuracy: 0.5166666666666667\n",
      "iteration no 65: Loss: 0.7660746765886454, accuracy: 0.5166666666666667\n",
      "iteration no 66: Loss: 0.7657122327112262, accuracy: 0.5166666666666667\n",
      "iteration no 67: Loss: 0.7653654336205058, accuracy: 0.5166666666666667\n",
      "iteration no 68: Loss: 0.7650335000186893, accuracy: 0.5166666666666667\n",
      "iteration no 69: Loss: 0.764715697817654, accuracy: 0.5166666666666667\n",
      "iteration no 70: Loss: 0.7644113350980388, accuracy: 0.5166666666666667\n",
      "iteration no 71: Loss: 0.7641197593024721, accuracy: 0.5166666666666667\n",
      "iteration no 72: Loss: 0.7638403546426156, accuracy: 0.5166666666666667\n",
      "iteration no 73: Loss: 0.7635725397016576, accuracy: 0.5166666666666667\n",
      "iteration no 74: Loss: 0.7633157652156443, accuracy: 0.5166666666666667\n",
      "iteration no 75: Loss: 0.7630695120186055, accuracy: 0.5166666666666667\n",
      "iteration no 76: Loss: 0.7628332891378428, accuracy: 0.5166666666666667\n",
      "iteration no 77: Loss: 0.7626066320270024, accuracy: 0.5166666666666667\n",
      "iteration no 78: Loss: 0.7623891009256984, accuracy: 0.5166666666666667\n",
      "iteration no 79: Loss: 0.7621802793354628, accuracy: 0.5133333333333333\n",
      "iteration no 80: Loss: 0.7619797726027254, accuracy: 0.5133333333333333\n",
      "iteration no 81: Loss: 0.7617872066003446, accuracy: 0.5133333333333333\n",
      "iteration no 82: Loss: 0.7616022264999672, accuracy: 0.5133333333333333\n",
      "iteration no 83: Loss: 0.7614244956281602, accuracy: 0.5133333333333333\n",
      "iteration no 84: Loss: 0.7612536943998719, accuracy: 0.5133333333333333\n",
      "iteration no 85: Loss: 0.761089519323332, accuracy: 0.5133333333333333\n",
      "iteration no 86: Loss: 0.7609316820709963, accuracy: 0.5133333333333333\n",
      "iteration no 87: Loss: 0.7607799086115966, accuracy: 0.5133333333333333\n",
      "iteration no 88: Loss: 0.7606339383987706, accuracy: 0.5133333333333333\n",
      "iteration no 89: Loss: 0.7604935236121113, accuracy: 0.5133333333333333\n",
      "iteration no 90: Loss: 0.7603584284468294, accuracy: 0.5133333333333333\n",
      "iteration no 91: Loss: 0.7602284284485128, accuracy: 0.5133333333333333\n",
      "iteration no 92: Loss: 0.760103309889766, accuracy: 0.5133333333333333\n",
      "iteration no 93: Loss: 0.7599828691857585, accuracy: 0.5133333333333333\n",
      "iteration no 94: Loss: 0.7598669123459488, accuracy: 0.5133333333333333\n",
      "iteration no 95: Loss: 0.7597552544594638, accuracy: 0.5133333333333333\n",
      "iteration no 96: Loss: 0.7596477192118085, accuracy: 0.5133333333333333\n",
      "iteration no 97: Loss: 0.7595441384307614, accuracy: 0.5133333333333333\n",
      "iteration no 98: Loss: 0.7594443516594721, accuracy: 0.5133333333333333\n",
      "iteration no 99: Loss: 0.7593482057549292, accuracy: 0.5133333333333333\n",
      "iteration no 100: Loss: 0.7592555545101013, accuracy: 0.5133333333333333\n",
      "iteration no 101: Loss: 0.7591662582981871, accuracy: 0.5133333333333333\n",
      "iteration no 102: Loss: 0.7590801837375132, accuracy: 0.5133333333333333\n",
      "iteration no 103: Loss: 0.758997203375742, accuracy: 0.5166666666666667\n",
      "iteration no 104: Loss: 0.7589171953921307, accuracy: 0.5166666666666667\n",
      "iteration no 105: Loss: 0.7588400433166905, accuracy: 0.5166666666666667\n",
      "iteration no 106: Loss: 0.7587656357651635, accuracy: 0.5166666666666667\n",
      "iteration no 107: Loss: 0.7586938661888188, accuracy: 0.5166666666666667\n",
      "iteration no 108: Loss: 0.7586246326381402, accuracy: 0.5166666666666667\n",
      "iteration no 109: Loss: 0.7585578375395363, accuracy: 0.5166666666666667\n",
      "iteration no 110: Loss: 0.7584933874842704, accuracy: 0.5166666666666667\n",
      "iteration no 111: Loss: 0.7584311930288606, accuracy: 0.5166666666666667\n",
      "iteration no 112: Loss: 0.7583711685062497, accuracy: 0.5166666666666667\n",
      "iteration no 113: Loss: 0.7583132318470952, accuracy: 0.5166666666666667\n",
      "iteration no 114: Loss: 0.7582573044105699, accuracy: 0.5166666666666667\n",
      "iteration no 115: Loss: 0.7582033108241093, accuracy: 0.5166666666666667\n",
      "iteration no 116: Loss: 0.7581511788315718, accuracy: 0.5166666666666667\n",
      "iteration no 117: Loss: 0.7581008391493211, accuracy: 0.5166666666666667\n",
      "iteration no 118: Loss: 0.7580522253297669, accuracy: 0.5166666666666667\n",
      "iteration no 119: Loss: 0.7580052736319287, accuracy: 0.5166666666666667\n",
      "iteration no 120: Loss: 0.7579599228986255, accuracy: 0.5166666666666667\n",
      "iteration no 121: Loss: 0.7579161144399029, accuracy: 0.5166666666666667\n",
      "iteration no 122: Loss: 0.7578737919223522, accuracy: 0.5166666666666667\n",
      "iteration no 123: Loss: 0.7578329012639824, accuracy: 0.5166666666666667\n",
      "iteration no 124: Loss: 0.7577933905343377, accuracy: 0.5166666666666667\n",
      "iteration no 125: Loss: 0.7577552098595666, accuracy: 0.5166666666666667\n",
      "iteration no 126: Loss: 0.7577183113321684, accuracy: 0.5166666666666667\n",
      "iteration no 127: Loss: 0.7576826489251617, accuracy: 0.5166666666666667\n",
      "iteration no 128: Loss: 0.7576481784104305, accuracy: 0.5166666666666667\n",
      "iteration no 129: Loss: 0.7576148572810247, accuracy: 0.5166666666666667\n",
      "iteration no 130: Loss: 0.7575826446771973, accuracy: 0.5166666666666667\n",
      "iteration no 131: Loss: 0.7575515013159819, accuracy: 0.5166666666666667\n",
      "iteration no 132: Loss: 0.7575213894241192, accuracy: 0.5166666666666667\n",
      "iteration no 133: Loss: 0.7574922726741572, accuracy: 0.5166666666666667\n",
      "iteration no 134: Loss: 0.7574641161235555, accuracy: 0.5133333333333333\n",
      "iteration no 135: Loss: 0.7574368861566392, accuracy: 0.5133333333333333\n",
      "iteration no 136: Loss: 0.7574105504292514, accuracy: 0.5133333333333333\n",
      "iteration no 137: Loss: 0.7573850778159683, accuracy: 0.5133333333333333\n",
      "iteration no 138: Loss: 0.7573604383597404, accuracy: 0.5133333333333333\n",
      "iteration no 139: Loss: 0.7573366032238414, accuracy: 0.5133333333333333\n",
      "iteration no 140: Loss: 0.7573135446460016, accuracy: 0.5133333333333333\n",
      "iteration no 141: Loss: 0.757291235894621, accuracy: 0.5133333333333333\n",
      "iteration no 142: Loss: 0.7572696512269528, accuracy: 0.5133333333333333\n",
      "iteration no 143: Loss: 0.757248765849163, accuracy: 0.5133333333333333\n",
      "iteration no 144: Loss: 0.7572285558781697, accuracy: 0.5133333333333333\n",
      "iteration no 145: Loss: 0.757208998305175, accuracy: 0.5133333333333333\n",
      "iteration no 146: Loss: 0.7571900709608086, accuracy: 0.5133333333333333\n",
      "iteration no 147: Loss: 0.7571717524818002, accuracy: 0.5133333333333333\n",
      "iteration no 148: Loss: 0.7571540222791107, accuracy: 0.5133333333333333\n",
      "iteration no 149: Loss: 0.7571368605074473, accuracy: 0.5133333333333333\n",
      "iteration no 150: Loss: 0.7571202480361013, accuracy: 0.5133333333333333\n",
      "iteration no 151: Loss: 0.757104166421039, accuracy: 0.5133333333333333\n",
      "iteration no 152: Loss: 0.7570885978781909, accuracy: 0.5133333333333333\n",
      "iteration no 153: Loss: 0.7570735252578822, accuracy: 0.5133333333333333\n",
      "iteration no 154: Loss: 0.7570589320203466, accuracy: 0.5133333333333333\n",
      "iteration no 155: Loss: 0.7570448022122774, accuracy: 0.5133333333333333\n",
      "iteration no 156: Loss: 0.7570311204443666, accuracy: 0.5133333333333333\n",
      "iteration no 157: Loss: 0.7570178718697826, accuracy: 0.5133333333333333\n",
      "iteration no 158: Loss: 0.7570050421635474, accuracy: 0.5133333333333333\n",
      "iteration no 159: Loss: 0.7569926175027717, accuracy: 0.5133333333333333\n",
      "iteration no 160: Loss: 0.7569805845477039, accuracy: 0.5133333333333333\n",
      "iteration no 161: Loss: 0.7569689304235634, accuracy: 0.5133333333333333\n",
      "iteration no 162: Loss: 0.7569576427031156, accuracy: 0.5133333333333333\n",
      "iteration no 163: Loss: 0.7569467093899606, accuracy: 0.5133333333333333\n",
      "iteration no 164: Loss: 0.7569361189024997, accuracy: 0.5133333333333333\n",
      "iteration no 165: Loss: 0.7569258600585519, accuracy: 0.5133333333333333\n",
      "iteration no 166: Loss: 0.7569159220605908, accuracy: 0.5133333333333333\n",
      "iteration no 167: Loss: 0.7569062944815745, accuracy: 0.5133333333333333\n",
      "iteration no 168: Loss: 0.7568969672513408, accuracy: 0.5133333333333333\n",
      "iteration no 169: Loss: 0.7568879306435471, accuracy: 0.5133333333333333\n",
      "iteration no 170: Loss: 0.7568791752631254, accuracy: 0.5133333333333333\n",
      "iteration no 171: Loss: 0.7568706920342358, accuracy: 0.5133333333333333\n",
      "iteration no 172: Loss: 0.7568624721886937, accuracy: 0.5133333333333333\n",
      "iteration no 173: Loss: 0.7568545072548494, accuracy: 0.5133333333333333\n",
      "iteration no 174: Loss: 0.7568467890469036, accuracy: 0.5133333333333333\n",
      "iteration no 175: Loss: 0.75683930965464, accuracy: 0.5133333333333333\n",
      "iteration no 176: Loss: 0.7568320614335529, accuracy: 0.5133333333333333\n",
      "iteration no 177: Loss: 0.7568250369953597, accuracy: 0.5133333333333333\n",
      "iteration no 178: Loss: 0.7568182291988768, accuracy: 0.5133333333333333\n",
      "iteration no 179: Loss: 0.756811631141245, accuracy: 0.5133333333333333\n",
      "iteration no 180: Loss: 0.756805236149493, accuracy: 0.5133333333333333\n",
      "iteration no 181: Loss: 0.7567990377724195, accuracy: 0.5133333333333333\n",
      "iteration no 182: Loss: 0.7567930297727853, accuracy: 0.5133333333333333\n",
      "iteration no 183: Loss: 0.7567872061197993, accuracy: 0.5133333333333333\n",
      "iteration no 184: Loss: 0.7567815609818898, accuracy: 0.5133333333333333\n",
      "iteration no 185: Loss: 0.7567760887197447, accuracy: 0.5133333333333333\n",
      "iteration no 186: Loss: 0.7567707838796159, accuracy: 0.5133333333333333\n",
      "iteration no 187: Loss: 0.7567656411868701, accuracy: 0.5133333333333333\n",
      "iteration no 188: Loss: 0.7567606555397817, accuracy: 0.5133333333333333\n",
      "iteration no 189: Loss: 0.7567558220035557, accuracy: 0.5133333333333333\n",
      "iteration no 190: Loss: 0.7567511358045709, accuracy: 0.5133333333333333\n",
      "iteration no 191: Loss: 0.7567465923248353, accuracy: 0.5133333333333333\n",
      "iteration no 192: Loss: 0.7567421870966466, accuracy: 0.5133333333333333\n",
      "iteration no 193: Loss: 0.7567379157974459, accuracy: 0.5133333333333333\n",
      "iteration no 194: Loss: 0.756733774244861, accuracy: 0.5133333333333333\n",
      "iteration no 195: Loss: 0.7567297583919302, accuracy: 0.5133333333333333\n",
      "iteration no 196: Loss: 0.7567258643224988, accuracy: 0.5133333333333333\n",
      "iteration no 197: Loss: 0.7567220882467817, accuracy: 0.5133333333333333\n",
      "iteration no 198: Loss: 0.7567184264970875, accuracy: 0.5133333333333333\n",
      "iteration no 199: Loss: 0.756714875523696, accuracy: 0.5133333333333333\n",
      "iteration no 200: Loss: 0.756711431890882, accuracy: 0.5166666666666667\n",
      "iteration no 1: Loss: 1.095146639887369, accuracy: 0.5066666666666667\n",
      "iteration no 2: Loss: 1.0648500404840109, accuracy: 0.5\n",
      "iteration no 3: Loss: 1.037900313371748, accuracy: 0.5033333333333333\n",
      "iteration no 4: Loss: 1.0138861072592966, accuracy: 0.5066666666666667\n",
      "iteration no 5: Loss: 0.9924478135178996, accuracy: 0.5066666666666667\n",
      "iteration no 6: Loss: 0.9732702978600729, accuracy: 0.5\n",
      "iteration no 7: Loss: 0.9560778146572615, accuracy: 0.5133333333333333\n",
      "iteration no 8: Loss: 0.9406296890655185, accuracy: 0.51\n",
      "iteration no 9: Loss: 0.9267163265488725, accuracy: 0.51\n",
      "iteration no 10: Loss: 0.9141554737289143, accuracy: 0.51\n",
      "iteration no 11: Loss: 0.9027887604994019, accuracy: 0.51\n",
      "iteration no 12: Loss: 0.8924785621522946, accuracy: 0.51\n",
      "iteration no 13: Loss: 0.8831052007182344, accuracy: 0.51\n",
      "iteration no 14: Loss: 0.8745644830870567, accuracy: 0.51\n",
      "iteration no 15: Loss: 0.866765557650795, accuracy: 0.51\n",
      "iteration no 16: Loss: 0.8596290620141024, accuracy: 0.51\n",
      "iteration no 17: Loss: 0.853085530334401, accuracy: 0.51\n",
      "iteration no 18: Loss: 0.8470740283628398, accuracy: 0.51\n",
      "iteration no 19: Loss: 0.8415409858403586, accuracy: 0.51\n",
      "iteration no 20: Loss: 0.836439198560055, accuracy: 0.51\n",
      "iteration no 21: Loss: 0.8317269754921142, accuracy: 0.51\n",
      "iteration no 22: Loss: 0.8273674094942012, accuracy: 0.51\n",
      "iteration no 23: Loss: 0.8233277530852605, accuracy: 0.51\n",
      "iteration no 24: Loss: 0.8195788834407701, accuracy: 0.51\n",
      "iteration no 25: Loss: 0.816094843135266, accuracy: 0.5133333333333333\n",
      "iteration no 26: Loss: 0.8128524452137228, accuracy: 0.5133333333333333\n",
      "iteration no 27: Loss: 0.8098309329372885, accuracy: 0.5133333333333333\n",
      "iteration no 28: Loss: 0.8070116860501627, accuracy: 0.5133333333333333\n",
      "iteration no 29: Loss: 0.8043779666852158, accuracy: 0.5133333333333333\n",
      "iteration no 30: Loss: 0.8019146990977009, accuracy: 0.5133333333333333\n",
      "iteration no 31: Loss: 0.7996082783182569, accuracy: 0.5166666666666667\n",
      "iteration no 32: Loss: 0.7974464035742724, accuracy: 0.5166666666666667\n",
      "iteration no 33: Loss: 0.7954179329652236, accuracy: 0.5166666666666667\n",
      "iteration no 34: Loss: 0.7935127564122811, accuracy: 0.5166666666666667\n",
      "iteration no 35: Loss: 0.7917216843517876, accuracy: 0.5166666666666667\n",
      "iteration no 36: Loss: 0.790036350020108, accuracy: 0.5166666666666667\n",
      "iteration no 37: Loss: 0.7884491234955392, accuracy: 0.5166666666666667\n",
      "iteration no 38: Loss: 0.786953035931209, accuracy: 0.5166666666666667\n",
      "iteration no 39: Loss: 0.7855417126393767, accuracy: 0.5166666666666667\n",
      "iteration no 40: Loss: 0.7842093138790679, accuracy: 0.5166666666666667\n",
      "iteration no 41: Loss: 0.7829504823612002, accuracy: 0.5166666666666667\n",
      "iteration no 42: Loss: 0.7817602966230096, accuracy: 0.5133333333333333\n",
      "iteration no 43: Loss: 0.7806342295405956, accuracy: 0.5133333333333333\n",
      "iteration no 44: Loss: 0.7795681113480543, accuracy: 0.5133333333333333\n",
      "iteration no 45: Loss: 0.7785580966166801, accuracy: 0.5133333333333333\n",
      "iteration no 46: Loss: 0.7776006347203889, accuracy: 0.5133333333333333\n",
      "iteration no 47: Loss: 0.7766924433757442, accuracy: 0.5133333333333333\n",
      "iteration no 48: Loss: 0.7758304848983653, accuracy: 0.5133333333333333\n",
      "iteration no 49: Loss: 0.7750119448633861, accuracy: 0.5133333333333333\n",
      "iteration no 50: Loss: 0.7742342128971597, accuracy: 0.5133333333333333\n",
      "iteration no 51: Loss: 0.7734948653614977, accuracy: 0.5133333333333333\n",
      "iteration no 52: Loss: 0.7727916497212064, accuracy: 0.5133333333333333\n",
      "iteration no 53: Loss: 0.7721224704111989, accuracy: 0.5133333333333333\n",
      "iteration no 54: Loss: 0.7714853760415877, accuracy: 0.5133333333333333\n",
      "iteration no 55: Loss: 0.7708785477984083, accuracy: 0.5133333333333333\n",
      "iteration no 56: Loss: 0.7703002889143432, accuracy: 0.5133333333333333\n",
      "iteration no 57: Loss: 0.769749015098431, accuracy: 0.5133333333333333\n",
      "iteration no 58: Loss: 0.7692232458264685, accuracy: 0.5166666666666667\n",
      "iteration no 59: Loss: 0.7687215964049825, accuracy: 0.5166666666666667\n",
      "iteration no 60: Loss: 0.7682427707313993, accuracy: 0.5166666666666667\n",
      "iteration no 61: Loss: 0.7677855546816251, accuracy: 0.5166666666666667\n",
      "iteration no 62: Loss: 0.7673488100637731, accuracy: 0.5166666666666667\n",
      "iteration no 63: Loss: 0.7669314690834156, accuracy: 0.5166666666666667\n",
      "iteration no 64: Loss: 0.7665325292715759, accuracy: 0.5166666666666667\n",
      "iteration no 65: Loss: 0.7661510488318456, accuracy: 0.5166666666666667\n",
      "iteration no 66: Loss: 0.7657861423675683, accuracy: 0.5166666666666667\n",
      "iteration no 67: Loss: 0.7654369769540778, accuracy: 0.5166666666666667\n",
      "iteration no 68: Loss: 0.7651027685245584, accuracy: 0.5166666666666667\n",
      "iteration no 69: Loss: 0.764782778541277, accuracy: 0.5166666666666667\n",
      "iteration no 70: Loss: 0.7644763109267632, accuracy: 0.5166666666666667\n",
      "iteration no 71: Loss: 0.7641827092320322, accuracy: 0.5166666666666667\n",
      "iteration no 72: Loss: 0.7639013540211896, accuracy: 0.5166666666666667\n",
      "iteration no 73: Loss: 0.7636316604537585, accuracy: 0.5166666666666667\n",
      "iteration no 74: Loss: 0.7633730760478599, accuracy: 0.5166666666666667\n",
      "iteration no 75: Loss: 0.7631250786089785, accuracy: 0.5166666666666667\n",
      "iteration no 76: Loss: 0.7628871743104787, accuracy: 0.5166666666666667\n",
      "iteration no 77: Loss: 0.7626588959133234, accuracy: 0.5166666666666667\n",
      "iteration no 78: Loss: 0.7624398011136045, accuracy: 0.5166666666666667\n",
      "iteration no 79: Loss: 0.7622294710075254, accuracy: 0.5166666666666667\n",
      "iteration no 80: Loss: 0.7620275086644189, accuracy: 0.5166666666666667\n",
      "iteration no 81: Loss: 0.7618335377992179, accuracy: 0.5133333333333333\n",
      "iteration no 82: Loss: 0.7616472015365586, accuracy: 0.5133333333333333\n",
      "iteration no 83: Loss: 0.7614681612593801, accuracy: 0.5133333333333333\n",
      "iteration no 84: Loss: 0.7612960955355013, accuracy: 0.5133333333333333\n",
      "iteration no 85: Loss: 0.7611306991162218, accuracy: 0.5133333333333333\n",
      "iteration no 86: Loss: 0.7609716820014917, accuracy: 0.5133333333333333\n",
      "iteration no 87: Loss: 0.760818768566661, accuracy: 0.5133333333333333\n",
      "iteration no 88: Loss: 0.7606716967462357, accuracy: 0.5133333333333333\n",
      "iteration no 89: Loss: 0.7605302172704422, accuracy: 0.5133333333333333\n",
      "iteration no 90: Loss: 0.7603940929507497, accuracy: 0.5133333333333333\n",
      "iteration no 91: Loss: 0.7602630980108116, accuracy: 0.5133333333333333\n",
      "iteration no 92: Loss: 0.7601370174595733, accuracy: 0.5133333333333333\n",
      "iteration no 93: Loss: 0.7600156465035474, accuracy: 0.5133333333333333\n",
      "iteration no 94: Loss: 0.7598987899955053, accuracy: 0.5133333333333333\n",
      "iteration no 95: Loss: 0.7597862619170394, accuracy: 0.5133333333333333\n",
      "iteration no 96: Loss: 0.7596778848926539, accuracy: 0.5133333333333333\n",
      "iteration no 97: Loss: 0.7595734897332216, accuracy: 0.5133333333333333\n",
      "iteration no 98: Loss: 0.7594729150068068, accuracy: 0.5133333333333333\n",
      "iteration no 99: Loss: 0.7593760066350116, accuracy: 0.5133333333333333\n",
      "iteration no 100: Loss: 0.7592826175131326, accuracy: 0.5133333333333333\n",
      "iteration no 101: Loss: 0.7591926071525545, accuracy: 0.5133333333333333\n",
      "iteration no 102: Loss: 0.7591058413439093, accuracy: 0.5133333333333333\n",
      "iteration no 103: Loss: 0.7590221918396504, accuracy: 0.5166666666666667\n",
      "iteration no 104: Loss: 0.7589415360547824, accuracy: 0.5166666666666667\n",
      "iteration no 105: Loss: 0.7588637567845766, accuracy: 0.5166666666666667\n",
      "iteration no 106: Loss: 0.7587887419381949, accuracy: 0.5166666666666667\n",
      "iteration no 107: Loss: 0.7587163842872076, accuracy: 0.5166666666666667\n",
      "iteration no 108: Loss: 0.758646581228076, accuracy: 0.5166666666666667\n",
      "iteration no 109: Loss: 0.7585792345577274, accuracy: 0.5166666666666667\n",
      "iteration no 110: Loss: 0.758514250261409, accuracy: 0.5166666666666667\n",
      "iteration no 111: Loss: 0.758451538312073, accuracy: 0.5166666666666667\n",
      "iteration no 112: Loss: 0.7583910124805852, accuracy: 0.5166666666666667\n",
      "iteration no 113: Loss: 0.7583325901561027, accuracy: 0.5166666666666667\n",
      "iteration no 114: Loss: 0.7582761921760129, accuracy: 0.5166666666666667\n",
      "iteration no 115: Loss: 0.7582217426648599, accuracy: 0.5166666666666667\n",
      "iteration no 116: Loss: 0.7581691688817269, accuracy: 0.5166666666666667\n",
      "iteration no 117: Loss: 0.7581184010755793, accuracy: 0.5166666666666667\n",
      "iteration no 118: Loss: 0.7580693723481011, accuracy: 0.5166666666666667\n",
      "iteration no 119: Loss: 0.7580220185235929, accuracy: 0.5166666666666667\n",
      "iteration no 120: Loss: 0.7579762780255199, accuracy: 0.5166666666666667\n",
      "iteration no 121: Loss: 0.7579320917593355, accuracy: 0.5166666666666667\n",
      "iteration no 122: Loss: 0.7578894030012199, accuracy: 0.5166666666666667\n",
      "iteration no 123: Loss: 0.7578481572924007, accuracy: 0.5166666666666667\n",
      "iteration no 124: Loss: 0.7578083023387439, accuracy: 0.5166666666666667\n",
      "iteration no 125: Loss: 0.7577697879153176, accuracy: 0.5166666666666667\n",
      "iteration no 126: Loss: 0.7577325657756618, accuracy: 0.5166666666666667\n",
      "iteration no 127: Loss: 0.7576965895654937, accuracy: 0.5166666666666667\n",
      "iteration no 128: Loss: 0.7576618147406168, accuracy: 0.5166666666666667\n",
      "iteration no 129: Loss: 0.7576281984887994, accuracy: 0.5166666666666667\n",
      "iteration no 130: Loss: 0.7575956996554103, accuracy: 0.5133333333333333\n",
      "iteration no 131: Loss: 0.757564278672611, accuracy: 0.5133333333333333\n",
      "iteration no 132: Loss: 0.7575338974919131, accuracy: 0.5133333333333333\n",
      "iteration no 133: Loss: 0.7575045195199243, accuracy: 0.5133333333333333\n",
      "iteration no 134: Loss: 0.757476109557114, accuracy: 0.5133333333333333\n",
      "iteration no 135: Loss: 0.7574486337394424, accuracy: 0.5133333333333333\n",
      "iteration no 136: Loss: 0.7574220594827009, accuracy: 0.5133333333333333\n",
      "iteration no 137: Loss: 0.7573963554294274, accuracy: 0.5133333333333333\n",
      "iteration no 138: Loss: 0.7573714913982611, accuracy: 0.5133333333333333\n",
      "iteration no 139: Loss: 0.7573474383356148, accuracy: 0.5133333333333333\n",
      "iteration no 140: Loss: 0.7573241682695441, accuracy: 0.5133333333333333\n",
      "iteration no 141: Loss: 0.7573016542657052, accuracy: 0.5133333333333333\n",
      "iteration no 142: Loss: 0.7572798703852947, accuracy: 0.5133333333333333\n",
      "iteration no 143: Loss: 0.7572587916448733, accuracy: 0.5133333333333333\n",
      "iteration no 144: Loss: 0.7572383939779795, accuracy: 0.5133333333333333\n",
      "iteration no 145: Loss: 0.7572186541984449, accuracy: 0.5133333333333333\n",
      "iteration no 146: Loss: 0.7571995499653271, accuracy: 0.5133333333333333\n",
      "iteration no 147: Loss: 0.7571810597493827, accuracy: 0.5133333333333333\n",
      "iteration no 148: Loss: 0.7571631628010035, accuracy: 0.5133333333333333\n",
      "iteration no 149: Loss: 0.7571458391195471, accuracy: 0.5133333333333333\n",
      "iteration no 150: Loss: 0.7571290694239934, accuracy: 0.5133333333333333\n",
      "iteration no 151: Loss: 0.7571128351248642, accuracy: 0.5133333333333333\n",
      "iteration no 152: Loss: 0.7570971182973465, accuracy: 0.5133333333333333\n",
      "iteration no 153: Loss: 0.7570819016555612, accuracy: 0.5133333333333333\n",
      "iteration no 154: Loss: 0.7570671685279243, accuracy: 0.5133333333333333\n",
      "iteration no 155: Loss: 0.7570529028335494, accuracy: 0.5133333333333333\n",
      "iteration no 156: Loss: 0.7570390890596411, accuracy: 0.5133333333333333\n",
      "iteration no 157: Loss: 0.7570257122398389, accuracy: 0.5133333333333333\n",
      "iteration no 158: Loss: 0.7570127579334577, accuracy: 0.5133333333333333\n",
      "iteration no 159: Loss: 0.757000212205596, accuracy: 0.5133333333333333\n",
      "iteration no 160: Loss: 0.7569880616080625, accuracy: 0.5133333333333333\n",
      "iteration no 161: Loss: 0.7569762931610877, accuracy: 0.5133333333333333\n",
      "iteration no 162: Loss: 0.7569648943357857, accuracy: 0.5133333333333333\n",
      "iteration no 163: Loss: 0.7569538530373314, accuracy: 0.5133333333333333\n",
      "iteration no 164: Loss: 0.7569431575888201, accuracy: 0.5133333333333333\n",
      "iteration no 165: Loss: 0.7569327967157807, accuracy: 0.5133333333333333\n",
      "iteration no 166: Loss: 0.756922759531313, accuracy: 0.5133333333333333\n",
      "iteration no 167: Loss: 0.7569130355218215, accuracy: 0.5133333333333333\n",
      "iteration no 168: Loss: 0.7569036145333181, accuracy: 0.5133333333333333\n",
      "iteration no 169: Loss: 0.7568944867582715, accuracy: 0.5133333333333333\n",
      "iteration no 170: Loss: 0.7568856427229774, accuracy: 0.5133333333333333\n",
      "iteration no 171: Loss: 0.7568770732754271, accuracy: 0.5133333333333333\n",
      "iteration no 172: Loss: 0.7568687695736545, accuracy: 0.5133333333333333\n",
      "iteration no 173: Loss: 0.7568607230745383, accuracy: 0.5133333333333333\n",
      "iteration no 174: Loss: 0.7568529255230426, accuracy: 0.5133333333333333\n",
      "iteration no 175: Loss: 0.7568453689418755, accuracy: 0.5133333333333333\n",
      "iteration no 176: Loss: 0.7568380456215485, accuracy: 0.5133333333333333\n",
      "iteration no 177: Loss: 0.7568309481108202, accuracy: 0.5133333333333333\n",
      "iteration no 178: Loss: 0.7568240692075063, accuracy: 0.5133333333333333\n",
      "iteration no 179: Loss: 0.7568174019496441, accuracy: 0.5133333333333333\n",
      "iteration no 180: Loss: 0.7568109396069921, accuracy: 0.5133333333333333\n",
      "iteration no 181: Loss: 0.7568046756728551, accuracy: 0.5133333333333333\n",
      "iteration no 182: Loss: 0.7567986038562183, accuracy: 0.5133333333333333\n",
      "iteration no 183: Loss: 0.7567927180741792, accuracy: 0.5133333333333333\n",
      "iteration no 184: Loss: 0.7567870124446643, accuracy: 0.5133333333333333\n",
      "iteration no 185: Loss: 0.7567814812794195, accuracy: 0.5133333333333333\n",
      "iteration no 186: Loss: 0.7567761190772627, accuracy: 0.5133333333333333\n",
      "iteration no 187: Loss: 0.7567709205175892, accuracy: 0.5133333333333333\n",
      "iteration no 188: Loss: 0.7567658804541154, accuracy: 0.5133333333333333\n",
      "iteration no 189: Loss: 0.7567609939088588, accuracy: 0.5133333333333333\n",
      "iteration no 190: Loss: 0.756756256066336, accuracy: 0.5133333333333333\n",
      "iteration no 191: Loss: 0.7567516622679785, accuracy: 0.5133333333333333\n",
      "iteration no 192: Loss: 0.7567472080067497, accuracy: 0.5133333333333333\n",
      "iteration no 193: Loss: 0.7567428889219613, accuracy: 0.5133333333333333\n",
      "iteration no 194: Loss: 0.7567387007942785, accuracy: 0.5133333333333333\n",
      "iteration no 195: Loss: 0.7567346395409059, accuracy: 0.5133333333333333\n",
      "iteration no 196: Loss: 0.7567307012109495, accuracy: 0.5133333333333333\n",
      "iteration no 197: Loss: 0.7567268819809443, accuracy: 0.5133333333333333\n",
      "iteration no 198: Loss: 0.7567231781505456, accuracy: 0.5133333333333333\n",
      "iteration no 199: Loss: 0.756719586138374, accuracy: 0.5133333333333333\n",
      "iteration no 200: Loss: 0.7567161024780082, accuracy: 0.5133333333333333\n",
      "iteration no 1: Loss: 1.0976981573626021, accuracy: 0.4066666666666667\n",
      "iteration no 2: Loss: 1.0673414566375388, accuracy: 0.5066666666666667\n",
      "iteration no 3: Loss: 1.0402882606531714, accuracy: 0.5066666666666667\n",
      "iteration no 4: Loss: 1.01614710765508, accuracy: 0.51\n",
      "iteration no 5: Loss: 0.9945688943858909, accuracy: 0.5133333333333333\n",
      "iteration no 6: Loss: 0.9752450032753809, accuracy: 0.5133333333333333\n",
      "iteration no 7: Loss: 0.9579042882755173, accuracy: 0.5133333333333333\n",
      "iteration no 8: Loss: 0.9423095172087983, accuracy: 0.5033333333333333\n",
      "iteration no 9: Loss: 0.9282536731295995, accuracy: 0.5066666666666667\n",
      "iteration no 10: Loss: 0.9155563635910021, accuracy: 0.51\n",
      "iteration no 11: Loss: 0.9040604771229317, accuracy: 0.51\n",
      "iteration no 12: Loss: 0.8936291532226996, accuracy: 0.51\n",
      "iteration no 13: Loss: 0.884143086257732, accuracy: 0.51\n",
      "iteration no 14: Loss: 0.8754981565140592, accuracy: 0.51\n",
      "iteration no 15: Loss: 0.8676033667946765, accuracy: 0.51\n",
      "iteration no 16: Loss: 0.8603790559499523, accuracy: 0.51\n",
      "iteration no 17: Loss: 0.8537553584513681, accuracy: 0.51\n",
      "iteration no 18: Loss: 0.8476708796049904, accuracy: 0.51\n",
      "iteration no 19: Loss: 0.8420715579744473, accuracy: 0.5133333333333333\n",
      "iteration no 20: Loss: 0.8369096892598394, accuracy: 0.5133333333333333\n",
      "iteration no 21: Loss: 0.8321430887874873, accuracy: 0.51\n",
      "iteration no 22: Loss: 0.8277343726341823, accuracy: 0.51\n",
      "iteration no 23: Loss: 0.8236503400926235, accuracy: 0.51\n",
      "iteration no 24: Loss: 0.8198614426129668, accuracy: 0.5066666666666667\n",
      "iteration no 25: Loss: 0.8163413265060124, accuracy: 0.5066666666666667\n",
      "iteration no 26: Loss: 0.813066438570152, accuracy: 0.5066666666666667\n",
      "iteration no 27: Loss: 0.810015685424626, accuracy: 0.5066666666666667\n",
      "iteration no 28: Loss: 0.8071701387205219, accuracy: 0.5066666666666667\n",
      "iteration no 29: Loss: 0.8045127795850533, accuracy: 0.5066666666666667\n",
      "iteration no 30: Loss: 0.8020282766604548, accuracy: 0.51\n",
      "iteration no 31: Loss: 0.7997027929510119, accuracy: 0.51\n",
      "iteration no 32: Loss: 0.7975238174125922, accuracy: 0.51\n",
      "iteration no 33: Loss: 0.7954800178282513, accuracy: 0.51\n",
      "iteration no 34: Loss: 0.7935611120281598, accuracy: 0.51\n",
      "iteration no 35: Loss: 0.7917577549469338, accuracy: 0.51\n",
      "iteration no 36: Loss: 0.7900614393789952, accuracy: 0.51\n",
      "iteration no 37: Loss: 0.7884644086034838, accuracy: 0.5133333333333333\n",
      "iteration no 38: Loss: 0.7869595793134491, accuracy: 0.5133333333333333\n",
      "iteration no 39: Loss: 0.7855404735071541, accuracy: 0.5133333333333333\n",
      "iteration no 40: Loss: 0.7842011581886511, accuracy: 0.5133333333333333\n",
      "iteration no 41: Loss: 0.7829361918856959, accuracy: 0.5133333333333333\n",
      "iteration no 42: Loss: 0.7817405771300026, accuracy: 0.5133333333333333\n",
      "iteration no 43: Loss: 0.780609718161572, accuracy: 0.5133333333333333\n",
      "iteration no 44: Loss: 0.7795393832184757, accuracy: 0.5133333333333333\n",
      "iteration no 45: Loss: 0.7785256708587034, accuracy: 0.5133333333333333\n",
      "iteration no 46: Loss: 0.7775649798336738, accuracy: 0.5133333333333333\n",
      "iteration no 47: Loss: 0.7766539820956417, accuracy: 0.5133333333333333\n",
      "iteration no 48: Loss: 0.7757895985750711, accuracy: 0.5133333333333333\n",
      "iteration no 49: Loss: 0.7749689774103784, accuracy: 0.5133333333333333\n",
      "iteration no 50: Loss: 0.7741894743524296, accuracy: 0.5133333333333333\n",
      "iteration no 51: Loss: 0.7734486351006972, accuracy: 0.5133333333333333\n",
      "iteration no 52: Loss: 0.772744179357862, accuracy: 0.5133333333333333\n",
      "iteration no 53: Loss: 0.7720739864155441, accuracy: 0.5133333333333333\n",
      "iteration no 54: Loss: 0.7714360821063353, accuracy: 0.5133333333333333\n",
      "iteration no 55: Loss: 0.7708286269768553, accuracy: 0.5166666666666667\n",
      "iteration no 56: Loss: 0.7702499055535961, accuracy: 0.5166666666666667\n",
      "iteration no 57: Loss: 0.7696983165881744, accuracy: 0.5166666666666667\n",
      "iteration no 58: Loss: 0.7691723641816082, accuracy: 0.5166666666666667\n",
      "iteration no 59: Loss: 0.7686706496985928, accuracy: 0.5166666666666667\n",
      "iteration no 60: Loss: 0.7681918643927264, accuracy: 0.5166666666666667\n",
      "iteration no 61: Loss: 0.767734782672377, accuracy: 0.5166666666666667\n",
      "iteration no 62: Loss: 0.7672982559445803, accuracy: 0.5166666666666667\n",
      "iteration no 63: Loss: 0.7668812069811306, accuracy: 0.5166666666666667\n",
      "iteration no 64: Loss: 0.7664826247570006, accuracy: 0.5166666666666667\n",
      "iteration no 65: Loss: 0.766101559716502, accuracy: 0.5166666666666667\n",
      "iteration no 66: Loss: 0.7657371194272627, accuracy: 0.5166666666666667\n",
      "iteration no 67: Loss: 0.7653884645862327, accuracy: 0.5166666666666667\n",
      "iteration no 68: Loss: 0.7650548053455858, accuracy: 0.5166666666666667\n",
      "iteration no 69: Loss: 0.7647353979296488, accuracy: 0.5166666666666667\n",
      "iteration no 70: Loss: 0.7644295415168723, accuracy: 0.5166666666666667\n",
      "iteration no 71: Loss: 0.7641365753634375, accuracy: 0.5166666666666667\n",
      "iteration no 72: Loss: 0.7638558761473911, accuracy: 0.5166666666666667\n",
      "iteration no 73: Loss: 0.7635868555142402, accuracy: 0.5166666666666667\n",
      "iteration no 74: Loss: 0.7633289578067832, accuracy: 0.5166666666666667\n",
      "iteration no 75: Loss: 0.763081657963574, accuracy: 0.5166666666666667\n",
      "iteration no 76: Loss: 0.7628444595719029, accuracy: 0.5133333333333333\n",
      "iteration no 77: Loss: 0.762616893062475, accuracy: 0.5133333333333333\n",
      "iteration no 78: Loss: 0.7623985140341603, accuracy: 0.5133333333333333\n",
      "iteration no 79: Loss: 0.7621889016982464, accuracy: 0.5133333333333333\n",
      "iteration no 80: Loss: 0.7619876574325801, accuracy: 0.5133333333333333\n",
      "iteration no 81: Loss: 0.7617944034368463, accuracy: 0.5133333333333333\n",
      "iteration no 82: Loss: 0.7616087814810039, accuracy: 0.5133333333333333\n",
      "iteration no 83: Loss: 0.7614304517396043, accuracy: 0.5133333333333333\n",
      "iteration no 84: Loss: 0.7612590917053482, accuracy: 0.5133333333333333\n",
      "iteration no 85: Loss: 0.7610943951758058, accuracy: 0.5133333333333333\n",
      "iteration no 86: Loss: 0.7609360713077461, accuracy: 0.5133333333333333\n",
      "iteration no 87: Loss: 0.7607838437339952, accuracy: 0.5133333333333333\n",
      "iteration no 88: Loss: 0.7606374497381554, accuracy: 0.5133333333333333\n",
      "iteration no 89: Loss: 0.7604966394829212, accuracy: 0.5133333333333333\n",
      "iteration no 90: Loss: 0.760361175288064, accuracy: 0.5133333333333333\n",
      "iteration no 91: Loss: 0.7602308309544864, accuracy: 0.5133333333333333\n",
      "iteration no 92: Loss: 0.7601053911310351, accuracy: 0.5133333333333333\n",
      "iteration no 93: Loss: 0.7599846507210247, accuracy: 0.5133333333333333\n",
      "iteration no 94: Loss: 0.7598684143256657, accuracy: 0.5133333333333333\n",
      "iteration no 95: Loss: 0.7597564957218195, accuracy: 0.5133333333333333\n",
      "iteration no 96: Loss: 0.7596487173716893, accuracy: 0.5133333333333333\n",
      "iteration no 97: Loss: 0.7595449099622529, accuracy: 0.5133333333333333\n",
      "iteration no 98: Loss: 0.7594449119724077, accuracy: 0.5133333333333333\n",
      "iteration no 99: Loss: 0.759348569265949, accuracy: 0.5133333333333333\n",
      "iteration no 100: Loss: 0.7592557347086495, accuracy: 0.5133333333333333\n",
      "iteration no 101: Loss: 0.7591662678078357, accuracy: 0.5166666666666667\n",
      "iteration no 102: Loss: 0.7590800343729707, accuracy: 0.5166666666666667\n",
      "iteration no 103: Loss: 0.758996906195874, accuracy: 0.5166666666666667\n",
      "iteration no 104: Loss: 0.7589167607492958, accuracy: 0.5166666666666667\n",
      "iteration no 105: Loss: 0.7588394809026631, accuracy: 0.5166666666666667\n",
      "iteration no 106: Loss: 0.7587649546539015, accuracy: 0.5166666666666667\n",
      "iteration no 107: Loss: 0.7586930748763095, accuracy: 0.5166666666666667\n",
      "iteration no 108: Loss: 0.7586237390795354, accuracy: 0.5166666666666667\n",
      "iteration no 109: Loss: 0.7585568491837791, accuracy: 0.5166666666666667\n",
      "iteration no 110: Loss: 0.7584923113063924, accuracy: 0.5166666666666667\n",
      "iteration no 111: Loss: 0.7584300355601192, accuracy: 0.5166666666666667\n",
      "iteration no 112: Loss: 0.7583699358622589, accuracy: 0.5166666666666667\n",
      "iteration no 113: Loss: 0.758311929754094, accuracy: 0.5166666666666667\n",
      "iteration no 114: Loss: 0.7582559382299602, accuracy: 0.5166666666666667\n",
      "iteration no 115: Loss: 0.7582018855753828, accuracy: 0.5166666666666667\n",
      "iteration no 116: Loss: 0.7581496992137414, accuracy: 0.5166666666666667\n",
      "iteration no 117: Loss: 0.7580993095609596, accuracy: 0.5166666666666667\n",
      "iteration no 118: Loss: 0.7580506498877481, accuracy: 0.5166666666666667\n",
      "iteration no 119: Loss: 0.7580036561889633, accuracy: 0.5166666666666667\n",
      "iteration no 120: Loss: 0.7579582670596695, accuracy: 0.5166666666666667\n",
      "iteration no 121: Loss: 0.7579144235775201, accuracy: 0.5166666666666667\n",
      "iteration no 122: Loss: 0.7578720691910982, accuracy: 0.5166666666666667\n",
      "iteration no 123: Loss: 0.7578311496138764, accuracy: 0.5166666666666667\n",
      "iteration no 124: Loss: 0.7577916127234848, accuracy: 0.5166666666666667\n",
      "iteration no 125: Loss: 0.757753408465985, accuracy: 0.5166666666666667\n",
      "iteration no 126: Loss: 0.7577164887648764, accuracy: 0.5166666666666667\n",
      "iteration no 127: Loss: 0.7576808074345703, accuracy: 0.5166666666666667\n",
      "iteration no 128: Loss: 0.7576463200980911, accuracy: 0.5166666666666667\n",
      "iteration no 129: Loss: 0.7576129841087694, accuracy: 0.5166666666666667\n",
      "iteration no 130: Loss: 0.7575807584757155, accuracy: 0.5166666666666667\n",
      "iteration no 131: Loss: 0.7575496037928673, accuracy: 0.5166666666666667\n",
      "iteration no 132: Loss: 0.7575194821714213, accuracy: 0.5166666666666667\n",
      "iteration no 133: Loss: 0.7574903571754703, accuracy: 0.5166666666666667\n",
      "iteration no 134: Loss: 0.7574621937606736, accuracy: 0.5166666666666667\n",
      "iteration no 135: Loss: 0.7574349582158048, accuracy: 0.5166666666666667\n",
      "iteration no 136: Loss: 0.7574086181070238, accuracy: 0.5166666666666667\n",
      "iteration no 137: Loss: 0.7573831422247356, accuracy: 0.5133333333333333\n",
      "iteration no 138: Loss: 0.757358500532898, accuracy: 0.5133333333333333\n",
      "iteration no 139: Loss: 0.7573346641206561, accuracy: 0.5133333333333333\n",
      "iteration no 140: Loss: 0.7573116051561829, accuracy: 0.5133333333333333\n",
      "iteration no 141: Loss: 0.7572892968426166, accuracy: 0.5133333333333333\n",
      "iteration no 142: Loss: 0.7572677133759865, accuracy: 0.5133333333333333\n",
      "iteration no 143: Loss: 0.7572468299050289, accuracy: 0.5133333333333333\n",
      "iteration no 144: Loss: 0.7572266224927993, accuracy: 0.5133333333333333\n",
      "iteration no 145: Loss: 0.7572070680799919, accuracy: 0.5133333333333333\n",
      "iteration no 146: Loss: 0.7571881444498801, accuracy: 0.5133333333333333\n",
      "iteration no 147: Loss: 0.7571698301948031, accuracy: 0.5133333333333333\n",
      "iteration no 148: Loss: 0.7571521046841183, accuracy: 0.5133333333333333\n",
      "iteration no 149: Loss: 0.7571349480335513, accuracy: 0.5133333333333333\n",
      "iteration no 150: Loss: 0.7571183410758767, accuracy: 0.5133333333333333\n",
      "iteration no 151: Loss: 0.7571022653328621, accuracy: 0.5133333333333333\n",
      "iteration no 152: Loss: 0.7570867029884204, accuracy: 0.5133333333333333\n",
      "iteration no 153: Loss: 0.7570716368629091, accuracy: 0.5133333333333333\n",
      "iteration no 154: Loss: 0.7570570503885231, accuracy: 0.5133333333333333\n",
      "iteration no 155: Loss: 0.757042927585731, accuracy: 0.5133333333333333\n",
      "iteration no 156: Loss: 0.757029253040705, accuracy: 0.5133333333333333\n",
      "iteration no 157: Loss: 0.7570160118836985, accuracy: 0.5133333333333333\n",
      "iteration no 158: Loss: 0.7570031897683276, accuracy: 0.5133333333333333\n",
      "iteration no 159: Loss: 0.7569907728517153, accuracy: 0.5133333333333333\n",
      "iteration no 160: Loss: 0.7569787477754574, accuracy: 0.5133333333333333\n",
      "iteration no 161: Loss: 0.7569671016473745, accuracy: 0.5133333333333333\n",
      "iteration no 162: Loss: 0.7569558220240133, accuracy: 0.5133333333333333\n",
      "iteration no 163: Loss: 0.7569448968938645, accuracy: 0.5133333333333333\n",
      "iteration no 164: Loss: 0.7569343146612625, accuracy: 0.5133333333333333\n",
      "iteration no 165: Loss: 0.7569240641309397, accuracy: 0.5133333333333333\n",
      "iteration no 166: Loss: 0.7569141344932045, accuracy: 0.5133333333333333\n",
      "iteration no 167: Loss: 0.7569045153097154, accuracy: 0.5133333333333333\n",
      "iteration no 168: Loss: 0.7568951964998247, accuracy: 0.5133333333333333\n",
      "iteration no 169: Loss: 0.7568861683274681, accuracy: 0.5133333333333333\n",
      "iteration no 170: Loss: 0.7568774213885747, accuracy: 0.5133333333333333\n",
      "iteration no 171: Loss: 0.756868946598976, accuracy: 0.5133333333333333\n",
      "iteration no 172: Loss: 0.7568607351827921, accuracy: 0.5133333333333333\n",
      "iteration no 173: Loss: 0.7568527786612724, accuracy: 0.5133333333333333\n",
      "iteration no 174: Loss: 0.7568450688420759, accuracy: 0.5133333333333333\n",
      "iteration no 175: Loss: 0.7568375978089672, accuracy: 0.5133333333333333\n",
      "iteration no 176: Loss: 0.7568303579119142, accuracy: 0.5133333333333333\n",
      "iteration no 177: Loss: 0.7568233417575683, accuracy: 0.5133333333333333\n",
      "iteration no 178: Loss: 0.7568165422001117, accuracy: 0.5133333333333333\n",
      "iteration no 179: Loss: 0.7568099523324575, accuracy: 0.5133333333333333\n",
      "iteration no 180: Loss: 0.7568035654777857, accuracy: 0.5133333333333333\n",
      "iteration no 181: Loss: 0.7567973751814023, accuracy: 0.5133333333333333\n",
      "iteration no 182: Loss: 0.7567913752029082, accuracy: 0.5133333333333333\n",
      "iteration no 183: Loss: 0.7567855595086644, accuracy: 0.5133333333333333\n",
      "iteration no 184: Loss: 0.7567799222645433, accuracy: 0.5133333333333333\n",
      "iteration no 185: Loss: 0.7567744578289496, accuracy: 0.5133333333333333\n",
      "iteration no 186: Loss: 0.7567691607461071, accuracy: 0.5133333333333333\n",
      "iteration no 187: Loss: 0.7567640257395928, accuracy: 0.5133333333333333\n",
      "iteration no 188: Loss: 0.7567590477061137, accuracy: 0.5133333333333333\n",
      "iteration no 189: Loss: 0.7567542217095155, accuracy: 0.5133333333333333\n",
      "iteration no 190: Loss: 0.756749542975011, accuracy: 0.5133333333333333\n",
      "iteration no 191: Loss: 0.7567450068836231, accuracy: 0.5133333333333333\n",
      "iteration no 192: Loss: 0.7567406089668319, accuracy: 0.5133333333333333\n",
      "iteration no 193: Loss: 0.7567363449014181, accuracy: 0.5133333333333333\n",
      "iteration no 194: Loss: 0.7567322105044944, accuracy: 0.5133333333333333\n",
      "iteration no 195: Loss: 0.7567282017287188, accuracy: 0.5133333333333333\n",
      "iteration no 196: Loss: 0.7567243146576819, accuracy: 0.5133333333333333\n",
      "iteration no 197: Loss: 0.7567205455014605, accuracy: 0.5133333333333333\n",
      "iteration no 198: Loss: 0.7567168905923325, accuracy: 0.5133333333333333\n",
      "iteration no 199: Loss: 0.7567133463806465, accuracy: 0.5133333333333333\n",
      "iteration no 200: Loss: 0.7567099094308387, accuracy: 0.5166666666666667\n",
      "iteration no 1: Loss: 1.0966686622115978, accuracy: 0.47\n",
      "iteration no 2: Loss: 1.06611965177754, accuracy: 0.4866666666666667\n",
      "iteration no 3: Loss: 1.0389626854435208, accuracy: 0.49333333333333335\n",
      "iteration no 4: Loss: 1.0147796761757901, accuracy: 0.49\n",
      "iteration no 5: Loss: 0.9932043371302033, accuracy: 0.5033333333333333\n",
      "iteration no 6: Loss: 0.9739155410427904, accuracy: 0.5133333333333333\n",
      "iteration no 7: Loss: 0.9566324436891092, accuracy: 0.5133333333333333\n",
      "iteration no 8: Loss: 0.941110170287677, accuracy: 0.5133333333333333\n",
      "iteration no 9: Loss: 0.9271357378919022, accuracy: 0.5166666666666667\n",
      "iteration no 10: Loss: 0.9145241968759193, accuracy: 0.5166666666666667\n",
      "iteration no 11: Loss: 0.9031150510575946, accuracy: 0.5133333333333333\n",
      "iteration no 12: Loss: 0.892769008769865, accuracy: 0.5133333333333333\n",
      "iteration no 13: Loss: 0.8833650891155933, accuracy: 0.5133333333333333\n",
      "iteration no 14: Loss: 0.8747980816531051, accuracy: 0.5166666666666667\n",
      "iteration no 15: Loss: 0.8669763398987582, accuracy: 0.5166666666666667\n",
      "iteration no 16: Loss: 0.8598198790482107, accuracy: 0.5166666666666667\n",
      "iteration no 17: Loss: 0.8532587441949089, accuracy: 0.5166666666666667\n",
      "iteration no 18: Loss: 0.8472316150001095, accuracy: 0.5166666666666667\n",
      "iteration no 19: Loss: 0.841684614641191, accuracy: 0.5166666666666667\n",
      "iteration no 20: Loss: 0.8365702938368332, accuracy: 0.5133333333333333\n",
      "iteration no 21: Loss: 0.8318467641270015, accuracy: 0.5166666666666667\n",
      "iteration no 22: Loss: 0.8274769579672511, accuracy: 0.5166666666666667\n",
      "iteration no 23: Loss: 0.8234279963631995, accuracy: 0.5166666666666667\n",
      "iteration no 24: Loss: 0.8196706476212662, accuracy: 0.5166666666666667\n",
      "iteration no 25: Loss: 0.81617886329404, accuracy: 0.5133333333333333\n",
      "iteration no 26: Loss: 0.8129293795592787, accuracy: 0.5133333333333333\n",
      "iteration no 27: Loss: 0.8099013741165078, accuracy: 0.5166666666666667\n",
      "iteration no 28: Loss: 0.8070761702487149, accuracy: 0.5166666666666667\n",
      "iteration no 29: Loss: 0.8044369810150083, accuracy: 0.5166666666666667\n",
      "iteration no 30: Loss: 0.8019686876481225, accuracy: 0.5166666666666667\n",
      "iteration no 31: Loss: 0.7996576471600807, accuracy: 0.5166666666666667\n",
      "iteration no 32: Loss: 0.797491524938149, accuracy: 0.5166666666666667\n",
      "iteration no 33: Loss: 0.7954591487656802, accuracy: 0.52\n",
      "iteration no 34: Loss: 0.7935503812492195, accuracy: 0.52\n",
      "iteration no 35: Loss: 0.7917560080917208, accuracy: 0.52\n",
      "iteration no 36: Loss: 0.7900676400366354, accuracy: 0.52\n",
      "iteration no 37: Loss: 0.7884776266310994, accuracy: 0.52\n",
      "iteration no 38: Loss: 0.786978980228764, accuracy: 0.52\n",
      "iteration no 39: Loss: 0.7855653088823693, accuracy: 0.5166666666666667\n",
      "iteration no 40: Loss: 0.784230756970042, accuracy: 0.5166666666666667\n",
      "iteration no 41: Loss: 0.7829699525633228, accuracy: 0.5166666666666667\n",
      "iteration no 42: Loss: 0.7817779606839669, accuracy: 0.5166666666666667\n",
      "iteration no 43: Loss: 0.7806502417146235, accuracy: 0.5166666666666667\n",
      "iteration no 44: Loss: 0.7795826143289629, accuracy: 0.5166666666666667\n",
      "iteration no 45: Loss: 0.7785712223924628, accuracy: 0.5166666666666667\n",
      "iteration no 46: Loss: 0.777612505358214, accuracy: 0.5166666666666667\n",
      "iteration no 47: Loss: 0.7767031717447135, accuracy: 0.5133333333333333\n",
      "iteration no 48: Loss: 0.7758401753362878, accuracy: 0.5133333333333333\n",
      "iteration no 49: Loss: 0.7750206937929195, accuracy: 0.5133333333333333\n",
      "iteration no 50: Loss: 0.7742421093959349, accuracy: 0.5133333333333333\n",
      "iteration no 51: Loss: 0.7735019916902494, accuracy: 0.5133333333333333\n",
      "iteration no 52: Loss: 0.7727980818134472, accuracy: 0.5133333333333333\n",
      "iteration no 53: Loss: 0.7721282783275655, accuracy: 0.5133333333333333\n",
      "iteration no 54: Loss: 0.7714906243916584, accuracy: 0.5133333333333333\n",
      "iteration no 55: Loss: 0.770883296132496, accuracy: 0.5133333333333333\n",
      "iteration no 56: Loss: 0.7703045920875372, accuracy: 0.5133333333333333\n",
      "iteration no 57: Loss: 0.7697529236089427, accuracy: 0.5133333333333333\n",
      "iteration no 58: Loss: 0.769226806130159, accuracy: 0.5133333333333333\n",
      "iteration no 59: Loss: 0.7687248512077949, accuracy: 0.5133333333333333\n",
      "iteration no 60: Loss: 0.7682457592612685, accuracy: 0.5133333333333333\n",
      "iteration no 61: Loss: 0.767788312941325, accuracy: 0.5133333333333333\n",
      "iteration no 62: Loss: 0.7673513710660401, accuracy: 0.5166666666666667\n",
      "iteration no 63: Loss: 0.7669338630695917, accuracy: 0.5166666666666667\n",
      "iteration no 64: Loss: 0.7665347839149188, accuracy: 0.5166666666666667\n",
      "iteration no 65: Loss: 0.7661531894265746, accuracy: 0.5166666666666667\n",
      "iteration no 66: Loss: 0.7657881920046343, accuracy: 0.5166666666666667\n",
      "iteration no 67: Loss: 0.7654389566845778, accuracy: 0.5166666666666667\n",
      "iteration no 68: Loss: 0.7651046975116491, accuracy: 0.5166666666666667\n",
      "iteration no 69: Loss: 0.764784674201383, accuracy: 0.5166666666666667\n",
      "iteration no 70: Loss: 0.7644781890608204, accuracy: 0.5166666666666667\n",
      "iteration no 71: Loss: 0.7641845841474543, accuracy: 0.5166666666666667\n",
      "iteration no 72: Loss: 0.7639032386452016, accuracy: 0.5166666666666667\n",
      "iteration no 73: Loss: 0.7636335664386913, accuracy: 0.5166666666666667\n",
      "iteration no 74: Loss: 0.7633750138689648, accuracy: 0.5166666666666667\n",
      "iteration no 75: Loss: 0.7631270576552788, accuracy: 0.5166666666666667\n",
      "iteration no 76: Loss: 0.7628892029691403, accuracy: 0.5166666666666667\n",
      "iteration no 77: Loss: 0.7626609816479972, accuracy: 0.5166666666666667\n",
      "iteration no 78: Loss: 0.7624419505371542, accuracy: 0.5166666666666667\n",
      "iteration no 79: Loss: 0.7622316899495356, accuracy: 0.5166666666666667\n",
      "iteration no 80: Loss: 0.7620298022338434, accuracy: 0.5166666666666667\n",
      "iteration no 81: Loss: 0.7618359104425081, accuracy: 0.5133333333333333\n",
      "iteration no 82: Loss: 0.7616496570915869, accuracy: 0.5133333333333333\n",
      "iteration no 83: Loss: 0.7614707030054505, accuracy: 0.5133333333333333\n",
      "iteration no 84: Loss: 0.7612987262397233, accuracy: 0.5133333333333333\n",
      "iteration no 85: Loss: 0.7611334210764988, accuracy: 0.5133333333333333\n",
      "iteration no 86: Loss: 0.7609744970863616, accuracy: 0.5133333333333333\n",
      "iteration no 87: Loss: 0.7608216782522108, accuracy: 0.5133333333333333\n",
      "iteration no 88: Loss: 0.7606747021502953, accuracy: 0.5133333333333333\n",
      "iteration no 89: Loss: 0.7605333191842459, accuracy: 0.5133333333333333\n",
      "iteration no 90: Loss: 0.7603972918682479, accuracy: 0.5133333333333333\n",
      "iteration no 91: Loss: 0.7602663941557937, accuracy: 0.5133333333333333\n",
      "iteration no 92: Loss: 0.760140410810758, accuracy: 0.5133333333333333\n",
      "iteration no 93: Loss: 0.760019136817786, accuracy: 0.5133333333333333\n",
      "iteration no 94: Loss: 0.7599023768292271, accuracy: 0.5133333333333333\n",
      "iteration no 95: Loss: 0.7597899446460674, accuracy: 0.5133333333333333\n",
      "iteration no 96: Loss: 0.7596816627305049, accuracy: 0.5133333333333333\n",
      "iteration no 97: Loss: 0.7595773617479968, accuracy: 0.5133333333333333\n",
      "iteration no 98: Loss: 0.7594768801367773, accuracy: 0.5133333333333333\n",
      "iteration no 99: Loss: 0.7593800637029873, accuracy: 0.5133333333333333\n",
      "iteration no 100: Loss: 0.7592867652397045, accuracy: 0.5133333333333333\n",
      "iteration no 101: Loss: 0.7591968441682895, accuracy: 0.5133333333333333\n",
      "iteration no 102: Loss: 0.759110166200573, accuracy: 0.5133333333333333\n",
      "iteration no 103: Loss: 0.7590266030205293, accuracy: 0.5133333333333333\n",
      "iteration no 104: Loss: 0.7589460319841661, accuracy: 0.5166666666666667\n",
      "iteration no 105: Loss: 0.758868335836467, accuracy: 0.5166666666666667\n",
      "iteration no 106: Loss: 0.7587934024442919, accuracy: 0.5166666666666667\n",
      "iteration no 107: Loss: 0.7587211245442291, accuracy: 0.5166666666666667\n",
      "iteration no 108: Loss: 0.758651399504458, accuracy: 0.5166666666666667\n",
      "iteration no 109: Loss: 0.7585841290997484, accuracy: 0.5166666666666667\n",
      "iteration no 110: Loss: 0.7585192192987856, accuracy: 0.5166666666666667\n",
      "iteration no 111: Loss: 0.7584565800630603, accuracy: 0.5166666666666667\n",
      "iteration no 112: Loss: 0.7583961251566216, accuracy: 0.5166666666666667\n",
      "iteration no 113: Loss: 0.7583377719660322, accuracy: 0.5166666666666667\n",
      "iteration no 114: Loss: 0.7582814413299171, accuracy: 0.5166666666666667\n",
      "iteration no 115: Loss: 0.7582270573775269, accuracy: 0.5166666666666667\n",
      "iteration no 116: Loss: 0.7581745473757867, accuracy: 0.5166666666666667\n",
      "iteration no 117: Loss: 0.7581238415843301, accuracy: 0.5166666666666667\n",
      "iteration no 118: Loss: 0.7580748731180498, accuracy: 0.5166666666666667\n",
      "iteration no 119: Loss: 0.7580275778167306, accuracy: 0.5166666666666667\n",
      "iteration no 120: Loss: 0.7579818941213557, accuracy: 0.5166666666666667\n",
      "iteration no 121: Loss: 0.7579377629567047, accuracy: 0.5166666666666667\n",
      "iteration no 122: Loss: 0.7578951276198852, accuracy: 0.5166666666666667\n",
      "iteration no 123: Loss: 0.7578539336744623, accuracy: 0.5166666666666667\n",
      "iteration no 124: Loss: 0.7578141288498741, accuracy: 0.5166666666666667\n",
      "iteration no 125: Loss: 0.757775662945835, accuracy: 0.5166666666666667\n",
      "iteration no 126: Loss: 0.7577384877414537, accuracy: 0.5166666666666667\n",
      "iteration no 127: Loss: 0.7577025569088052, accuracy: 0.5166666666666667\n",
      "iteration no 128: Loss: 0.7576678259307129, accuracy: 0.5166666666666667\n",
      "iteration no 129: Loss: 0.7576342520225128, accuracy: 0.5133333333333333\n",
      "iteration no 130: Loss: 0.7576017940575835, accuracy: 0.5133333333333333\n",
      "iteration no 131: Loss: 0.7575704124964431, accuracy: 0.5133333333333333\n",
      "iteration no 132: Loss: 0.7575400693192172, accuracy: 0.5133333333333333\n",
      "iteration no 133: Loss: 0.7575107279613056, accuracy: 0.5133333333333333\n",
      "iteration no 134: Loss: 0.7574823532520759, accuracy: 0.5133333333333333\n",
      "iteration no 135: Loss: 0.757454911356423, accuracy: 0.5133333333333333\n",
      "iteration no 136: Loss: 0.7574283697190521, accuracy: 0.5133333333333333\n",
      "iteration no 137: Loss: 0.7574026970113387, accuracy: 0.5133333333333333\n",
      "iteration no 138: Loss: 0.7573778630806325, accuracy: 0.5133333333333333\n",
      "iteration no 139: Loss: 0.7573538389018867, accuracy: 0.5133333333333333\n",
      "iteration no 140: Loss: 0.757330596531486, accuracy: 0.5133333333333333\n",
      "iteration no 141: Loss: 0.7573081090631697, accuracy: 0.5133333333333333\n",
      "iteration no 142: Loss: 0.7572863505859382, accuracy: 0.5133333333333333\n",
      "iteration no 143: Loss: 0.7572652961438481, accuracy: 0.5133333333333333\n",
      "iteration no 144: Loss: 0.7572449216976016, accuracy: 0.5133333333333333\n",
      "iteration no 145: Loss: 0.7572252040878384, accuracy: 0.5133333333333333\n",
      "iteration no 146: Loss: 0.7572061210000497, accuracy: 0.5133333333333333\n",
      "iteration no 147: Loss: 0.7571876509310336, accuracy: 0.5133333333333333\n",
      "iteration no 148: Loss: 0.7571697731568172, accuracy: 0.5133333333333333\n",
      "iteration no 149: Loss: 0.7571524677019741, accuracy: 0.5133333333333333\n",
      "iteration no 150: Loss: 0.7571357153102709, accuracy: 0.5133333333333333\n",
      "iteration no 151: Loss: 0.7571194974165779, accuracy: 0.5133333333333333\n",
      "iteration no 152: Loss: 0.7571037961199848, accuracy: 0.5133333333333333\n",
      "iteration no 153: Loss: 0.7570885941580651, accuracy: 0.5133333333333333\n",
      "iteration no 154: Loss: 0.7570738748822314, accuracy: 0.5133333333333333\n",
      "iteration no 155: Loss: 0.7570596222341365, accuracy: 0.5133333333333333\n",
      "iteration no 156: Loss: 0.7570458207230653, accuracy: 0.5133333333333333\n",
      "iteration no 157: Loss: 0.7570324554042751, accuracy: 0.5133333333333333\n",
      "iteration no 158: Loss: 0.7570195118582416, accuracy: 0.5133333333333333\n",
      "iteration no 159: Loss: 0.7570069761707626, accuracy: 0.5133333333333333\n",
      "iteration no 160: Loss: 0.7569948349138907, accuracy: 0.5133333333333333\n",
      "iteration no 161: Loss: 0.7569830751276455, accuracy: 0.5133333333333333\n",
      "iteration no 162: Loss: 0.7569716843024793, accuracy: 0.5133333333333333\n",
      "iteration no 163: Loss: 0.7569606503624585, accuracy: 0.5133333333333333\n",
      "iteration no 164: Loss: 0.7569499616491265, accuracy: 0.5133333333333333\n",
      "iteration no 165: Loss: 0.756939606906023, accuracy: 0.5133333333333333\n",
      "iteration no 166: Loss: 0.7569295752638263, accuracy: 0.5133333333333333\n",
      "iteration no 167: Loss: 0.7569198562260911, accuracy: 0.5133333333333333\n",
      "iteration no 168: Loss: 0.7569104396555598, accuracy: 0.5133333333333333\n",
      "iteration no 169: Loss: 0.7569013157610159, accuracy: 0.5133333333333333\n",
      "iteration no 170: Loss: 0.7568924750846617, accuracy: 0.5133333333333333\n",
      "iteration no 171: Loss: 0.7568839084899932, accuracy: 0.5133333333333333\n",
      "iteration no 172: Loss: 0.7568756071501537, accuracy: 0.5133333333333333\n",
      "iteration no 173: Loss: 0.7568675625367438, accuracy: 0.5133333333333333\n",
      "iteration no 174: Loss: 0.7568597664090676, accuracy: 0.5133333333333333\n",
      "iteration no 175: Loss: 0.7568522108038, accuracy: 0.5133333333333333\n",
      "iteration no 176: Loss: 0.7568448880250521, accuracy: 0.5133333333333333\n",
      "iteration no 177: Loss: 0.756837790634823, accuracy: 0.5133333333333333\n",
      "iteration no 178: Loss: 0.7568309114438165, accuracy: 0.5133333333333333\n",
      "iteration no 179: Loss: 0.7568242435026133, accuracy: 0.5133333333333333\n",
      "iteration no 180: Loss: 0.7568177800931776, accuracy: 0.5133333333333333\n",
      "iteration no 181: Loss: 0.75681151472069, accuracy: 0.5133333333333333\n",
      "iteration no 182: Loss: 0.7568054411056878, accuracy: 0.5133333333333333\n",
      "iteration no 183: Loss: 0.7567995531765053, accuracy: 0.5133333333333333\n",
      "iteration no 184: Loss: 0.756793845061997, accuracy: 0.5133333333333333\n",
      "iteration no 185: Loss: 0.7567883110845356, accuracy: 0.5133333333333333\n",
      "iteration no 186: Loss: 0.756782945753271, accuracy: 0.5133333333333333\n",
      "iteration no 187: Loss: 0.7567777437576424, accuracy: 0.5133333333333333\n",
      "iteration no 188: Loss: 0.7567726999611311, accuracy: 0.5133333333333333\n",
      "iteration no 189: Loss: 0.7567678093952434, accuracy: 0.5133333333333333\n",
      "iteration no 190: Loss: 0.7567630672537196, accuracy: 0.5133333333333333\n",
      "iteration no 191: Loss: 0.7567584688869531, accuracy: 0.5133333333333333\n",
      "iteration no 192: Loss: 0.7567540097966153, accuracy: 0.5133333333333333\n",
      "iteration no 193: Loss: 0.756749685630479, accuracy: 0.5133333333333333\n",
      "iteration no 194: Loss: 0.7567454921774278, accuracy: 0.5133333333333333\n",
      "iteration no 195: Loss: 0.7567414253626509, accuracy: 0.5133333333333333\n",
      "iteration no 196: Loss: 0.7567374812430081, accuracy: 0.5133333333333333\n",
      "iteration no 197: Loss: 0.7567336560025669, accuracy: 0.5133333333333333\n",
      "iteration no 198: Loss: 0.7567299459482966, accuracy: 0.5133333333333333\n",
      "iteration no 199: Loss: 0.75672634750592, accuracy: 0.5133333333333333\n",
      "iteration no 200: Loss: 0.7567228572159133, accuracy: 0.5133333333333333\n",
      "iteration no 1: Loss: 1.1096050017099548, accuracy: 0.4\n",
      "iteration no 2: Loss: 1.0779917988224306, accuracy: 0.4766666666666667\n",
      "iteration no 3: Loss: 1.0498447653600695, accuracy: 0.49666666666666665\n",
      "iteration no 4: Loss: 1.024748545262488, accuracy: 0.48\n",
      "iteration no 5: Loss: 1.002334931338909, accuracy: 0.51\n",
      "iteration no 6: Loss: 0.9822790589135366, accuracy: 0.5066666666666667\n",
      "iteration no 7: Loss: 0.964295662825626, accuracy: 0.5066666666666667\n",
      "iteration no 8: Loss: 0.9481352154408658, accuracy: 0.5066666666666667\n",
      "iteration no 9: Loss: 0.933580045876699, accuracy: 0.5066666666666667\n",
      "iteration no 10: Loss: 0.92044058844402, accuracy: 0.5033333333333333\n",
      "iteration no 11: Loss: 0.9085518761334669, accuracy: 0.5133333333333333\n",
      "iteration no 12: Loss: 0.8977703472232473, accuracy: 0.5133333333333333\n",
      "iteration no 13: Loss: 0.8879709924221388, accuracy: 0.51\n",
      "iteration no 14: Loss: 0.8790448415247828, accuracy: 0.51\n",
      "iteration no 15: Loss: 0.8708967711399817, accuracy: 0.51\n",
      "iteration no 16: Loss: 0.8634436058279603, accuracy: 0.51\n",
      "iteration no 17: Loss: 0.8566124812475828, accuracy: 0.5066666666666667\n",
      "iteration no 18: Loss: 0.8503394375824697, accuracy: 0.51\n",
      "iteration no 19: Loss: 0.844568213127923, accuracy: 0.51\n",
      "iteration no 20: Loss: 0.8392492105222544, accuracy: 0.5133333333333333\n",
      "iteration no 21: Loss: 0.8343386111000004, accuracy: 0.51\n",
      "iteration no 22: Loss: 0.8297976158770575, accuracy: 0.51\n",
      "iteration no 23: Loss: 0.8255917945530161, accuracy: 0.51\n",
      "iteration no 24: Loss: 0.821690526536742, accuracy: 0.51\n",
      "iteration no 25: Loss: 0.8180665203302031, accuracy: 0.51\n",
      "iteration no 26: Loss: 0.8146953996400551, accuracy: 0.51\n",
      "iteration no 27: Loss: 0.81155534634287, accuracy: 0.51\n",
      "iteration no 28: Loss: 0.8086267919336351, accuracy: 0.51\n",
      "iteration no 29: Loss: 0.8058921503670821, accuracy: 0.51\n",
      "iteration no 30: Loss: 0.8033355862865276, accuracy: 0.51\n",
      "iteration no 31: Loss: 0.8009428135523161, accuracy: 0.5133333333333333\n",
      "iteration no 32: Loss: 0.7987009197563252, accuracy: 0.5133333333333333\n",
      "iteration no 33: Loss: 0.796598213061975, accuracy: 0.5133333333333333\n",
      "iteration no 34: Loss: 0.7946240882596423, accuracy: 0.5133333333333333\n",
      "iteration no 35: Loss: 0.7927689093914821, accuracy: 0.5133333333333333\n",
      "iteration no 36: Loss: 0.7910239066911252, accuracy: 0.5133333333333333\n",
      "iteration no 37: Loss: 0.7893810859142116, accuracy: 0.5166666666666667\n",
      "iteration no 38: Loss: 0.7878331484149976, accuracy: 0.5166666666666667\n",
      "iteration no 39: Loss: 0.7863734205605621, accuracy: 0.5166666666666667\n",
      "iteration no 40: Loss: 0.7849957912743385, accuracy: 0.5166666666666667\n",
      "iteration no 41: Loss: 0.7836946566705598, accuracy: 0.5133333333333333\n",
      "iteration no 42: Loss: 0.7824648708855387, accuracy: 0.5133333333333333\n",
      "iteration no 43: Loss: 0.7813017023345826, accuracy: 0.5133333333333333\n",
      "iteration no 44: Loss: 0.780200794728083, accuracy: 0.5133333333333333\n",
      "iteration no 45: Loss: 0.7791581322697907, accuracy: 0.5133333333333333\n",
      "iteration no 46: Loss: 0.7781700085368266, accuracy: 0.5133333333333333\n",
      "iteration no 47: Loss: 0.7772329986065781, accuracy: 0.5133333333333333\n",
      "iteration no 48: Loss: 0.77634393405195, accuracy: 0.5133333333333333\n",
      "iteration no 49: Loss: 0.775499880474881, accuracy: 0.5133333333333333\n",
      "iteration no 50: Loss: 0.7746981172897729, accuracy: 0.5133333333333333\n",
      "iteration no 51: Loss: 0.7739361195044966, accuracy: 0.5133333333333333\n",
      "iteration no 52: Loss: 0.7732115412777916, accuracy: 0.5133333333333333\n",
      "iteration no 53: Loss: 0.7725222010588446, accuracy: 0.5133333333333333\n",
      "iteration no 54: Loss: 0.7718660681382399, accuracy: 0.5133333333333333\n",
      "iteration no 55: Loss: 0.7712412504598097, accuracy: 0.5166666666666667\n",
      "iteration no 56: Loss: 0.7706459835606213, accuracy: 0.5166666666666667\n",
      "iteration no 57: Loss: 0.7700786205217728, accuracy: 0.5166666666666667\n",
      "iteration no 58: Loss: 0.7695376228261552, accuracy: 0.5166666666666667\n",
      "iteration no 59: Loss: 0.7690215520311304, accuracy: 0.5166666666666667\n",
      "iteration no 60: Loss: 0.7685290621744107, accuracy: 0.5166666666666667\n",
      "iteration no 61: Loss: 0.7680588928404948, accuracy: 0.5166666666666667\n",
      "iteration no 62: Loss: 0.7676098628229794, accuracy: 0.5166666666666667\n",
      "iteration no 63: Loss: 0.7671808643250876, accuracy: 0.5166666666666667\n",
      "iteration no 64: Loss: 0.7667708576469351, accuracy: 0.5166666666666667\n",
      "iteration no 65: Loss: 0.7663788663135106, accuracy: 0.5166666666666667\n",
      "iteration no 66: Loss: 0.7660039726021802, accuracy: 0.5166666666666667\n",
      "iteration no 67: Loss: 0.7656453134327874, accuracy: 0.5166666666666667\n",
      "iteration no 68: Loss: 0.7653020765872183, accuracy: 0.5166666666666667\n",
      "iteration no 69: Loss: 0.7649734972286505, accuracy: 0.5166666666666667\n",
      "iteration no 70: Loss: 0.7646588546937065, accuracy: 0.5166666666666667\n",
      "iteration no 71: Loss: 0.7643574695333766, accuracy: 0.5166666666666667\n",
      "iteration no 72: Loss: 0.764068700780958, accuracy: 0.5166666666666667\n",
      "iteration no 73: Loss: 0.7637919434273687, accuracy: 0.5166666666666667\n",
      "iteration no 74: Loss: 0.7635266260860754, accuracy: 0.5166666666666667\n",
      "iteration no 75: Loss: 0.7632722088315798, accuracy: 0.5166666666666667\n",
      "iteration no 76: Loss: 0.7630281811969057, accuracy: 0.5166666666666667\n",
      "iteration no 77: Loss: 0.762794060316898, accuracy: 0.5166666666666667\n",
      "iteration no 78: Loss: 0.7625693892053522, accuracy: 0.5166666666666667\n",
      "iteration no 79: Loss: 0.7623537351550962, accuracy: 0.5166666666666667\n",
      "iteration no 80: Loss: 0.762146688251127, accuracy: 0.5166666666666667\n",
      "iteration no 81: Loss: 0.7619478599877877, accuracy: 0.5133333333333333\n",
      "iteration no 82: Loss: 0.7617568819817776, accuracy: 0.5133333333333333\n",
      "iteration no 83: Loss: 0.7615734047735024, accuracy: 0.5133333333333333\n",
      "iteration no 84: Loss: 0.7613970967099261, accuracy: 0.5133333333333333\n",
      "iteration no 85: Loss: 0.7612276429026775, accuracy: 0.5133333333333333\n",
      "iteration no 86: Loss: 0.7610647442556914, accuracy: 0.5133333333333333\n",
      "iteration no 87: Loss: 0.7609081165571558, accuracy: 0.5133333333333333\n",
      "iteration no 88: Loss: 0.7607574896309716, accuracy: 0.5133333333333333\n",
      "iteration no 89: Loss: 0.7606126065433203, accuracy: 0.5133333333333333\n",
      "iteration no 90: Loss: 0.7604732228603193, accuracy: 0.5133333333333333\n",
      "iteration no 91: Loss: 0.7603391059530454, accuracy: 0.5133333333333333\n",
      "iteration no 92: Loss: 0.7602100343465287, accuracy: 0.5133333333333333\n",
      "iteration no 93: Loss: 0.7600857971095797, accuracy: 0.5133333333333333\n",
      "iteration no 94: Loss: 0.7599661932825654, accuracy: 0.5133333333333333\n",
      "iteration no 95: Loss: 0.7598510313404775, accuracy: 0.5133333333333333\n",
      "iteration no 96: Loss: 0.7597401286888394, accuracy: 0.5133333333333333\n",
      "iteration no 97: Loss: 0.7596333111901948, accuracy: 0.5133333333333333\n",
      "iteration no 98: Loss: 0.7595304127190846, accuracy: 0.5133333333333333\n",
      "iteration no 99: Loss: 0.7594312747435892, accuracy: 0.5133333333333333\n",
      "iteration no 100: Loss: 0.7593357459316469, accuracy: 0.5133333333333333\n",
      "iteration no 101: Loss: 0.7592436817805016, accuracy: 0.5166666666666667\n",
      "iteration no 102: Loss: 0.759154944267752, accuracy: 0.5166666666666667\n",
      "iteration no 103: Loss: 0.7590694015225805, accuracy: 0.5166666666666667\n",
      "iteration no 104: Loss: 0.7589869275158606, accuracy: 0.5166666666666667\n",
      "iteration no 105: Loss: 0.7589074017679143, accuracy: 0.5166666666666667\n",
      "iteration no 106: Loss: 0.7588307090727963, accuracy: 0.5166666666666667\n",
      "iteration no 107: Loss: 0.7587567392380525, accuracy: 0.5166666666666667\n",
      "iteration no 108: Loss: 0.7586853868389796, accuracy: 0.5166666666666667\n",
      "iteration no 109: Loss: 0.7586165509864754, accuracy: 0.5166666666666667\n",
      "iteration no 110: Loss: 0.7585501351076402, accuracy: 0.5166666666666667\n",
      "iteration no 111: Loss: 0.7584860467383406, accuracy: 0.5166666666666667\n",
      "iteration no 112: Loss: 0.7584241973270056, accuracy: 0.5166666666666667\n",
      "iteration no 113: Loss: 0.7583645020489703, accuracy: 0.5166666666666667\n",
      "iteration no 114: Loss: 0.7583068796307374, accuracy: 0.5166666666666667\n",
      "iteration no 115: Loss: 0.7582512521835552, accuracy: 0.5166666666666667\n",
      "iteration no 116: Loss: 0.7581975450457636, accuracy: 0.5166666666666667\n",
      "iteration no 117: Loss: 0.7581456866333913, accuracy: 0.5166666666666667\n",
      "iteration no 118: Loss: 0.7580956082985153, accuracy: 0.5166666666666667\n",
      "iteration no 119: Loss: 0.7580472441949386, accuracy: 0.5166666666666667\n",
      "iteration no 120: Loss: 0.7580005311507558, accuracy: 0.5166666666666667\n",
      "iteration no 121: Loss: 0.7579554085474179, accuracy: 0.5166666666666667\n",
      "iteration no 122: Loss: 0.75791181820492, accuracy: 0.5166666666666667\n",
      "iteration no 123: Loss: 0.7578697042727698, accuracy: 0.5166666666666667\n",
      "iteration no 124: Loss: 0.7578290131264088, accuracy: 0.5166666666666667\n",
      "iteration no 125: Loss: 0.7577896932687829, accuracy: 0.5166666666666667\n",
      "iteration no 126: Loss: 0.7577516952367755, accuracy: 0.5166666666666667\n",
      "iteration no 127: Loss: 0.7577149715122375, accuracy: 0.5166666666666667\n",
      "iteration no 128: Loss: 0.7576794764373576, accuracy: 0.5166666666666667\n",
      "iteration no 129: Loss: 0.7576451661341403, accuracy: 0.5166666666666667\n",
      "iteration no 130: Loss: 0.7576119984277695, accuracy: 0.5166666666666667\n",
      "iteration no 131: Loss: 0.7575799327736461, accuracy: 0.5166666666666667\n",
      "iteration no 132: Loss: 0.757548930187905, accuracy: 0.5166666666666667\n",
      "iteration no 133: Loss: 0.7575189531812265, accuracy: 0.5166666666666667\n",
      "iteration no 134: Loss: 0.7574899656957675, accuracy: 0.5166666666666667\n",
      "iteration no 135: Loss: 0.7574619330450506, accuracy: 0.5166666666666667\n",
      "iteration no 136: Loss: 0.7574348218566538, accuracy: 0.5133333333333333\n",
      "iteration no 137: Loss: 0.7574086000175589, accuracy: 0.5133333333333333\n",
      "iteration no 138: Loss: 0.757383236622019, accuracy: 0.5133333333333333\n",
      "iteration no 139: Loss: 0.7573587019218173, accuracy: 0.5133333333333333\n",
      "iteration no 140: Loss: 0.7573349672787953, accuracy: 0.5133333333333333\n",
      "iteration no 141: Loss: 0.7573120051195353, accuracy: 0.5133333333333333\n",
      "iteration no 142: Loss: 0.7572897888920871, accuracy: 0.5133333333333333\n",
      "iteration no 143: Loss: 0.7572682930246408, accuracy: 0.5133333333333333\n",
      "iteration no 144: Loss: 0.757247492886043, accuracy: 0.5133333333333333\n",
      "iteration no 145: Loss: 0.7572273647480708, accuracy: 0.5133333333333333\n",
      "iteration no 146: Loss: 0.7572078857493726, accuracy: 0.5133333333333333\n",
      "iteration no 147: Loss: 0.757189033860996, accuracy: 0.5133333333333333\n",
      "iteration no 148: Loss: 0.7571707878534258, accuracy: 0.5133333333333333\n",
      "iteration no 149: Loss: 0.7571531272650588, accuracy: 0.5133333333333333\n",
      "iteration no 150: Loss: 0.7571360323720437, accuracy: 0.5133333333333333\n",
      "iteration no 151: Loss: 0.7571194841594258, accuracy: 0.5133333333333333\n",
      "iteration no 152: Loss: 0.7571034642935284, accuracy: 0.5133333333333333\n",
      "iteration no 153: Loss: 0.7570879550955163, accuracy: 0.5133333333333333\n",
      "iteration no 154: Loss: 0.7570729395160847, accuracy: 0.5133333333333333\n",
      "iteration no 155: Loss: 0.7570584011112187, accuracy: 0.5133333333333333\n",
      "iteration no 156: Loss: 0.7570443240189766, accuracy: 0.5133333333333333\n",
      "iteration no 157: Loss: 0.7570306929372476, accuracy: 0.5133333333333333\n",
      "iteration no 158: Loss: 0.7570174931024372, accuracy: 0.5133333333333333\n",
      "iteration no 159: Loss: 0.7570047102690415, accuracy: 0.5133333333333333\n",
      "iteration no 160: Loss: 0.7569923306900669, accuracy: 0.5133333333333333\n",
      "iteration no 161: Loss: 0.7569803410982568, accuracy: 0.5133333333333333\n",
      "iteration no 162: Loss: 0.7569687286880898, accuracy: 0.5133333333333333\n",
      "iteration no 163: Loss: 0.7569574810985142, accuracy: 0.5133333333333333\n",
      "iteration no 164: Loss: 0.7569465863963847, accuracy: 0.5133333333333333\n",
      "iteration no 165: Loss: 0.7569360330605723, accuracy: 0.5133333333333333\n",
      "iteration no 166: Loss: 0.756925809966714, accuracy: 0.5133333333333333\n",
      "iteration no 167: Loss: 0.7569159063725772, accuracy: 0.5133333333333333\n",
      "iteration no 168: Loss: 0.7569063119040103, accuracy: 0.5133333333333333\n",
      "iteration no 169: Loss: 0.7568970165414532, accuracy: 0.5133333333333333\n",
      "iteration no 170: Loss: 0.7568880106069845, accuracy: 0.5133333333333333\n",
      "iteration no 171: Loss: 0.7568792847518807, accuracy: 0.5133333333333333\n",
      "iteration no 172: Loss: 0.756870829944667, accuracy: 0.5133333333333333\n",
      "iteration no 173: Loss: 0.7568626374596363, accuracy: 0.5133333333333333\n",
      "iteration no 174: Loss: 0.7568546988658188, accuracy: 0.5133333333333333\n",
      "iteration no 175: Loss: 0.75684700601638, accuracy: 0.5133333333333333\n",
      "iteration no 176: Loss: 0.7568395510384323, accuracy: 0.5133333333333333\n",
      "iteration no 177: Loss: 0.7568323263232393, accuracy: 0.5133333333333333\n",
      "iteration no 178: Loss: 0.7568253245167983, accuracy: 0.5133333333333333\n",
      "iteration no 179: Loss: 0.7568185385107858, accuracy: 0.5133333333333333\n",
      "iteration no 180: Loss: 0.7568119614338474, accuracy: 0.5133333333333333\n",
      "iteration no 181: Loss: 0.7568055866432213, accuracy: 0.5133333333333333\n",
      "iteration no 182: Loss: 0.7567994077166806, accuracy: 0.5133333333333333\n",
      "iteration no 183: Loss: 0.7567934184447805, accuracy: 0.5133333333333333\n",
      "iteration no 184: Loss: 0.7567876128233967, accuracy: 0.5133333333333333\n",
      "iteration no 185: Loss: 0.7567819850465471, accuracy: 0.5133333333333333\n",
      "iteration no 186: Loss: 0.7567765294994812, accuracy: 0.5133333333333333\n",
      "iteration no 187: Loss: 0.7567712407520271, accuracy: 0.5133333333333333\n",
      "iteration no 188: Loss: 0.7567661135521894, accuracy: 0.5133333333333333\n",
      "iteration no 189: Loss: 0.7567611428199803, accuracy: 0.5133333333333333\n",
      "iteration no 190: Loss: 0.756756323641483, accuracy: 0.5133333333333333\n",
      "iteration no 191: Loss: 0.7567516512631328, accuracy: 0.5133333333333333\n",
      "iteration no 192: Loss: 0.7567471210862073, accuracy: 0.5133333333333333\n",
      "iteration no 193: Loss: 0.756742728661521, accuracy: 0.5133333333333333\n",
      "iteration no 194: Loss: 0.7567384696843114, accuracy: 0.5133333333333333\n",
      "iteration no 195: Loss: 0.7567343399893138, accuracy: 0.5133333333333333\n",
      "iteration no 196: Loss: 0.7567303355460138, accuracy: 0.5133333333333333\n",
      "iteration no 197: Loss: 0.7567264524540711, accuracy: 0.5133333333333333\n",
      "iteration no 198: Loss: 0.7567226869389109, accuracy: 0.5133333333333333\n",
      "iteration no 199: Loss: 0.7567190353474716, accuracy: 0.5133333333333333\n",
      "iteration no 200: Loss: 0.7567154941441072, accuracy: 0.5133333333333333\n",
      "iteration no 1: Loss: 1.1090378355528918, accuracy: 0.37666666666666665\n",
      "iteration no 2: Loss: 1.0773625945954188, accuracy: 0.44666666666666666\n",
      "iteration no 3: Loss: 1.0492019512055473, accuracy: 0.4866666666666667\n",
      "iteration no 4: Loss: 1.0241222062252717, accuracy: 0.4866666666666667\n",
      "iteration no 5: Loss: 1.0017438886760646, accuracy: 0.5033333333333333\n",
      "iteration no 6: Loss: 0.981734648662349, accuracy: 0.5033333333333333\n",
      "iteration no 7: Loss: 0.9638040019257135, accuracy: 0.5\n",
      "iteration no 8: Loss: 0.9476987234197705, accuracy: 0.5\n",
      "iteration no 9: Loss: 0.9331985480218639, accuracy: 0.5\n",
      "iteration no 10: Loss: 0.9201121440024882, accuracy: 0.5\n",
      "iteration no 11: Loss: 0.9082734034288309, accuracy: 0.5033333333333333\n",
      "iteration no 12: Loss: 0.897538090105345, accuracy: 0.5033333333333333\n",
      "iteration no 13: Loss: 0.8877808611537601, accuracy: 0.5066666666666667\n",
      "iteration no 14: Loss: 0.8788926551774026, accuracy: 0.51\n",
      "iteration no 15: Loss: 0.870778424071035, accuracy: 0.5166666666666667\n",
      "iteration no 16: Loss: 0.8633551768159264, accuracy: 0.5166666666666667\n",
      "iteration no 17: Loss: 0.8565503002293045, accuracy: 0.5166666666666667\n",
      "iteration no 18: Loss: 0.850300121736878, accuracy: 0.5166666666666667\n",
      "iteration no 19: Loss: 0.8445486813386018, accuracy: 0.5133333333333333\n",
      "iteration no 20: Loss: 0.8392466830320062, accuracy: 0.5133333333333333\n",
      "iteration no 21: Loss: 0.8343505994052486, accuracy: 0.5133333333333333\n",
      "iteration no 22: Loss: 0.8298219065375685, accuracy: 0.5133333333333333\n",
      "iteration no 23: Loss: 0.82562642954545, accuracy: 0.5133333333333333\n",
      "iteration no 24: Loss: 0.8217337819943848, accuracy: 0.5133333333333333\n",
      "iteration no 25: Loss: 0.8181168849291885, accuracy: 0.5133333333333333\n",
      "iteration no 26: Loss: 0.8147515534670905, accuracy: 0.51\n",
      "iteration no 27: Loss: 0.8116161407728487, accuracy: 0.5133333333333333\n",
      "iteration no 28: Loss: 0.8086912308275438, accuracy: 0.5133333333333333\n",
      "iteration no 29: Loss: 0.8059593727482468, accuracy: 0.5133333333333333\n",
      "iteration no 30: Loss: 0.8034048505488659, accuracy: 0.5133333333333333\n",
      "iteration no 31: Loss: 0.8010134831847185, accuracy: 0.5133333333333333\n",
      "iteration no 32: Loss: 0.7987724505227007, accuracy: 0.5133333333333333\n",
      "iteration no 33: Loss: 0.7966701415496211, accuracy: 0.5166666666666667\n",
      "iteration no 34: Loss: 0.7946960216941009, accuracy: 0.5166666666666667\n",
      "iteration no 35: Loss: 0.7928405166100435, accuracy: 0.5166666666666667\n",
      "iteration no 36: Loss: 0.7910949101668509, accuracy: 0.5166666666666667\n",
      "iteration no 37: Loss: 0.7894512547257422, accuracy: 0.5166666666666667\n",
      "iteration no 38: Loss: 0.7879022920630905, accuracy: 0.5166666666666667\n",
      "iteration no 39: Loss: 0.7864413835392623, accuracy: 0.5166666666666667\n",
      "iteration no 40: Loss: 0.7850624483122395, accuracy: 0.5166666666666667\n",
      "iteration no 41: Loss: 0.7837599085652903, accuracy: 0.5166666666666667\n",
      "iteration no 42: Loss: 0.782528640862138, accuracy: 0.5133333333333333\n",
      "iteration no 43: Loss: 0.7813639328655694, accuracy: 0.5133333333333333\n",
      "iteration no 44: Loss: 0.780261444759714, accuracy: 0.5133333333333333\n",
      "iteration no 45: Loss: 0.7792171748051581, accuracy: 0.5133333333333333\n",
      "iteration no 46: Loss: 0.7782274285320545, accuracy: 0.5133333333333333\n",
      "iteration no 47: Loss: 0.7772887911414545, accuracy: 0.5133333333333333\n",
      "iteration no 48: Loss: 0.7763981027408835, accuracy: 0.5133333333333333\n",
      "iteration no 49: Loss: 0.7755524360881457, accuracy: 0.5133333333333333\n",
      "iteration no 50: Loss: 0.7747490765586272, accuracy: 0.5133333333333333\n",
      "iteration no 51: Loss: 0.7739855040869766, accuracy: 0.5133333333333333\n",
      "iteration no 52: Loss: 0.7732593768648237, accuracy: 0.5133333333333333\n",
      "iteration no 53: Loss: 0.772568516602829, accuracy: 0.5133333333333333\n",
      "iteration no 54: Loss: 0.7719108951884668, accuracy: 0.5133333333333333\n",
      "iteration no 55: Loss: 0.7712846225910206, accuracy: 0.5133333333333333\n",
      "iteration no 56: Loss: 0.7706879358827268, accuracy: 0.5133333333333333\n",
      "iteration no 57: Loss: 0.7701191892602472, accuracy: 0.5133333333333333\n",
      "iteration no 58: Loss: 0.7695768449639362, accuracy: 0.5133333333333333\n",
      "iteration no 59: Loss: 0.7690594650040107, accuracy: 0.5133333333333333\n",
      "iteration no 60: Loss: 0.7685657036129148, accuracy: 0.5133333333333333\n",
      "iteration no 61: Loss: 0.7680943003521187, accuracy: 0.5133333333333333\n",
      "iteration no 62: Loss: 0.7676440738094471, accuracy: 0.5166666666666667\n",
      "iteration no 63: Loss: 0.767213915829954, accuracy: 0.5166666666666667\n",
      "iteration no 64: Loss: 0.7668027862294574, accuracy: 0.5166666666666667\n",
      "iteration no 65: Loss: 0.7664097079452297, accuracy: 0.5166666666666667\n",
      "iteration no 66: Loss: 0.7660337625831063, accuracy: 0.5166666666666667\n",
      "iteration no 67: Loss: 0.7656740863244825, accuracy: 0.5166666666666667\n",
      "iteration no 68: Loss: 0.7653298661604109, accuracy: 0.5166666666666667\n",
      "iteration no 69: Loss: 0.765000336423328, accuracy: 0.5166666666666667\n",
      "iteration no 70: Loss: 0.7646847755898866, accuracy: 0.5166666666666667\n",
      "iteration no 71: Loss: 0.7643825033310018, accuracy: 0.5166666666666667\n",
      "iteration no 72: Loss: 0.7640928777875513, accuracy: 0.5166666666666667\n",
      "iteration no 73: Loss: 0.7638152930522686, accuracy: 0.5166666666666667\n",
      "iteration no 74: Loss: 0.7635491768402264, accuracy: 0.5166666666666667\n",
      "iteration no 75: Loss: 0.7632939883319851, accuracy: 0.5166666666666667\n",
      "iteration no 76: Loss: 0.7630492161749701, accuracy: 0.5166666666666667\n",
      "iteration no 77: Loss: 0.7628143766299914, accuracy: 0.5166666666666667\n",
      "iteration no 78: Loss: 0.7625890118510154, accuracy: 0.5166666666666667\n",
      "iteration no 79: Loss: 0.7623726882873885, accuracy: 0.5166666666666667\n",
      "iteration no 80: Loss: 0.7621649951986832, accuracy: 0.5166666666666667\n",
      "iteration no 81: Loss: 0.7619655432732162, accuracy: 0.5166666666666667\n",
      "iteration no 82: Loss: 0.761773963342078, accuracy: 0.5166666666666667\n",
      "iteration no 83: Loss: 0.7615899051812306, accuracy: 0.5133333333333333\n",
      "iteration no 84: Loss: 0.7614130363948742, accuracy: 0.5133333333333333\n",
      "iteration no 85: Loss: 0.7612430413738693, accuracy: 0.5133333333333333\n",
      "iteration no 86: Loss: 0.7610796203235269, accuracy: 0.5133333333333333\n",
      "iteration no 87: Loss: 0.7609224883555603, accuracy: 0.5133333333333333\n",
      "iteration no 88: Loss: 0.7607713746394306, accuracy: 0.5133333333333333\n",
      "iteration no 89: Loss: 0.7606260216087043, accuracy: 0.5133333333333333\n",
      "iteration no 90: Loss: 0.7604861842184106, accuracy: 0.5133333333333333\n",
      "iteration no 91: Loss: 0.7603516292497041, accuracy: 0.5133333333333333\n",
      "iteration no 92: Loss: 0.7602221346584412, accuracy: 0.5133333333333333\n",
      "iteration no 93: Loss: 0.7600974889645468, accuracy: 0.5133333333333333\n",
      "iteration no 94: Loss: 0.7599774906792954, accuracy: 0.5133333333333333\n",
      "iteration no 95: Loss: 0.7598619477678604, accuracy: 0.5133333333333333\n",
      "iteration no 96: Loss: 0.7597506771446808, accuracy: 0.5133333333333333\n",
      "iteration no 97: Loss: 0.7596435041993983, accuracy: 0.5133333333333333\n",
      "iteration no 98: Loss: 0.7595402623512755, accuracy: 0.5133333333333333\n",
      "iteration no 99: Loss: 0.7594407926301737, accuracy: 0.5133333333333333\n",
      "iteration no 100: Loss: 0.7593449432823118, accuracy: 0.5133333333333333\n",
      "iteration no 101: Loss: 0.7592525693991522, accuracy: 0.5133333333333333\n",
      "iteration no 102: Loss: 0.759163532567896, accuracy: 0.5133333333333333\n",
      "iteration no 103: Loss: 0.7590777005421703, accuracy: 0.5133333333333333\n",
      "iteration no 104: Loss: 0.7589949469315967, accuracy: 0.5166666666666667\n",
      "iteration no 105: Loss: 0.7589151509090253, accuracy: 0.5166666666666667\n",
      "iteration no 106: Loss: 0.7588381969343065, accuracy: 0.5166666666666667\n",
      "iteration no 107: Loss: 0.7587639744935506, accuracy: 0.5166666666666667\n",
      "iteration no 108: Loss: 0.7586923778529007, accuracy: 0.5166666666666667\n",
      "iteration no 109: Loss: 0.7586233058259126, accuracy: 0.5166666666666667\n",
      "iteration no 110: Loss: 0.7585566615536992, accuracy: 0.5166666666666667\n",
      "iteration no 111: Loss: 0.7584923522970508, accuracy: 0.5166666666666667\n",
      "iteration no 112: Loss: 0.7584302892398044, accuracy: 0.5166666666666667\n",
      "iteration no 113: Loss: 0.7583703873027752, accuracy: 0.5166666666666667\n",
      "iteration no 114: Loss: 0.758312564967617, accuracy: 0.5166666666666667\n",
      "iteration no 115: Loss: 0.7582567441100188, accuracy: 0.5166666666666667\n",
      "iteration no 116: Loss: 0.7582028498416802, accuracy: 0.5166666666666667\n",
      "iteration no 117: Loss: 0.758150810360551, accuracy: 0.5166666666666667\n",
      "iteration no 118: Loss: 0.7581005568088509, accuracy: 0.5166666666666667\n",
      "iteration no 119: Loss: 0.7580520231384154, accuracy: 0.5166666666666667\n",
      "iteration no 120: Loss: 0.7580051459829473, accuracy: 0.5166666666666667\n",
      "iteration no 121: Loss: 0.7579598645367762, accuracy: 0.5166666666666667\n",
      "iteration no 122: Loss: 0.7579161204397558, accuracy: 0.5166666666666667\n",
      "iteration no 123: Loss: 0.7578738576679515, accuracy: 0.5166666666666667\n",
      "iteration no 124: Loss: 0.7578330224297936, accuracy: 0.5166666666666667\n",
      "iteration no 125: Loss: 0.7577935630673908, accuracy: 0.5166666666666667\n",
      "iteration no 126: Loss: 0.7577554299627159, accuracy: 0.5166666666666667\n",
      "iteration no 127: Loss: 0.7577185754483995, accuracy: 0.5166666666666667\n",
      "iteration no 128: Loss: 0.7576829537228722, accuracy: 0.5166666666666667\n",
      "iteration no 129: Loss: 0.7576485207696277, accuracy: 0.5166666666666667\n",
      "iteration no 130: Loss: 0.7576152342803757, accuracy: 0.5166666666666667\n",
      "iteration no 131: Loss: 0.7575830535818803, accuracy: 0.5166666666666667\n",
      "iteration no 132: Loss: 0.7575519395662871, accuracy: 0.5166666666666667\n",
      "iteration no 133: Loss: 0.7575218546247491, accuracy: 0.5133333333333333\n",
      "iteration no 134: Loss: 0.7574927625841847, accuracy: 0.5133333333333333\n",
      "iteration no 135: Loss: 0.7574646286469969, accuracy: 0.5133333333333333\n",
      "iteration no 136: Loss: 0.7574374193336026, accuracy: 0.5133333333333333\n",
      "iteration no 137: Loss: 0.7574111024276268, accuracy: 0.5133333333333333\n",
      "iteration no 138: Loss: 0.7573856469236215, accuracy: 0.5133333333333333\n",
      "iteration no 139: Loss: 0.7573610229771842, accuracy: 0.5133333333333333\n",
      "iteration no 140: Loss: 0.7573372018573482, accuracy: 0.5133333333333333\n",
      "iteration no 141: Loss: 0.7573141559011372, accuracy: 0.5133333333333333\n",
      "iteration no 142: Loss: 0.7572918584701663, accuracy: 0.5133333333333333\n",
      "iteration no 143: Loss: 0.7572702839091957, accuracy: 0.5133333333333333\n",
      "iteration no 144: Loss: 0.7572494075065324, accuracy: 0.5133333333333333\n",
      "iteration no 145: Loss: 0.7572292054561941, accuracy: 0.5133333333333333\n",
      "iteration no 146: Loss: 0.7572096548217454, accuracy: 0.5133333333333333\n",
      "iteration no 147: Loss: 0.7571907335017248, accuracy: 0.5133333333333333\n",
      "iteration no 148: Loss: 0.7571724201965863, accuracy: 0.5133333333333333\n",
      "iteration no 149: Loss: 0.7571546943770799, accuracy: 0.5133333333333333\n",
      "iteration no 150: Loss: 0.7571375362540058, accuracy: 0.5133333333333333\n",
      "iteration no 151: Loss: 0.7571209267492707, accuracy: 0.5133333333333333\n",
      "iteration no 152: Loss: 0.7571048474681887, accuracy: 0.5133333333333333\n",
      "iteration no 153: Loss: 0.7570892806729672, accuracy: 0.5133333333333333\n",
      "iteration no 154: Loss: 0.7570742092573189, accuracy: 0.5133333333333333\n",
      "iteration no 155: Loss: 0.7570596167221502, accuracy: 0.5133333333333333\n",
      "iteration no 156: Loss: 0.7570454871522756, accuracy: 0.5133333333333333\n",
      "iteration no 157: Loss: 0.7570318051941068, accuracy: 0.5133333333333333\n",
      "iteration no 158: Loss: 0.7570185560342764, accuracy: 0.5133333333333333\n",
      "iteration no 159: Loss: 0.7570057253791499, accuracy: 0.5133333333333333\n",
      "iteration no 160: Loss: 0.7569932994351861, accuracy: 0.5133333333333333\n",
      "iteration no 161: Loss: 0.7569812648901081, accuracy: 0.5133333333333333\n",
      "iteration no 162: Loss: 0.7569696088948469, accuracy: 0.5133333333333333\n",
      "iteration no 163: Loss: 0.756958319046223, accuracy: 0.5133333333333333\n",
      "iteration no 164: Loss: 0.756947383370334, accuracy: 0.5133333333333333\n",
      "iteration no 165: Loss: 0.7569367903066149, accuracy: 0.5133333333333333\n",
      "iteration no 166: Loss: 0.756926528692543, accuracy: 0.5133333333333333\n",
      "iteration no 167: Loss: 0.7569165877489562, accuracy: 0.5133333333333333\n",
      "iteration no 168: Loss: 0.7569069570659622, accuracy: 0.5133333333333333\n",
      "iteration no 169: Loss: 0.7568976265894062, accuracy: 0.5133333333333333\n",
      "iteration no 170: Loss: 0.7568885866078798, accuracy: 0.5133333333333333\n",
      "iteration no 171: Loss: 0.7568798277402413, accuracy: 0.5133333333333333\n",
      "iteration no 172: Loss: 0.7568713409236301, accuracy: 0.5133333333333333\n",
      "iteration no 173: Loss: 0.7568631174019503, accuracy: 0.5133333333333333\n",
      "iteration no 174: Loss: 0.7568551487148057, accuracy: 0.5133333333333333\n",
      "iteration no 175: Loss: 0.756847426686867, accuracy: 0.5133333333333333\n",
      "iteration no 176: Loss: 0.756839943417649, accuracy: 0.5133333333333333\n",
      "iteration no 177: Loss: 0.7568326912716872, accuracy: 0.5133333333333333\n",
      "iteration no 178: Loss: 0.7568256628690893, accuracy: 0.5133333333333333\n",
      "iteration no 179: Loss: 0.7568188510764532, accuracy: 0.5133333333333333\n",
      "iteration no 180: Loss: 0.7568122489981305, accuracy: 0.5133333333333333\n",
      "iteration no 181: Loss: 0.7568058499678226, accuracy: 0.5133333333333333\n",
      "iteration no 182: Loss: 0.7567996475404987, accuracy: 0.5133333333333333\n",
      "iteration no 183: Loss: 0.7567936354846178, accuracy: 0.5133333333333333\n",
      "iteration no 184: Loss: 0.7567878077746452, accuracy: 0.5133333333333333\n",
      "iteration no 185: Loss: 0.75678215858385, accuracy: 0.5133333333333333\n",
      "iteration no 186: Loss: 0.7567766822773743, accuracy: 0.5133333333333333\n",
      "iteration no 187: Loss: 0.7567713734055582, accuracy: 0.5133333333333333\n",
      "iteration no 188: Loss: 0.7567662266975173, accuracy: 0.5133333333333333\n",
      "iteration no 189: Loss: 0.7567612370549552, accuracy: 0.5133333333333333\n",
      "iteration no 190: Loss: 0.7567563995462078, accuracy: 0.5133333333333333\n",
      "iteration no 191: Loss: 0.7567517094005053, accuracy: 0.5133333333333333\n",
      "iteration no 192: Loss: 0.7567471620024463, accuracy: 0.5133333333333333\n",
      "iteration no 193: Loss: 0.7567427528866745, accuracy: 0.5133333333333333\n",
      "iteration no 194: Loss: 0.756738477732749, accuracy: 0.5133333333333333\n",
      "iteration no 195: Loss: 0.7567343323602034, accuracy: 0.5133333333333333\n",
      "iteration no 196: Loss: 0.7567303127237816, accuracy: 0.5133333333333333\n",
      "iteration no 197: Loss: 0.7567264149088495, accuracy: 0.5133333333333333\n",
      "iteration no 198: Loss: 0.7567226351269705, accuracy: 0.5133333333333333\n",
      "iteration no 199: Loss: 0.7567189697116394, accuracy: 0.5133333333333333\n",
      "iteration no 200: Loss: 0.7567154151141721, accuracy: 0.5133333333333333\n",
      "iteration no 1: Loss: 1.131541696461061, accuracy: 0.33666666666666667\n",
      "iteration no 2: Loss: 1.0972686676274095, accuracy: 0.49\n",
      "iteration no 3: Loss: 1.066809216940402, accuracy: 0.5033333333333333\n",
      "iteration no 4: Loss: 1.0397057114107058, accuracy: 0.5133333333333333\n",
      "iteration no 5: Loss: 1.0155512667590718, accuracy: 0.5066666666666667\n",
      "iteration no 6: Loss: 0.9939857719650588, accuracy: 0.51\n",
      "iteration no 7: Loss: 0.9746923207064464, accuracy: 0.51\n",
      "iteration no 8: Loss: 0.9573934336522165, accuracy: 0.51\n",
      "iteration no 9: Loss: 0.9418470814106814, accuracy: 0.5133333333333333\n",
      "iteration no 10: Loss: 0.9278426896978726, accuracy: 0.5166666666666667\n",
      "iteration no 11: Loss: 0.9151973037756758, accuracy: 0.5133333333333333\n",
      "iteration no 12: Loss: 0.9037520334467737, accuracy: 0.5133333333333333\n",
      "iteration no 13: Loss: 0.8933688424822874, accuracy: 0.5133333333333333\n",
      "iteration no 14: Loss: 0.8839277026375857, accuracy: 0.5133333333333333\n",
      "iteration no 15: Loss: 0.8753241038892529, accuracy: 0.5133333333333333\n",
      "iteration no 16: Loss: 0.8674668962022458, accuracy: 0.5133333333333333\n",
      "iteration no 17: Loss: 0.8602764303192028, accuracy: 0.5133333333333333\n",
      "iteration no 18: Loss: 0.8536829626637294, accuracy: 0.51\n",
      "iteration no 19: Loss: 0.8476252902319387, accuracy: 0.5133333333333333\n",
      "iteration no 20: Loss: 0.8420495838235489, accuracy: 0.5133333333333333\n",
      "iteration no 21: Loss: 0.8369083912011394, accuracy: 0.5133333333333333\n",
      "iteration no 22: Loss: 0.8321597852077087, accuracy: 0.5133333333333333\n",
      "iteration no 23: Loss: 0.8277666352076188, accuracy: 0.5133333333333333\n",
      "iteration no 24: Loss: 0.8236959832861958, accuracy: 0.5133333333333333\n",
      "iteration no 25: Loss: 0.819918509382216, accuracy: 0.5133333333333333\n",
      "iteration no 26: Loss: 0.8164080719214954, accuracy: 0.5133333333333333\n",
      "iteration no 27: Loss: 0.8131413125836796, accuracy: 0.5133333333333333\n",
      "iteration no 28: Loss: 0.8100973155969612, accuracy: 0.5133333333333333\n",
      "iteration no 29: Loss: 0.8072573134511302, accuracy: 0.5133333333333333\n",
      "iteration no 30: Loss: 0.8046044321829989, accuracy: 0.5166666666666667\n",
      "iteration no 31: Loss: 0.8021234704527941, accuracy: 0.5166666666666667\n",
      "iteration no 32: Loss: 0.79980070752539, accuracy: 0.5166666666666667\n",
      "iteration no 33: Loss: 0.7976237360225128, accuracy: 0.5166666666666667\n",
      "iteration no 34: Loss: 0.7955813159439528, accuracy: 0.5166666666666667\n",
      "iteration no 35: Loss: 0.7936632469867754, accuracy: 0.5166666666666667\n",
      "iteration no 36: Loss: 0.7918602566379195, accuracy: 0.5133333333333333\n",
      "iteration no 37: Loss: 0.7901639018912229, accuracy: 0.5133333333333333\n",
      "iteration no 38: Loss: 0.7885664827564032, accuracy: 0.5133333333333333\n",
      "iteration no 39: Loss: 0.7870609659945093, accuracy: 0.5133333333333333\n",
      "iteration no 40: Loss: 0.7856409177399384, accuracy: 0.5133333333333333\n",
      "iteration no 41: Loss: 0.7843004438599882, accuracy: 0.5133333333333333\n",
      "iteration no 42: Loss: 0.7830341370647066, accuracy: 0.5133333333333333\n",
      "iteration no 43: Loss: 0.7818370299171821, accuracy: 0.5133333333333333\n",
      "iteration no 44: Loss: 0.7807045530112633, accuracy: 0.5133333333333333\n",
      "iteration no 45: Loss: 0.7796324976832767, accuracy: 0.5133333333333333\n",
      "iteration no 46: Loss: 0.7786169827093093, accuracy: 0.5133333333333333\n",
      "iteration no 47: Loss: 0.7776544245123336, accuracy: 0.5133333333333333\n",
      "iteration no 48: Loss: 0.7767415104657457, accuracy: 0.5133333333333333\n",
      "iteration no 49: Loss: 0.775875174933361, accuracy: 0.5133333333333333\n",
      "iteration no 50: Loss: 0.7750525777319087, accuracy: 0.5133333333333333\n",
      "iteration no 51: Loss: 0.7742710847416858, accuracy: 0.5133333333333333\n",
      "iteration no 52: Loss: 0.7735282504252408, accuracy: 0.5133333333333333\n",
      "iteration no 53: Loss: 0.7728218020435198, accuracy: 0.5133333333333333\n",
      "iteration no 54: Loss: 0.7721496253845406, accuracy: 0.5133333333333333\n",
      "iteration no 55: Loss: 0.7715097518418756, accuracy: 0.5133333333333333\n",
      "iteration no 56: Loss: 0.7709003466995601, accuracy: 0.5133333333333333\n",
      "iteration no 57: Loss: 0.7703196984968557, accuracy: 0.5133333333333333\n",
      "iteration no 58: Loss: 0.7697662093609832, accuracy: 0.5133333333333333\n",
      "iteration no 59: Loss: 0.769238386208753, accuracy: 0.5133333333333333\n",
      "iteration no 60: Loss: 0.768734832729239, accuracy: 0.5133333333333333\n",
      "iteration no 61: Loss: 0.7682542420694777, accuracy: 0.5166666666666667\n",
      "iteration no 62: Loss: 0.7677953901537975, accuracy: 0.5166666666666667\n",
      "iteration no 63: Loss: 0.7673571295749823, accuracy: 0.5166666666666667\n",
      "iteration no 64: Loss: 0.7669383840021345, accuracy: 0.5166666666666667\n",
      "iteration no 65: Loss: 0.7665381430560204, accuracy: 0.5166666666666667\n",
      "iteration no 66: Loss: 0.7661554576078552, accuracy: 0.5166666666666667\n",
      "iteration no 67: Loss: 0.765789435462107, accuracy: 0.5166666666666667\n",
      "iteration no 68: Loss: 0.7654392373879612, accuracy: 0.5166666666666667\n",
      "iteration no 69: Loss: 0.7651040734677076, accuracy: 0.5166666666666667\n",
      "iteration no 70: Loss: 0.7647831997335124, accuracy: 0.5166666666666667\n",
      "iteration no 71: Loss: 0.7644759150669004, accuracy: 0.5166666666666667\n",
      "iteration no 72: Loss: 0.7641815583378047, accuracy: 0.5166666666666667\n",
      "iteration no 73: Loss: 0.7638995057623122, accuracy: 0.5166666666666667\n",
      "iteration no 74: Loss: 0.7636291684602495, accuracy: 0.5166666666666667\n",
      "iteration no 75: Loss: 0.7633699901955628, accuracy: 0.5166666666666667\n",
      "iteration no 76: Loss: 0.7631214452840662, accuracy: 0.5166666666666667\n",
      "iteration no 77: Loss: 0.7628830366545711, accuracy: 0.5166666666666667\n",
      "iteration no 78: Loss: 0.7626542940507178, accuracy: 0.5166666666666667\n",
      "iteration no 79: Loss: 0.7624347723619954, accuracy: 0.5166666666666667\n",
      "iteration no 80: Loss: 0.762224050073477, accuracy: 0.5133333333333333\n",
      "iteration no 81: Loss: 0.7620217278247517, accuracy: 0.5133333333333333\n",
      "iteration no 82: Loss: 0.7618274270693777, accuracy: 0.5133333333333333\n",
      "iteration no 83: Loss: 0.761640788826949, accuracy: 0.5133333333333333\n",
      "iteration no 84: Loss: 0.7614614725205641, accuracy: 0.5133333333333333\n",
      "iteration no 85: Loss: 0.7612891548931021, accuracy: 0.5133333333333333\n",
      "iteration no 86: Loss: 0.7611235289962898, accuracy: 0.5133333333333333\n",
      "iteration no 87: Loss: 0.7609643032470432, accuracy: 0.5133333333333333\n",
      "iteration no 88: Loss: 0.7608112005460403, accuracy: 0.5133333333333333\n",
      "iteration no 89: Loss: 0.7606639574538987, accuracy: 0.5133333333333333\n",
      "iteration no 90: Loss: 0.7605223234207177, accuracy: 0.5133333333333333\n",
      "iteration no 91: Loss: 0.7603860600650876, accuracy: 0.5133333333333333\n",
      "iteration no 92: Loss: 0.7602549404989896, accuracy: 0.5133333333333333\n",
      "iteration no 93: Loss: 0.7601287486952977, accuracy: 0.5133333333333333\n",
      "iteration no 94: Loss: 0.7600072788948524, accuracy: 0.5133333333333333\n",
      "iteration no 95: Loss: 0.7598903350503213, accuracy: 0.5133333333333333\n",
      "iteration no 96: Loss: 0.7597777303042738, accuracy: 0.5133333333333333\n",
      "iteration no 97: Loss: 0.7596692864991047, accuracy: 0.5133333333333333\n",
      "iteration no 98: Loss: 0.7595648337166171, accuracy: 0.5133333333333333\n",
      "iteration no 99: Loss: 0.7594642098452448, accuracy: 0.5133333333333333\n",
      "iteration no 100: Loss: 0.7593672601730493, accuracy: 0.5133333333333333\n",
      "iteration no 101: Loss: 0.7592738370047648, accuracy: 0.5133333333333333\n",
      "iteration no 102: Loss: 0.7591837993012914, accuracy: 0.5133333333333333\n",
      "iteration no 103: Loss: 0.7590970123401592, accuracy: 0.5133333333333333\n",
      "iteration no 104: Loss: 0.7590133473955943, accuracy: 0.5166666666666667\n",
      "iteration no 105: Loss: 0.758932681436909, accuracy: 0.5166666666666667\n",
      "iteration no 106: Loss: 0.7588548968440432, accuracy: 0.5166666666666667\n",
      "iteration no 107: Loss: 0.758779881139159, accuracy: 0.5166666666666667\n",
      "iteration no 108: Loss: 0.7587075267332717, accuracy: 0.5166666666666667\n",
      "iteration no 109: Loss: 0.7586377306869709, accuracy: 0.5166666666666667\n",
      "iteration no 110: Loss: 0.7585703944843538, accuracy: 0.5166666666666667\n",
      "iteration no 111: Loss: 0.7585054238193507, accuracy: 0.5166666666666667\n",
      "iteration no 112: Loss: 0.7584427283936819, accuracy: 0.5166666666666667\n",
      "iteration no 113: Loss: 0.7583822217257364, accuracy: 0.5166666666666667\n",
      "iteration no 114: Loss: 0.7583238209697083, accuracy: 0.5166666666666667\n",
      "iteration no 115: Loss: 0.758267446744376, accuracy: 0.5166666666666667\n",
      "iteration no 116: Loss: 0.758213022970948, accuracy: 0.5166666666666667\n",
      "iteration no 117: Loss: 0.7581604767194368, accuracy: 0.5166666666666667\n",
      "iteration no 118: Loss: 0.7581097380630594, accuracy: 0.5166666666666667\n",
      "iteration no 119: Loss: 0.7580607399401939, accuracy: 0.5166666666666667\n",
      "iteration no 120: Loss: 0.7580134180234551, accuracy: 0.5166666666666667\n",
      "iteration no 121: Loss: 0.7579677105954765, accuracy: 0.5166666666666667\n",
      "iteration no 122: Loss: 0.7579235584310167, accuracy: 0.5166666666666667\n",
      "iteration no 123: Loss: 0.7578809046850274, accuracy: 0.5166666666666667\n",
      "iteration no 124: Loss: 0.7578396947863495, accuracy: 0.5166666666666667\n",
      "iteration no 125: Loss: 0.7577998763367171, accuracy: 0.5166666666666667\n",
      "iteration no 126: Loss: 0.7577613990147782, accuracy: 0.5166666666666667\n",
      "iteration no 127: Loss: 0.7577242144848497, accuracy: 0.5166666666666667\n",
      "iteration no 128: Loss: 0.7576882763101473, accuracy: 0.5166666666666667\n",
      "iteration no 129: Loss: 0.7576535398702475, accuracy: 0.5166666666666667\n",
      "iteration no 130: Loss: 0.7576199622825481, accuracy: 0.5166666666666667\n",
      "iteration no 131: Loss: 0.7575875023275148, accuracy: 0.5166666666666667\n",
      "iteration no 132: Loss: 0.7575561203775065, accuracy: 0.5166666666666667\n",
      "iteration no 133: Loss: 0.7575257783289924, accuracy: 0.5166666666666667\n",
      "iteration no 134: Loss: 0.7574964395379785, accuracy: 0.5133333333333333\n",
      "iteration no 135: Loss: 0.7574680687584744, accuracy: 0.5133333333333333\n",
      "iteration no 136: Loss: 0.7574406320838419, accuracy: 0.5133333333333333\n",
      "iteration no 137: Loss: 0.7574140968908766, accuracy: 0.5133333333333333\n",
      "iteration no 138: Loss: 0.7573884317864754, accuracy: 0.5133333333333333\n",
      "iteration no 139: Loss: 0.7573636065567658, accuracy: 0.5133333333333333\n",
      "iteration no 140: Loss: 0.7573395921185615, accuracy: 0.5133333333333333\n",
      "iteration no 141: Loss: 0.7573163604730332, accuracy: 0.5133333333333333\n",
      "iteration no 142: Loss: 0.7572938846614784, accuracy: 0.5133333333333333\n",
      "iteration no 143: Loss: 0.7572721387230863, accuracy: 0.5133333333333333\n",
      "iteration no 144: Loss: 0.7572510976545973, accuracy: 0.5133333333333333\n",
      "iteration no 145: Loss: 0.7572307373717643, accuracy: 0.5133333333333333\n",
      "iteration no 146: Loss: 0.7572110346725228, accuracy: 0.5133333333333333\n",
      "iteration no 147: Loss: 0.7571919672017916, accuracy: 0.5133333333333333\n",
      "iteration no 148: Loss: 0.7571735134178192, accuracy: 0.5133333333333333\n",
      "iteration no 149: Loss: 0.7571556525600034, accuracy: 0.5133333333333333\n",
      "iteration no 150: Loss: 0.7571383646181112, accuracy: 0.5133333333333333\n",
      "iteration no 151: Loss: 0.7571216303028335, accuracy: 0.5133333333333333\n",
      "iteration no 152: Loss: 0.7571054310176091, accuracy: 0.5133333333333333\n",
      "iteration no 153: Loss: 0.7570897488316565, accuracy: 0.5133333333333333\n",
      "iteration no 154: Loss: 0.7570745664541612, accuracy: 0.5133333333333333\n",
      "iteration no 155: Loss: 0.7570598672095569, accuracy: 0.5133333333333333\n",
      "iteration no 156: Loss: 0.7570456350138546, accuracy: 0.5133333333333333\n",
      "iteration no 157: Loss: 0.7570318543519697, accuracy: 0.5133333333333333\n",
      "iteration no 158: Loss: 0.7570185102559983, accuracy: 0.5133333333333333\n",
      "iteration no 159: Loss: 0.7570055882844021, accuracy: 0.5133333333333333\n",
      "iteration no 160: Loss: 0.7569930745020576, accuracy: 0.5133333333333333\n",
      "iteration no 161: Loss: 0.7569809554611324, accuracy: 0.5133333333333333\n",
      "iteration no 162: Loss: 0.7569692181827474, accuracy: 0.5133333333333333\n",
      "iteration no 163: Loss: 0.7569578501393942, accuracy: 0.5133333333333333\n",
      "iteration no 164: Loss: 0.7569468392380687, accuracy: 0.5133333333333333\n",
      "iteration no 165: Loss: 0.7569361738040917, accuracy: 0.5133333333333333\n",
      "iteration no 166: Loss: 0.7569258425655867, accuracy: 0.5133333333333333\n",
      "iteration no 167: Loss: 0.7569158346385808, accuracy: 0.5133333333333333\n",
      "iteration no 168: Loss: 0.7569061395127074, accuracy: 0.5133333333333333\n",
      "iteration no 169: Loss: 0.7568967470374794, accuracy: 0.5133333333333333\n",
      "iteration no 170: Loss: 0.7568876474091119, accuracy: 0.5133333333333333\n",
      "iteration no 171: Loss: 0.7568788311578635, accuracy: 0.5133333333333333\n",
      "iteration no 172: Loss: 0.756870289135885, accuracy: 0.5133333333333333\n",
      "iteration no 173: Loss: 0.7568620125055406, accuracy: 0.5133333333333333\n",
      "iteration no 174: Loss: 0.7568539927281916, accuracy: 0.5133333333333333\n",
      "iteration no 175: Loss: 0.7568462215534174, accuracy: 0.5133333333333333\n",
      "iteration no 176: Loss: 0.7568386910086551, accuracy: 0.5133333333333333\n",
      "iteration no 177: Loss: 0.7568313933892441, accuracy: 0.5133333333333333\n",
      "iteration no 178: Loss: 0.7568243212488532, accuracy: 0.5133333333333333\n",
      "iteration no 179: Loss: 0.7568174673902776, accuracy: 0.5133333333333333\n",
      "iteration no 180: Loss: 0.7568108248565886, accuracy: 0.5133333333333333\n",
      "iteration no 181: Loss: 0.7568043869226228, accuracy: 0.5133333333333333\n",
      "iteration no 182: Loss: 0.756798147086795, accuracy: 0.5133333333333333\n",
      "iteration no 183: Loss: 0.7567920990632229, accuracy: 0.5133333333333333\n",
      "iteration no 184: Loss: 0.756786236774148, accuracy: 0.5133333333333333\n",
      "iteration no 185: Loss: 0.756780554342645, accuracy: 0.5133333333333333\n",
      "iteration no 186: Loss: 0.7567750460856039, accuracy: 0.5133333333333333\n",
      "iteration no 187: Loss: 0.7567697065069747, accuracy: 0.5133333333333333\n",
      "iteration no 188: Loss: 0.7567645302912659, accuracy: 0.5133333333333333\n",
      "iteration no 189: Loss: 0.7567595122972836, accuracy: 0.5133333333333333\n",
      "iteration no 190: Loss: 0.7567546475521038, accuracy: 0.5133333333333333\n",
      "iteration no 191: Loss: 0.7567499312452672, accuracy: 0.5133333333333333\n",
      "iteration no 192: Loss: 0.7567453587231886, accuracy: 0.5133333333333333\n",
      "iteration no 193: Loss: 0.7567409254837707, accuracy: 0.5133333333333333\n",
      "iteration no 194: Loss: 0.7567366271712173, accuracy: 0.5133333333333333\n",
      "iteration no 195: Loss: 0.756732459571033, accuracy: 0.5133333333333333\n",
      "iteration no 196: Loss: 0.7567284186052091, accuracy: 0.5133333333333333\n",
      "iteration no 197: Loss: 0.7567245003275792, accuracy: 0.5133333333333333\n",
      "iteration no 198: Loss: 0.756720700919348, accuracy: 0.5133333333333333\n",
      "iteration no 199: Loss: 0.7567170166847783, accuracy: 0.5133333333333333\n",
      "iteration no 200: Loss: 0.7567134440470339, accuracy: 0.5133333333333333\n",
      "iteration no 1: Loss: 1.1203858939168767, accuracy: 0.24333333333333335\n",
      "iteration no 2: Loss: 1.087559363779604, accuracy: 0.43333333333333335\n",
      "iteration no 3: Loss: 1.0583626682103964, accuracy: 0.45666666666666667\n",
      "iteration no 4: Loss: 1.032357874518503, accuracy: 0.4866666666666667\n",
      "iteration no 5: Loss: 1.0091562819440585, accuracy: 0.4866666666666667\n",
      "iteration no 6: Loss: 0.9884150650594532, accuracy: 0.48333333333333334\n",
      "iteration no 7: Loss: 0.9698335451843374, accuracy: 0.49666666666666665\n",
      "iteration no 8: Loss: 0.9531491338310829, accuracy: 0.49666666666666665\n",
      "iteration no 9: Loss: 0.9381331735704989, accuracy: 0.5033333333333333\n",
      "iteration no 10: Loss: 0.9245868912536317, accuracy: 0.5033333333333333\n",
      "iteration no 11: Loss: 0.9123376127057214, accuracy: 0.5066666666666667\n",
      "iteration no 12: Loss: 0.9012353212929938, accuracy: 0.5133333333333333\n",
      "iteration no 13: Loss: 0.8911495918654253, accuracy: 0.5133333333333333\n",
      "iteration no 14: Loss: 0.8819668979792437, accuracy: 0.5133333333333333\n",
      "iteration no 15: Loss: 0.8735882705749358, accuracy: 0.5166666666666667\n",
      "iteration no 16: Loss: 0.8659272762875325, accuracy: 0.5133333333333333\n",
      "iteration no 17: Loss: 0.8589082798382519, accuracy: 0.5133333333333333\n",
      "iteration no 18: Loss: 0.8524649549914493, accuracy: 0.5133333333333333\n",
      "iteration no 19: Loss: 0.8465390106731926, accuracy: 0.5133333333333333\n",
      "iteration no 20: Loss: 0.8410791019651681, accuracy: 0.5133333333333333\n",
      "iteration no 21: Loss: 0.8360398991587079, accuracy: 0.5133333333333333\n",
      "iteration no 22: Loss: 0.831381291503508, accuracy: 0.51\n",
      "iteration no 23: Loss: 0.8270677055136001, accuracy: 0.5133333333333333\n",
      "iteration no 24: Loss: 0.8230675206059836, accuracy: 0.5133333333333333\n",
      "iteration no 25: Loss: 0.819352567414847, accuracy: 0.5133333333333333\n",
      "iteration no 26: Loss: 0.8158976963519589, accuracy: 0.5133333333333333\n",
      "iteration no 27: Loss: 0.8126804058956844, accuracy: 0.5133333333333333\n",
      "iteration no 28: Loss: 0.8096805217195341, accuracy: 0.5133333333333333\n",
      "iteration no 29: Loss: 0.8068799191510101, accuracy: 0.5133333333333333\n",
      "iteration no 30: Loss: 0.8042622826165422, accuracy: 0.5133333333333333\n",
      "iteration no 31: Loss: 0.8018128967097381, accuracy: 0.5133333333333333\n",
      "iteration no 32: Loss: 0.7995184643458163, accuracy: 0.5133333333333333\n",
      "iteration no 33: Loss: 0.7973669481592334, accuracy: 0.5133333333333333\n",
      "iteration no 34: Loss: 0.7953474318850912, accuracy: 0.5133333333333333\n",
      "iteration no 35: Loss: 0.7934499989556937, accuracy: 0.5166666666666667\n",
      "iteration no 36: Loss: 0.7916656259566652, accuracy: 0.5133333333333333\n",
      "iteration no 37: Loss: 0.789986088935017, accuracy: 0.51\n",
      "iteration no 38: Loss: 0.7884038808450476, accuracy: 0.51\n",
      "iteration no 39: Loss: 0.7869121386658392, accuracy: 0.51\n",
      "iteration no 40: Loss: 0.7855045789338081, accuracy: 0.51\n",
      "iteration no 41: Loss: 0.7841754406114059, accuracy: 0.5133333333333333\n",
      "iteration no 42: Loss: 0.7829194343638269, accuracy: 0.5133333333333333\n",
      "iteration no 43: Loss: 0.7817316974437474, accuracy: 0.5133333333333333\n",
      "iteration no 44: Loss: 0.7806077534932603, accuracy: 0.5133333333333333\n",
      "iteration no 45: Loss: 0.7795434766652999, accuracy: 0.5133333333333333\n",
      "iteration no 46: Loss: 0.7785350595464299, accuracy: 0.5133333333333333\n",
      "iteration no 47: Loss: 0.7775789844310385, accuracy: 0.5133333333333333\n",
      "iteration no 48: Loss: 0.7766719975554391, accuracy: 0.5133333333333333\n",
      "iteration no 49: Loss: 0.7758110859506274, accuracy: 0.5133333333333333\n",
      "iteration no 50: Loss: 0.7749934566157157, accuracy: 0.5133333333333333\n",
      "iteration no 51: Loss: 0.7742165177513742, accuracy: 0.5133333333333333\n",
      "iteration no 52: Loss: 0.7734778618248699, accuracy: 0.5133333333333333\n",
      "iteration no 53: Loss: 0.7727752502661921, accuracy: 0.5133333333333333\n",
      "iteration no 54: Loss: 0.7721065996189834, accuracy: 0.5133333333333333\n",
      "iteration no 55: Loss: 0.7714699689910015, accuracy: 0.5133333333333333\n",
      "iteration no 56: Loss: 0.7708635486671519, accuracy: 0.5133333333333333\n",
      "iteration no 57: Loss: 0.7702856497640749, accuracy: 0.5133333333333333\n",
      "iteration no 58: Loss: 0.769734694819198, accuracy: 0.5133333333333333\n",
      "iteration no 59: Loss: 0.7692092092193393, accuracy: 0.5133333333333333\n",
      "iteration no 60: Loss: 0.7687078133846207, accuracy: 0.5133333333333333\n",
      "iteration no 61: Loss: 0.7682292156328022, accuracy: 0.5166666666666667\n",
      "iteration no 62: Loss: 0.7677722056573759, accuracy: 0.5166666666666667\n",
      "iteration no 63: Loss: 0.7673356485599916, accuracy: 0.5166666666666667\n",
      "iteration no 64: Loss: 0.7669184793841618, accuracy: 0.5166666666666667\n",
      "iteration no 65: Loss: 0.7665196981028305, accuracy: 0.5166666666666667\n",
      "iteration no 66: Loss: 0.7661383650173541, accuracy: 0.5166666666666667\n",
      "iteration no 67: Loss: 0.7657735965298564, accuracy: 0.5166666666666667\n",
      "iteration no 68: Loss: 0.7654245612548185, accuracy: 0.5166666666666667\n",
      "iteration no 69: Loss: 0.7650904764392277, accuracy: 0.5166666666666667\n",
      "iteration no 70: Loss: 0.764770604663696, accuracy: 0.5166666666666667\n",
      "iteration no 71: Loss: 0.7644642507996892, accuracy: 0.5166666666666667\n",
      "iteration no 72: Loss: 0.7641707592004592, accuracy: 0.5166666666666667\n",
      "iteration no 73: Loss: 0.7638895111054443, accuracy: 0.5166666666666667\n",
      "iteration no 74: Loss: 0.7636199222398524, accuracy: 0.5166666666666667\n",
      "iteration no 75: Loss: 0.7633614405928811, accuracy: 0.5166666666666667\n",
      "iteration no 76: Loss: 0.7631135443595897, accuracy: 0.5166666666666667\n",
      "iteration no 77: Loss: 0.7628757400328333, accuracy: 0.5166666666666667\n",
      "iteration no 78: Loss: 0.7626475606329257, accuracy: 0.5166666666666667\n",
      "iteration no 79: Loss: 0.762428564063823, accuracy: 0.5166666666666667\n",
      "iteration no 80: Loss: 0.7622183315856398, accuracy: 0.5166666666666667\n",
      "iteration no 81: Loss: 0.7620164663942134, accuracy: 0.5133333333333333\n",
      "iteration no 82: Loss: 0.7618225922992616, accuracy: 0.5133333333333333\n",
      "iteration no 83: Loss: 0.7616363524934258, accuracy: 0.5133333333333333\n",
      "iteration no 84: Loss: 0.7614574084051504, accuracy: 0.5133333333333333\n",
      "iteration no 85: Loss: 0.7612854386289702, accuracy: 0.5133333333333333\n",
      "iteration no 86: Loss: 0.7611201379273178, accuracy: 0.5133333333333333\n",
      "iteration no 87: Loss: 0.7609612162984632, accuracy: 0.5133333333333333\n",
      "iteration no 88: Loss: 0.7608083981056533, accuracy: 0.5133333333333333\n",
      "iteration no 89: Loss: 0.7606614212629207, accuracy: 0.5133333333333333\n",
      "iteration no 90: Loss: 0.7605200364734134, accuracy: 0.5133333333333333\n",
      "iteration no 91: Loss: 0.760384006516426, accuracy: 0.5133333333333333\n",
      "iteration no 92: Loss: 0.7602531055796321, accuracy: 0.5133333333333333\n",
      "iteration no 93: Loss: 0.7601271186332849, accuracy: 0.5133333333333333\n",
      "iteration no 94: Loss: 0.7600058408434234, accuracy: 0.5133333333333333\n",
      "iteration no 95: Loss: 0.7598890770213463, accuracy: 0.5133333333333333\n",
      "iteration no 96: Loss: 0.7597766411068291, accuracy: 0.5133333333333333\n",
      "iteration no 97: Loss: 0.7596683556827623, accuracy: 0.5133333333333333\n",
      "iteration no 98: Loss: 0.7595640515190581, accuracy: 0.5133333333333333\n",
      "iteration no 99: Loss: 0.7594635671438438, accuracy: 0.5133333333333333\n",
      "iteration no 100: Loss: 0.7593667484401034, accuracy: 0.5133333333333333\n",
      "iteration no 101: Loss: 0.7592734482660737, accuracy: 0.5133333333333333\n",
      "iteration no 102: Loss: 0.7591835260978186, accuracy: 0.5133333333333333\n",
      "iteration no 103: Loss: 0.7590968476925298, accuracy: 0.5133333333333333\n",
      "iteration no 104: Loss: 0.7590132847712007, accuracy: 0.5166666666666667\n",
      "iteration no 105: Loss: 0.7589327147194216, accuracy: 0.5166666666666667\n",
      "iteration no 106: Loss: 0.758855020305139, accuracy: 0.5166666666666667\n",
      "iteration no 107: Loss: 0.7587800894122911, accuracy: 0.5166666666666667\n",
      "iteration no 108: Loss: 0.7587078147893263, accuracy: 0.5166666666666667\n",
      "iteration no 109: Loss: 0.7586380938116638, accuracy: 0.5166666666666667\n",
      "iteration no 110: Loss: 0.7585708282572354, accuracy: 0.5166666666666667\n",
      "iteration no 111: Loss: 0.7585059240942963, accuracy: 0.5166666666666667\n",
      "iteration no 112: Loss: 0.7584432912807558, accuracy: 0.5166666666666667\n",
      "iteration no 113: Loss: 0.7583828435743261, accuracy: 0.5166666666666667\n",
      "iteration no 114: Loss: 0.7583244983528338, accuracy: 0.5166666666666667\n",
      "iteration no 115: Loss: 0.7582681764440896, accuracy: 0.5166666666666667\n",
      "iteration no 116: Loss: 0.7582138019647405, accuracy: 0.5166666666666667\n",
      "iteration no 117: Loss: 0.7581613021675793, accuracy: 0.5166666666666667\n",
      "iteration no 118: Loss: 0.7581106072968096, accuracy: 0.5166666666666667\n",
      "iteration no 119: Loss: 0.7580616504508078, accuracy: 0.5166666666666667\n",
      "iteration no 120: Loss: 0.7580143674519422, accuracy: 0.5166666666666667\n",
      "iteration no 121: Loss: 0.7579686967230486, accuracy: 0.5166666666666667\n",
      "iteration no 122: Loss: 0.7579245791701773, accuracy: 0.5166666666666667\n",
      "iteration no 123: Loss: 0.7578819580712596, accuracy: 0.5166666666666667\n",
      "iteration no 124: Loss: 0.7578407789703567, accuracy: 0.5166666666666667\n",
      "iteration no 125: Loss: 0.7578009895771814, accuracy: 0.5166666666666667\n",
      "iteration no 126: Loss: 0.7577625396715963, accuracy: 0.5166666666666667\n",
      "iteration no 127: Loss: 0.7577253810128152, accuracy: 0.5166666666666667\n",
      "iteration no 128: Loss: 0.7576894672530491, accuracy: 0.5166666666666667\n",
      "iteration no 129: Loss: 0.7576547538553534, accuracy: 0.5166666666666667\n",
      "iteration no 130: Loss: 0.7576211980154491, accuracy: 0.5166666666666667\n",
      "iteration no 131: Loss: 0.7575887585873038, accuracy: 0.5166666666666667\n",
      "iteration no 132: Loss: 0.7575573960122693, accuracy: 0.5166666666666667\n",
      "iteration no 133: Loss: 0.7575270722515907, accuracy: 0.5166666666666667\n",
      "iteration no 134: Loss: 0.757497750722103, accuracy: 0.5166666666666667\n",
      "iteration no 135: Loss: 0.757469396234952, accuracy: 0.5166666666666667\n",
      "iteration no 136: Loss: 0.7574419749371791, accuracy: 0.5133333333333333\n",
      "iteration no 137: Loss: 0.7574154542560215, accuracy: 0.5133333333333333\n",
      "iteration no 138: Loss: 0.7573898028457887, accuracy: 0.5133333333333333\n",
      "iteration no 139: Loss: 0.7573649905371794, accuracy: 0.5133333333333333\n",
      "iteration no 140: Loss: 0.7573409882889208, accuracy: 0.5133333333333333\n",
      "iteration no 141: Loss: 0.7573177681416032, accuracy: 0.5133333333333333\n",
      "iteration no 142: Loss: 0.7572953031736094, accuracy: 0.5133333333333333\n",
      "iteration no 143: Loss: 0.7572735674590232, accuracy: 0.5133333333333333\n",
      "iteration no 144: Loss: 0.7572525360274277, accuracy: 0.5133333333333333\n",
      "iteration no 145: Loss: 0.7572321848254923, accuracy: 0.5133333333333333\n",
      "iteration no 146: Loss: 0.7572124906802646, accuracy: 0.5133333333333333\n",
      "iteration no 147: Loss: 0.7571934312640812, accuracy: 0.5133333333333333\n",
      "iteration no 148: Loss: 0.7571749850610197, accuracy: 0.5133333333333333\n",
      "iteration no 149: Loss: 0.7571571313348152, accuracy: 0.5133333333333333\n",
      "iteration no 150: Loss: 0.7571398500981728, accuracy: 0.5133333333333333\n",
      "iteration no 151: Loss: 0.7571231220834077, accuracy: 0.5133333333333333\n",
      "iteration no 152: Loss: 0.7571069287143484, accuracy: 0.5133333333333333\n",
      "iteration no 153: Loss: 0.7570912520794459, accuracy: 0.5133333333333333\n",
      "iteration no 154: Loss: 0.7570760749060296, accuracy: 0.5133333333333333\n",
      "iteration no 155: Loss: 0.7570613805356547, accuracy: 0.5133333333333333\n",
      "iteration no 156: Loss: 0.7570471529004942, accuracy: 0.5133333333333333\n",
      "iteration no 157: Loss: 0.7570333765007222, accuracy: 0.5133333333333333\n",
      "iteration no 158: Loss: 0.7570200363828464, accuracy: 0.5133333333333333\n",
      "iteration no 159: Loss: 0.7570071181189421, accuracy: 0.5133333333333333\n",
      "iteration no 160: Loss: 0.7569946077867504, accuracy: 0.5133333333333333\n",
      "iteration no 161: Loss: 0.7569824919505986, accuracy: 0.5133333333333333\n",
      "iteration no 162: Loss: 0.7569707576431043, accuracy: 0.5133333333333333\n",
      "iteration no 163: Loss: 0.7569593923476324, accuracy: 0.5133333333333333\n",
      "iteration no 164: Loss: 0.7569483839814658, accuracy: 0.5133333333333333\n",
      "iteration no 165: Loss: 0.7569377208796602, accuracy: 0.5133333333333333\n",
      "iteration no 166: Loss: 0.7569273917795539, accuracy: 0.5133333333333333\n",
      "iteration no 167: Loss: 0.7569173858059011, accuracy: 0.5133333333333333\n",
      "iteration no 168: Loss: 0.756907692456601, accuracy: 0.5133333333333333\n",
      "iteration no 169: Loss: 0.7568983015889995, accuracy: 0.5133333333333333\n",
      "iteration no 170: Loss: 0.7568892034067352, accuracy: 0.5133333333333333\n",
      "iteration no 171: Loss: 0.7568803884471075, accuracy: 0.5133333333333333\n",
      "iteration no 172: Loss: 0.756871847568944, accuracy: 0.5133333333333333\n",
      "iteration no 173: Loss: 0.7568635719409452, accuracy: 0.5133333333333333\n",
      "iteration no 174: Loss: 0.756855553030487, accuracy: 0.5133333333333333\n",
      "iteration no 175: Loss: 0.7568477825928585, accuracy: 0.5133333333333333\n",
      "iteration no 176: Loss: 0.7568402526609229, accuracy: 0.5133333333333333\n",
      "iteration no 177: Loss: 0.7568329555351738, accuracy: 0.5133333333333333\n",
      "iteration no 178: Loss: 0.7568258837741806, accuracy: 0.5133333333333333\n",
      "iteration no 179: Loss: 0.7568190301853989, accuracy: 0.5133333333333333\n",
      "iteration no 180: Loss: 0.7568123878163333, accuracy: 0.5133333333333333\n",
      "iteration no 181: Loss: 0.7568059499460409, accuracy: 0.5133333333333333\n",
      "iteration no 182: Loss: 0.7567997100769539, accuracy: 0.5133333333333333\n",
      "iteration no 183: Loss: 0.7567936619270166, accuracy: 0.5133333333333333\n",
      "iteration no 184: Loss: 0.7567877994221176, accuracy: 0.5133333333333333\n",
      "iteration no 185: Loss: 0.7567821166888082, accuracy: 0.5133333333333333\n",
      "iteration no 186: Loss: 0.7567766080472939, accuracy: 0.5133333333333333\n",
      "iteration no 187: Loss: 0.7567712680046882, accuracy: 0.5133333333333333\n",
      "iteration no 188: Loss: 0.7567660912485191, accuracy: 0.5133333333333333\n",
      "iteration no 189: Loss: 0.7567610726404764, accuracy: 0.5133333333333333\n",
      "iteration no 190: Loss: 0.7567562072103906, accuracy: 0.5133333333333333\n",
      "iteration no 191: Loss: 0.7567514901504354, accuracy: 0.5133333333333333\n",
      "iteration no 192: Loss: 0.7567469168095428, accuracy: 0.5133333333333333\n",
      "iteration no 193: Loss: 0.756742482688024, accuracy: 0.5133333333333333\n",
      "iteration no 194: Loss: 0.7567381834323873, accuracy: 0.5133333333333333\n",
      "iteration no 195: Loss: 0.7567340148303442, accuracy: 0.5133333333333333\n",
      "iteration no 196: Loss: 0.7567299728059991, accuracy: 0.5133333333333333\n",
      "iteration no 197: Loss: 0.7567260534152113, accuracy: 0.5133333333333333\n",
      "iteration no 198: Loss: 0.7567222528411274, accuracy: 0.5133333333333333\n",
      "iteration no 199: Loss: 0.7567185673898722, accuracy: 0.5133333333333333\n",
      "iteration no 200: Loss: 0.756714993486397, accuracy: 0.5133333333333333\n",
      "iteration no 1: Loss: 1.1113385940729636, accuracy: 0.54\n",
      "iteration no 2: Loss: 1.0792713594934569, accuracy: 0.5266666666666666\n",
      "iteration no 3: Loss: 1.0507379810887978, accuracy: 0.5366666666666666\n",
      "iteration no 4: Loss: 1.0253165207163963, accuracy: 0.52\n",
      "iteration no 5: Loss: 1.0026319428211066, accuracy: 0.52\n",
      "iteration no 6: Loss: 0.982352533556372, accuracy: 0.5166666666666667\n",
      "iteration no 7: Loss: 0.9641864669674479, accuracy: 0.52\n",
      "iteration no 8: Loss: 0.9478781734058489, accuracy: 0.5133333333333333\n",
      "iteration no 9: Loss: 0.9332045818804714, accuracy: 0.51\n",
      "iteration no 10: Loss: 0.919971404215045, accuracy: 0.51\n",
      "iteration no 11: Loss: 0.9080096067237732, accuracy: 0.5133333333333333\n",
      "iteration no 12: Loss: 0.8971721631166447, accuracy: 0.51\n",
      "iteration no 13: Loss: 0.8873311340622803, accuracy: 0.51\n",
      "iteration no 14: Loss: 0.8783750835575974, accuracy: 0.5133333333333333\n",
      "iteration no 15: Loss: 0.8702068198526618, accuracy: 0.51\n",
      "iteration no 16: Loss: 0.862741436217137, accuracy: 0.51\n",
      "iteration no 17: Loss: 0.8559046211686618, accuracy: 0.51\n",
      "iteration no 18: Loss: 0.8496312063901752, accuracy: 0.51\n",
      "iteration no 19: Loss: 0.8438639216518082, accuracy: 0.51\n",
      "iteration no 20: Loss: 0.8385523284418198, accuracy: 0.51\n",
      "iteration no 21: Loss: 0.8336519069619013, accuracy: 0.51\n",
      "iteration no 22: Loss: 0.8291232742170889, accuracy: 0.5133333333333333\n",
      "iteration no 23: Loss: 0.8249315138854787, accuracy: 0.5133333333333333\n",
      "iteration no 24: Loss: 0.8210456013648265, accuracy: 0.5133333333333333\n",
      "iteration no 25: Loss: 0.8174379098113188, accuracy: 0.5133333333333333\n",
      "iteration no 26: Loss: 0.8140837851017223, accuracy: 0.5133333333333333\n",
      "iteration no 27: Loss: 0.8109611794778353, accuracy: 0.5133333333333333\n",
      "iteration no 28: Loss: 0.8080503351969817, accuracy: 0.5133333333333333\n",
      "iteration no 29: Loss: 0.80533351084382, accuracy: 0.5133333333333333\n",
      "iteration no 30: Loss: 0.8027947440869668, accuracy: 0.5166666666666667\n",
      "iteration no 31: Loss: 0.8004196456172742, accuracy: 0.5166666666666667\n",
      "iteration no 32: Loss: 0.7981952198087259, accuracy: 0.5166666666666667\n",
      "iteration no 33: Loss: 0.7961097083205549, accuracy: 0.5166666666666667\n",
      "iteration no 34: Loss: 0.7941524534300423, accuracy: 0.5133333333333333\n",
      "iteration no 35: Loss: 0.7923137783664166, accuracy: 0.5133333333333333\n",
      "iteration no 36: Loss: 0.7905848823217162, accuracy: 0.5133333333333333\n",
      "iteration no 37: Loss: 0.788957748156512, accuracy: 0.5133333333333333\n",
      "iteration no 38: Loss: 0.7874250611072563, accuracy: 0.5133333333333333\n",
      "iteration no 39: Loss: 0.7859801370462614, accuracy: 0.5133333333333333\n",
      "iteration no 40: Loss: 0.7846168590521138, accuracy: 0.5133333333333333\n",
      "iteration no 41: Loss: 0.7833296212236799, accuracy: 0.5133333333333333\n",
      "iteration no 42: Loss: 0.7821132788197803, accuracy: 0.5133333333333333\n",
      "iteration no 43: Loss: 0.7809631039332837, accuracy: 0.5133333333333333\n",
      "iteration no 44: Loss: 0.7798747460163122, accuracy: 0.5133333333333333\n",
      "iteration no 45: Loss: 0.7788441966653745, accuracy: 0.5133333333333333\n",
      "iteration no 46: Loss: 0.7778677581540113, accuracy: 0.5133333333333333\n",
      "iteration no 47: Loss: 0.7769420152680094, accuracy: 0.5133333333333333\n",
      "iteration no 48: Loss: 0.7760638100561269, accuracy: 0.5133333333333333\n",
      "iteration no 49: Loss: 0.7752302191590261, accuracy: 0.5133333333333333\n",
      "iteration no 50: Loss: 0.7744385334219607, accuracy: 0.5133333333333333\n",
      "iteration no 51: Loss: 0.773686239533717, accuracy: 0.5133333333333333\n",
      "iteration no 52: Loss: 0.7729710034662358, accuracy: 0.5133333333333333\n",
      "iteration no 53: Loss: 0.7722906555169958, accuracy: 0.5133333333333333\n",
      "iteration no 54: Loss: 0.7716431767801832, accuracy: 0.5133333333333333\n",
      "iteration no 55: Loss: 0.7710266868935086, accuracy: 0.5133333333333333\n",
      "iteration no 56: Loss: 0.7704394329256212, accuracy: 0.5166666666666667\n",
      "iteration no 57: Loss: 0.7698797792848636, accuracy: 0.5166666666666667\n",
      "iteration no 58: Loss: 0.7693461985438721, accuracy: 0.5166666666666667\n",
      "iteration no 59: Loss: 0.7688372630865772, accuracy: 0.5166666666666667\n",
      "iteration no 60: Loss: 0.7683516374946935, accuracy: 0.5166666666666667\n",
      "iteration no 61: Loss: 0.7678880716000427, accuracy: 0.5166666666666667\n",
      "iteration no 62: Loss: 0.7674453941371657, accuracy: 0.5166666666666667\n",
      "iteration no 63: Loss: 0.7670225069378299, accuracy: 0.5166666666666667\n",
      "iteration no 64: Loss: 0.7666183796153284, accuracy: 0.5166666666666667\n",
      "iteration no 65: Loss: 0.7662320446920192, accuracy: 0.5166666666666667\n",
      "iteration no 66: Loss: 0.7658625931284588, accuracy: 0.5166666666666667\n",
      "iteration no 67: Loss: 0.7655091702168265, accuracy: 0.5166666666666667\n",
      "iteration no 68: Loss: 0.7651709718051731, accuracy: 0.5166666666666667\n",
      "iteration no 69: Loss: 0.764847240822452, accuracy: 0.5166666666666667\n",
      "iteration no 70: Loss: 0.7645372640773054, accuracy: 0.5166666666666667\n",
      "iteration no 71: Loss: 0.7642403693062889, accuracy: 0.5166666666666667\n",
      "iteration no 72: Loss: 0.7639559224496066, accuracy: 0.5166666666666667\n",
      "iteration no 73: Loss: 0.763683325134576, accuracy: 0.5166666666666667\n",
      "iteration no 74: Loss: 0.7634220123489542, accuracy: 0.5166666666666667\n",
      "iteration no 75: Loss: 0.7631714502879628, accuracy: 0.5166666666666667\n",
      "iteration no 76: Loss: 0.7629311343603788, accuracy: 0.5166666666666667\n",
      "iteration no 77: Loss: 0.7627005873404333, accuracy: 0.5166666666666667\n",
      "iteration no 78: Loss: 0.7624793576534843, accuracy: 0.5133333333333333\n",
      "iteration no 79: Loss: 0.7622670177845398, accuracy: 0.5133333333333333\n",
      "iteration no 80: Loss: 0.7620631627996948, accuracy: 0.5133333333333333\n",
      "iteration no 81: Loss: 0.7618674089714446, accuracy: 0.5133333333333333\n",
      "iteration no 82: Loss: 0.7616793924996381, accuracy: 0.5133333333333333\n",
      "iteration no 83: Loss: 0.7614987683205688, accuracy: 0.5133333333333333\n",
      "iteration no 84: Loss: 0.7613252089973429, accuracy: 0.5133333333333333\n",
      "iteration no 85: Loss: 0.7611584036852758, accuracy: 0.5133333333333333\n",
      "iteration no 86: Loss: 0.7609980571665889, accuracy: 0.5133333333333333\n",
      "iteration no 87: Loss: 0.7608438889491729, accuracy: 0.5133333333333333\n",
      "iteration no 88: Loss: 0.7606956324246228, accuracy: 0.5133333333333333\n",
      "iteration no 89: Loss: 0.7605530340811465, accuracy: 0.5133333333333333\n",
      "iteration no 90: Loss: 0.7604158527673182, accuracy: 0.5133333333333333\n",
      "iteration no 91: Loss: 0.7602838590029711, accuracy: 0.5133333333333333\n",
      "iteration no 92: Loss: 0.7601568343338276, accuracy: 0.5133333333333333\n",
      "iteration no 93: Loss: 0.7600345707267401, accuracy: 0.5133333333333333\n",
      "iteration no 94: Loss: 0.7599168700026617, accuracy: 0.5133333333333333\n",
      "iteration no 95: Loss: 0.7598035433046935, accuracy: 0.5133333333333333\n",
      "iteration no 96: Loss: 0.7596944105987676, accuracy: 0.5133333333333333\n",
      "iteration no 97: Loss: 0.7595893002047077, accuracy: 0.5133333333333333\n",
      "iteration no 98: Loss: 0.7594880483555881, accuracy: 0.5133333333333333\n",
      "iteration no 99: Loss: 0.7593904987834676, accuracy: 0.5133333333333333\n",
      "iteration no 100: Loss: 0.759296502329723, accuracy: 0.5133333333333333\n",
      "iteration no 101: Loss: 0.7592059165783362, accuracy: 0.5133333333333333\n",
      "iteration no 102: Loss: 0.7591186055106163, accuracy: 0.5166666666666667\n",
      "iteration no 103: Loss: 0.7590344391799452, accuracy: 0.5166666666666667\n",
      "iteration no 104: Loss: 0.7589532934052416, accuracy: 0.5166666666666667\n",
      "iteration no 105: Loss: 0.7588750494819346, accuracy: 0.5166666666666667\n",
      "iteration no 106: Loss: 0.7587995939093183, accuracy: 0.5166666666666667\n",
      "iteration no 107: Loss: 0.7587268181332482, accuracy: 0.5166666666666667\n",
      "iteration no 108: Loss: 0.7586566183032074, accuracy: 0.5166666666666667\n",
      "iteration no 109: Loss: 0.7585888950428435, accuracy: 0.5166666666666667\n",
      "iteration no 110: Loss: 0.7585235532331336, accuracy: 0.5166666666666667\n",
      "iteration no 111: Loss: 0.758460501807402, accuracy: 0.5166666666666667\n",
      "iteration no 112: Loss: 0.7583996535574604, accuracy: 0.5166666666666667\n",
      "iteration no 113: Loss: 0.7583409249501954, accuracy: 0.5166666666666667\n",
      "iteration no 114: Loss: 0.7582842359539701, accuracy: 0.5166666666666667\n",
      "iteration no 115: Loss: 0.7582295098742523, accuracy: 0.5166666666666667\n",
      "iteration no 116: Loss: 0.7581766731979209, accuracy: 0.5166666666666667\n",
      "iteration no 117: Loss: 0.7581256554457331, accuracy: 0.5166666666666667\n",
      "iteration no 118: Loss: 0.7580763890324786, accuracy: 0.5166666666666667\n",
      "iteration no 119: Loss: 0.7580288091343677, accuracy: 0.5166666666666667\n",
      "iteration no 120: Loss: 0.7579828535632364, accuracy: 0.5166666666666667\n",
      "iteration no 121: Loss: 0.757938462647178, accuracy: 0.5166666666666667\n",
      "iteration no 122: Loss: 0.7578955791172309, accuracy: 0.5166666666666667\n",
      "iteration no 123: Loss: 0.7578541479997798, accuracy: 0.5166666666666667\n",
      "iteration no 124: Loss: 0.75781411651435, accuracy: 0.5166666666666667\n",
      "iteration no 125: Loss: 0.7577754339764915, accuracy: 0.5166666666666667\n",
      "iteration no 126: Loss: 0.7577380517054687, accuracy: 0.5166666666666667\n",
      "iteration no 127: Loss: 0.7577019229364931, accuracy: 0.5166666666666667\n",
      "iteration no 128: Loss: 0.7576670027372446, accuracy: 0.5166666666666667\n",
      "iteration no 129: Loss: 0.7576332479284548, accuracy: 0.5166666666666667\n",
      "iteration no 130: Loss: 0.7576006170083223, accuracy: 0.5166666666666667\n",
      "iteration no 131: Loss: 0.7575690700805638, accuracy: 0.5166666666666667\n",
      "iteration no 132: Loss: 0.7575385687858994, accuracy: 0.5166666666666667\n",
      "iteration no 133: Loss: 0.7575090762367901, accuracy: 0.5133333333333333\n",
      "iteration no 134: Loss: 0.757480556955259, accuracy: 0.5133333333333333\n",
      "iteration no 135: Loss: 0.7574529768136306, accuracy: 0.5133333333333333\n",
      "iteration no 136: Loss: 0.7574263029780378, accuracy: 0.5133333333333333\n",
      "iteration no 137: Loss: 0.7574005038545506, accuracy: 0.5133333333333333\n",
      "iteration no 138: Loss: 0.757375549037795, accuracy: 0.5133333333333333\n",
      "iteration no 139: Loss: 0.7573514092619283, accuracy: 0.5133333333333333\n",
      "iteration no 140: Loss: 0.7573280563538585, accuracy: 0.5133333333333333\n",
      "iteration no 141: Loss: 0.7573054631885849, accuracy: 0.5133333333333333\n",
      "iteration no 142: Loss: 0.7572836036465611, accuracy: 0.5133333333333333\n",
      "iteration no 143: Loss: 0.7572624525729738, accuracy: 0.5133333333333333\n",
      "iteration no 144: Loss: 0.7572419857388429, accuracy: 0.5133333333333333\n",
      "iteration no 145: Loss: 0.7572221798038543, accuracy: 0.5133333333333333\n",
      "iteration no 146: Loss: 0.7572030122808379, accuracy: 0.5133333333333333\n",
      "iteration no 147: Loss: 0.7571844615018107, accuracy: 0.5133333333333333\n",
      "iteration no 148: Loss: 0.7571665065855098, accuracy: 0.5133333333333333\n",
      "iteration no 149: Loss: 0.7571491274063399, accuracy: 0.5133333333333333\n",
      "iteration no 150: Loss: 0.7571323045646722, accuracy: 0.5133333333333333\n",
      "iteration no 151: Loss: 0.7571160193584234, accuracy: 0.5133333333333333\n",
      "iteration no 152: Loss: 0.7571002537558598, accuracy: 0.5133333333333333\n",
      "iteration no 153: Loss: 0.7570849903695627, accuracy: 0.5133333333333333\n",
      "iteration no 154: Loss: 0.7570702124315062, accuracy: 0.5133333333333333\n",
      "iteration no 155: Loss: 0.7570559037691894, accuracy: 0.5133333333333333\n",
      "iteration no 156: Loss: 0.7570420487827765, accuracy: 0.5133333333333333\n",
      "iteration no 157: Loss: 0.7570286324231985, accuracy: 0.5133333333333333\n",
      "iteration no 158: Loss: 0.75701564017117, accuracy: 0.5133333333333333\n",
      "iteration no 159: Loss: 0.7570030580170811, accuracy: 0.5133333333333333\n",
      "iteration no 160: Loss: 0.7569908724417215, accuracy: 0.5133333333333333\n",
      "iteration no 161: Loss: 0.7569790703978019, accuracy: 0.5133333333333333\n",
      "iteration no 162: Loss: 0.7569676392922335, accuracy: 0.5133333333333333\n",
      "iteration no 163: Loss: 0.756956566969135, accuracy: 0.5133333333333333\n",
      "iteration no 164: Loss: 0.7569458416935299, accuracy: 0.5133333333333333\n",
      "iteration no 165: Loss: 0.7569354521357066, accuracy: 0.5133333333333333\n",
      "iteration no 166: Loss: 0.7569253873562114, accuracy: 0.5133333333333333\n",
      "iteration no 167: Loss: 0.7569156367914442, accuracy: 0.5133333333333333\n",
      "iteration no 168: Loss: 0.7569061902398329, accuracy: 0.5133333333333333\n",
      "iteration no 169: Loss: 0.7568970378485586, accuracy: 0.5133333333333333\n",
      "iteration no 170: Loss: 0.7568881701008106, accuracy: 0.5133333333333333\n",
      "iteration no 171: Loss: 0.7568795778035444, accuracy: 0.5133333333333333\n",
      "iteration no 172: Loss: 0.7568712520757249, accuracy: 0.5133333333333333\n",
      "iteration no 173: Loss: 0.7568631843370287, accuracy: 0.5133333333333333\n",
      "iteration no 174: Loss: 0.7568553662969935, accuracy: 0.5133333333333333\n",
      "iteration no 175: Loss: 0.7568477899445853, accuracy: 0.5133333333333333\n",
      "iteration no 176: Loss: 0.7568404475381758, accuracy: 0.5133333333333333\n",
      "iteration no 177: Loss: 0.7568333315959056, accuracy: 0.5133333333333333\n",
      "iteration no 178: Loss: 0.756826434886419, accuracy: 0.5133333333333333\n",
      "iteration no 179: Loss: 0.756819750419957, accuracy: 0.5133333333333333\n",
      "iteration no 180: Loss: 0.756813271439789, accuracy: 0.5133333333333333\n",
      "iteration no 181: Loss: 0.7568069914139736, accuracy: 0.5133333333333333\n",
      "iteration no 182: Loss: 0.7568009040274314, accuracy: 0.5133333333333333\n",
      "iteration no 183: Loss: 0.7567950031743197, accuracy: 0.5133333333333333\n",
      "iteration no 184: Loss: 0.7567892829506931, accuracy: 0.5133333333333333\n",
      "iteration no 185: Loss: 0.7567837376474434, accuracy: 0.5133333333333333\n",
      "iteration no 186: Loss: 0.7567783617435023, accuracy: 0.5133333333333333\n",
      "iteration no 187: Loss: 0.7567731498992972, accuracy: 0.5133333333333333\n",
      "iteration no 188: Loss: 0.7567680969504552, accuracy: 0.5133333333333333\n",
      "iteration no 189: Loss: 0.7567631979017367, accuracy: 0.5133333333333333\n",
      "iteration no 190: Loss: 0.7567584479211954, accuracy: 0.5133333333333333\n",
      "iteration no 191: Loss: 0.7567538423345549, accuracy: 0.5133333333333333\n",
      "iteration no 192: Loss: 0.7567493766197896, accuracy: 0.5133333333333333\n",
      "iteration no 193: Loss: 0.7567450464019081, accuracy: 0.5133333333333333\n",
      "iteration no 194: Loss: 0.7567408474479231, accuracy: 0.5133333333333333\n",
      "iteration no 195: Loss: 0.7567367756620087, accuracy: 0.5133333333333333\n",
      "iteration no 196: Loss: 0.7567328270808311, accuracy: 0.5133333333333333\n",
      "iteration no 197: Loss: 0.7567289978690498, accuracy: 0.5133333333333333\n",
      "iteration no 198: Loss: 0.756725284314981, accuracy: 0.5133333333333333\n",
      "iteration no 199: Loss: 0.7567216828264168, accuracy: 0.5133333333333333\n",
      "iteration no 200: Loss: 0.7567181899265965, accuracy: 0.5133333333333333\n",
      "iteration no 1: Loss: 1.1330563806610559, accuracy: 0.27\n",
      "iteration no 2: Loss: 1.098476042147715, accuracy: 0.4766666666666667\n",
      "iteration no 3: Loss: 1.0677568182243247, accuracy: 0.5033333333333333\n",
      "iteration no 4: Loss: 1.040435979925674, accuracy: 0.5\n",
      "iteration no 5: Loss: 1.0161015778178182, accuracy: 0.5033333333333333\n",
      "iteration no 6: Loss: 0.9943884562983821, accuracy: 0.51\n",
      "iteration no 7: Loss: 0.9749748760477114, accuracy: 0.52\n",
      "iteration no 8: Loss: 0.9575789167002888, accuracy: 0.5233333333333333\n",
      "iteration no 9: Loss: 0.9419546084958196, accuracy: 0.5233333333333333\n",
      "iteration no 10: Loss: 0.9278879749801979, accuracy: 0.5233333333333333\n",
      "iteration no 11: Loss: 0.9151931835090369, accuracy: 0.52\n",
      "iteration no 12: Loss: 0.903708945873203, accuracy: 0.5166666666666667\n",
      "iteration no 13: Loss: 0.8932952492750332, accuracy: 0.5166666666666667\n",
      "iteration no 14: Loss: 0.8838304486322422, accuracy: 0.5166666666666667\n",
      "iteration no 15: Loss: 0.8752087181264315, accuracy: 0.5166666666666667\n",
      "iteration no 16: Loss: 0.8673378404141561, accuracy: 0.5133333333333333\n",
      "iteration no 17: Loss: 0.8601373021036121, accuracy: 0.5166666666666667\n",
      "iteration no 18: Loss: 0.8535366605581209, accuracy: 0.5166666666666667\n",
      "iteration no 19: Loss: 0.8474741472909606, accuracy: 0.5166666666666667\n",
      "iteration no 20: Loss: 0.8418954754562092, accuracy: 0.5166666666666667\n",
      "iteration no 21: Loss: 0.8367528221336779, accuracy: 0.5166666666666667\n",
      "iteration no 22: Loss: 0.8320039596026587, accuracy: 0.5166666666666667\n",
      "iteration no 23: Loss: 0.8276115132324637, accuracy: 0.5166666666666667\n",
      "iteration no 24: Loss: 0.8235423267982799, accuracy: 0.5166666666666667\n",
      "iteration no 25: Loss: 0.8197669188761897, accuracy: 0.5166666666666667\n",
      "iteration no 26: Loss: 0.8162590164602387, accuracy: 0.5133333333333333\n",
      "iteration no 27: Loss: 0.8129951540896421, accuracy: 0.5133333333333333\n",
      "iteration no 28: Loss: 0.8099543286045829, accuracy: 0.5133333333333333\n",
      "iteration no 29: Loss: 0.8071177012001185, accuracy: 0.5166666666666667\n",
      "iteration no 30: Loss: 0.8044683397561051, accuracy: 0.5166666666666667\n",
      "iteration no 31: Loss: 0.8019909955215082, accuracy: 0.5166666666666667\n",
      "iteration no 32: Loss: 0.7996719091553576, accuracy: 0.5166666666666667\n",
      "iteration no 33: Loss: 0.7974986419016225, accuracy: 0.5133333333333333\n",
      "iteration no 34: Loss: 0.7954599283252447, accuracy: 0.5133333333333333\n",
      "iteration no 35: Loss: 0.7935455475818337, accuracy: 0.5166666666666667\n",
      "iteration no 36: Loss: 0.7917462106512739, accuracy: 0.5166666666666667\n",
      "iteration no 37: Loss: 0.7900534613501432, accuracy: 0.5166666666666667\n",
      "iteration no 38: Loss: 0.7884595892614861, accuracy: 0.5166666666666667\n",
      "iteration no 39: Loss: 0.7869575529931486, accuracy: 0.5166666666666667\n",
      "iteration no 40: Loss: 0.7855409124059959, accuracy: 0.5166666666666667\n",
      "iteration no 41: Loss: 0.7842037686478108, accuracy: 0.5166666666666667\n",
      "iteration no 42: Loss: 0.7829407109933693, accuracy: 0.5166666666666667\n",
      "iteration no 43: Loss: 0.7817467696308671, accuracy: 0.5166666666666667\n",
      "iteration no 44: Loss: 0.7806173736535927, accuracy: 0.5166666666666667\n",
      "iteration no 45: Loss: 0.7795483136168047, accuracy: 0.5166666666666667\n",
      "iteration no 46: Loss: 0.7785357081059885, accuracy: 0.5166666666666667\n",
      "iteration no 47: Loss: 0.7775759738363481, accuracy: 0.5166666666666667\n",
      "iteration no 48: Loss: 0.7766657988664728, accuracy: 0.5133333333333333\n",
      "iteration no 49: Loss: 0.775802118563241, accuracy: 0.5133333333333333\n",
      "iteration no 50: Loss: 0.7749820940015388, accuracy: 0.5133333333333333\n",
      "iteration no 51: Loss: 0.7742030925224225, accuracy: 0.5133333333333333\n",
      "iteration no 52: Loss: 0.773462670207904, accuracy: 0.5133333333333333\n",
      "iteration no 53: Loss: 0.7727585560604028, accuracy: 0.5133333333333333\n",
      "iteration no 54: Loss: 0.7720886377007609, accuracy: 0.5133333333333333\n",
      "iteration no 55: Loss: 0.771450948421139, accuracy: 0.5133333333333333\n",
      "iteration no 56: Loss: 0.7708436554485971, accuracy: 0.5133333333333333\n",
      "iteration no 57: Loss: 0.7702650492921188, accuracy: 0.5133333333333333\n",
      "iteration no 58: Loss: 0.769713534060626, accuracy: 0.5133333333333333\n",
      "iteration no 59: Loss: 0.7691876186524358, accuracy: 0.5133333333333333\n",
      "iteration no 60: Loss: 0.7686859087279115, accuracy: 0.5133333333333333\n",
      "iteration no 61: Loss: 0.7682070993869476, accuracy: 0.5133333333333333\n",
      "iteration no 62: Loss: 0.7677499684816201, accuracy: 0.5133333333333333\n",
      "iteration no 63: Loss: 0.7673133705019545, accuracy: 0.5166666666666667\n",
      "iteration no 64: Loss: 0.7668962309794924, accuracy: 0.5166666666666667\n",
      "iteration no 65: Loss: 0.7664975413592517, accuracy: 0.5166666666666667\n",
      "iteration no 66: Loss: 0.7661163542959062, accuracy: 0.5166666666666667\n",
      "iteration no 67: Loss: 0.7657517793346356, accuracy: 0.5166666666666667\n",
      "iteration no 68: Loss: 0.7654029789411858, accuracy: 0.5166666666666667\n",
      "iteration no 69: Loss: 0.7650691648493111, accuracy: 0.5166666666666667\n",
      "iteration no 70: Loss: 0.7647495946969886, accuracy: 0.5166666666666667\n",
      "iteration no 71: Loss: 0.7644435689256662, accuracy: 0.5166666666666667\n",
      "iteration no 72: Loss: 0.7641504279193458, accuracy: 0.5166666666666667\n",
      "iteration no 73: Loss: 0.763869549362584, accuracy: 0.5166666666666667\n",
      "iteration no 74: Loss: 0.7636003457985177, accuracy: 0.5166666666666667\n",
      "iteration no 75: Loss: 0.7633422623698342, accuracy: 0.5166666666666667\n",
      "iteration no 76: Loss: 0.7630947747272295, accuracy: 0.5166666666666667\n",
      "iteration no 77: Loss: 0.7628573870913475, accuracy: 0.5166666666666667\n",
      "iteration no 78: Loss: 0.7626296304554989, accuracy: 0.5166666666666667\n",
      "iteration no 79: Loss: 0.762411060917625, accuracy: 0.5166666666666667\n",
      "iteration no 80: Loss: 0.7622012581310268, accuracy: 0.5133333333333333\n",
      "iteration no 81: Loss: 0.7619998238643195, accuracy: 0.5133333333333333\n",
      "iteration no 82: Loss: 0.7618063806619342, accuracy: 0.5133333333333333\n",
      "iteration no 83: Loss: 0.7616205705972423, accuracy: 0.5133333333333333\n",
      "iteration no 84: Loss: 0.761442054111092, accuracy: 0.5133333333333333\n",
      "iteration no 85: Loss: 0.76127050892915, accuracy: 0.5133333333333333\n",
      "iteration no 86: Loss: 0.7611056290520283, accuracy: 0.5133333333333333\n",
      "iteration no 87: Loss: 0.7609471238126765, accuracy: 0.5133333333333333\n",
      "iteration no 88: Loss: 0.7607947169959921, accuracy: 0.5133333333333333\n",
      "iteration no 89: Loss: 0.76064814601602, accuracy: 0.5133333333333333\n",
      "iteration no 90: Loss: 0.7605071611464956, accuracy: 0.5133333333333333\n",
      "iteration no 91: Loss: 0.7603715248008374, accuracy: 0.5133333333333333\n",
      "iteration no 92: Loss: 0.7602410108580075, accuracy: 0.5133333333333333\n",
      "iteration no 93: Loss: 0.7601154040309505, accuracy: 0.5133333333333333\n",
      "iteration no 94: Loss: 0.759994499274581, accuracy: 0.5133333333333333\n",
      "iteration no 95: Loss: 0.759878101230533, accuracy: 0.5133333333333333\n",
      "iteration no 96: Loss: 0.7597660237061007, accuracy: 0.5133333333333333\n",
      "iteration no 97: Loss: 0.7596580891850007, accuracy: 0.5133333333333333\n",
      "iteration no 98: Loss: 0.7595541283677694, accuracy: 0.5133333333333333\n",
      "iteration no 99: Loss: 0.759453979739777, accuracy: 0.5133333333333333\n",
      "iteration no 100: Loss: 0.7593574891649888, accuracy: 0.5133333333333333\n",
      "iteration no 101: Loss: 0.759264509503751, accuracy: 0.5133333333333333\n",
      "iteration no 102: Loss: 0.759174900253002, accuracy: 0.5133333333333333\n",
      "iteration no 103: Loss: 0.759088527207431, accuracy: 0.5133333333333333\n",
      "iteration no 104: Loss: 0.7590052621402116, accuracy: 0.5133333333333333\n",
      "iteration no 105: Loss: 0.758924982502043, accuracy: 0.5166666666666667\n",
      "iteration no 106: Loss: 0.7588475711373156, accuracy: 0.5166666666666667\n",
      "iteration no 107: Loss: 0.7587729160163118, accuracy: 0.5166666666666667\n",
      "iteration no 108: Loss: 0.758700909982418, accuracy: 0.5166666666666667\n",
      "iteration no 109: Loss: 0.7586314505134116, accuracy: 0.5166666666666667\n",
      "iteration no 110: Loss: 0.7585644394959358, accuracy: 0.5166666666666667\n",
      "iteration no 111: Loss: 0.7584997830123508, accuracy: 0.5166666666666667\n",
      "iteration no 112: Loss: 0.7584373911391961, accuracy: 0.5166666666666667\n",
      "iteration no 113: Loss: 0.7583771777565559, accuracy: 0.5166666666666667\n",
      "iteration no 114: Loss: 0.7583190603676672, accuracy: 0.5166666666666667\n",
      "iteration no 115: Loss: 0.7582629599281513, accuracy: 0.5166666666666667\n",
      "iteration no 116: Loss: 0.758208800684298, accuracy: 0.5166666666666667\n",
      "iteration no 117: Loss: 0.7581565100198597, accuracy: 0.5166666666666667\n",
      "iteration no 118: Loss: 0.7581060183108582, accuracy: 0.5166666666666667\n",
      "iteration no 119: Loss: 0.7580572587879334, accuracy: 0.5166666666666667\n",
      "iteration no 120: Loss: 0.7580101674057956, accuracy: 0.5166666666666667\n",
      "iteration no 121: Loss: 0.757964682719371, accuracy: 0.5166666666666667\n",
      "iteration no 122: Loss: 0.7579207457662582, accuracy: 0.5166666666666667\n",
      "iteration no 123: Loss: 0.7578782999551353, accuracy: 0.5166666666666667\n",
      "iteration no 124: Loss: 0.7578372909597788, accuracy: 0.5166666666666667\n",
      "iteration no 125: Loss: 0.7577976666183834, accuracy: 0.5166666666666667\n",
      "iteration no 126: Loss: 0.7577593768378829, accuracy: 0.5166666666666667\n",
      "iteration no 127: Loss: 0.7577223735029972, accuracy: 0.5166666666666667\n",
      "iteration no 128: Loss: 0.7576866103897425, accuracy: 0.5166666666666667\n",
      "iteration no 129: Loss: 0.7576520430831631, accuracy: 0.5166666666666667\n",
      "iteration no 130: Loss: 0.7576186288990532, accuracy: 0.5166666666666667\n",
      "iteration no 131: Loss: 0.7575863268094521, accuracy: 0.5166666666666667\n",
      "iteration no 132: Loss: 0.7575550973717142, accuracy: 0.5133333333333333\n",
      "iteration no 133: Loss: 0.7575249026609565, accuracy: 0.5133333333333333\n",
      "iteration no 134: Loss: 0.7574957062057109, accuracy: 0.5133333333333333\n",
      "iteration no 135: Loss: 0.7574674729266065, accuracy: 0.5133333333333333\n",
      "iteration no 136: Loss: 0.7574401690779279, accuracy: 0.5133333333333333\n",
      "iteration no 137: Loss: 0.7574137621918952, accuracy: 0.5133333333333333\n",
      "iteration no 138: Loss: 0.7573882210255282, accuracy: 0.5133333333333333\n",
      "iteration no 139: Loss: 0.7573635155099587, accuracy: 0.5133333333333333\n",
      "iteration no 140: Loss: 0.7573396167020696, accuracy: 0.5133333333333333\n",
      "iteration no 141: Loss: 0.7573164967383358, accuracy: 0.5133333333333333\n",
      "iteration no 142: Loss: 0.7572941287907627, accuracy: 0.5133333333333333\n",
      "iteration no 143: Loss: 0.7572724870248115, accuracy: 0.5133333333333333\n",
      "iteration no 144: Loss: 0.757251546559214, accuracy: 0.5133333333333333\n",
      "iteration no 145: Loss: 0.7572312834275831, accuracy: 0.5133333333333333\n",
      "iteration no 146: Loss: 0.7572116745417276, accuracy: 0.5133333333333333\n",
      "iteration no 147: Loss: 0.7571926976565918, accuracy: 0.5133333333333333\n",
      "iteration no 148: Loss: 0.7571743313367352, accuracy: 0.5133333333333333\n",
      "iteration no 149: Loss: 0.7571565549242819, accuracy: 0.5133333333333333\n",
      "iteration no 150: Loss: 0.7571393485082656, accuracy: 0.5133333333333333\n",
      "iteration no 151: Loss: 0.7571226928953019, accuracy: 0.5133333333333333\n",
      "iteration no 152: Loss: 0.7571065695815313, accuracy: 0.5133333333333333\n",
      "iteration no 153: Loss: 0.757090960725761, accuracy: 0.5133333333333333\n",
      "iteration no 154: Loss: 0.757075849123759, accuracy: 0.5133333333333333\n",
      "iteration no 155: Loss: 0.7570612181836408, accuracy: 0.5133333333333333\n",
      "iteration no 156: Loss: 0.7570470519022978, accuracy: 0.5133333333333333\n",
      "iteration no 157: Loss: 0.7570333348428206, accuracy: 0.5133333333333333\n",
      "iteration no 158: Loss: 0.7570200521128696, accuracy: 0.5133333333333333\n",
      "iteration no 159: Loss: 0.7570071893439476, accuracy: 0.5133333333333333\n",
      "iteration no 160: Loss: 0.7569947326715385, accuracy: 0.5133333333333333\n",
      "iteration no 161: Loss: 0.7569826687160638, accuracy: 0.5133333333333333\n",
      "iteration no 162: Loss: 0.7569709845646289, accuracy: 0.5133333333333333\n",
      "iteration no 163: Loss: 0.7569596677535151, accuracy: 0.5133333333333333\n",
      "iteration no 164: Loss: 0.7569487062513904, accuracy: 0.5133333333333333\n",
      "iteration no 165: Loss: 0.7569380884432015, accuracy: 0.5133333333333333\n",
      "iteration no 166: Loss: 0.7569278031147212, accuracy: 0.5133333333333333\n",
      "iteration no 167: Loss: 0.7569178394377174, accuracy: 0.5133333333333333\n",
      "iteration no 168: Loss: 0.7569081869557204, accuracy: 0.5133333333333333\n",
      "iteration no 169: Loss: 0.7568988355703588, accuracy: 0.5133333333333333\n",
      "iteration no 170: Loss: 0.7568897755282417, accuracy: 0.5133333333333333\n",
      "iteration no 171: Loss: 0.7568809974083612, accuracy: 0.5133333333333333\n",
      "iteration no 172: Loss: 0.7568724921099943, accuracy: 0.5133333333333333\n",
      "iteration no 173: Loss: 0.7568642508410802, accuracy: 0.5133333333333333\n",
      "iteration no 174: Loss: 0.7568562651070565, accuracy: 0.5133333333333333\n",
      "iteration no 175: Loss: 0.7568485267001295, accuracy: 0.5133333333333333\n",
      "iteration no 176: Loss: 0.7568410276889649, accuracy: 0.5133333333333333\n",
      "iteration no 177: Loss: 0.7568337604087775, accuracy: 0.5133333333333333\n",
      "iteration no 178: Loss: 0.7568267174518045, accuracy: 0.5133333333333333\n",
      "iteration no 179: Loss: 0.7568198916581472, accuracy: 0.5133333333333333\n",
      "iteration no 180: Loss: 0.7568132761069629, accuracy: 0.5133333333333333\n",
      "iteration no 181: Loss: 0.756806864107994, accuracy: 0.5133333333333333\n",
      "iteration no 182: Loss: 0.7568006491934218, accuracy: 0.5133333333333333\n",
      "iteration no 183: Loss: 0.7567946251100284, accuracy: 0.5133333333333333\n",
      "iteration no 184: Loss: 0.7567887858116555, accuracy: 0.5133333333333333\n",
      "iteration no 185: Loss: 0.7567831254519484, accuracy: 0.5133333333333333\n",
      "iteration no 186: Loss: 0.7567776383773731, accuracy: 0.5133333333333333\n",
      "iteration no 187: Loss: 0.756772319120494, accuracy: 0.5133333333333333\n",
      "iteration no 188: Loss: 0.7567671623935046, accuracy: 0.5133333333333333\n",
      "iteration no 189: Loss: 0.7567621630819975, accuracy: 0.5133333333333333\n",
      "iteration no 190: Loss: 0.7567573162389661, accuracy: 0.5133333333333333\n",
      "iteration no 191: Loss: 0.7567526170790294, accuracy: 0.5133333333333333\n",
      "iteration no 192: Loss: 0.7567480609728681, accuracy: 0.5133333333333333\n",
      "iteration no 193: Loss: 0.756743643441866, accuracy: 0.5133333333333333\n",
      "iteration no 194: Loss: 0.756739360152949, accuracy: 0.5133333333333333\n",
      "iteration no 195: Loss: 0.7567352069136106, accuracy: 0.5133333333333333\n",
      "iteration no 196: Loss: 0.7567311796671204, accuracy: 0.5133333333333333\n",
      "iteration no 197: Loss: 0.7567272744879054, accuracy: 0.5133333333333333\n",
      "iteration no 198: Loss: 0.7567234875770996, accuracy: 0.5133333333333333\n",
      "iteration no 199: Loss: 0.7567198152582536, accuracy: 0.5133333333333333\n",
      "iteration no 200: Loss: 0.756716253973199, accuracy: 0.5133333333333333\n",
      "iteration no 1: Loss: 1.1164175096635047, accuracy: 0.42333333333333334\n",
      "iteration no 2: Loss: 1.0836628397983146, accuracy: 0.5066666666666667\n",
      "iteration no 3: Loss: 1.0545294515269321, accuracy: 0.5133333333333333\n",
      "iteration no 4: Loss: 1.0285881514070012, accuracy: 0.5366666666666666\n",
      "iteration no 5: Loss: 1.0054555127558813, accuracy: 0.5233333333333333\n",
      "iteration no 6: Loss: 0.9847911872072396, accuracy: 0.5233333333333333\n",
      "iteration no 7: Loss: 0.9662950474629858, accuracy: 0.5233333333333333\n",
      "iteration no 8: Loss: 0.9497038856539189, accuracy: 0.52\n",
      "iteration no 9: Loss: 0.9347878091893292, accuracy: 0.5233333333333333\n",
      "iteration no 10: Loss: 0.9213465622526902, accuracy: 0.5166666666666667\n",
      "iteration no 11: Loss: 0.9092059649421085, accuracy: 0.5133333333333333\n",
      "iteration no 12: Loss: 0.8982145964138555, accuracy: 0.5133333333333333\n",
      "iteration no 13: Loss: 0.8882407888652534, accuracy: 0.5133333333333333\n",
      "iteration no 14: Loss: 0.8791699555610215, accuracy: 0.5166666666666667\n",
      "iteration no 15: Loss: 0.8709022479467677, accuracy: 0.5166666666666667\n",
      "iteration no 16: Loss: 0.863350520701854, accuracy: 0.5133333333333333\n",
      "iteration no 17: Loss: 0.8564385756434704, accuracy: 0.5133333333333333\n",
      "iteration no 18: Loss: 0.8500996527044722, accuracy: 0.5133333333333333\n",
      "iteration no 19: Loss: 0.8442751366238102, accuracy: 0.51\n",
      "iteration no 20: Loss: 0.8389134500832502, accuracy: 0.51\n",
      "iteration no 21: Loss: 0.8339691068983057, accuracy: 0.51\n",
      "iteration no 22: Loss: 0.8294019019859163, accuracy: 0.5133333333333333\n",
      "iteration no 23: Loss: 0.8251762178815388, accuracy: 0.5133333333333333\n",
      "iteration no 24: Loss: 0.8212604304057114, accuracy: 0.5133333333333333\n",
      "iteration no 25: Loss: 0.8176263986150837, accuracy: 0.5133333333333333\n",
      "iteration no 26: Loss: 0.8142490263971784, accuracy: 0.5133333333333333\n",
      "iteration no 27: Loss: 0.8111058849917651, accuracy: 0.5133333333333333\n",
      "iteration no 28: Loss: 0.8081768873689881, accuracy: 0.5133333333333333\n",
      "iteration no 29: Loss: 0.8054440067954511, accuracy: 0.5133333333333333\n",
      "iteration no 30: Loss: 0.8028910331056245, accuracy: 0.5166666666666667\n",
      "iteration no 31: Loss: 0.8005033611971033, accuracy: 0.5166666666666667\n",
      "iteration no 32: Loss: 0.7982678071116808, accuracy: 0.5166666666666667\n",
      "iteration no 33: Loss: 0.7961724477739947, accuracy: 0.5166666666666667\n",
      "iteration no 34: Loss: 0.7942064810566334, accuracy: 0.5166666666666667\n",
      "iteration no 35: Loss: 0.7923601033429936, accuracy: 0.5166666666666667\n",
      "iteration no 36: Loss: 0.7906244021821044, accuracy: 0.5166666666666667\n",
      "iteration no 37: Loss: 0.7889912619859595, accuracy: 0.5166666666666667\n",
      "iteration no 38: Loss: 0.7874532810204223, accuracy: 0.5166666666666667\n",
      "iteration no 39: Loss: 0.7860036981945595, accuracy: 0.5166666666666667\n",
      "iteration no 40: Loss: 0.7846363283678771, accuracy: 0.5166666666666667\n",
      "iteration no 41: Loss: 0.7833455050767052, accuracy: 0.5166666666666667\n",
      "iteration no 42: Loss: 0.7821260297351815, accuracy: 0.5133333333333333\n",
      "iteration no 43: Loss: 0.7809731264973088, accuracy: 0.5133333333333333\n",
      "iteration no 44: Loss: 0.7798824020780971, accuracy: 0.5133333333333333\n",
      "iteration no 45: Loss: 0.7788498099269068, accuracy: 0.5133333333333333\n",
      "iteration no 46: Loss: 0.777871618227344, accuracy: 0.5133333333333333\n",
      "iteration no 47: Loss: 0.7769443812675891, accuracy: 0.5133333333333333\n",
      "iteration no 48: Loss: 0.7760649137846334, accuracy: 0.5133333333333333\n",
      "iteration no 49: Loss: 0.7752302679370934, accuracy: 0.5133333333333333\n",
      "iteration no 50: Loss: 0.7744377126053131, accuracy: 0.5133333333333333\n",
      "iteration no 51: Loss: 0.7736847147554288, accuracy: 0.5133333333333333\n",
      "iteration no 52: Loss: 0.7729689226368459, accuracy: 0.5133333333333333\n",
      "iteration no 53: Loss: 0.7722881506109397, accuracy: 0.5133333333333333\n",
      "iteration no 54: Loss: 0.7716403654333541, accuracy: 0.5133333333333333\n",
      "iteration no 55: Loss: 0.7710236738336008, accuracy: 0.5133333333333333\n",
      "iteration no 56: Loss: 0.770436311254205, accuracy: 0.5133333333333333\n",
      "iteration no 57: Loss: 0.7698766316277895, accuracy: 0.5133333333333333\n",
      "iteration no 58: Loss: 0.7693430980845829, accuracy: 0.5133333333333333\n",
      "iteration no 59: Loss: 0.7688342744951393, accuracy: 0.5133333333333333\n",
      "iteration no 60: Loss: 0.7683488177638396, accuracy: 0.5166666666666667\n",
      "iteration no 61: Loss: 0.7678854707981813, accuracy: 0.5166666666666667\n",
      "iteration no 62: Loss: 0.7674430560871609, accuracy: 0.5166666666666667\n",
      "iteration no 63: Loss: 0.7670204698293362, accuracy: 0.5166666666666667\n",
      "iteration no 64: Loss: 0.7666166765575841, accuracy: 0.5166666666666667\n",
      "iteration no 65: Loss: 0.7662307042132233, accuracy: 0.5166666666666667\n",
      "iteration no 66: Loss: 0.7658616396271765, accuracy: 0.5166666666666667\n",
      "iteration no 67: Loss: 0.7655086243702673, accuracy: 0.5166666666666667\n",
      "iteration no 68: Loss: 0.7651708509386612, accuracy: 0.5166666666666667\n",
      "iteration no 69: Loss: 0.7648475592439388, accuracy: 0.5166666666666667\n",
      "iteration no 70: Loss: 0.7645380333803679, accuracy: 0.5166666666666667\n",
      "iteration no 71: Loss: 0.7642415986446912, accuracy: 0.5166666666666667\n",
      "iteration no 72: Loss: 0.7639576187861805, accuracy: 0.5166666666666667\n",
      "iteration no 73: Loss: 0.7636854934668915, accuracy: 0.5166666666666667\n",
      "iteration no 74: Loss: 0.7634246559139978, accuracy: 0.5166666666666667\n",
      "iteration no 75: Loss: 0.7631745707478133, accuracy: 0.5166666666666667\n",
      "iteration no 76: Loss: 0.762934731970674, accuracy: 0.5133333333333333\n",
      "iteration no 77: Loss: 0.7627046611032385, accuracy: 0.5133333333333333\n",
      "iteration no 78: Loss: 0.7624839054560147, accuracy: 0.5133333333333333\n",
      "iteration no 79: Loss: 0.7622720365250459, accuracy: 0.5133333333333333\n",
      "iteration no 80: Loss: 0.7620686485016946, accuracy: 0.5133333333333333\n",
      "iteration no 81: Loss: 0.7618733568873689, accuracy: 0.5133333333333333\n",
      "iteration no 82: Loss: 0.7616857972048541, accuracy: 0.5133333333333333\n",
      "iteration no 83: Loss: 0.7615056237986549, accuracy: 0.5133333333333333\n",
      "iteration no 84: Loss: 0.7613325087174082, accuracy: 0.5133333333333333\n",
      "iteration no 85: Loss: 0.7611661406720375, accuracy: 0.5133333333333333\n",
      "iteration no 86: Loss: 0.7610062240638578, accuracy: 0.5133333333333333\n",
      "iteration no 87: Loss: 0.760852478077338, accuracy: 0.5133333333333333\n",
      "iteration no 88: Loss: 0.7607046358326689, accuracy: 0.5133333333333333\n",
      "iteration no 89: Loss: 0.7605624435936916, accuracy: 0.5133333333333333\n",
      "iteration no 90: Loss: 0.7604256600271118, accuracy: 0.5133333333333333\n",
      "iteration no 91: Loss: 0.7602940555092563, accuracy: 0.5133333333333333\n",
      "iteration no 92: Loss: 0.7601674114769329, accuracy: 0.5133333333333333\n",
      "iteration no 93: Loss: 0.7600455198192329, accuracy: 0.5133333333333333\n",
      "iteration no 94: Loss: 0.7599281823073677, accuracy: 0.5133333333333333\n",
      "iteration no 95: Loss: 0.7598152100598593, accuracy: 0.5133333333333333\n",
      "iteration no 96: Loss: 0.759706423040617, accuracy: 0.5133333333333333\n",
      "iteration no 97: Loss: 0.7596016495876226, accuracy: 0.5133333333333333\n",
      "iteration no 98: Loss: 0.7595007259701241, accuracy: 0.5133333333333333\n",
      "iteration no 99: Loss: 0.7594034959723962, accuracy: 0.5133333333333333\n",
      "iteration no 100: Loss: 0.7593098105022731, accuracy: 0.5133333333333333\n",
      "iteration no 101: Loss: 0.7592195272227991, accuracy: 0.5133333333333333\n",
      "iteration no 102: Loss: 0.7591325102054551, accuracy: 0.5133333333333333\n",
      "iteration no 103: Loss: 0.7590486296035461, accuracy: 0.5133333333333333\n",
      "iteration no 104: Loss: 0.7589677613444274, accuracy: 0.5166666666666667\n",
      "iteration no 105: Loss: 0.7588897868393528, accuracy: 0.5166666666666667\n",
      "iteration no 106: Loss: 0.7588145927098036, accuracy: 0.5166666666666667\n",
      "iteration no 107: Loss: 0.7587420705292569, accuracy: 0.5166666666666667\n",
      "iteration no 108: Loss: 0.7586721165794063, accuracy: 0.5166666666666667\n",
      "iteration no 109: Loss: 0.7586046316199299, accuracy: 0.5166666666666667\n",
      "iteration no 110: Loss: 0.7585395206709634, accuracy: 0.5166666666666667\n",
      "iteration no 111: Loss: 0.7584766928074859, accuracy: 0.5166666666666667\n",
      "iteration no 112: Loss: 0.7584160609648904, accuracy: 0.5166666666666667\n",
      "iteration no 113: Loss: 0.7583575417550571, accuracy: 0.5166666666666667\n",
      "iteration no 114: Loss: 0.7583010552922897, accuracy: 0.5166666666666667\n",
      "iteration no 115: Loss: 0.7582465250285252, accuracy: 0.5166666666666667\n",
      "iteration no 116: Loss: 0.7581938775972626, accuracy: 0.5166666666666667\n",
      "iteration no 117: Loss: 0.7581430426656934, accuracy: 0.5166666666666667\n",
      "iteration no 118: Loss: 0.7580939527945508, accuracy: 0.5166666666666667\n",
      "iteration no 119: Loss: 0.7580465433052276, accuracy: 0.5166666666666667\n",
      "iteration no 120: Loss: 0.7580007521537404, accuracy: 0.5166666666666667\n",
      "iteration no 121: Loss: 0.7579565198111459, accuracy: 0.5166666666666667\n",
      "iteration no 122: Loss: 0.7579137891500379, accuracy: 0.5166666666666667\n",
      "iteration no 123: Loss: 0.7578725053367823, accuracy: 0.5166666666666667\n",
      "iteration no 124: Loss: 0.7578326157291637, accuracy: 0.5166666666666667\n",
      "iteration no 125: Loss: 0.7577940697791405, accuracy: 0.5166666666666667\n",
      "iteration no 126: Loss: 0.7577568189404247, accuracy: 0.5166666666666667\n",
      "iteration no 127: Loss: 0.7577208165806157, accuracy: 0.5166666666666667\n",
      "iteration no 128: Loss: 0.757686017897645, accuracy: 0.5166666666666667\n",
      "iteration no 129: Loss: 0.7576523798402852, accuracy: 0.5166666666666667\n",
      "iteration no 130: Loss: 0.7576198610325126, accuracy: 0.5166666666666667\n",
      "iteration no 131: Loss: 0.7575884217015102, accuracy: 0.5166666666666667\n",
      "iteration no 132: Loss: 0.7575580236091157, accuracy: 0.5133333333333333\n",
      "iteration no 133: Loss: 0.7575286299865324, accuracy: 0.5133333333333333\n",
      "iteration no 134: Loss: 0.7575002054721295, accuracy: 0.5133333333333333\n",
      "iteration no 135: Loss: 0.7574727160521695, accuracy: 0.5133333333333333\n",
      "iteration no 136: Loss: 0.7574461290043081, accuracy: 0.5133333333333333\n",
      "iteration no 137: Loss: 0.7574204128437231, accuracy: 0.5133333333333333\n",
      "iteration no 138: Loss: 0.7573955372717365, accuracy: 0.5133333333333333\n",
      "iteration no 139: Loss: 0.7573714731268026, accuracy: 0.5133333333333333\n",
      "iteration no 140: Loss: 0.7573481923377372, accuracy: 0.5133333333333333\n",
      "iteration no 141: Loss: 0.7573256678790783, accuracy: 0.5133333333333333\n",
      "iteration no 142: Loss: 0.7573038737284697, accuracy: 0.5133333333333333\n",
      "iteration no 143: Loss: 0.7572827848259621, accuracy: 0.5133333333333333\n",
      "iteration no 144: Loss: 0.7572623770351415, accuracy: 0.5133333333333333\n",
      "iteration no 145: Loss: 0.7572426271059894, accuracy: 0.5133333333333333\n",
      "iteration no 146: Loss: 0.7572235126393924, accuracy: 0.5133333333333333\n",
      "iteration no 147: Loss: 0.7572050120532168, accuracy: 0.5133333333333333\n",
      "iteration no 148: Loss: 0.7571871045498766, accuracy: 0.5133333333333333\n",
      "iteration no 149: Loss: 0.7571697700853162, accuracy: 0.5133333333333333\n",
      "iteration no 150: Loss: 0.7571529893393448, accuracy: 0.5133333333333333\n",
      "iteration no 151: Loss: 0.7571367436872534, accuracy: 0.5133333333333333\n",
      "iteration no 152: Loss: 0.7571210151726565, accuracy: 0.5133333333333333\n",
      "iteration no 153: Loss: 0.757105786481497, accuracy: 0.5133333333333333\n",
      "iteration no 154: Loss: 0.7570910409171613, accuracy: 0.5133333333333333\n",
      "iteration no 155: Loss: 0.7570767623766517, accuracy: 0.5133333333333333\n",
      "iteration no 156: Loss: 0.7570629353277664, accuracy: 0.5133333333333333\n",
      "iteration no 157: Loss: 0.75704954478724, accuracy: 0.5133333333333333\n",
      "iteration no 158: Loss: 0.757036576299801, accuracy: 0.5133333333333333\n",
      "iteration no 159: Loss: 0.757024015918101, accuracy: 0.5133333333333333\n",
      "iteration no 160: Loss: 0.7570118501834806, accuracy: 0.5133333333333333\n",
      "iteration no 161: Loss: 0.7570000661075285, accuracy: 0.5133333333333333\n",
      "iteration no 162: Loss: 0.7569886511544003, accuracy: 0.5133333333333333\n",
      "iteration no 163: Loss: 0.7569775932238634, accuracy: 0.5133333333333333\n",
      "iteration no 164: Loss: 0.7569668806350328, accuracy: 0.5133333333333333\n",
      "iteration no 165: Loss: 0.7569565021107696, accuracy: 0.5133333333333333\n",
      "iteration no 166: Loss: 0.7569464467627095, accuracy: 0.5133333333333333\n",
      "iteration no 167: Loss: 0.7569367040768965, accuracy: 0.5133333333333333\n",
      "iteration no 168: Loss: 0.7569272638999929, accuracy: 0.5133333333333333\n",
      "iteration no 169: Loss: 0.7569181164260401, accuracy: 0.5133333333333333\n",
      "iteration no 170: Loss: 0.7569092521837488, accuracy: 0.5133333333333333\n",
      "iteration no 171: Loss: 0.7569006620242921, accuracy: 0.5133333333333333\n",
      "iteration no 172: Loss: 0.7568923371095803, accuracy: 0.5133333333333333\n",
      "iteration no 173: Loss: 0.7568842689009992, accuracy: 0.5133333333333333\n",
      "iteration no 174: Loss: 0.7568764491485898, accuracy: 0.5133333333333333\n",
      "iteration no 175: Loss: 0.7568688698806488, accuracy: 0.5133333333333333\n",
      "iteration no 176: Loss: 0.7568615233937368, accuracy: 0.5133333333333333\n",
      "iteration no 177: Loss: 0.7568544022430724, accuracy: 0.5133333333333333\n",
      "iteration no 178: Loss: 0.7568474992332965, accuracy: 0.5133333333333333\n",
      "iteration no 179: Loss: 0.7568408074095948, accuracy: 0.5133333333333333\n",
      "iteration no 180: Loss: 0.7568343200491588, accuracy: 0.5133333333333333\n",
      "iteration no 181: Loss: 0.7568280306529741, accuracy: 0.5133333333333333\n",
      "iteration no 182: Loss: 0.7568219329379208, accuracy: 0.5133333333333333\n",
      "iteration no 183: Loss: 0.756816020829174, accuracy: 0.5133333333333333\n",
      "iteration no 184: Loss: 0.7568102884528937, accuracy: 0.5133333333333333\n",
      "iteration no 185: Loss: 0.7568047301291858, accuracy: 0.5133333333333333\n",
      "iteration no 186: Loss: 0.7567993403653327, accuracy: 0.5133333333333333\n",
      "iteration no 187: Loss: 0.7567941138492736, accuracy: 0.5133333333333333\n",
      "iteration no 188: Loss: 0.7567890454433303, accuracy: 0.5133333333333333\n",
      "iteration no 189: Loss: 0.7567841301781651, accuracy: 0.5133333333333333\n",
      "iteration no 190: Loss: 0.7567793632469638, accuracy: 0.5133333333333333\n",
      "iteration no 191: Loss: 0.7567747399998324, accuracy: 0.5133333333333333\n",
      "iteration no 192: Loss: 0.7567702559384025, accuracy: 0.5133333333333333\n",
      "iteration no 193: Loss: 0.7567659067106317, accuracy: 0.5133333333333333\n",
      "iteration no 194: Loss: 0.7567616881057975, accuracy: 0.5133333333333333\n",
      "iteration no 195: Loss: 0.7567575960496716, accuracy: 0.5133333333333333\n",
      "iteration no 196: Loss: 0.7567536265998719, accuracy: 0.5133333333333333\n",
      "iteration no 197: Loss: 0.7567497759413804, accuracy: 0.5133333333333333\n",
      "iteration no 198: Loss: 0.7567460403822266, accuracy: 0.5133333333333333\n",
      "iteration no 199: Loss: 0.756742416349324, accuracy: 0.5133333333333333\n",
      "iteration no 200: Loss: 0.7567389003844578, accuracy: 0.5166666666666667\n",
      "iteration no 1: Loss: 1.1033676547963145, accuracy: 0.41\n",
      "iteration no 2: Loss: 1.072061438271489, accuracy: 0.48\n",
      "iteration no 3: Loss: 1.0442420367748901, accuracy: 0.49333333333333335\n",
      "iteration no 4: Loss: 1.019481620722347, accuracy: 0.48333333333333334\n",
      "iteration no 5: Loss: 0.9974034001283937, accuracy: 0.4866666666666667\n",
      "iteration no 6: Loss: 0.9776762363394499, accuracy: 0.49333333333333335\n",
      "iteration no 7: Loss: 0.9600102682925117, accuracy: 0.5\n",
      "iteration no 8: Loss: 0.9441527381177502, accuracy: 0.5066666666666667\n",
      "iteration no 9: Loss: 0.9298838834728861, accuracy: 0.51\n",
      "iteration no 10: Loss: 0.9170129773276572, accuracy: 0.5166666666666667\n",
      "iteration no 11: Loss: 0.9053746230947791, accuracy: 0.5233333333333333\n",
      "iteration no 12: Loss: 0.8948253805738531, accuracy: 0.5233333333333333\n",
      "iteration no 13: Loss: 0.8852407572183543, accuracy: 0.5233333333333333\n",
      "iteration no 14: Loss: 0.8765125667280496, accuracy: 0.52\n",
      "iteration no 15: Loss: 0.8685466358960786, accuracy: 0.52\n",
      "iteration no 16: Loss: 0.8612608291671858, accuracy: 0.5133333333333333\n",
      "iteration no 17: Loss: 0.8545833556455102, accuracy: 0.5133333333333333\n",
      "iteration no 18: Loss: 0.8484513228235517, accuracy: 0.5166666666666667\n",
      "iteration no 19: Loss: 0.8428095032514064, accuracy: 0.5166666666666667\n",
      "iteration no 20: Loss: 0.837609283505474, accuracy: 0.5166666666666667\n",
      "iteration no 21: Loss: 0.8328077683921384, accuracy: 0.5133333333333333\n",
      "iteration no 22: Loss: 0.8283670168970813, accuracy: 0.5166666666666667\n",
      "iteration no 23: Loss: 0.8242533897329128, accuracy: 0.5166666666666667\n",
      "iteration no 24: Loss: 0.8204369913407257, accuracy: 0.5166666666666667\n",
      "iteration no 25: Loss: 0.8168911918326413, accuracy: 0.5166666666666667\n",
      "iteration no 26: Loss: 0.8135922166306072, accuracy: 0.5166666666666667\n",
      "iteration no 27: Loss: 0.8105187934902126, accuracy: 0.5166666666666667\n",
      "iteration no 28: Loss: 0.807651848234254, accuracy: 0.5166666666666667\n",
      "iteration no 29: Loss: 0.8049742418981393, accuracy: 0.5166666666666667\n",
      "iteration no 30: Loss: 0.8024705431451644, accuracy: 0.52\n",
      "iteration no 31: Loss: 0.8001268307780337, accuracy: 0.5166666666666667\n",
      "iteration no 32: Loss: 0.7979305219834038, accuracy: 0.5166666666666667\n",
      "iteration no 33: Loss: 0.7958702226243668, accuracy: 0.5166666666666667\n",
      "iteration no 34: Loss: 0.793935596463419, accuracy: 0.5166666666666667\n",
      "iteration no 35: Loss: 0.7921172506739823, accuracy: 0.52\n",
      "iteration no 36: Loss: 0.790406635397309, accuracy: 0.5166666666666667\n",
      "iteration no 37: Loss: 0.7887959554364821, accuracy: 0.5166666666666667\n",
      "iteration no 38: Loss: 0.7872780924608336, accuracy: 0.5166666666666667\n",
      "iteration no 39: Loss: 0.7858465363313588, accuracy: 0.5166666666666667\n",
      "iteration no 40: Loss: 0.7844953243578981, accuracy: 0.5166666666666667\n",
      "iteration no 41: Loss: 0.7832189874681399, accuracy: 0.5166666666666667\n",
      "iteration no 42: Loss: 0.7820125024118543, accuracy: 0.5166666666666667\n",
      "iteration no 43: Loss: 0.7808712492454647, accuracy: 0.5166666666666667\n",
      "iteration no 44: Loss: 0.7797909734455266, accuracy: 0.5166666666666667\n",
      "iteration no 45: Loss: 0.7787677520878484, accuracy: 0.5166666666666667\n",
      "iteration no 46: Loss: 0.7777979636042549, accuracy: 0.5166666666666667\n",
      "iteration no 47: Loss: 0.7768782606933746, accuracy: 0.5166666666666667\n",
      "iteration no 48: Loss: 0.7760055460170107, accuracy: 0.5166666666666667\n",
      "iteration no 49: Loss: 0.7751769503610457, accuracy: 0.5166666666666667\n",
      "iteration no 50: Loss: 0.774389812980597, accuracy: 0.5166666666666667\n",
      "iteration no 51: Loss: 0.7736416638842851, accuracy: 0.5166666666666667\n",
      "iteration no 52: Loss: 0.7729302078428398, accuracy: 0.5133333333333333\n",
      "iteration no 53: Loss: 0.7722533099335235, accuracy: 0.5133333333333333\n",
      "iteration no 54: Loss: 0.7716089824546244, accuracy: 0.5133333333333333\n",
      "iteration no 55: Loss: 0.7709953730640463, accuracy: 0.5133333333333333\n",
      "iteration no 56: Loss: 0.7704107540132159, accuracy: 0.5133333333333333\n",
      "iteration no 57: Loss: 0.7698535123625219, accuracy: 0.5133333333333333\n",
      "iteration no 58: Loss: 0.7693221410775858, accuracy: 0.5133333333333333\n",
      "iteration no 59: Loss: 0.7688152309171038, accuracy: 0.5133333333333333\n",
      "iteration no 60: Loss: 0.7683314630330202, accuracy: 0.5133333333333333\n",
      "iteration no 61: Loss: 0.7678696022125876, accuracy: 0.5133333333333333\n",
      "iteration no 62: Loss: 0.7674284906995885, accuracy: 0.5133333333333333\n",
      "iteration no 63: Loss: 0.7670070425387951, accuracy: 0.5133333333333333\n",
      "iteration no 64: Loss: 0.7666042383937351, accuracy: 0.5166666666666667\n",
      "iteration no 65: Loss: 0.7662191207931143, accuracy: 0.5166666666666667\n",
      "iteration no 66: Loss: 0.765850789765932, accuracy: 0.5166666666666667\n",
      "iteration no 67: Loss: 0.765498398829452, accuracy: 0.5166666666666667\n",
      "iteration no 68: Loss: 0.7651611512978719, accuracy: 0.5166666666666667\n",
      "iteration no 69: Loss: 0.7648382968827796, accuracy: 0.5166666666666667\n",
      "iteration no 70: Loss: 0.7645291285593914, accuracy: 0.5166666666666667\n",
      "iteration no 71: Loss: 0.7642329796751344, accuracy: 0.5166666666666667\n",
      "iteration no 72: Loss: 0.7639492212794385, accuracy: 0.5166666666666667\n",
      "iteration no 73: Loss: 0.7636772596556498, accuracy: 0.5166666666666667\n",
      "iteration no 74: Loss: 0.7634165340378103, accuracy: 0.5166666666666667\n",
      "iteration no 75: Loss: 0.7631665144966853, accuracy: 0.5166666666666667\n",
      "iteration no 76: Loss: 0.7629266999808902, accuracy: 0.5166666666666667\n",
      "iteration no 77: Loss: 0.7626966165002833, accuracy: 0.5166666666666667\n",
      "iteration no 78: Loss: 0.7624758154399691, accuracy: 0.5166666666666667\n",
      "iteration no 79: Loss: 0.762263871994325, accuracy: 0.5166666666666667\n",
      "iteration no 80: Loss: 0.7620603837114164, accuracy: 0.5166666666666667\n",
      "iteration no 81: Loss: 0.7618649691390231, accuracy: 0.5133333333333333\n",
      "iteration no 82: Loss: 0.7616772665642831, accuracy: 0.5133333333333333\n",
      "iteration no 83: Loss: 0.7614969328396534, accuracy: 0.5133333333333333\n",
      "iteration no 84: Loss: 0.7613236422885246, accuracy: 0.5133333333333333\n",
      "iteration no 85: Loss: 0.7611570856843991, accuracy: 0.5133333333333333\n",
      "iteration no 86: Loss: 0.7609969692980585, accuracy: 0.5133333333333333\n",
      "iteration no 87: Loss: 0.7608430140076173, accuracy: 0.5133333333333333\n",
      "iteration no 88: Loss: 0.7606949544667874, accuracy: 0.5133333333333333\n",
      "iteration no 89: Loss: 0.7605525383270629, accuracy: 0.5133333333333333\n",
      "iteration no 90: Loss: 0.7604155255098881, accuracy: 0.5133333333333333\n",
      "iteration no 91: Loss: 0.7602836875251916, accuracy: 0.5133333333333333\n",
      "iteration no 92: Loss: 0.7601568068329586, accuracy: 0.5133333333333333\n",
      "iteration no 93: Loss: 0.7600346762447848, accuracy: 0.5133333333333333\n",
      "iteration no 94: Loss: 0.7599170983625884, accuracy: 0.5133333333333333\n",
      "iteration no 95: Loss: 0.7598038850518878, accuracy: 0.5133333333333333\n",
      "iteration no 96: Loss: 0.7596948569472476, accuracy: 0.5133333333333333\n",
      "iteration no 97: Loss: 0.7595898429876824, accuracy: 0.5133333333333333\n",
      "iteration no 98: Loss: 0.7594886799799793, accuracy: 0.5133333333333333\n",
      "iteration no 99: Loss: 0.7593912121880477, accuracy: 0.5133333333333333\n",
      "iteration no 100: Loss: 0.7592972909465598, accuracy: 0.5133333333333333\n",
      "iteration no 101: Loss: 0.759206774297257, accuracy: 0.5133333333333333\n",
      "iteration no 102: Loss: 0.7591195266464369, accuracy: 0.5133333333333333\n",
      "iteration no 103: Loss: 0.7590354184422283, accuracy: 0.5133333333333333\n",
      "iteration no 104: Loss: 0.758954325870374, accuracy: 0.5133333333333333\n",
      "iteration no 105: Loss: 0.7588761305673271, accuracy: 0.5166666666666667\n",
      "iteration no 106: Loss: 0.7588007193495543, accuracy: 0.5166666666666667\n",
      "iteration no 107: Loss: 0.7587279839580215, accuracy: 0.5166666666666667\n",
      "iteration no 108: Loss: 0.7586578208169019, accuracy: 0.5166666666666667\n",
      "iteration no 109: Loss: 0.7585901308056215, accuracy: 0.5166666666666667\n",
      "iteration no 110: Loss: 0.758524819043413, accuracy: 0.5166666666666667\n",
      "iteration no 111: Loss: 0.75846179468561, accuracy: 0.5166666666666667\n",
      "iteration no 112: Loss: 0.7584009707309646, accuracy: 0.5166666666666667\n",
      "iteration no 113: Loss: 0.7583422638393157, accuracy: 0.5166666666666667\n",
      "iteration no 114: Loss: 0.7582855941589919, accuracy: 0.5166666666666667\n",
      "iteration no 115: Loss: 0.7582308851633601, accuracy: 0.5166666666666667\n",
      "iteration no 116: Loss: 0.7581780634959817, accuracy: 0.5166666666666667\n",
      "iteration no 117: Loss: 0.7581270588238672, accuracy: 0.5166666666666667\n",
      "iteration no 118: Loss: 0.7580778036983533, accuracy: 0.5166666666666667\n",
      "iteration no 119: Loss: 0.758030233423165, accuracy: 0.5166666666666667\n",
      "iteration no 120: Loss: 0.7579842859292407, accuracy: 0.5166666666666667\n",
      "iteration no 121: Loss: 0.7579399016559383, accuracy: 0.5166666666666667\n",
      "iteration no 122: Loss: 0.757897023438255, accuracy: 0.5166666666666667\n",
      "iteration no 123: Loss: 0.757855596399724, accuracy: 0.5166666666666667\n",
      "iteration no 124: Loss: 0.7578155678506646, accuracy: 0.5166666666666667\n",
      "iteration no 125: Loss: 0.7577768871914917, accuracy: 0.5166666666666667\n",
      "iteration no 126: Loss: 0.7577395058207989, accuracy: 0.5166666666666667\n",
      "iteration no 127: Loss: 0.7577033770479582, accuracy: 0.5166666666666667\n",
      "iteration no 128: Loss: 0.7576684560099813, accuracy: 0.5166666666666667\n",
      "iteration no 129: Loss: 0.7576346995924181, accuracy: 0.5166666666666667\n",
      "iteration no 130: Loss: 0.7576020663540687, accuracy: 0.5133333333333333\n",
      "iteration no 131: Loss: 0.7575705164553048, accuracy: 0.5133333333333333\n",
      "iteration no 132: Loss: 0.7575400115898127, accuracy: 0.5133333333333333\n",
      "iteration no 133: Loss: 0.7575105149195673, accuracy: 0.5133333333333333\n",
      "iteration no 134: Loss: 0.7574819910128745, accuracy: 0.5133333333333333\n",
      "iteration no 135: Loss: 0.7574544057853178, accuracy: 0.5133333333333333\n",
      "iteration no 136: Loss: 0.757427726443456, accuracy: 0.5133333333333333\n",
      "iteration no 137: Loss: 0.7574019214311336, accuracy: 0.5133333333333333\n",
      "iteration no 138: Loss: 0.757376960378265, accuracy: 0.5133333333333333\n",
      "iteration no 139: Loss: 0.7573528140519695, accuracy: 0.5133333333333333\n",
      "iteration no 140: Loss: 0.7573294543099343, accuracy: 0.5133333333333333\n",
      "iteration no 141: Loss: 0.7573068540558932, accuracy: 0.5133333333333333\n",
      "iteration no 142: Loss: 0.7572849871971175, accuracy: 0.5133333333333333\n",
      "iteration no 143: Loss: 0.7572638286038138, accuracy: 0.5133333333333333\n",
      "iteration no 144: Loss: 0.757243354070337, accuracy: 0.5133333333333333\n",
      "iteration no 145: Loss: 0.7572235402781272, accuracy: 0.5133333333333333\n",
      "iteration no 146: Loss: 0.7572043647602855, accuracy: 0.5133333333333333\n",
      "iteration no 147: Loss: 0.7571858058677106, accuracy: 0.5133333333333333\n",
      "iteration no 148: Loss: 0.7571678427367151, accuracy: 0.5133333333333333\n",
      "iteration no 149: Loss: 0.7571504552580576, accuracy: 0.5133333333333333\n",
      "iteration no 150: Loss: 0.7571336240473132, accuracy: 0.5133333333333333\n",
      "iteration no 151: Loss: 0.7571173304165265, accuracy: 0.5133333333333333\n",
      "iteration no 152: Loss: 0.7571015563470808, accuracy: 0.5133333333333333\n",
      "iteration no 153: Loss: 0.7570862844637264, accuracy: 0.5133333333333333\n",
      "iteration no 154: Loss: 0.7570714980097165, accuracy: 0.5133333333333333\n",
      "iteration no 155: Loss: 0.7570571808229954, accuracy: 0.5133333333333333\n",
      "iteration no 156: Loss: 0.757043317313389, accuracy: 0.5133333333333333\n",
      "iteration no 157: Loss: 0.7570298924407564, accuracy: 0.5133333333333333\n",
      "iteration no 158: Loss: 0.7570168916940512, accuracy: 0.5133333333333333\n",
      "iteration no 159: Loss: 0.7570043010712563, accuracy: 0.5133333333333333\n",
      "iteration no 160: Loss: 0.7569921070601489, accuracy: 0.5133333333333333\n",
      "iteration no 161: Loss: 0.756980296619859, accuracy: 0.5133333333333333\n",
      "iteration no 162: Loss: 0.7569688571631852, accuracy: 0.5133333333333333\n",
      "iteration no 163: Loss: 0.7569577765396339, accuracy: 0.5133333333333333\n",
      "iteration no 164: Loss: 0.7569470430191491, accuracy: 0.5133333333333333\n",
      "iteration no 165: Loss: 0.7569366452765021, accuracy: 0.5133333333333333\n",
      "iteration no 166: Loss: 0.756926572376311, accuracy: 0.5133333333333333\n",
      "iteration no 167: Loss: 0.7569168137586638, accuracy: 0.5133333333333333\n",
      "iteration no 168: Loss: 0.7569073592253172, accuracy: 0.5133333333333333\n",
      "iteration no 169: Loss: 0.7568981989264443, accuracy: 0.5133333333333333\n",
      "iteration no 170: Loss: 0.7568893233479114, accuracy: 0.5133333333333333\n",
      "iteration no 171: Loss: 0.7568807232990575, accuracy: 0.5133333333333333\n",
      "iteration no 172: Loss: 0.7568723899009554, accuracy: 0.5133333333333333\n",
      "iteration no 173: Loss: 0.7568643145751335, accuracy: 0.5133333333333333\n",
      "iteration no 174: Loss: 0.7568564890327397, accuracy: 0.5133333333333333\n",
      "iteration no 175: Loss: 0.7568489052641284, accuracy: 0.5133333333333333\n",
      "iteration no 176: Loss: 0.7568415555288497, accuracy: 0.5133333333333333\n",
      "iteration no 177: Loss: 0.7568344323460281, accuracy: 0.5133333333333333\n",
      "iteration no 178: Loss: 0.7568275284851116, accuracy: 0.5133333333333333\n",
      "iteration no 179: Loss: 0.7568208369569746, accuracy: 0.5133333333333333\n",
      "iteration no 180: Loss: 0.7568143510053644, accuracy: 0.5133333333333333\n",
      "iteration no 181: Loss: 0.7568080640986711, accuracy: 0.5133333333333333\n",
      "iteration no 182: Loss: 0.756801969922012, accuracy: 0.5133333333333333\n",
      "iteration no 183: Loss: 0.7567960623696148, accuracy: 0.5133333333333333\n",
      "iteration no 184: Loss: 0.7567903355374898, accuracy: 0.5133333333333333\n",
      "iteration no 185: Loss: 0.756784783716375, accuracy: 0.5133333333333333\n",
      "iteration no 186: Loss: 0.7567794013849499, accuracy: 0.5133333333333333\n",
      "iteration no 187: Loss: 0.7567741832032988, accuracy: 0.5133333333333333\n",
      "iteration no 188: Loss: 0.7567691240066203, accuracy: 0.5133333333333333\n",
      "iteration no 189: Loss: 0.7567642187991683, accuracy: 0.5133333333333333\n",
      "iteration no 190: Loss: 0.75675946274842, accuracy: 0.5133333333333333\n",
      "iteration no 191: Loss: 0.7567548511794557, accuracy: 0.5133333333333333\n",
      "iteration no 192: Loss: 0.7567503795695478, accuracy: 0.5133333333333333\n",
      "iteration no 193: Loss: 0.7567460435429478, accuracy: 0.5133333333333333\n",
      "iteration no 194: Loss: 0.7567418388658625, accuracy: 0.5133333333333333\n",
      "iteration no 195: Loss: 0.7567377614416146, accuracy: 0.5133333333333333\n",
      "iteration no 196: Loss: 0.7567338073059787, accuracy: 0.5133333333333333\n",
      "iteration no 197: Loss: 0.7567299726226859, accuracy: 0.5133333333333333\n",
      "iteration no 198: Loss: 0.7567262536790922, accuracy: 0.5133333333333333\n",
      "iteration no 199: Loss: 0.7567226468819995, accuracy: 0.5133333333333333\n",
      "iteration no 200: Loss: 0.7567191487536318, accuracy: 0.5133333333333333\n",
      "iteration no 1: Loss: 1.1061427717451746, accuracy: 0.44333333333333336\n",
      "iteration no 2: Loss: 1.0745699410398728, accuracy: 0.4866666666666667\n",
      "iteration no 3: Loss: 1.0465168806800698, accuracy: 0.49333333333333335\n",
      "iteration no 4: Loss: 1.0215468008187218, accuracy: 0.49666666666666665\n",
      "iteration no 5: Loss: 0.9992792770353628, accuracy: 0.5066666666666667\n",
      "iteration no 6: Loss: 0.979381365417012, accuracy: 0.51\n",
      "iteration no 7: Loss: 0.9615618723170268, accuracy: 0.51\n",
      "iteration no 8: Loss: 0.9455667360236982, accuracy: 0.5066666666666667\n",
      "iteration no 9: Loss: 0.9311748316420011, accuracy: 0.51\n",
      "iteration no 10: Loss: 0.9181940409693363, accuracy: 0.5066666666666667\n",
      "iteration no 11: Loss: 0.9064576007472963, accuracy: 0.5066666666666667\n",
      "iteration no 12: Loss: 0.895820772713398, accuracy: 0.5066666666666667\n",
      "iteration no 13: Loss: 0.8861578629473742, accuracy: 0.5133333333333333\n",
      "iteration no 14: Loss: 0.8773595942496064, accuracy: 0.5133333333333333\n",
      "iteration no 15: Loss: 0.8693308163753651, accuracy: 0.5166666666666667\n",
      "iteration no 16: Loss: 0.8619885272652724, accuracy: 0.5166666666666667\n",
      "iteration no 17: Loss: 0.8552601728385657, accuracy: 0.5166666666666667\n",
      "iteration no 18: Loss: 0.8490821915824243, accuracy: 0.5166666666666667\n",
      "iteration no 19: Loss: 0.8433987714272912, accuracy: 0.5166666666666667\n",
      "iteration no 20: Loss: 0.8381607890415643, accuracy: 0.5166666666666667\n",
      "iteration no 21: Loss: 0.8333249049190019, accuracy: 0.5166666666666667\n",
      "iteration no 22: Loss: 0.8288527909891042, accuracy: 0.5166666666666667\n",
      "iteration no 23: Loss: 0.8247104706865073, accuracy: 0.51\n",
      "iteration no 24: Loss: 0.820867754336877, accuracy: 0.5133333333333333\n",
      "iteration no 25: Loss: 0.817297755302362, accuracy: 0.5133333333333333\n",
      "iteration no 26: Loss: 0.8139764745745173, accuracy: 0.5133333333333333\n",
      "iteration no 27: Loss: 0.8108824434266778, accuracy: 0.5133333333333333\n",
      "iteration no 28: Loss: 0.8079964153725556, accuracy: 0.5166666666666667\n",
      "iteration no 29: Loss: 0.8053011000586845, accuracy: 0.5166666666666667\n",
      "iteration no 30: Loss: 0.8027809328801967, accuracy: 0.5166666666666667\n",
      "iteration no 31: Loss: 0.8004218750846775, accuracy: 0.5166666666666667\n",
      "iteration no 32: Loss: 0.7982112399463706, accuracy: 0.5166666666666667\n",
      "iteration no 33: Loss: 0.796137541277989, accuracy: 0.5166666666666667\n",
      "iteration no 34: Loss: 0.7941903611213379, accuracy: 0.52\n",
      "iteration no 35: Loss: 0.7923602339391604, accuracy: 0.52\n",
      "iteration no 36: Loss: 0.7906385450344247, accuracy: 0.52\n",
      "iteration no 37: Loss: 0.7890174412625399, accuracy: 0.52\n",
      "iteration no 38: Loss: 0.7874897523874258, accuracy: 0.52\n",
      "iteration no 39: Loss: 0.7860489216728797, accuracy: 0.52\n",
      "iteration no 40: Loss: 0.7846889445037157, accuracy: 0.52\n",
      "iteration no 41: Loss: 0.7834043140028091, accuracy: 0.52\n",
      "iteration no 42: Loss: 0.7821899727556085, accuracy: 0.5166666666666667\n",
      "iteration no 43: Loss: 0.781041269877099, accuracy: 0.5166666666666667\n",
      "iteration no 44: Loss: 0.7799539227611603, accuracy: 0.5166666666666667\n",
      "iteration no 45: Loss: 0.7789239829416762, accuracy: 0.5133333333333333\n",
      "iteration no 46: Loss: 0.7779478055710959, accuracy: 0.5133333333333333\n",
      "iteration no 47: Loss: 0.7770220220874339, accuracy: 0.5133333333333333\n",
      "iteration no 48: Loss: 0.7761435156966504, accuracy: 0.5133333333333333\n",
      "iteration no 49: Loss: 0.7753093993453977, accuracy: 0.5133333333333333\n",
      "iteration no 50: Loss: 0.774516995900449, accuracy: 0.5133333333333333\n",
      "iteration no 51: Loss: 0.7737638202867491, accuracy: 0.5133333333333333\n",
      "iteration no 52: Loss: 0.7730475633667901, accuracy: 0.5133333333333333\n",
      "iteration no 53: Loss: 0.7723660773706224, accuracy: 0.5133333333333333\n",
      "iteration no 54: Loss: 0.7717173627088849, accuracy: 0.5133333333333333\n",
      "iteration no 55: Loss: 0.7710995560212535, accuracy: 0.5133333333333333\n",
      "iteration no 56: Loss: 0.7705109193301357, accuracy: 0.5133333333333333\n",
      "iteration no 57: Loss: 0.769949830184604, accuracy: 0.5133333333333333\n",
      "iteration no 58: Loss: 0.7694147726928195, accuracy: 0.5133333333333333\n",
      "iteration no 59: Loss: 0.7689043293527625, accuracy: 0.5133333333333333\n",
      "iteration no 60: Loss: 0.7684171736012391, accuracy: 0.5133333333333333\n",
      "iteration no 61: Loss: 0.7679520630100172, accuracy: 0.5133333333333333\n",
      "iteration no 62: Loss: 0.7675078330657671, accuracy: 0.5166666666666667\n",
      "iteration no 63: Loss: 0.767083391477346, accuracy: 0.5166666666666667\n",
      "iteration no 64: Loss: 0.7666777129600328, accuracy: 0.5166666666666667\n",
      "iteration no 65: Loss: 0.76628983445166, accuracy: 0.5166666666666667\n",
      "iteration no 66: Loss: 0.7659188507203146, accuracy: 0.5166666666666667\n",
      "iteration no 67: Loss: 0.7655639103274644, accuracy: 0.5166666666666667\n",
      "iteration no 68: Loss: 0.7652242119140693, accuracy: 0.5166666666666667\n",
      "iteration no 69: Loss: 0.7648990007805293, accuracy: 0.5166666666666667\n",
      "iteration no 70: Loss: 0.7645875657342426, accuracy: 0.5166666666666667\n",
      "iteration no 71: Loss: 0.7642892361811512, accuracy: 0.5166666666666667\n",
      "iteration no 72: Loss: 0.7640033794399695, accuracy: 0.5166666666666667\n",
      "iteration no 73: Loss: 0.7637293982598575, accuracy: 0.5166666666666667\n",
      "iteration no 74: Loss: 0.7634667285241534, accuracy: 0.5166666666666667\n",
      "iteration no 75: Loss: 0.7632148371244295, accuracy: 0.5166666666666667\n",
      "iteration no 76: Loss: 0.7629732199906161, accuracy: 0.5166666666666667\n",
      "iteration no 77: Loss: 0.7627414002642708, accuracy: 0.5166666666666667\n",
      "iteration no 78: Loss: 0.7625189266032503, accuracy: 0.5166666666666667\n",
      "iteration no 79: Loss: 0.7623053716071276, accuracy: 0.5166666666666667\n",
      "iteration no 80: Loss: 0.7621003303536525, accuracy: 0.5166666666666667\n",
      "iteration no 81: Loss: 0.761903419037414, accuracy: 0.5166666666666667\n",
      "iteration no 82: Loss: 0.7617142737026649, accuracy: 0.5133333333333333\n",
      "iteration no 83: Loss: 0.7615325490629555, accuracy: 0.5133333333333333\n",
      "iteration no 84: Loss: 0.7613579174008729, accuracy: 0.5133333333333333\n",
      "iteration no 85: Loss: 0.7611900675417531, accuracy: 0.5133333333333333\n",
      "iteration no 86: Loss: 0.7610287038957616, accuracy: 0.5133333333333333\n",
      "iteration no 87: Loss: 0.7608735455632054, accuracy: 0.5133333333333333\n",
      "iteration no 88: Loss: 0.7607243254983715, accuracy: 0.5133333333333333\n",
      "iteration no 89: Loss: 0.7605807897275796, accuracy: 0.5133333333333333\n",
      "iteration no 90: Loss: 0.7604426966174826, accuracy: 0.5133333333333333\n",
      "iteration no 91: Loss: 0.7603098161899842, accuracy: 0.5133333333333333\n",
      "iteration no 92: Loss: 0.760181929480419, accuracy: 0.5133333333333333\n",
      "iteration no 93: Loss: 0.760058827935926, accuracy: 0.5133333333333333\n",
      "iteration no 94: Loss: 0.7599403128511757, accuracy: 0.5133333333333333\n",
      "iteration no 95: Loss: 0.7598261948388403, accuracy: 0.5133333333333333\n",
      "iteration no 96: Loss: 0.7597162933324018, accuracy: 0.5133333333333333\n",
      "iteration no 97: Loss: 0.7596104361190704, accuracy: 0.5133333333333333\n",
      "iteration no 98: Loss: 0.759508458900766, accuracy: 0.5133333333333333\n",
      "iteration no 99: Loss: 0.7594102048812624, accuracy: 0.5133333333333333\n",
      "iteration no 100: Loss: 0.7593155243777427, accuracy: 0.5133333333333333\n",
      "iteration no 101: Loss: 0.7592242744551406, accuracy: 0.5133333333333333\n",
      "iteration no 102: Loss: 0.7591363185817673, accuracy: 0.5133333333333333\n",
      "iteration no 103: Loss: 0.7590515263048292, accuracy: 0.5133333333333333\n",
      "iteration no 104: Loss: 0.758969772944546, accuracy: 0.5166666666666667\n",
      "iteration no 105: Loss: 0.7588909393056718, accuracy: 0.5166666666666667\n",
      "iteration no 106: Loss: 0.7588149114053051, accuracy: 0.5166666666666667\n",
      "iteration no 107: Loss: 0.7587415802159587, accuracy: 0.5166666666666667\n",
      "iteration no 108: Loss: 0.7586708414229241, accuracy: 0.5166666666666667\n",
      "iteration no 109: Loss: 0.7586025951950418, accuracy: 0.5166666666666667\n",
      "iteration no 110: Loss: 0.7585367459680441, accuracy: 0.5166666666666667\n",
      "iteration no 111: Loss: 0.7584732022396985, accuracy: 0.5166666666666667\n",
      "iteration no 112: Loss: 0.7584118763760281, accuracy: 0.5166666666666667\n",
      "iteration no 113: Loss: 0.7583526844279426, accuracy: 0.5166666666666667\n",
      "iteration no 114: Loss: 0.7582955459576458, accuracy: 0.5166666666666667\n",
      "iteration no 115: Loss: 0.7582403838742438, accuracy: 0.5166666666666667\n",
      "iteration no 116: Loss: 0.7581871242779991, accuracy: 0.5166666666666667\n",
      "iteration no 117: Loss: 0.7581356963127293, accuracy: 0.5166666666666667\n",
      "iteration no 118: Loss: 0.7580860320258681, accuracy: 0.5166666666666667\n",
      "iteration no 119: Loss: 0.7580380662357455, accuracy: 0.5166666666666667\n",
      "iteration no 120: Loss: 0.7579917364056706, accuracy: 0.5166666666666667\n",
      "iteration no 121: Loss: 0.7579469825244269, accuracy: 0.5166666666666667\n",
      "iteration no 122: Loss: 0.757903746992814, accuracy: 0.5166666666666667\n",
      "iteration no 123: Loss: 0.7578619745158962, accuracy: 0.5166666666666667\n",
      "iteration no 124: Loss: 0.7578216120006339, accuracy: 0.5166666666666667\n",
      "iteration no 125: Loss: 0.7577826084586001, accuracy: 0.5166666666666667\n",
      "iteration no 126: Loss: 0.7577449149134989, accuracy: 0.5166666666666667\n",
      "iteration no 127: Loss: 0.7577084843132222, accuracy: 0.5166666666666667\n",
      "iteration no 128: Loss: 0.7576732714461941, accuracy: 0.5166666666666667\n",
      "iteration no 129: Loss: 0.7576392328617721, accuracy: 0.5133333333333333\n",
      "iteration no 130: Loss: 0.7576063267944851, accuracy: 0.5133333333333333\n",
      "iteration no 131: Loss: 0.7575745130919009, accuracy: 0.5133333333333333\n",
      "iteration no 132: Loss: 0.7575437531459314, accuracy: 0.5133333333333333\n",
      "iteration no 133: Loss: 0.7575140098273907, accuracy: 0.5133333333333333\n",
      "iteration no 134: Loss: 0.7574852474236375, accuracy: 0.5133333333333333\n",
      "iteration no 135: Loss: 0.7574574315791369, accuracy: 0.5133333333333333\n",
      "iteration no 136: Loss: 0.7574305292387921, accuracy: 0.5133333333333333\n",
      "iteration no 137: Loss: 0.7574045085939003, accuracy: 0.5133333333333333\n",
      "iteration no 138: Loss: 0.7573793390306001, accuracy: 0.5133333333333333\n",
      "iteration no 139: Loss: 0.7573549910806804, accuracy: 0.5133333333333333\n",
      "iteration no 140: Loss: 0.7573314363746322, accuracy: 0.5133333333333333\n",
      "iteration no 141: Loss: 0.7573086475968299, accuracy: 0.5133333333333333\n",
      "iteration no 142: Loss: 0.7572865984427316, accuracy: 0.5133333333333333\n",
      "iteration no 143: Loss: 0.7572652635780043, accuracy: 0.5133333333333333\n",
      "iteration no 144: Loss: 0.75724461859947, accuracy: 0.5133333333333333\n",
      "iteration no 145: Loss: 0.7572246399977891, accuracy: 0.5133333333333333\n",
      "iteration no 146: Loss: 0.7572053051217921, accuracy: 0.5133333333333333\n",
      "iteration no 147: Loss: 0.757186592144379, accuracy: 0.5133333333333333\n",
      "iteration no 148: Loss: 0.7571684800299135, accuracy: 0.5133333333333333\n",
      "iteration no 149: Loss: 0.7571509485030339, accuracy: 0.5133333333333333\n",
      "iteration no 150: Loss: 0.7571339780188181, accuracy: 0.5133333333333333\n",
      "iteration no 151: Loss: 0.7571175497342344, accuracy: 0.5133333333333333\n",
      "iteration no 152: Loss: 0.757101645480817, accuracy: 0.5133333333333333\n",
      "iteration no 153: Loss: 0.7570862477385119, accuracy: 0.5133333333333333\n",
      "iteration no 154: Loss: 0.7570713396106293, accuracy: 0.5133333333333333\n",
      "iteration no 155: Loss: 0.7570569047998635, accuracy: 0.5133333333333333\n",
      "iteration no 156: Loss: 0.7570429275853178, accuracy: 0.5133333333333333\n",
      "iteration no 157: Loss: 0.7570293928004946, accuracy: 0.5133333333333333\n",
      "iteration no 158: Loss: 0.7570162858122067, accuracy: 0.5133333333333333\n",
      "iteration no 159: Loss: 0.7570035925003618, accuracy: 0.5133333333333333\n",
      "iteration no 160: Loss: 0.7569912992385872, accuracy: 0.5133333333333333\n",
      "iteration no 161: Loss: 0.7569793928756509, accuracy: 0.5133333333333333\n",
      "iteration no 162: Loss: 0.7569678607176461, accuracy: 0.5133333333333333\n",
      "iteration no 163: Loss: 0.7569566905109031, accuracy: 0.5133333333333333\n",
      "iteration no 164: Loss: 0.7569458704255974, accuracy: 0.5133333333333333\n",
      "iteration no 165: Loss: 0.7569353890400218, accuracy: 0.5133333333333333\n",
      "iteration no 166: Loss: 0.7569252353254919, accuracy: 0.5133333333333333\n",
      "iteration no 167: Loss: 0.7569153986318611, accuracy: 0.5133333333333333\n",
      "iteration no 168: Loss: 0.7569058686736119, accuracy: 0.5133333333333333\n",
      "iteration no 169: Loss: 0.7568966355165052, accuracy: 0.5133333333333333\n",
      "iteration no 170: Loss: 0.7568876895647573, accuracy: 0.5133333333333333\n",
      "iteration no 171: Loss: 0.7568790215487253, accuracy: 0.5133333333333333\n",
      "iteration no 172: Loss: 0.7568706225130776, accuracy: 0.5133333333333333\n",
      "iteration no 173: Loss: 0.7568624838054284, accuracy: 0.5133333333333333\n",
      "iteration no 174: Loss: 0.7568545970654172, accuracy: 0.5133333333333333\n",
      "iteration no 175: Loss: 0.756846954214213, accuracy: 0.5133333333333333\n",
      "iteration no 176: Loss: 0.7568395474444268, accuracy: 0.5133333333333333\n",
      "iteration no 177: Loss: 0.7568323692104135, accuracy: 0.5133333333333333\n",
      "iteration no 178: Loss: 0.7568254122189477, accuracy: 0.5133333333333333\n",
      "iteration no 179: Loss: 0.756818669420257, accuracy: 0.5133333333333333\n",
      "iteration no 180: Loss: 0.7568121339993995, accuracy: 0.5133333333333333\n",
      "iteration no 181: Loss: 0.7568057993679692, accuracy: 0.5133333333333333\n",
      "iteration no 182: Loss: 0.7567996591561157, accuracy: 0.5133333333333333\n",
      "iteration no 183: Loss: 0.7567937072048675, accuracy: 0.5133333333333333\n",
      "iteration no 184: Loss: 0.7567879375587444, accuracy: 0.5133333333333333\n",
      "iteration no 185: Loss: 0.7567823444586471, accuracy: 0.5133333333333333\n",
      "iteration no 186: Loss: 0.7567769223350139, accuracy: 0.5133333333333333\n",
      "iteration no 187: Loss: 0.7567716658012329, accuracy: 0.5133333333333333\n",
      "iteration no 188: Loss: 0.756766569647299, accuracy: 0.5133333333333333\n",
      "iteration no 189: Loss: 0.7567616288337071, accuracy: 0.5133333333333333\n",
      "iteration no 190: Loss: 0.75675683848557, accuracy: 0.5133333333333333\n",
      "iteration no 191: Loss: 0.7567521938869547, accuracy: 0.5133333333333333\n",
      "iteration no 192: Loss: 0.7567476904754258, accuracy: 0.5133333333333333\n",
      "iteration no 193: Loss: 0.7567433238367888, accuracy: 0.5133333333333333\n",
      "iteration no 194: Loss: 0.756739089700026, accuracy: 0.5133333333333333\n",
      "iteration no 195: Loss: 0.7567349839324166, accuracy: 0.5133333333333333\n",
      "iteration no 196: Loss: 0.7567310025348342, accuracy: 0.5133333333333333\n",
      "iteration no 197: Loss: 0.7567271416372141, accuracy: 0.5133333333333333\n",
      "iteration no 198: Loss: 0.7567233974941844, accuracy: 0.5133333333333333\n",
      "iteration no 199: Loss: 0.7567197664808548, accuracy: 0.5133333333333333\n",
      "iteration no 200: Loss: 0.756716245088756, accuracy: 0.5133333333333333\n",
      "iteration no 1: Loss: 1.0793880206424165, accuracy: 0.49\n",
      "iteration no 2: Loss: 1.0509023537267805, accuracy: 0.49333333333333335\n",
      "iteration no 3: Loss: 1.0255359538586293, accuracy: 0.51\n",
      "iteration no 4: Loss: 1.002908386466752, accuracy: 0.52\n",
      "iteration no 5: Loss: 0.9826840072414537, accuracy: 0.52\n",
      "iteration no 6: Loss: 0.9645684181347952, accuracy: 0.5166666666666667\n",
      "iteration no 7: Loss: 0.9483045627225826, accuracy: 0.5133333333333333\n",
      "iteration no 8: Loss: 0.9336686821138068, accuracy: 0.51\n",
      "iteration no 9: Loss: 0.920466353962503, accuracy: 0.5133333333333333\n",
      "iteration no 10: Loss: 0.9085287729889878, accuracy: 0.5133333333333333\n",
      "iteration no 11: Loss: 0.8977093627634324, accuracy: 0.5166666666666667\n",
      "iteration no 12: Loss: 0.887880755046281, accuracy: 0.5166666666666667\n",
      "iteration no 13: Loss: 0.8789321373100448, accuracy: 0.5166666666666667\n",
      "iteration no 14: Loss: 0.870766947981311, accuracy: 0.5166666666666667\n",
      "iteration no 15: Loss: 0.8633008881792504, accuracy: 0.5166666666666667\n",
      "iteration no 16: Loss: 0.85646021463446, accuracy: 0.5166666666666667\n",
      "iteration no 17: Loss: 0.850180278372987, accuracy: 0.5166666666666667\n",
      "iteration no 18: Loss: 0.8444042758440987, accuracy: 0.51\n",
      "iteration no 19: Loss: 0.8390821823151743, accuracy: 0.5133333333333333\n",
      "iteration no 20: Loss: 0.8341698408653181, accuracy: 0.5133333333333333\n",
      "iteration no 21: Loss: 0.8296281837895284, accuracy: 0.5133333333333333\n",
      "iteration no 22: Loss: 0.8254225664718903, accuracy: 0.5133333333333333\n",
      "iteration no 23: Loss: 0.8215221967059796, accuracy: 0.5133333333333333\n",
      "iteration no 24: Loss: 0.8178996450056254, accuracy: 0.5133333333333333\n",
      "iteration no 25: Loss: 0.8145304236675676, accuracy: 0.5133333333333333\n",
      "iteration no 26: Loss: 0.811392624245981, accuracy: 0.5133333333333333\n",
      "iteration no 27: Loss: 0.8084666047116658, accuracy: 0.5133333333333333\n",
      "iteration no 28: Loss: 0.8057347189321673, accuracy: 0.5133333333333333\n",
      "iteration no 29: Loss: 0.8031810822579781, accuracy: 0.5133333333333333\n",
      "iteration no 30: Loss: 0.8007913679660914, accuracy: 0.5166666666666667\n",
      "iteration no 31: Loss: 0.7985526301236969, accuracy: 0.5166666666666667\n",
      "iteration no 32: Loss: 0.7964531491161805, accuracy: 0.5166666666666667\n",
      "iteration no 33: Loss: 0.7944822966557571, accuracy: 0.5133333333333333\n",
      "iteration no 34: Loss: 0.7926304175677825, accuracy: 0.5133333333333333\n",
      "iteration no 35: Loss: 0.790888726056027, accuracy: 0.5133333333333333\n",
      "iteration no 36: Loss: 0.7892492144884744, accuracy: 0.5166666666666667\n",
      "iteration no 37: Loss: 0.7877045730320595, accuracy: 0.5166666666666667\n",
      "iteration no 38: Loss: 0.7862481187068717, accuracy: 0.5166666666666667\n",
      "iteration no 39: Loss: 0.7848737326350799, accuracy: 0.5166666666666667\n",
      "iteration no 40: Loss: 0.7835758044331936, accuracy: 0.5133333333333333\n",
      "iteration no 41: Loss: 0.7823491828433533, accuracy: 0.5133333333333333\n",
      "iteration no 42: Loss: 0.781189131824334, accuracy: 0.5133333333333333\n",
      "iteration no 43: Loss: 0.7800912914293705, accuracy: 0.5133333333333333\n",
      "iteration no 44: Loss: 0.7790516428886751, accuracy: 0.5133333333333333\n",
      "iteration no 45: Loss: 0.7780664773921012, accuracy: 0.5133333333333333\n",
      "iteration no 46: Loss: 0.7771323681338038, accuracy: 0.5133333333333333\n",
      "iteration no 47: Loss: 0.7762461452377208, accuracy: 0.5133333333333333\n",
      "iteration no 48: Loss: 0.7754048732316421, accuracy: 0.5133333333333333\n",
      "iteration no 49: Loss: 0.7746058307797772, accuracy: 0.5133333333333333\n",
      "iteration no 50: Loss: 0.7738464924200773, accuracy: 0.5133333333333333\n",
      "iteration no 51: Loss: 0.7731245120839696, accuracy: 0.5133333333333333\n",
      "iteration no 52: Loss: 0.7724377082033476, accuracy: 0.5133333333333333\n",
      "iteration no 53: Loss: 0.7717840502332293, accuracy: 0.5133333333333333\n",
      "iteration no 54: Loss: 0.7711616464389698, accuracy: 0.5133333333333333\n",
      "iteration no 55: Loss: 0.770568732814731, accuracy: 0.5133333333333333\n",
      "iteration no 56: Loss: 0.770003663015435, accuracy: 0.5133333333333333\n",
      "iteration no 57: Loss: 0.769464899197987, accuracy: 0.5133333333333333\n",
      "iteration no 58: Loss: 0.7689510036794069, accuracy: 0.5133333333333333\n",
      "iteration no 59: Loss: 0.7684606313298895, accuracy: 0.5133333333333333\n",
      "iteration no 60: Loss: 0.767992522627923, accuracy: 0.5166666666666667\n",
      "iteration no 61: Loss: 0.7675454973126026, accuracy: 0.5166666666666667\n",
      "iteration no 62: Loss: 0.7671184485753094, accuracy: 0.5166666666666667\n",
      "iteration no 63: Loss: 0.766710337739142, accuracy: 0.5166666666666667\n",
      "iteration no 64: Loss: 0.7663201893799544, accuracy: 0.5166666666666667\n",
      "iteration no 65: Loss: 0.7659470868477101, accuracy: 0.5166666666666667\n",
      "iteration no 66: Loss: 0.7655901681511296, accuracy: 0.5166666666666667\n",
      "iteration no 67: Loss: 0.765248622172426, accuracy: 0.5166666666666667\n",
      "iteration no 68: Loss: 0.7649216851822787, accuracy: 0.5166666666666667\n",
      "iteration no 69: Loss: 0.7646086376282019, accuracy: 0.5166666666666667\n",
      "iteration no 70: Loss: 0.7643088011721272, accuracy: 0.5166666666666667\n",
      "iteration no 71: Loss: 0.7640215359553949, accuracy: 0.5166666666666667\n",
      "iteration no 72: Loss: 0.7637462380714709, accuracy: 0.5166666666666667\n",
      "iteration no 73: Loss: 0.7634823372285987, accuracy: 0.5166666666666667\n",
      "iteration no 74: Loss: 0.7632292945862896, accuracy: 0.5166666666666667\n",
      "iteration no 75: Loss: 0.7629866007510703, accuracy: 0.5166666666666667\n",
      "iteration no 76: Loss: 0.7627537739182708, accuracy: 0.5166666666666667\n",
      "iteration no 77: Loss: 0.7625303581478501, accuracy: 0.5166666666666667\n",
      "iteration no 78: Loss: 0.7623159217633572, accuracy: 0.5166666666666667\n",
      "iteration no 79: Loss: 0.7621100558641143, accuracy: 0.5133333333333333\n",
      "iteration no 80: Loss: 0.7619123729415904, accuracy: 0.5133333333333333\n",
      "iteration no 81: Loss: 0.7617225055917407, accuracy: 0.5133333333333333\n",
      "iteration no 82: Loss: 0.7615401053158084, accuracy: 0.5133333333333333\n",
      "iteration no 83: Loss: 0.7613648414027361, accuracy: 0.5133333333333333\n",
      "iteration no 84: Loss: 0.7611963998869276, accuracy: 0.5133333333333333\n",
      "iteration no 85: Loss: 0.761034482575634, accuracy: 0.5133333333333333\n",
      "iteration no 86: Loss: 0.7608788061407228, accuracy: 0.5133333333333333\n",
      "iteration no 87: Loss: 0.7607291012700266, accuracy: 0.5133333333333333\n",
      "iteration no 88: Loss: 0.7605851118738672, accuracy: 0.5133333333333333\n",
      "iteration no 89: Loss: 0.7604465943427179, accuracy: 0.5133333333333333\n",
      "iteration no 90: Loss: 0.7603133168522856, accuracy: 0.5133333333333333\n",
      "iteration no 91: Loss: 0.7601850587126067, accuracy: 0.5133333333333333\n",
      "iteration no 92: Loss: 0.7600616097580146, accuracy: 0.5133333333333333\n",
      "iteration no 93: Loss: 0.7599427697750906, accuracy: 0.5133333333333333\n",
      "iteration no 94: Loss: 0.7598283479659351, accuracy: 0.5133333333333333\n",
      "iteration no 95: Loss: 0.7597181624443069, accuracy: 0.5133333333333333\n",
      "iteration no 96: Loss: 0.7596120397623629, accuracy: 0.5133333333333333\n",
      "iteration no 97: Loss: 0.7595098144659087, accuracy: 0.5133333333333333\n",
      "iteration no 98: Loss: 0.7594113286762297, accuracy: 0.5133333333333333\n",
      "iteration no 99: Loss: 0.7593164316967131, accuracy: 0.5133333333333333\n",
      "iteration no 100: Loss: 0.7592249796426114, accuracy: 0.5133333333333333\n",
      "iteration no 101: Loss: 0.7591368350924145, accuracy: 0.5133333333333333\n",
      "iteration no 102: Loss: 0.7590518667594153, accuracy: 0.5133333333333333\n",
      "iteration no 103: Loss: 0.7589699491821535, accuracy: 0.5166666666666667\n",
      "iteration no 104: Loss: 0.7588909624325186, accuracy: 0.5166666666666667\n",
      "iteration no 105: Loss: 0.7588147918403806, accuracy: 0.5166666666666667\n",
      "iteration no 106: Loss: 0.758741327733699, accuracy: 0.5166666666666667\n",
      "iteration no 107: Loss: 0.7586704651931315, accuracy: 0.5166666666666667\n",
      "iteration no 108: Loss: 0.7586021038202371, accuracy: 0.5166666666666667\n",
      "iteration no 109: Loss: 0.7585361475184254, accuracy: 0.5166666666666667\n",
      "iteration no 110: Loss: 0.7584725042858693, accuracy: 0.5166666666666667\n",
      "iteration no 111: Loss: 0.7584110860196474, accuracy: 0.5166666666666667\n",
      "iteration no 112: Loss: 0.75835180833043, accuracy: 0.5166666666666667\n",
      "iteration no 113: Loss: 0.7582945903670806, accuracy: 0.5166666666666667\n",
      "iteration no 114: Loss: 0.7582393546505689, accuracy: 0.5166666666666667\n",
      "iteration no 115: Loss: 0.758186026916649, accuracy: 0.5166666666666667\n",
      "iteration no 116: Loss: 0.7581345359667814, accuracy: 0.5166666666666667\n",
      "iteration no 117: Loss: 0.7580848135268163, accuracy: 0.5166666666666667\n",
      "iteration no 118: Loss: 0.7580367941129854, accuracy: 0.5166666666666667\n",
      "iteration no 119: Loss: 0.7579904149047799, accuracy: 0.5166666666666667\n",
      "iteration no 120: Loss: 0.7579456156243187, accuracy: 0.5166666666666667\n",
      "iteration no 121: Loss: 0.757902338421837, accuracy: 0.5166666666666667\n",
      "iteration no 122: Loss: 0.7578605277669465, accuracy: 0.5166666666666667\n",
      "iteration no 123: Loss: 0.7578201303453447, accuracy: 0.5166666666666667\n",
      "iteration no 124: Loss: 0.7577810949606668, accuracy: 0.5166666666666667\n",
      "iteration no 125: Loss: 0.7577433724411929, accuracy: 0.5166666666666667\n",
      "iteration no 126: Loss: 0.7577069155511467, accuracy: 0.5166666666666667\n",
      "iteration no 127: Loss: 0.7576716789063287, accuracy: 0.5166666666666667\n",
      "iteration no 128: Loss: 0.7576376188938514, accuracy: 0.5166666666666667\n",
      "iteration no 129: Loss: 0.7576046935957511, accuracy: 0.5166666666666667\n",
      "iteration no 130: Loss: 0.7575728627162712, accuracy: 0.5166666666666667\n",
      "iteration no 131: Loss: 0.757542087512616, accuracy: 0.5166666666666667\n",
      "iteration no 132: Loss: 0.7575123307289947, accuracy: 0.5133333333333333\n",
      "iteration no 133: Loss: 0.7574835565337784, accuracy: 0.5133333333333333\n",
      "iteration no 134: Loss: 0.7574557304596088, accuracy: 0.5133333333333333\n",
      "iteration no 135: Loss: 0.757428819346303, accuracy: 0.5133333333333333\n",
      "iteration no 136: Loss: 0.7574027912864097, accuracy: 0.5133333333333333\n",
      "iteration no 137: Loss: 0.7573776155732795, accuracy: 0.5133333333333333\n",
      "iteration no 138: Loss: 0.7573532626515219, accuracy: 0.5133333333333333\n",
      "iteration no 139: Loss: 0.7573297040697246, accuracy: 0.5133333333333333\n",
      "iteration no 140: Loss: 0.7573069124353219, accuracy: 0.5133333333333333\n",
      "iteration no 141: Loss: 0.7572848613715051, accuracy: 0.5133333333333333\n",
      "iteration no 142: Loss: 0.7572635254760691, accuracy: 0.5133333333333333\n",
      "iteration no 143: Loss: 0.7572428802821012, accuracy: 0.5133333333333333\n",
      "iteration no 144: Loss: 0.7572229022204202, accuracy: 0.5133333333333333\n",
      "iteration no 145: Loss: 0.7572035685836799, accuracy: 0.5133333333333333\n",
      "iteration no 146: Loss: 0.7571848574920528, accuracy: 0.5133333333333333\n",
      "iteration no 147: Loss: 0.7571667478604212, accuracy: 0.5133333333333333\n",
      "iteration no 148: Loss: 0.7571492193669987, accuracy: 0.5133333333333333\n",
      "iteration no 149: Loss: 0.7571322524233145, accuracy: 0.5133333333333333\n",
      "iteration no 150: Loss: 0.7571158281454958, accuracy: 0.5133333333333333\n",
      "iteration no 151: Loss: 0.7570999283267857, accuracy: 0.5133333333333333\n",
      "iteration no 152: Loss: 0.757084535411237, accuracy: 0.5133333333333333\n",
      "iteration no 153: Loss: 0.7570696324685269, accuracy: 0.5133333333333333\n",
      "iteration no 154: Loss: 0.7570552031698418, accuracy: 0.5133333333333333\n",
      "iteration no 155: Loss: 0.7570412317647783, accuracy: 0.5133333333333333\n",
      "iteration no 156: Loss: 0.7570277030592171, accuracy: 0.5133333333333333\n",
      "iteration no 157: Loss: 0.7570146023941208, accuracy: 0.5133333333333333\n",
      "iteration no 158: Loss: 0.7570019156252167, accuracy: 0.5133333333333333\n",
      "iteration no 159: Loss: 0.7569896291035223, accuracy: 0.5133333333333333\n",
      "iteration no 160: Loss: 0.7569777296566738, accuracy: 0.5133333333333333\n",
      "iteration no 161: Loss: 0.7569662045710234, accuracy: 0.5133333333333333\n",
      "iteration no 162: Loss: 0.7569550415744697, accuracy: 0.5133333333333333\n",
      "iteration no 163: Loss: 0.7569442288199879, accuracy: 0.5133333333333333\n",
      "iteration no 164: Loss: 0.7569337548698291, accuracy: 0.5133333333333333\n",
      "iteration no 165: Loss: 0.756923608680357, accuracy: 0.5133333333333333\n",
      "iteration no 166: Loss: 0.7569137795874985, accuracy: 0.5133333333333333\n",
      "iteration no 167: Loss: 0.7569042572927738, accuracy: 0.5133333333333333\n",
      "iteration no 168: Loss: 0.756895031849888, accuracy: 0.5133333333333333\n",
      "iteration no 169: Loss: 0.7568860936518548, accuracy: 0.5133333333333333\n",
      "iteration no 170: Loss: 0.7568774334186292, accuracy: 0.5133333333333333\n",
      "iteration no 171: Loss: 0.7568690421852314, accuracy: 0.5133333333333333\n",
      "iteration no 172: Loss: 0.7568609112903352, accuracy: 0.5133333333333333\n",
      "iteration no 173: Loss: 0.7568530323653048, accuracy: 0.5133333333333333\n",
      "iteration no 174: Loss: 0.7568453973236589, accuracy: 0.5133333333333333\n",
      "iteration no 175: Loss: 0.756837998350945, accuracy: 0.5133333333333333\n",
      "iteration no 176: Loss: 0.7568308278950057, accuracy: 0.5133333333333333\n",
      "iteration no 177: Loss: 0.7568238786566198, accuracy: 0.5133333333333333\n",
      "iteration no 178: Loss: 0.7568171435805061, accuracy: 0.5133333333333333\n",
      "iteration no 179: Loss: 0.7568106158466683, accuracy: 0.5133333333333333\n",
      "iteration no 180: Loss: 0.7568042888620726, accuracy: 0.5133333333333333\n",
      "iteration no 181: Loss: 0.756798156252643, accuracy: 0.5133333333333333\n",
      "iteration no 182: Loss: 0.7567922118555569, accuracy: 0.5133333333333333\n",
      "iteration no 183: Loss: 0.7567864497118348, accuracy: 0.5133333333333333\n",
      "iteration no 184: Loss: 0.7567808640592081, accuracy: 0.5133333333333333\n",
      "iteration no 185: Loss: 0.7567754493252536, accuracy: 0.5133333333333333\n",
      "iteration no 186: Loss: 0.7567702001207876, accuracy: 0.5133333333333333\n",
      "iteration no 187: Loss: 0.756765111233502, accuracy: 0.5133333333333333\n",
      "iteration no 188: Loss: 0.7567601776218424, accuracy: 0.5133333333333333\n",
      "iteration no 189: Loss: 0.7567553944091079, accuracy: 0.5133333333333333\n",
      "iteration no 190: Loss: 0.7567507568777717, accuracy: 0.5133333333333333\n",
      "iteration no 191: Loss: 0.7567462604640108, accuracy: 0.5133333333333333\n",
      "iteration no 192: Loss: 0.7567419007524349, accuracy: 0.5133333333333333\n",
      "iteration no 193: Loss: 0.7567376734710092, accuracy: 0.5133333333333333\n",
      "iteration no 194: Loss: 0.7567335744861624, accuracy: 0.5133333333333333\n",
      "iteration no 195: Loss: 0.7567295997980729, accuracy: 0.5133333333333333\n",
      "iteration no 196: Loss: 0.7567257455361248, accuracy: 0.5133333333333333\n",
      "iteration no 197: Loss: 0.756722007954529, accuracy: 0.5133333333333333\n",
      "iteration no 198: Loss: 0.7567183834281026, accuracy: 0.5133333333333333\n",
      "iteration no 199: Loss: 0.7567148684481986, accuracy: 0.5133333333333333\n",
      "iteration no 200: Loss: 0.7567114596187831, accuracy: 0.5166666666666667\n",
      "iteration no 1: Loss: 1.1105445727623986, accuracy: 0.32666666666666666\n",
      "iteration no 2: Loss: 1.07873636226577, accuracy: 0.43\n",
      "iteration no 3: Loss: 1.0504570811635099, accuracy: 0.4633333333333333\n",
      "iteration no 4: Loss: 1.0252727998718822, accuracy: 0.4766666666666667\n",
      "iteration no 5: Loss: 1.0028021806199787, accuracy: 0.4766666666666667\n",
      "iteration no 6: Loss: 0.9827108163148882, accuracy: 0.49333333333333335\n",
      "iteration no 7: Loss: 0.9647064525949972, accuracy: 0.5033333333333333\n",
      "iteration no 8: Loss: 0.9485344705300636, accuracy: 0.5066666666666667\n",
      "iteration no 9: Loss: 0.9339735420913476, accuracy: 0.5066666666666667\n",
      "iteration no 10: Loss: 0.9208315275160566, accuracy: 0.5033333333333333\n",
      "iteration no 11: Loss: 0.9089416957898443, accuracy: 0.5066666666666667\n",
      "iteration no 12: Loss: 0.8981593184903912, accuracy: 0.5033333333333333\n",
      "iteration no 13: Loss: 0.8883586526697151, accuracy: 0.5033333333333333\n",
      "iteration no 14: Loss: 0.8794303024451576, accuracy: 0.5033333333333333\n",
      "iteration no 15: Loss: 0.8712789329368589, accuracy: 0.51\n",
      "iteration no 16: Loss: 0.8638213021326795, accuracy: 0.5166666666666667\n",
      "iteration no 17: Loss: 0.8569845736507989, accuracy: 0.5166666666666667\n",
      "iteration no 18: Loss: 0.8507048740987796, accuracy: 0.5166666666666667\n",
      "iteration no 19: Loss: 0.8449260612905835, accuracy: 0.5166666666666667\n",
      "iteration no 20: Loss: 0.8395986729951496, accuracy: 0.5166666666666667\n",
      "iteration no 21: Loss: 0.8346790295496355, accuracy: 0.5166666666666667\n",
      "iteration no 22: Loss: 0.8301284672337561, accuracy: 0.5166666666666667\n",
      "iteration no 23: Loss: 0.825912682590786, accuracy: 0.5166666666666667\n",
      "iteration no 24: Loss: 0.8220011708186519, accuracy: 0.5166666666666667\n",
      "iteration no 25: Loss: 0.8183667439231618, accuracy: 0.5133333333333333\n",
      "iteration no 26: Loss: 0.8149851165390158, accuracy: 0.51\n",
      "iteration no 27: Loss: 0.8118345492132395, accuracy: 0.51\n",
      "iteration no 28: Loss: 0.8088955405467756, accuracy: 0.5133333333333333\n",
      "iteration no 29: Loss: 0.8061505609408973, accuracy: 0.5133333333333333\n",
      "iteration no 30: Loss: 0.8035838218315469, accuracy: 0.5133333333333333\n",
      "iteration no 31: Loss: 0.8011810752489762, accuracy: 0.5133333333333333\n",
      "iteration no 32: Loss: 0.7989294393406631, accuracy: 0.5133333333333333\n",
      "iteration no 33: Loss: 0.7968172461669352, accuracy: 0.5133333333333333\n",
      "iteration no 34: Loss: 0.7948339086420643, accuracy: 0.5133333333333333\n",
      "iteration no 35: Loss: 0.7929698039664984, accuracy: 0.5166666666666667\n",
      "iteration no 36: Loss: 0.7912161712932934, accuracy: 0.5166666666666667\n",
      "iteration no 37: Loss: 0.7895650217061386, accuracy: 0.5166666666666667\n",
      "iteration no 38: Loss: 0.7880090588680533, accuracy: 0.5166666666666667\n",
      "iteration no 39: Loss: 0.7865416089375106, accuracy: 0.5166666666666667\n",
      "iteration no 40: Loss: 0.7851565585496425, accuracy: 0.5133333333333333\n",
      "iteration no 41: Loss: 0.7838482998302561, accuracy: 0.5166666666666667\n",
      "iteration no 42: Loss: 0.7826116815546685, accuracy: 0.5166666666666667\n",
      "iteration no 43: Loss: 0.7814419656859567, accuracy: 0.5133333333333333\n",
      "iteration no 44: Loss: 0.7803347886315859, accuracy: 0.5133333333333333\n",
      "iteration no 45: Loss: 0.7792861266464087, accuracy: 0.5133333333333333\n",
      "iteration no 46: Loss: 0.7782922648861038, accuracy: 0.5133333333333333\n",
      "iteration no 47: Loss: 0.7773497696802615, accuracy: 0.5133333333333333\n",
      "iteration no 48: Loss: 0.7764554636502083, accuracy: 0.5133333333333333\n",
      "iteration no 49: Loss: 0.7756064033446848, accuracy: 0.5133333333333333\n",
      "iteration no 50: Loss: 0.7747998591078483, accuracy: 0.5133333333333333\n",
      "iteration no 51: Loss: 0.7740332969297509, accuracy: 0.5133333333333333\n",
      "iteration no 52: Loss: 0.7733043620602678, accuracy: 0.5133333333333333\n",
      "iteration no 53: Loss: 0.7726108641941607, accuracy: 0.5133333333333333\n",
      "iteration no 54: Loss: 0.7719507640581013, accuracy: 0.5133333333333333\n",
      "iteration no 55: Loss: 0.7713221612506141, accuracy: 0.5133333333333333\n",
      "iteration no 56: Loss: 0.7707232832034029, accuracy: 0.5133333333333333\n",
      "iteration no 57: Loss: 0.770152475147793, accuracy: 0.5133333333333333\n",
      "iteration no 58: Loss: 0.7696081909833666, accuracy: 0.5133333333333333\n",
      "iteration no 59: Loss: 0.7690889849575238, accuracy: 0.5133333333333333\n",
      "iteration no 60: Loss: 0.7685935040749312, accuracy: 0.5133333333333333\n",
      "iteration no 61: Loss: 0.76812048116479, accuracy: 0.5133333333333333\n",
      "iteration no 62: Loss: 0.7676687285417375, accuracy: 0.5133333333333333\n",
      "iteration no 63: Loss: 0.7672371322031443, accuracy: 0.5166666666666667\n",
      "iteration no 64: Loss: 0.7668246465116835, accuracy: 0.5166666666666667\n",
      "iteration no 65: Loss: 0.7664302893174577, accuracy: 0.5166666666666667\n",
      "iteration no 66: Loss: 0.7660531374787458, accuracy: 0.5166666666666667\n",
      "iteration no 67: Loss: 0.7656923227446614, accuracy: 0.5166666666666667\n",
      "iteration no 68: Loss: 0.7653470279667739, accuracy: 0.5166666666666667\n",
      "iteration no 69: Loss: 0.7650164836100651, accuracy: 0.5166666666666667\n",
      "iteration no 70: Loss: 0.764699964536566, accuracy: 0.5166666666666667\n",
      "iteration no 71: Loss: 0.764396787037651, accuracy: 0.5166666666666667\n",
      "iteration no 72: Loss: 0.7641063060933186, accuracy: 0.5166666666666667\n",
      "iteration no 73: Loss: 0.7638279128388886, accuracy: 0.5166666666666667\n",
      "iteration no 74: Loss: 0.7635610322214177, accuracy: 0.5166666666666667\n",
      "iteration no 75: Loss: 0.7633051208298189, accuracy: 0.5166666666666667\n",
      "iteration no 76: Loss: 0.7630596648841685, accuracy: 0.5166666666666667\n",
      "iteration no 77: Loss: 0.7628241783710357, accuracy: 0.5166666666666667\n",
      "iteration no 78: Loss: 0.7625982013128813, accuracy: 0.5166666666666667\n",
      "iteration no 79: Loss: 0.7623812981606584, accuracy: 0.5166666666666667\n",
      "iteration no 80: Loss: 0.7621730562997291, accuracy: 0.5166666666666667\n",
      "iteration no 81: Loss: 0.7619730846600924, accuracy: 0.5166666666666667\n",
      "iteration no 82: Loss: 0.7617810124227168, accuracy: 0.5133333333333333\n",
      "iteration no 83: Loss: 0.7615964878144873, accuracy: 0.5133333333333333\n",
      "iteration no 84: Loss: 0.7614191769849266, accuracy: 0.5133333333333333\n",
      "iteration no 85: Loss: 0.7612487629584378, accuracy: 0.5133333333333333\n",
      "iteration no 86: Loss: 0.7610849446563486, accuracy: 0.5133333333333333\n",
      "iteration no 87: Loss: 0.7609274359835191, accuracy: 0.5133333333333333\n",
      "iteration no 88: Loss: 0.760775964974709, accuracy: 0.5133333333333333\n",
      "iteration no 89: Loss: 0.7606302729963059, accuracy: 0.5133333333333333\n",
      "iteration no 90: Loss: 0.7604901139993671, accuracy: 0.5133333333333333\n",
      "iteration no 91: Loss: 0.760355253820262, accuracy: 0.5133333333333333\n",
      "iteration no 92: Loss: 0.7602254695255038, accuracy: 0.5133333333333333\n",
      "iteration no 93: Loss: 0.7601005487976233, accuracy: 0.5133333333333333\n",
      "iteration no 94: Loss: 0.7599802893591926, accuracy: 0.5133333333333333\n",
      "iteration no 95: Loss: 0.7598644984323341, accuracy: 0.5133333333333333\n",
      "iteration no 96: Loss: 0.7597529922312534, accuracy: 0.5133333333333333\n",
      "iteration no 97: Loss: 0.759645595485526, accuracy: 0.5133333333333333\n",
      "iteration no 98: Loss: 0.7595421409920452, accuracy: 0.5133333333333333\n",
      "iteration no 99: Loss: 0.7594424691936886, accuracy: 0.5133333333333333\n",
      "iteration no 100: Loss: 0.7593464277829173, accuracy: 0.5133333333333333\n",
      "iteration no 101: Loss: 0.7592538713286456, accuracy: 0.5133333333333333\n",
      "iteration no 102: Loss: 0.7591646609248494, accuracy: 0.5133333333333333\n",
      "iteration no 103: Loss: 0.7590786638594863, accuracy: 0.5133333333333333\n",
      "iteration no 104: Loss: 0.7589957533024153, accuracy: 0.5166666666666667\n",
      "iteration no 105: Loss: 0.758915808011083, accuracy: 0.5166666666666667\n",
      "iteration no 106: Loss: 0.75883871205285, accuracy: 0.5166666666666667\n",
      "iteration no 107: Loss: 0.7587643545428941, accuracy: 0.5166666666666667\n",
      "iteration no 108: Loss: 0.7586926293967138, accuracy: 0.5166666666666667\n",
      "iteration no 109: Loss: 0.7586234350963191, accuracy: 0.5166666666666667\n",
      "iteration no 110: Loss: 0.7585566744692596, accuracy: 0.5166666666666667\n",
      "iteration no 111: Loss: 0.7584922544797006, accuracy: 0.5166666666666667\n",
      "iteration no 112: Loss: 0.7584300860308107, accuracy: 0.5166666666666667\n",
      "iteration no 113: Loss: 0.7583700837777739, accuracy: 0.5166666666666667\n",
      "iteration no 114: Loss: 0.7583121659507861, accuracy: 0.5166666666666667\n",
      "iteration no 115: Loss: 0.7582562541874417, accuracy: 0.5166666666666667\n",
      "iteration no 116: Loss: 0.7582022733739482, accuracy: 0.5166666666666667\n",
      "iteration no 117: Loss: 0.7581501514946508, accuracy: 0.5166666666666667\n",
      "iteration no 118: Loss: 0.7580998194893809, accuracy: 0.5166666666666667\n",
      "iteration no 119: Loss: 0.7580512111181713, accuracy: 0.5166666666666667\n",
      "iteration no 120: Loss: 0.7580042628329143, accuracy: 0.5166666666666667\n",
      "iteration no 121: Loss: 0.7579589136555642, accuracy: 0.5166666666666667\n",
      "iteration no 122: Loss: 0.7579151050625097, accuracy: 0.5166666666666667\n",
      "iteration no 123: Loss: 0.7578727808747686, accuracy: 0.5166666666666667\n",
      "iteration no 124: Loss: 0.7578318871536777, accuracy: 0.5166666666666667\n",
      "iteration no 125: Loss: 0.7577923721017675, accuracy: 0.5166666666666667\n",
      "iteration no 126: Loss: 0.7577541859685394, accuracy: 0.5166666666666667\n",
      "iteration no 127: Loss: 0.7577172809608692, accuracy: 0.5166666666666667\n",
      "iteration no 128: Loss: 0.7576816111577881, accuracy: 0.5166666666666667\n",
      "iteration no 129: Loss: 0.7576471324293994, accuracy: 0.5166666666666667\n",
      "iteration no 130: Loss: 0.7576138023597089, accuracy: 0.5166666666666667\n",
      "iteration no 131: Loss: 0.7575815801731587, accuracy: 0.5166666666666667\n",
      "iteration no 132: Loss: 0.7575504266646648, accuracy: 0.5166666666666667\n",
      "iteration no 133: Loss: 0.7575203041329744, accuracy: 0.5166666666666667\n",
      "iteration no 134: Loss: 0.7574911763171656, accuracy: 0.5133333333333333\n",
      "iteration no 135: Loss: 0.7574630083361268, accuracy: 0.5133333333333333\n",
      "iteration no 136: Loss: 0.757435766630858, accuracy: 0.5133333333333333\n",
      "iteration no 137: Loss: 0.7574094189094491, accuracy: 0.5133333333333333\n",
      "iteration no 138: Loss: 0.7573839340945971, accuracy: 0.5133333333333333\n",
      "iteration no 139: Loss: 0.7573592822735321, accuracy: 0.5133333333333333\n",
      "iteration no 140: Loss: 0.7573354346502276, accuracy: 0.5133333333333333\n",
      "iteration no 141: Loss: 0.7573123634997825, accuracy: 0.5133333333333333\n",
      "iteration no 142: Loss: 0.7572900421248625, accuracy: 0.5133333333333333\n",
      "iteration no 143: Loss: 0.7572684448140994, accuracy: 0.5133333333333333\n",
      "iteration no 144: Loss: 0.7572475468023496, accuracy: 0.5133333333333333\n",
      "iteration no 145: Loss: 0.7572273242327215, accuracy: 0.5133333333333333\n",
      "iteration no 146: Loss: 0.757207754120282, accuracy: 0.5133333333333333\n",
      "iteration no 147: Loss: 0.7571888143173615, accuracy: 0.5133333333333333\n",
      "iteration no 148: Loss: 0.7571704834803805, accuracy: 0.5133333333333333\n",
      "iteration no 149: Loss: 0.7571527410381215, accuracy: 0.5133333333333333\n",
      "iteration no 150: Loss: 0.7571355671613779, accuracy: 0.5133333333333333\n",
      "iteration no 151: Loss: 0.7571189427339131, accuracy: 0.5133333333333333\n",
      "iteration no 152: Loss: 0.757102849324671, accuracy: 0.5133333333333333\n",
      "iteration no 153: Loss: 0.75708726916117, accuracy: 0.5133333333333333\n",
      "iteration no 154: Loss: 0.7570721851040347, accuracy: 0.5133333333333333\n",
      "iteration no 155: Loss: 0.7570575806226058, accuracy: 0.5133333333333333\n",
      "iteration no 156: Loss: 0.7570434397715772, accuracy: 0.5133333333333333\n",
      "iteration no 157: Loss: 0.7570297471686176, accuracy: 0.5133333333333333\n",
      "iteration no 158: Loss: 0.7570164879729243, accuracy: 0.5133333333333333\n",
      "iteration no 159: Loss: 0.757003647864674, accuracy: 0.5133333333333333\n",
      "iteration no 160: Loss: 0.7569912130253212, accuracy: 0.5133333333333333\n",
      "iteration no 161: Loss: 0.756979170118713, accuracy: 0.5133333333333333\n",
      "iteration no 162: Loss: 0.7569675062729775, accuracy: 0.5133333333333333\n",
      "iteration no 163: Loss: 0.7569562090631549, accuracy: 0.5133333333333333\n",
      "iteration no 164: Loss: 0.7569452664945362, accuracy: 0.5133333333333333\n",
      "iteration no 165: Loss: 0.7569346669866763, accuracy: 0.5133333333333333\n",
      "iteration no 166: Loss: 0.7569243993580554, accuracy: 0.5133333333333333\n",
      "iteration no 167: Loss: 0.7569144528113563, accuracy: 0.5133333333333333\n",
      "iteration no 168: Loss: 0.7569048169193318, accuracy: 0.5133333333333333\n",
      "iteration no 169: Loss: 0.7568954816112374, accuracy: 0.5133333333333333\n",
      "iteration no 170: Loss: 0.7568864371598023, accuracy: 0.5133333333333333\n",
      "iteration no 171: Loss: 0.7568776741687172, accuracy: 0.5133333333333333\n",
      "iteration no 172: Loss: 0.7568691835606143, accuracy: 0.5133333333333333\n",
      "iteration no 173: Loss: 0.7568609565655227, accuracy: 0.5133333333333333\n",
      "iteration no 174: Loss: 0.7568529847097728, accuracy: 0.5133333333333333\n",
      "iteration no 175: Loss: 0.7568452598053351, accuracy: 0.5133333333333333\n",
      "iteration no 176: Loss: 0.756837773939573, accuracy: 0.5133333333333333\n",
      "iteration no 177: Loss: 0.7568305194653921, accuracy: 0.5133333333333333\n",
      "iteration no 178: Loss: 0.7568234889917698, accuracy: 0.5133333333333333\n",
      "iteration no 179: Loss: 0.7568166753746486, accuracy: 0.5133333333333333\n",
      "iteration no 180: Loss: 0.7568100717081793, accuracy: 0.5133333333333333\n",
      "iteration no 181: Loss: 0.7568036713162966, accuracy: 0.5133333333333333\n",
      "iteration no 182: Loss: 0.7567974677446168, accuracy: 0.5133333333333333\n",
      "iteration no 183: Loss: 0.7567914547526413, accuracy: 0.5133333333333333\n",
      "iteration no 184: Loss: 0.7567856263062555, accuracy: 0.5133333333333333\n",
      "iteration no 185: Loss: 0.75677997657051, accuracy: 0.5133333333333333\n",
      "iteration no 186: Loss: 0.756774499902672, accuracy: 0.5133333333333333\n",
      "iteration no 187: Loss: 0.7567691908455371, accuracy: 0.5133333333333333\n",
      "iteration no 188: Loss: 0.7567640441209902, accuracy: 0.5133333333333333\n",
      "iteration no 189: Loss: 0.7567590546238058, accuracy: 0.5133333333333333\n",
      "iteration no 190: Loss: 0.7567542174156776, accuracy: 0.5133333333333333\n",
      "iteration no 191: Loss: 0.756749527719469, accuracy: 0.5133333333333333\n",
      "iteration no 192: Loss: 0.7567449809136743, accuracy: 0.5133333333333333\n",
      "iteration no 193: Loss: 0.7567405725270844, accuracy: 0.5133333333333333\n",
      "iteration no 194: Loss: 0.7567362982336457, accuracy: 0.5133333333333333\n",
      "iteration no 195: Loss: 0.756732153847509, accuracy: 0.5133333333333333\n",
      "iteration no 196: Loss: 0.756728135318256, accuracy: 0.5133333333333333\n",
      "iteration no 197: Loss: 0.7567242387263005, accuracy: 0.5133333333333333\n",
      "iteration no 198: Loss: 0.7567204602784542, accuracy: 0.5133333333333333\n",
      "iteration no 199: Loss: 0.756716796303654, accuracy: 0.5133333333333333\n",
      "iteration no 200: Loss: 0.756713243248842, accuracy: 0.5133333333333333\n",
      "iteration no 1: Loss: 1.101808980594831, accuracy: 0.5166666666666667\n",
      "iteration no 2: Loss: 1.0708466791458002, accuracy: 0.5\n",
      "iteration no 3: Loss: 1.0433042686411955, accuracy: 0.49333333333333335\n",
      "iteration no 4: Loss: 1.0187626843666655, accuracy: 0.5\n",
      "iteration no 5: Loss: 0.9968552747259226, accuracy: 0.49666666666666665\n",
      "iteration no 6: Loss: 0.9772604603443305, accuracy: 0.49666666666666665\n",
      "iteration no 7: Loss: 0.9596966286321281, accuracy: 0.5033333333333333\n",
      "iteration no 8: Loss: 0.9439178045671689, accuracy: 0.5133333333333333\n",
      "iteration no 9: Loss: 0.9297096417903403, accuracy: 0.51\n",
      "iteration no 10: Loss: 0.9168856551322722, accuracy: 0.5066666666666667\n",
      "iteration no 11: Loss: 0.9052837264595145, accuracy: 0.51\n",
      "iteration no 12: Loss: 0.8947629257371896, accuracy: 0.51\n",
      "iteration no 13: Loss: 0.8852006694384664, accuracy: 0.51\n",
      "iteration no 14: Loss: 0.8764902161207375, accuracy: 0.51\n",
      "iteration no 15: Loss: 0.868538482452907, accuracy: 0.51\n",
      "iteration no 16: Loss: 0.8612641531782546, accuracy: 0.51\n",
      "iteration no 17: Loss: 0.8545960540573725, accuracy: 0.51\n",
      "iteration no 18: Loss: 0.8484717560215667, accuracy: 0.5133333333333333\n",
      "iteration no 19: Loss: 0.8428363801365956, accuracy: 0.51\n",
      "iteration no 20: Loss: 0.8376415755020598, accuracy: 0.51\n",
      "iteration no 21: Loss: 0.8328446452241959, accuracy: 0.51\n",
      "iteration no 22: Loss: 0.8284077986940974, accuracy: 0.51\n",
      "iteration no 23: Loss: 0.8242975113523883, accuracy: 0.51\n",
      "iteration no 24: Loss: 0.8204839758116981, accuracy: 0.51\n",
      "iteration no 25: Loss: 0.816940630595597, accuracy: 0.51\n",
      "iteration no 26: Loss: 0.8136437548325718, accuracy: 0.5133333333333333\n",
      "iteration no 27: Loss: 0.8105721190332466, accuracy: 0.5133333333333333\n",
      "iteration no 28: Loss: 0.8077066836057532, accuracy: 0.5133333333333333\n",
      "iteration no 29: Loss: 0.8050303380589139, accuracy: 0.5133333333333333\n",
      "iteration no 30: Loss: 0.8025276749366458, accuracy: 0.5133333333333333\n",
      "iteration no 31: Loss: 0.8001847934485454, accuracy: 0.5133333333333333\n",
      "iteration no 32: Loss: 0.7979891285369511, accuracy: 0.5133333333333333\n",
      "iteration no 33: Loss: 0.7959293017726163, accuracy: 0.5166666666666667\n",
      "iteration no 34: Loss: 0.7939949910190781, accuracy: 0.5166666666666667\n",
      "iteration no 35: Loss: 0.7921768162665908, accuracy: 0.5166666666666667\n",
      "iteration no 36: Loss: 0.79046623942427, accuracy: 0.5166666666666667\n",
      "iteration no 37: Loss: 0.7888554761857225, accuracy: 0.5166666666666667\n",
      "iteration no 38: Loss: 0.787337418358935, accuracy: 0.5166666666666667\n",
      "iteration no 39: Loss: 0.7859055652838501, accuracy: 0.5166666666666667\n",
      "iteration no 40: Loss: 0.7845539631578616, accuracy: 0.5166666666666667\n",
      "iteration no 41: Loss: 0.7832771512561845, accuracy: 0.5166666666666667\n",
      "iteration no 42: Loss: 0.7820701141755478, accuracy: 0.5166666666666667\n",
      "iteration no 43: Loss: 0.7809282393499478, accuracy: 0.5133333333333333\n",
      "iteration no 44: Loss: 0.779847279189637, accuracy: 0.5133333333333333\n",
      "iteration no 45: Loss: 0.7788233172819309, accuracy: 0.5133333333333333\n",
      "iteration no 46: Loss: 0.7778527381671261, accuracy: 0.5133333333333333\n",
      "iteration no 47: Loss: 0.7769322002667984, accuracy: 0.5133333333333333\n",
      "iteration no 48: Loss: 0.7760586115966374, accuracy: 0.5133333333333333\n",
      "iteration no 49: Loss: 0.7752291079431556, accuracy: 0.5133333333333333\n",
      "iteration no 50: Loss: 0.7744410332242264, accuracy: 0.5133333333333333\n",
      "iteration no 51: Loss: 0.7736919217884557, accuracy: 0.5133333333333333\n",
      "iteration no 52: Loss: 0.7729794824386688, accuracy: 0.5133333333333333\n",
      "iteration no 53: Loss: 0.7723015839910208, accuracy: 0.5133333333333333\n",
      "iteration no 54: Loss: 0.7716562422039697, accuracy: 0.5133333333333333\n",
      "iteration no 55: Loss: 0.7710416079311113, accuracy: 0.5133333333333333\n",
      "iteration no 56: Loss: 0.7704559563690618, accuracy: 0.5133333333333333\n",
      "iteration no 57: Loss: 0.7698976772865657, accuracy: 0.5166666666666667\n",
      "iteration no 58: Loss: 0.7693652661340866, accuracy: 0.5166666666666667\n",
      "iteration no 59: Loss: 0.7688573159445888, accuracy: 0.5166666666666667\n",
      "iteration no 60: Loss: 0.7683725099462367, accuracy: 0.5166666666666667\n",
      "iteration no 61: Loss: 0.7679096148165436, accuracy: 0.5166666666666667\n",
      "iteration no 62: Loss: 0.7674674745152257, accuracy: 0.5166666666666667\n",
      "iteration no 63: Loss: 0.7670450046398198, accuracy: 0.5166666666666667\n",
      "iteration no 64: Loss: 0.7666411872541287, accuracy: 0.5166666666666667\n",
      "iteration no 65: Loss: 0.766255066144834, accuracy: 0.5166666666666667\n",
      "iteration no 66: Loss: 0.7658857424663184, accuracy: 0.5166666666666667\n",
      "iteration no 67: Loss: 0.7655323707378578, accuracy: 0.5166666666666667\n",
      "iteration no 68: Loss: 0.7651941551610362, accuracy: 0.5166666666666667\n",
      "iteration no 69: Loss: 0.7648703462284793, accuracy: 0.5166666666666667\n",
      "iteration no 70: Loss: 0.7645602375979128, accuracy: 0.5166666666666667\n",
      "iteration no 71: Loss: 0.7642631632081216, accuracy: 0.5166666666666667\n",
      "iteration no 72: Loss: 0.7639784946156882, accuracy: 0.5166666666666667\n",
      "iteration no 73: Loss: 0.7637056385334388, accuracy: 0.5166666666666667\n",
      "iteration no 74: Loss: 0.7634440345533575, accuracy: 0.5166666666666667\n",
      "iteration no 75: Loss: 0.7631931530383683, accuracy: 0.5166666666666667\n",
      "iteration no 76: Loss: 0.7629524931688496, accuracy: 0.5166666666666667\n",
      "iteration no 77: Loss: 0.7627215811310686, accuracy: 0.5166666666666667\n",
      "iteration no 78: Loss: 0.7624999684358961, accuracy: 0.5166666666666667\n",
      "iteration no 79: Loss: 0.7622872303572321, accuracy: 0.5166666666666667\n",
      "iteration no 80: Loss: 0.7620829644805188, accuracy: 0.5166666666666667\n",
      "iteration no 81: Loss: 0.7618867893525886, accuracy: 0.5133333333333333\n",
      "iteration no 82: Loss: 0.7616983432248614, accuracy: 0.5133333333333333\n",
      "iteration no 83: Loss: 0.7615172828826084, accuracy: 0.5133333333333333\n",
      "iteration no 84: Loss: 0.7613432825536363, accuracy: 0.5133333333333333\n",
      "iteration no 85: Loss: 0.7611760328903111, accuracy: 0.5133333333333333\n",
      "iteration no 86: Loss: 0.7610152400193639, accuracy: 0.5133333333333333\n",
      "iteration no 87: Loss: 0.7608606246543881, accuracy: 0.5133333333333333\n",
      "iteration no 88: Loss: 0.7607119212663632, accuracy: 0.5133333333333333\n",
      "iteration no 89: Loss: 0.7605688773079287, accuracy: 0.5133333333333333\n",
      "iteration no 90: Loss: 0.7604312524874786, accuracy: 0.5133333333333333\n",
      "iteration no 91: Loss: 0.7602988180894752, accuracy: 0.5133333333333333\n",
      "iteration no 92: Loss: 0.760171356337658, accuracy: 0.5133333333333333\n",
      "iteration no 93: Loss: 0.7600486597981044, accuracy: 0.5133333333333333\n",
      "iteration no 94: Loss: 0.7599305308193282, accuracy: 0.5133333333333333\n",
      "iteration no 95: Loss: 0.7598167810068274, accuracy: 0.5133333333333333\n",
      "iteration no 96: Loss: 0.7597072307296961, accuracy: 0.5133333333333333\n",
      "iteration no 97: Loss: 0.7596017086570968, accuracy: 0.5133333333333333\n",
      "iteration no 98: Loss: 0.7595000513225564, accuracy: 0.5133333333333333\n",
      "iteration no 99: Loss: 0.7594021027142092, accuracy: 0.5133333333333333\n",
      "iteration no 100: Loss: 0.7593077138892463, accuracy: 0.5133333333333333\n",
      "iteration no 101: Loss: 0.7592167426109633, accuracy: 0.5133333333333333\n",
      "iteration no 102: Loss: 0.759129053006915, accuracy: 0.5166666666666667\n",
      "iteration no 103: Loss: 0.7590445152467994, accuracy: 0.5166666666666667\n",
      "iteration no 104: Loss: 0.7589630052387891, accuracy: 0.5166666666666667\n",
      "iteration no 105: Loss: 0.758884404343122, accuracy: 0.5166666666666667\n",
      "iteration no 106: Loss: 0.7588085991018514, accuracy: 0.5166666666666667\n",
      "iteration no 107: Loss: 0.7587354809837284, accuracy: 0.5166666666666667\n",
      "iteration no 108: Loss: 0.7586649461432686, accuracy: 0.5166666666666667\n",
      "iteration no 109: Loss: 0.7585968951931128, accuracy: 0.5166666666666667\n",
      "iteration no 110: Loss: 0.7585312329888627, accuracy: 0.5166666666666667\n",
      "iteration no 111: Loss: 0.7584678684256229, accuracy: 0.5166666666666667\n",
      "iteration no 112: Loss: 0.7584067142455333, accuracy: 0.5166666666666667\n",
      "iteration no 113: Loss: 0.7583476868556288, accuracy: 0.5166666666666667\n",
      "iteration no 114: Loss: 0.7582907061554018, accuracy: 0.5166666666666667\n",
      "iteration no 115: Loss: 0.7582356953734916, accuracy: 0.5166666666666667\n",
      "iteration no 116: Loss: 0.7581825809129554, accuracy: 0.5166666666666667\n",
      "iteration no 117: Loss: 0.7581312922046216, accuracy: 0.5166666666666667\n",
      "iteration no 118: Loss: 0.7580817615680446, accuracy: 0.5166666666666667\n",
      "iteration no 119: Loss: 0.7580339240796284, accuracy: 0.5166666666666667\n",
      "iteration no 120: Loss: 0.7579877174474982, accuracy: 0.5166666666666667\n",
      "iteration no 121: Loss: 0.7579430818927396, accuracy: 0.5166666666666667\n",
      "iteration no 122: Loss: 0.7578999600366388, accuracy: 0.5166666666666667\n",
      "iteration no 123: Loss: 0.7578582967935863, accuracy: 0.5166666666666667\n",
      "iteration no 124: Loss: 0.7578180392693269, accuracy: 0.5166666666666667\n",
      "iteration no 125: Loss: 0.7577791366642568, accuracy: 0.5166666666666667\n",
      "iteration no 126: Loss: 0.7577415401814888, accuracy: 0.5166666666666667\n",
      "iteration no 127: Loss: 0.7577052029394215, accuracy: 0.5166666666666667\n",
      "iteration no 128: Loss: 0.7576700798885708, accuracy: 0.5166666666666667\n",
      "iteration no 129: Loss: 0.7576361277324241, accuracy: 0.5166666666666667\n",
      "iteration no 130: Loss: 0.7576033048521089, accuracy: 0.5166666666666667\n",
      "iteration no 131: Loss: 0.7575715712346651, accuracy: 0.5133333333333333\n",
      "iteration no 132: Loss: 0.7575408884047299, accuracy: 0.5133333333333333\n",
      "iteration no 133: Loss: 0.7575112193594563, accuracy: 0.5133333333333333\n",
      "iteration no 134: Loss: 0.7574825285064924, accuracy: 0.5133333333333333\n",
      "iteration no 135: Loss: 0.7574547816048643, accuracy: 0.5133333333333333\n",
      "iteration no 136: Loss: 0.7574279457086069, accuracy: 0.5133333333333333\n",
      "iteration no 137: Loss: 0.7574019891130049, accuracy: 0.5133333333333333\n",
      "iteration no 138: Loss: 0.7573768813033079, accuracy: 0.5133333333333333\n",
      "iteration no 139: Loss: 0.7573525929057917, accuracy: 0.5133333333333333\n",
      "iteration no 140: Loss: 0.7573290956410491, accuracy: 0.5133333333333333\n",
      "iteration no 141: Loss: 0.7573063622793955, accuracy: 0.5133333333333333\n",
      "iteration no 142: Loss: 0.7572843665982846, accuracy: 0.5133333333333333\n",
      "iteration no 143: Loss: 0.7572630833416308, accuracy: 0.5133333333333333\n",
      "iteration no 144: Loss: 0.7572424881809486, accuracy: 0.5133333333333333\n",
      "iteration no 145: Loss: 0.7572225576782136, accuracy: 0.5133333333333333\n",
      "iteration no 146: Loss: 0.7572032692503636, accuracy: 0.5133333333333333\n",
      "iteration no 147: Loss: 0.7571846011353599, accuracy: 0.5133333333333333\n",
      "iteration no 148: Loss: 0.7571665323597312, accuracy: 0.5133333333333333\n",
      "iteration no 149: Loss: 0.7571490427075301, accuracy: 0.5133333333333333\n",
      "iteration no 150: Loss: 0.7571321126906316, accuracy: 0.5133333333333333\n",
      "iteration no 151: Loss: 0.7571157235203138, accuracy: 0.5133333333333333\n",
      "iteration no 152: Loss: 0.7570998570800541, accuracy: 0.5133333333333333\n",
      "iteration no 153: Loss: 0.7570844958994902, accuracy: 0.5133333333333333\n",
      "iteration no 154: Loss: 0.757069623129483, accuracy: 0.5133333333333333\n",
      "iteration no 155: Loss: 0.757055222518237, accuracy: 0.5133333333333333\n",
      "iteration no 156: Loss: 0.7570412783884246, accuracy: 0.5133333333333333\n",
      "iteration no 157: Loss: 0.757027775615269, accuracy: 0.5133333333333333\n",
      "iteration no 158: Loss: 0.7570146996055421, accuracy: 0.5133333333333333\n",
      "iteration no 159: Loss: 0.7570020362774337, accuracy: 0.5133333333333333\n",
      "iteration no 160: Loss: 0.7569897720412535, accuracy: 0.5133333333333333\n",
      "iteration no 161: Loss: 0.7569778937809281, accuracy: 0.5133333333333333\n",
      "iteration no 162: Loss: 0.7569663888362564, accuracy: 0.5133333333333333\n",
      "iteration no 163: Loss: 0.7569552449858884, accuracy: 0.5133333333333333\n",
      "iteration no 164: Loss: 0.7569444504309972, accuracy: 0.5133333333333333\n",
      "iteration no 165: Loss: 0.756933993779612, accuracy: 0.5133333333333333\n",
      "iteration no 166: Loss: 0.7569238640315815, accuracy: 0.5133333333333333\n",
      "iteration no 167: Loss: 0.7569140505641427, accuracy: 0.5133333333333333\n",
      "iteration no 168: Loss: 0.7569045431180653, accuracy: 0.5133333333333333\n",
      "iteration no 169: Loss: 0.7568953317843494, accuracy: 0.5133333333333333\n",
      "iteration no 170: Loss: 0.7568864069914494, accuracy: 0.5133333333333333\n",
      "iteration no 171: Loss: 0.7568777594930048, accuracy: 0.5133333333333333\n",
      "iteration no 172: Loss: 0.7568693803560523, accuracy: 0.5133333333333333\n",
      "iteration no 173: Loss: 0.7568612609497012, accuracy: 0.5133333333333333\n",
      "iteration no 174: Loss: 0.7568533929342501, accuracy: 0.5133333333333333\n",
      "iteration no 175: Loss: 0.7568457682507295, accuracy: 0.5133333333333333\n",
      "iteration no 176: Loss: 0.756838379110847, accuracy: 0.5133333333333333\n",
      "iteration no 177: Loss: 0.7568312179873233, accuracy: 0.5133333333333333\n",
      "iteration no 178: Loss: 0.7568242776045994, accuracy: 0.5133333333333333\n",
      "iteration no 179: Loss: 0.7568175509298996, accuracy: 0.5133333333333333\n",
      "iteration no 180: Loss: 0.7568110311646372, accuracy: 0.5133333333333333\n",
      "iteration no 181: Loss: 0.756804711736148, accuracy: 0.5133333333333333\n",
      "iteration no 182: Loss: 0.7567985862897346, accuracy: 0.5133333333333333\n",
      "iteration no 183: Loss: 0.7567926486810158, accuracy: 0.5133333333333333\n",
      "iteration no 184: Loss: 0.7567868929685609, accuracy: 0.5133333333333333\n",
      "iteration no 185: Loss: 0.7567813134068038, accuracy: 0.5133333333333333\n",
      "iteration no 186: Loss: 0.7567759044392185, accuracy: 0.5133333333333333\n",
      "iteration no 187: Loss: 0.7567706606917537, accuracy: 0.5133333333333333\n",
      "iteration no 188: Loss: 0.7567655769665085, accuracy: 0.5133333333333333\n",
      "iteration no 189: Loss: 0.7567606482356444, accuracy: 0.5133333333333333\n",
      "iteration no 190: Loss: 0.7567558696355204, accuracy: 0.5133333333333333\n",
      "iteration no 191: Loss: 0.7567512364610472, accuracy: 0.5133333333333333\n",
      "iteration no 192: Loss: 0.7567467441602456, accuracy: 0.5133333333333333\n",
      "iteration no 193: Loss: 0.7567423883290058, accuracy: 0.5133333333333333\n",
      "iteration no 194: Loss: 0.7567381647060384, accuracy: 0.5133333333333333\n",
      "iteration no 195: Loss: 0.756734069168008, accuracy: 0.5133333333333333\n",
      "iteration no 196: Loss: 0.7567300977248441, accuracy: 0.5133333333333333\n",
      "iteration no 197: Loss: 0.7567262465152224, accuracy: 0.5133333333333333\n",
      "iteration no 198: Loss: 0.756722511802206, accuracy: 0.5133333333333333\n",
      "iteration no 199: Loss: 0.7567188899690487, accuracy: 0.5133333333333333\n",
      "iteration no 200: Loss: 0.7567153775151432, accuracy: 0.5133333333333333\n",
      "iteration no 1: Loss: 1.0900199254219667, accuracy: 0.49\n",
      "iteration no 2: Loss: 1.0600590184225696, accuracy: 0.5033333333333333\n",
      "iteration no 3: Loss: 1.0334203787444507, accuracy: 0.5166666666666667\n",
      "iteration no 4: Loss: 1.0096994823451075, accuracy: 0.5\n",
      "iteration no 5: Loss: 0.9885386615875899, accuracy: 0.5066666666666667\n",
      "iteration no 6: Loss: 0.9696229622173238, accuracy: 0.5066666666666667\n",
      "iteration no 7: Loss: 0.9526764049169155, accuracy: 0.5133333333333333\n",
      "iteration no 8: Loss: 0.9374581916885909, accuracy: 0.5166666666666667\n",
      "iteration no 9: Loss: 0.9237588720627263, accuracy: 0.52\n",
      "iteration no 10: Loss: 0.91139660677306, accuracy: 0.5233333333333333\n",
      "iteration no 11: Loss: 0.9002136569091458, accuracy: 0.5233333333333333\n",
      "iteration no 12: Loss: 0.8900731806161596, accuracy: 0.5233333333333333\n",
      "iteration no 13: Loss: 0.8808563742551967, accuracy: 0.5233333333333333\n",
      "iteration no 14: Loss: 0.8724599615452024, accuracy: 0.5233333333333333\n",
      "iteration no 15: Loss: 0.8647940132009289, accuracy: 0.52\n",
      "iteration no 16: Loss: 0.8577800682985094, accuracy: 0.52\n",
      "iteration no 17: Loss: 0.8513495239693913, accuracy: 0.5166666666666667\n",
      "iteration no 18: Loss: 0.8454422595220258, accuracy: 0.5166666666666667\n",
      "iteration no 19: Loss: 0.8400054629180514, accuracy: 0.5166666666666667\n",
      "iteration no 20: Loss: 0.8349926305036849, accuracy: 0.5166666666666667\n",
      "iteration no 21: Loss: 0.8303627142907033, accuracy: 0.5166666666666667\n",
      "iteration no 22: Loss: 0.8260793944759458, accuracy: 0.5166666666666667\n",
      "iteration no 23: Loss: 0.8221104580620173, accuracy: 0.5166666666666667\n",
      "iteration no 24: Loss: 0.8184272672936501, accuracy: 0.5166666666666667\n",
      "iteration no 25: Loss: 0.815004304123112, accuracy: 0.5166666666666667\n",
      "iteration no 26: Loss: 0.811818779071876, accuracy: 0.52\n",
      "iteration no 27: Loss: 0.8088502946917236, accuracy: 0.52\n",
      "iteration no 28: Loss: 0.8060805553818146, accuracy: 0.52\n",
      "iteration no 29: Loss: 0.8034931166260713, accuracy: 0.52\n",
      "iteration no 30: Loss: 0.8010731678128438, accuracy: 0.5166666666666667\n",
      "iteration no 31: Loss: 0.7988073437183215, accuracy: 0.52\n",
      "iteration no 32: Loss: 0.7966835605047891, accuracy: 0.52\n",
      "iteration no 33: Loss: 0.7946908727288762, accuracy: 0.5166666666666667\n",
      "iteration no 34: Loss: 0.7928193483941501, accuracy: 0.5166666666666667\n",
      "iteration no 35: Loss: 0.7910599595341513, accuracy: 0.5166666666666667\n",
      "iteration no 36: Loss: 0.7894044861908905, accuracy: 0.5166666666666667\n",
      "iteration no 37: Loss: 0.7878454319720791, accuracy: 0.5166666666666667\n",
      "iteration no 38: Loss: 0.7863759496380684, accuracy: 0.5166666666666667\n",
      "iteration no 39: Loss: 0.7849897753950491, accuracy: 0.5166666666666667\n",
      "iteration no 40: Loss: 0.7836811707614518, accuracy: 0.5166666666666667\n",
      "iteration no 41: Loss: 0.7824448710355079, accuracy: 0.5166666666666667\n",
      "iteration no 42: Loss: 0.7812760395283407, accuracy: 0.5166666666666667\n",
      "iteration no 43: Loss: 0.7801702268427663, accuracy: 0.5166666666666667\n",
      "iteration no 44: Loss: 0.7791233345764771, accuracy: 0.5166666666666667\n",
      "iteration no 45: Loss: 0.7781315829122355, accuracy: 0.5166666666666667\n",
      "iteration no 46: Loss: 0.777191481629382, accuracy: 0.5166666666666667\n",
      "iteration no 47: Loss: 0.7762998041323015, accuracy: 0.5166666666666667\n",
      "iteration no 48: Loss: 0.7754535641440728, accuracy: 0.5166666666666667\n",
      "iteration no 49: Loss: 0.7746499947586899, accuracy: 0.5166666666666667\n",
      "iteration no 50: Loss: 0.7738865295841176, accuracy: 0.5166666666666667\n",
      "iteration no 51: Loss: 0.7731607857419617, accuracy: 0.5166666666666667\n",
      "iteration no 52: Loss: 0.7724705485184808, accuracy: 0.5166666666666667\n",
      "iteration no 53: Loss: 0.7718137574867382, accuracy: 0.5166666666666667\n",
      "iteration no 54: Loss: 0.7711884939414094, accuracy: 0.5166666666666667\n",
      "iteration no 55: Loss: 0.7705929695066402, accuracy: 0.5166666666666667\n",
      "iteration no 56: Loss: 0.7700255157937705, accuracy: 0.5133333333333333\n",
      "iteration no 57: Loss: 0.7694845750000576, accuracy: 0.5133333333333333\n",
      "iteration no 58: Loss: 0.7689686913520228, accuracy: 0.5133333333333333\n",
      "iteration no 59: Loss: 0.7684765033079887, accuracy: 0.5133333333333333\n",
      "iteration no 60: Loss: 0.7680067364439422, accuracy: 0.5133333333333333\n",
      "iteration no 61: Loss: 0.7675581969552661, accuracy: 0.5133333333333333\n",
      "iteration no 62: Loss: 0.7671297657142611, accuracy: 0.5133333333333333\n",
      "iteration no 63: Loss: 0.7667203928298925, accuracy: 0.5133333333333333\n",
      "iteration no 64: Loss: 0.7663290926619103, accuracy: 0.5166666666666667\n",
      "iteration no 65: Loss: 0.7659549392465635, accuracy: 0.5166666666666667\n",
      "iteration no 66: Loss: 0.7655970620955965, accuracy: 0.5166666666666667\n",
      "iteration no 67: Loss: 0.7652546423341753, accuracy: 0.5166666666666667\n",
      "iteration no 68: Loss: 0.7649269091469068, accuracy: 0.5166666666666667\n",
      "iteration no 69: Loss: 0.7646131365042297, accuracy: 0.5166666666666667\n",
      "iteration no 70: Loss: 0.7643126401442258, accuracy: 0.5166666666666667\n",
      "iteration no 71: Loss: 0.7640247747873755, accuracy: 0.5166666666666667\n",
      "iteration no 72: Loss: 0.7637489315639701, accuracy: 0.5166666666666667\n",
      "iteration no 73: Loss: 0.7634845356358698, accuracy: 0.5166666666666667\n",
      "iteration no 74: Loss: 0.7632310439960414, accuracy: 0.5166666666666667\n",
      "iteration no 75: Loss: 0.7629879434308838, accuracy: 0.5166666666666667\n",
      "iteration no 76: Loss: 0.7627547486317557, accuracy: 0.5166666666666667\n",
      "iteration no 77: Loss: 0.7625310004433821, accuracy: 0.5166666666666667\n",
      "iteration no 78: Loss: 0.7623162642379475, accuracy: 0.5166666666666667\n",
      "iteration no 79: Loss: 0.7621101284047022, accuracy: 0.5133333333333333\n",
      "iteration no 80: Loss: 0.7619122029458275, accuracy: 0.5133333333333333\n",
      "iteration no 81: Loss: 0.7617221181701246, accuracy: 0.5133333333333333\n",
      "iteration no 82: Loss: 0.7615395234768474, accuracy: 0.5133333333333333\n",
      "iteration no 83: Loss: 0.7613640862226582, accuracy: 0.5133333333333333\n",
      "iteration no 84: Loss: 0.7611954906653052, accuracy: 0.5133333333333333\n",
      "iteration no 85: Loss: 0.7610334369781642, accuracy: 0.5133333333333333\n",
      "iteration no 86: Loss: 0.7608776403302807, accuracy: 0.5133333333333333\n",
      "iteration no 87: Loss: 0.7607278300270148, accuracy: 0.5133333333333333\n",
      "iteration no 88: Loss: 0.76058374870678, accuracy: 0.5133333333333333\n",
      "iteration no 89: Loss: 0.760445151589755, accuracy: 0.5133333333333333\n",
      "iteration no 90: Loss: 0.7603118057747819, accuracy: 0.5133333333333333\n",
      "iteration no 91: Loss: 0.7601834895809625, accuracy: 0.5133333333333333\n",
      "iteration no 92: Loss: 0.7600599919307616, accuracy: 0.5133333333333333\n",
      "iteration no 93: Loss: 0.75994111177166, accuracy: 0.5133333333333333\n",
      "iteration no 94: Loss: 0.7598266575336554, accuracy: 0.5133333333333333\n",
      "iteration no 95: Loss: 0.759716446620103, accuracy: 0.5133333333333333\n",
      "iteration no 96: Loss: 0.7596103049295929, accuracy: 0.5133333333333333\n",
      "iteration no 97: Loss: 0.7595080664067347, accuracy: 0.5133333333333333\n",
      "iteration no 98: Loss: 0.7594095726198827, accuracy: 0.5133333333333333\n",
      "iteration no 99: Loss: 0.7593146723639863, accuracy: 0.5133333333333333\n",
      "iteration no 100: Loss: 0.7592232212868815, accuracy: 0.5133333333333333\n",
      "iteration no 101: Loss: 0.7591350815374712, accuracy: 0.5133333333333333\n",
      "iteration no 102: Loss: 0.7590501214343512, accuracy: 0.5133333333333333\n",
      "iteration no 103: Loss: 0.7589682151535468, accuracy: 0.5133333333333333\n",
      "iteration no 104: Loss: 0.7588892424341233, accuracy: 0.5133333333333333\n",
      "iteration no 105: Loss: 0.7588130883005193, accuracy: 0.5166666666666667\n",
      "iteration no 106: Loss: 0.7587396428005382, accuracy: 0.5166666666666667\n",
      "iteration no 107: Loss: 0.7586688007580034, accuracy: 0.5166666666666667\n",
      "iteration no 108: Loss: 0.7586004615391606, accuracy: 0.5166666666666667\n",
      "iteration no 109: Loss: 0.7585345288319658, accuracy: 0.5166666666666667\n",
      "iteration no 110: Loss: 0.7584709104374636, accuracy: 0.5166666666666667\n",
      "iteration no 111: Loss: 0.7584095180725141, accuracy: 0.5166666666666667\n",
      "iteration no 112: Loss: 0.7583502671831722, accuracy: 0.5166666666666667\n",
      "iteration no 113: Loss: 0.7582930767680806, accuracy: 0.5166666666666667\n",
      "iteration no 114: Loss: 0.7582378692112686, accuracy: 0.5166666666666667\n",
      "iteration no 115: Loss: 0.7581845701238, accuracy: 0.5166666666666667\n",
      "iteration no 116: Loss: 0.7581331081937422, accuracy: 0.5166666666666667\n",
      "iteration no 117: Loss: 0.7580834150439698, accuracy: 0.5166666666666667\n",
      "iteration no 118: Loss: 0.7580354250973426, accuracy: 0.5166666666666667\n",
      "iteration no 119: Loss: 0.7579890754488321, accuracy: 0.5166666666666667\n",
      "iteration no 120: Loss: 0.7579443057441944, accuracy: 0.5166666666666667\n",
      "iteration no 121: Loss: 0.7579010580648137, accuracy: 0.5166666666666667\n",
      "iteration no 122: Loss: 0.7578592768183717, accuracy: 0.5166666666666667\n",
      "iteration no 123: Loss: 0.7578189086350027, accuracy: 0.5166666666666667\n",
      "iteration no 124: Loss: 0.7577799022686394, accuracy: 0.5166666666666667\n",
      "iteration no 125: Loss: 0.7577422085032497, accuracy: 0.5166666666666667\n",
      "iteration no 126: Loss: 0.7577057800636985, accuracy: 0.5166666666666667\n",
      "iteration no 127: Loss: 0.7576705715309794, accuracy: 0.5166666666666667\n",
      "iteration no 128: Loss: 0.7576365392615776, accuracy: 0.5133333333333333\n",
      "iteration no 129: Loss: 0.7576036413107375, accuracy: 0.5133333333333333\n",
      "iteration no 130: Loss: 0.7575718373594272, accuracy: 0.5133333333333333\n",
      "iteration no 131: Loss: 0.7575410886447971, accuracy: 0.5133333333333333\n",
      "iteration no 132: Loss: 0.7575113578939527, accuracy: 0.5133333333333333\n",
      "iteration no 133: Loss: 0.7574826092608595, accuracy: 0.5133333333333333\n",
      "iteration no 134: Loss: 0.7574548082662185, accuracy: 0.5133333333333333\n",
      "iteration no 135: Loss: 0.7574279217401555, accuracy: 0.5133333333333333\n",
      "iteration no 136: Loss: 0.7574019177675795, accuracy: 0.5133333333333333\n",
      "iteration no 137: Loss: 0.7573767656360685, accuracy: 0.5133333333333333\n",
      "iteration no 138: Loss: 0.7573524357861569, accuracy: 0.5133333333333333\n",
      "iteration no 139: Loss: 0.7573288997638981, accuracy: 0.5133333333333333\n",
      "iteration no 140: Loss: 0.757306130175588, accuracy: 0.5133333333333333\n",
      "iteration no 141: Loss: 0.757284100644542, accuracy: 0.5133333333333333\n",
      "iteration no 142: Loss: 0.7572627857698182, accuracy: 0.5133333333333333\n",
      "iteration no 143: Loss: 0.7572421610867927, accuracy: 0.5133333333333333\n",
      "iteration no 144: Loss: 0.757222203029494, accuracy: 0.5133333333333333\n",
      "iteration no 145: Loss: 0.7572028888946094, accuracy: 0.5133333333333333\n",
      "iteration no 146: Loss: 0.7571841968070802, accuracy: 0.5133333333333333\n",
      "iteration no 147: Loss: 0.7571661056872105, accuracy: 0.5133333333333333\n",
      "iteration no 148: Loss: 0.7571485952192146, accuracy: 0.5133333333333333\n",
      "iteration no 149: Loss: 0.7571316458211298, accuracy: 0.5133333333333333\n",
      "iteration no 150: Loss: 0.7571152386160377, accuracy: 0.5133333333333333\n",
      "iteration no 151: Loss: 0.7570993554045216, accuracy: 0.5133333333333333\n",
      "iteration no 152: Loss: 0.7570839786383075, accuracy: 0.5133333333333333\n",
      "iteration no 153: Loss: 0.7570690913950302, accuracy: 0.5133333333333333\n",
      "iteration no 154: Loss: 0.7570546773540723, accuracy: 0.5133333333333333\n",
      "iteration no 155: Loss: 0.7570407207734243, accuracy: 0.5133333333333333\n",
      "iteration no 156: Loss: 0.7570272064675205, accuracy: 0.5133333333333333\n",
      "iteration no 157: Loss: 0.7570141197860034, accuracy: 0.5133333333333333\n",
      "iteration no 158: Loss: 0.7570014465933749, accuracy: 0.5133333333333333\n",
      "iteration no 159: Loss: 0.7569891732494922, accuracy: 0.5133333333333333\n",
      "iteration no 160: Loss: 0.7569772865908719, accuracy: 0.5133333333333333\n",
      "iteration no 161: Loss: 0.7569657739127635, accuracy: 0.5133333333333333\n",
      "iteration no 162: Loss: 0.7569546229519577, accuracy: 0.5133333333333333\n",
      "iteration no 163: Loss: 0.7569438218702985, accuracy: 0.5133333333333333\n",
      "iteration no 164: Loss: 0.756933359238864, accuracy: 0.5133333333333333\n",
      "iteration no 165: Loss: 0.7569232240227893, accuracy: 0.5133333333333333\n",
      "iteration no 166: Loss: 0.7569134055667012, accuracy: 0.5133333333333333\n",
      "iteration no 167: Loss: 0.7569038935807376, accuracy: 0.5133333333333333\n",
      "iteration no 168: Loss: 0.7568946781271269, accuracy: 0.5133333333333333\n",
      "iteration no 169: Loss: 0.7568857496073014, accuracy: 0.5133333333333333\n",
      "iteration no 170: Loss: 0.7568770987495238, accuracy: 0.5133333333333333\n",
      "iteration no 171: Loss: 0.7568687165970008, accuracy: 0.5133333333333333\n",
      "iteration no 172: Loss: 0.7568605944964665, accuracy: 0.5133333333333333\n",
      "iteration no 173: Loss: 0.7568527240872128, accuracy: 0.5133333333333333\n",
      "iteration no 174: Loss: 0.7568450972905493, accuracy: 0.5133333333333333\n",
      "iteration no 175: Loss: 0.7568377062996728, accuracy: 0.5133333333333333\n",
      "iteration no 176: Loss: 0.7568305435699305, accuracy: 0.5133333333333333\n",
      "iteration no 177: Loss: 0.7568236018094594, accuracy: 0.5133333333333333\n",
      "iteration no 178: Loss: 0.7568168739701854, accuracy: 0.5133333333333333\n",
      "iteration no 179: Loss: 0.7568103532391693, accuracy: 0.5133333333333333\n",
      "iteration no 180: Loss: 0.756804033030282, accuracy: 0.5133333333333333\n",
      "iteration no 181: Loss: 0.7567979069761986, accuracy: 0.5133333333333333\n",
      "iteration no 182: Loss: 0.7567919689206947, accuracy: 0.5133333333333333\n",
      "iteration no 183: Loss: 0.7567862129112356, accuracy: 0.5133333333333333\n",
      "iteration no 184: Loss: 0.7567806331918444, accuracy: 0.5133333333333333\n",
      "iteration no 185: Loss: 0.7567752241962381, accuracy: 0.5133333333333333\n",
      "iteration no 186: Loss: 0.7567699805412209, accuracy: 0.5133333333333333\n",
      "iteration no 187: Loss: 0.7567648970203236, accuracy: 0.5133333333333333\n",
      "iteration no 188: Loss: 0.7567599685976809, accuracy: 0.5133333333333333\n",
      "iteration no 189: Loss: 0.7567551904021351, accuracy: 0.5133333333333333\n",
      "iteration no 190: Loss: 0.7567505577215569, accuracy: 0.5133333333333333\n",
      "iteration no 191: Loss: 0.7567460659973764, accuracy: 0.5133333333333333\n",
      "iteration no 192: Loss: 0.7567417108193162, accuracy: 0.5133333333333333\n",
      "iteration no 193: Loss: 0.7567374879203149, accuracy: 0.5133333333333333\n",
      "iteration no 194: Loss: 0.7567333931716379, accuracy: 0.5133333333333333\n",
      "iteration no 195: Loss: 0.7567294225781647, accuracy: 0.5133333333333333\n",
      "iteration no 196: Loss: 0.7567255722738491, accuracy: 0.5133333333333333\n",
      "iteration no 197: Loss: 0.7567218385173423, accuracy: 0.5133333333333333\n",
      "iteration no 198: Loss: 0.7567182176877733, accuracy: 0.5133333333333333\n",
      "iteration no 199: Loss: 0.7567147062806835, accuracy: 0.5133333333333333\n",
      "iteration no 200: Loss: 0.7567113009041043, accuracy: 0.5166666666666667\n",
      "iteration no 1: Loss: 1.0900802583878544, accuracy: 0.42333333333333334\n",
      "iteration no 2: Loss: 1.0606958196308107, accuracy: 0.47333333333333333\n",
      "iteration no 3: Loss: 1.0345054808749972, accuracy: 0.48333333333333334\n",
      "iteration no 4: Loss: 1.0111243049293286, accuracy: 0.4766666666666667\n",
      "iteration no 5: Loss: 0.990212658015541, accuracy: 0.48333333333333334\n",
      "iteration no 6: Loss: 0.9714719991256169, accuracy: 0.49333333333333335\n",
      "iteration no 7: Loss: 0.9546408668168251, accuracy: 0.49666666666666665\n",
      "iteration no 8: Loss: 0.9394909326984995, accuracy: 0.49666666666666665\n",
      "iteration no 9: Loss: 0.925823187705725, accuracy: 0.5\n",
      "iteration no 10: Loss: 0.9134643562201372, accuracy: 0.51\n",
      "iteration no 11: Loss: 0.9022636074928989, accuracy: 0.51\n",
      "iteration no 12: Loss: 0.8920895990961318, accuracy: 0.51\n",
      "iteration no 13: Loss: 0.8828278585727352, accuracy: 0.51\n",
      "iteration no 14: Loss: 0.874378490145175, accuracy: 0.51\n",
      "iteration no 15: Loss: 0.8666541821023942, accuracy: 0.5066666666666667\n",
      "iteration no 16: Loss: 0.8595784851343652, accuracy: 0.5066666666666667\n",
      "iteration no 17: Loss: 0.8530843304194805, accuracy: 0.5066666666666667\n",
      "iteration no 18: Loss: 0.8471127571609226, accuracy: 0.51\n",
      "iteration no 19: Loss: 0.8416118214525833, accuracy: 0.51\n",
      "iteration no 20: Loss: 0.8365356611407956, accuracy: 0.5066666666666667\n",
      "iteration no 21: Loss: 0.831843694307354, accuracy: 0.51\n",
      "iteration no 22: Loss: 0.8274999318830111, accuracy: 0.5133333333333333\n",
      "iteration no 23: Loss: 0.8234723875757086, accuracy: 0.51\n",
      "iteration no 24: Loss: 0.8197325707035978, accuracy: 0.51\n",
      "iteration no 25: Loss: 0.8162550496422638, accuracy: 0.51\n",
      "iteration no 26: Loss: 0.8130170754362704, accuracy: 0.51\n",
      "iteration no 27: Loss: 0.809998256707932, accuracy: 0.51\n",
      "iteration no 28: Loss: 0.8071802783477174, accuracy: 0.51\n",
      "iteration no 29: Loss: 0.8045466576190183, accuracy: 0.51\n",
      "iteration no 30: Loss: 0.802082532282426, accuracy: 0.51\n",
      "iteration no 31: Loss: 0.7997744761663023, accuracy: 0.51\n",
      "iteration no 32: Loss: 0.7976103383037297, accuracy: 0.5133333333333333\n",
      "iteration no 33: Loss: 0.7955791023405941, accuracy: 0.5133333333333333\n",
      "iteration no 34: Loss: 0.7936707634125292, accuracy: 0.5133333333333333\n",
      "iteration no 35: Loss: 0.7918762201042661, accuracy: 0.5133333333333333\n",
      "iteration no 36: Loss: 0.7901871794558846, accuracy: 0.5133333333333333\n",
      "iteration no 37: Loss: 0.7885960732769376, accuracy: 0.5133333333333333\n",
      "iteration no 38: Loss: 0.7870959842801667, accuracy: 0.5133333333333333\n",
      "iteration no 39: Loss: 0.7856805807588539, accuracy: 0.5166666666666667\n",
      "iteration no 40: Loss: 0.7843440587119214, accuracy: 0.5166666666666667\n",
      "iteration no 41: Loss: 0.7830810904738221, accuracy: 0.5166666666666667\n",
      "iteration no 42: Loss: 0.7818867790363503, accuracy: 0.5133333333333333\n",
      "iteration no 43: Loss: 0.7807566173603573, accuracy: 0.5133333333333333\n",
      "iteration no 44: Loss: 0.7796864520699601, accuracy: 0.5133333333333333\n",
      "iteration no 45: Loss: 0.7786724510027414, accuracy: 0.5133333333333333\n",
      "iteration no 46: Loss: 0.777711074158706, accuracy: 0.5133333333333333\n",
      "iteration no 47: Loss: 0.7767990476502279, accuracy: 0.5133333333333333\n",
      "iteration no 48: Loss: 0.7759333403063069, accuracy: 0.5133333333333333\n",
      "iteration no 49: Loss: 0.7751111426284655, accuracy: 0.5133333333333333\n",
      "iteration no 50: Loss: 0.7743298478335643, accuracy: 0.5133333333333333\n",
      "iteration no 51: Loss: 0.7735870347516114, accuracy: 0.5133333333333333\n",
      "iteration no 52: Loss: 0.7728804523750323, accuracy: 0.5133333333333333\n",
      "iteration no 53: Loss: 0.7722080058804804, accuracy: 0.5133333333333333\n",
      "iteration no 54: Loss: 0.77156774396565, accuracy: 0.5133333333333333\n",
      "iteration no 55: Loss: 0.7709578473621566, accuracy: 0.5166666666666667\n",
      "iteration no 56: Loss: 0.7703766184017565, accuracy: 0.5166666666666667\n",
      "iteration no 57: Loss: 0.76982247152734, accuracy: 0.5166666666666667\n",
      "iteration no 58: Loss: 0.7692939246524894, accuracy: 0.5166666666666667\n",
      "iteration no 59: Loss: 0.7687895912842493, accuracy: 0.5166666666666667\n",
      "iteration no 60: Loss: 0.7683081733332374, accuracy: 0.5166666666666667\n",
      "iteration no 61: Loss: 0.7678484545435975, accuracy: 0.5166666666666667\n",
      "iteration no 62: Loss: 0.7674092944826163, accuracy: 0.5166666666666667\n",
      "iteration no 63: Loss: 0.7669896230363218, accuracy: 0.5166666666666667\n",
      "iteration no 64: Loss: 0.7665884353630735, accuracy: 0.5166666666666667\n",
      "iteration no 65: Loss: 0.7662047872622083, accuracy: 0.5166666666666667\n",
      "iteration no 66: Loss: 0.7658377909192745, accuracy: 0.5166666666666667\n",
      "iteration no 67: Loss: 0.7654866109933366, accuracy: 0.5166666666666667\n",
      "iteration no 68: Loss: 0.7651504610153499, accuracy: 0.5166666666666667\n",
      "iteration no 69: Loss: 0.7648286000697218, accuracy: 0.5166666666666667\n",
      "iteration no 70: Loss: 0.764520329733954, accuracy: 0.5166666666666667\n",
      "iteration no 71: Loss: 0.7642249912537299, accuracy: 0.5166666666666667\n",
      "iteration no 72: Loss: 0.7639419629330222, accuracy: 0.5166666666666667\n",
      "iteration no 73: Loss: 0.7636706577207583, accuracy: 0.5166666666666667\n",
      "iteration no 74: Loss: 0.7634105209773512, accuracy: 0.5166666666666667\n",
      "iteration no 75: Loss: 0.7631610284059728, accuracy: 0.5166666666666667\n",
      "iteration no 76: Loss: 0.7629216841348657, accuracy: 0.5166666666666667\n",
      "iteration no 77: Loss: 0.762692018938254, accuracy: 0.5166666666666667\n",
      "iteration no 78: Loss: 0.7624715885845543, accuracy: 0.5166666666666667\n",
      "iteration no 79: Loss: 0.7622599723016158, accuracy: 0.5166666666666667\n",
      "iteration no 80: Loss: 0.7620567713496337, accuracy: 0.5166666666666667\n",
      "iteration no 81: Loss: 0.7618616076932211, accuracy: 0.5133333333333333\n",
      "iteration no 82: Loss: 0.7616741227648656, accuracy: 0.5133333333333333\n",
      "iteration no 83: Loss: 0.7614939763126811, accuracy: 0.5133333333333333\n",
      "iteration no 84: Loss: 0.761320845325976, accuracy: 0.5133333333333333\n",
      "iteration no 85: Loss: 0.7611544230327092, accuracy: 0.5133333333333333\n",
      "iteration no 86: Loss: 0.7609944179634168, accuracy: 0.5133333333333333\n",
      "iteration no 87: Loss: 0.7608405530766363, accuracy: 0.5133333333333333\n",
      "iteration no 88: Loss: 0.760692564941279, accuracy: 0.5133333333333333\n",
      "iteration no 89: Loss: 0.7605502029717688, accuracy: 0.5133333333333333\n",
      "iteration no 90: Loss: 0.7604132287121119, accuracy: 0.5133333333333333\n",
      "iteration no 91: Loss: 0.7602814151653702, accuracy: 0.5133333333333333\n",
      "iteration no 92: Loss: 0.7601545461652944, accuracy: 0.5133333333333333\n",
      "iteration no 93: Loss: 0.7600324157871329, accuracy: 0.5133333333333333\n",
      "iteration no 94: Loss: 0.759914827794864, accuracy: 0.5133333333333333\n",
      "iteration no 95: Loss: 0.7598015951223178, accuracy: 0.5133333333333333\n",
      "iteration no 96: Loss: 0.759692539385849, accuracy: 0.5133333333333333\n",
      "iteration no 97: Loss: 0.7595874904263997, accuracy: 0.5133333333333333\n",
      "iteration no 98: Loss: 0.7594862858789609, accuracy: 0.5133333333333333\n",
      "iteration no 99: Loss: 0.759388770767585, accuracy: 0.5133333333333333\n",
      "iteration no 100: Loss: 0.7592947971242462, accuracy: 0.5133333333333333\n",
      "iteration no 101: Loss: 0.7592042236299693, accuracy: 0.5166666666666667\n",
      "iteration no 102: Loss: 0.7591169152767632, accuracy: 0.5166666666666667\n",
      "iteration no 103: Loss: 0.7590327430490035, accuracy: 0.5166666666666667\n",
      "iteration no 104: Loss: 0.758951583623009, accuracy: 0.5166666666666667\n",
      "iteration no 105: Loss: 0.7588733190836421, accuracy: 0.5166666666666667\n",
      "iteration no 106: Loss: 0.7587978366568499, accuracy: 0.5166666666666667\n",
      "iteration no 107: Loss: 0.758725028457143, accuracy: 0.5166666666666667\n",
      "iteration no 108: Loss: 0.7586547912490683, accuracy: 0.5166666666666667\n",
      "iteration no 109: Loss: 0.7585870262218138, accuracy: 0.5166666666666667\n",
      "iteration no 110: Loss: 0.7585216387761269, accuracy: 0.5166666666666667\n",
      "iteration no 111: Loss: 0.7584585383227959, accuracy: 0.5166666666666667\n",
      "iteration no 112: Loss: 0.7583976380919901, accuracy: 0.5166666666666667\n",
      "iteration no 113: Loss: 0.7583388549528022, accuracy: 0.5166666666666667\n",
      "iteration no 114: Loss: 0.7582821092423806, accuracy: 0.5166666666666667\n",
      "iteration no 115: Loss: 0.7582273246040816, accuracy: 0.5166666666666667\n",
      "iteration no 116: Loss: 0.758174427834109, accuracy: 0.5166666666666667\n",
      "iteration no 117: Loss: 0.7581233487361396, accuracy: 0.5166666666666667\n",
      "iteration no 118: Loss: 0.7580740199834725, accuracy: 0.5166666666666667\n",
      "iteration no 119: Loss: 0.758026376988264, accuracy: 0.5166666666666667\n",
      "iteration no 120: Loss: 0.757980357777443, accuracy: 0.5166666666666667\n",
      "iteration no 121: Loss: 0.7579359028749226, accuracy: 0.5166666666666667\n",
      "iteration no 122: Loss: 0.7578929551897527, accuracy: 0.5166666666666667\n",
      "iteration no 123: Loss: 0.7578514599098778, accuracy: 0.5166666666666667\n",
      "iteration no 124: Loss: 0.7578113644011869, accuracy: 0.5166666666666667\n",
      "iteration no 125: Loss: 0.7577726181115599, accuracy: 0.5166666666666667\n",
      "iteration no 126: Loss: 0.757735172479636, accuracy: 0.5166666666666667\n",
      "iteration no 127: Loss: 0.7576989808480427, accuracy: 0.5166666666666667\n",
      "iteration no 128: Loss: 0.7576639983808449, accuracy: 0.5166666666666667\n",
      "iteration no 129: Loss: 0.7576301819849818, accuracy: 0.5166666666666667\n",
      "iteration no 130: Loss: 0.7575974902354798, accuracy: 0.5166666666666667\n",
      "iteration no 131: Loss: 0.7575658833042384, accuracy: 0.5166666666666667\n",
      "iteration no 132: Loss: 0.7575353228921962, accuracy: 0.5166666666666667\n",
      "iteration no 133: Loss: 0.7575057721647034, accuracy: 0.5166666666666667\n",
      "iteration no 134: Loss: 0.7574771956899279, accuracy: 0.5166666666666667\n",
      "iteration no 135: Loss: 0.757449559380138, accuracy: 0.5166666666666667\n",
      "iteration no 136: Loss: 0.7574228304357118, accuracy: 0.5133333333333333\n",
      "iteration no 137: Loss: 0.7573969772917354, accuracy: 0.5133333333333333\n",
      "iteration no 138: Loss: 0.7573719695670508, accuracy: 0.5133333333333333\n",
      "iteration no 139: Loss: 0.7573477780156349, accuracy: 0.5133333333333333\n",
      "iteration no 140: Loss: 0.7573243744801875, accuracy: 0.5133333333333333\n",
      "iteration no 141: Loss: 0.757301731847819, accuracy: 0.5133333333333333\n",
      "iteration no 142: Loss: 0.7572798240077298, accuracy: 0.5133333333333333\n",
      "iteration no 143: Loss: 0.7572586258107844, accuracy: 0.5133333333333333\n",
      "iteration no 144: Loss: 0.7572381130308864, accuracy: 0.5133333333333333\n",
      "iteration no 145: Loss: 0.7572182623280627, accuracy: 0.5133333333333333\n",
      "iteration no 146: Loss: 0.7571990512131764, accuracy: 0.5133333333333333\n",
      "iteration no 147: Loss: 0.7571804580141868, accuracy: 0.5133333333333333\n",
      "iteration no 148: Loss: 0.7571624618438823, accuracy: 0.5133333333333333\n",
      "iteration no 149: Loss: 0.7571450425690145, accuracy: 0.5133333333333333\n",
      "iteration no 150: Loss: 0.7571281807807666, accuracy: 0.5133333333333333\n",
      "iteration no 151: Loss: 0.7571118577664931, accuracy: 0.5133333333333333\n",
      "iteration no 152: Loss: 0.7570960554826679, accuracy: 0.5133333333333333\n",
      "iteration no 153: Loss: 0.7570807565289872, accuracy: 0.5133333333333333\n",
      "iteration no 154: Loss: 0.7570659441235696, accuracy: 0.5133333333333333\n",
      "iteration no 155: Loss: 0.7570516020792046, accuracy: 0.5133333333333333\n",
      "iteration no 156: Loss: 0.7570377147805989, accuracy: 0.5133333333333333\n",
      "iteration no 157: Loss: 0.7570242671625745, accuracy: 0.5133333333333333\n",
      "iteration no 158: Loss: 0.757011244689177, accuracy: 0.5133333333333333\n",
      "iteration no 159: Loss: 0.7569986333336484, accuracy: 0.5133333333333333\n",
      "iteration no 160: Loss: 0.7569864195592285, accuracy: 0.5133333333333333\n",
      "iteration no 161: Loss: 0.7569745903007451, accuracy: 0.5133333333333333\n",
      "iteration no 162: Loss: 0.7569631329469584, accuracy: 0.5133333333333333\n",
      "iteration no 163: Loss: 0.7569520353236259, accuracy: 0.5133333333333333\n",
      "iteration no 164: Loss: 0.7569412856772534, accuracy: 0.5133333333333333\n",
      "iteration no 165: Loss: 0.7569308726595045, accuracy: 0.5133333333333333\n",
      "iteration no 166: Loss: 0.7569207853122383, accuracy: 0.5133333333333333\n",
      "iteration no 167: Loss: 0.7569110130531453, accuracy: 0.5133333333333333\n",
      "iteration no 168: Loss: 0.7569015456619596, accuracy: 0.5133333333333333\n",
      "iteration no 169: Loss: 0.7568923732672176, accuracy: 0.5133333333333333\n",
      "iteration no 170: Loss: 0.7568834863335434, accuracy: 0.5133333333333333\n",
      "iteration no 171: Loss: 0.756874875649435, accuracy: 0.5133333333333333\n",
      "iteration no 172: Loss: 0.7568665323155308, accuracy: 0.5133333333333333\n",
      "iteration no 173: Loss: 0.7568584477333372, accuracy: 0.5133333333333333\n",
      "iteration no 174: Loss: 0.7568506135943941, accuracy: 0.5133333333333333\n",
      "iteration no 175: Loss: 0.7568430218698637, accuracy: 0.5133333333333333\n",
      "iteration no 176: Loss: 0.7568356648005224, accuracy: 0.5133333333333333\n",
      "iteration no 177: Loss: 0.7568285348871373, accuracy: 0.5133333333333333\n",
      "iteration no 178: Loss: 0.7568216248812152, accuracy: 0.5133333333333333\n",
      "iteration no 179: Loss: 0.7568149277761043, accuracy: 0.5133333333333333\n",
      "iteration no 180: Loss: 0.7568084367984372, accuracy: 0.5133333333333333\n",
      "iteration no 181: Loss: 0.7568021453998992, accuracy: 0.5133333333333333\n",
      "iteration no 182: Loss: 0.7567960472493078, accuracy: 0.5133333333333333\n",
      "iteration no 183: Loss: 0.7567901362249922, accuracy: 0.5133333333333333\n",
      "iteration no 184: Loss: 0.7567844064074616, accuracy: 0.5133333333333333\n",
      "iteration no 185: Loss: 0.7567788520723452, accuracy: 0.5133333333333333\n",
      "iteration no 186: Loss: 0.7567734676835997, accuracy: 0.5133333333333333\n",
      "iteration no 187: Loss: 0.7567682478869681, accuracy: 0.5133333333333333\n",
      "iteration no 188: Loss: 0.756763187503683, accuracy: 0.5133333333333333\n",
      "iteration no 189: Loss: 0.7567582815244019, accuracy: 0.5133333333333333\n",
      "iteration no 190: Loss: 0.756753525103368, accuracy: 0.5133333333333333\n",
      "iteration no 191: Loss: 0.7567489135527851, accuracy: 0.5133333333333333\n",
      "iteration no 192: Loss: 0.7567444423373983, accuracy: 0.5133333333333333\n",
      "iteration no 193: Loss: 0.7567401070692757, accuracy: 0.5133333333333333\n",
      "iteration no 194: Loss: 0.7567359035027771, accuracy: 0.5133333333333333\n",
      "iteration no 195: Loss: 0.756731827529708, accuracy: 0.5133333333333333\n",
      "iteration no 196: Loss: 0.7567278751746492, accuracy: 0.5133333333333333\n",
      "iteration no 197: Loss: 0.7567240425904537, accuracy: 0.5133333333333333\n",
      "iteration no 198: Loss: 0.756720326053908, accuracy: 0.5133333333333333\n",
      "iteration no 199: Loss: 0.7567167219615482, accuracy: 0.5133333333333333\n",
      "iteration no 200: Loss: 0.7567132268256263, accuracy: 0.5133333333333333\n",
      "iteration no 1: Loss: 1.0811309996827834, accuracy: 0.44666666666666666\n",
      "iteration no 2: Loss: 1.052310576429016, accuracy: 0.47333333333333333\n",
      "iteration no 3: Loss: 1.0266710936122798, accuracy: 0.49333333333333335\n",
      "iteration no 4: Loss: 1.0038221435677892, accuracy: 0.48333333333333334\n",
      "iteration no 5: Loss: 0.983419344186395, accuracy: 0.49\n",
      "iteration no 6: Loss: 0.9651606384601689, accuracy: 0.49666666666666665\n",
      "iteration no 7: Loss: 0.9487823663617784, accuracy: 0.5\n",
      "iteration no 8: Loss: 0.9340551883855287, accuracy: 0.5033333333333333\n",
      "iteration no 9: Loss: 0.9207800526065245, accuracy: 0.52\n",
      "iteration no 10: Loss: 0.9087843720240484, accuracy: 0.5233333333333333\n",
      "iteration no 11: Loss: 0.8979185186968133, accuracy: 0.5233333333333333\n",
      "iteration no 12: Loss: 0.8880526851171154, accuracy: 0.5233333333333333\n",
      "iteration no 13: Loss: 0.8790741224841646, accuracy: 0.5233333333333333\n",
      "iteration no 14: Loss: 0.8708847399357328, accuracy: 0.5233333333333333\n",
      "iteration no 15: Loss: 0.8633990348932024, accuracy: 0.52\n",
      "iteration no 16: Loss: 0.8565423187075312, accuracy: 0.52\n",
      "iteration no 17: Loss: 0.8502492007425777, accuracy: 0.5166666666666667\n",
      "iteration no 18: Loss: 0.8444622957646846, accuracy: 0.5133333333333333\n",
      "iteration no 19: Loss: 0.8391311226311848, accuracy: 0.5133333333333333\n",
      "iteration no 20: Loss: 0.8342111659310785, accuracy: 0.5133333333333333\n",
      "iteration no 21: Loss: 0.8296630759352575, accuracy: 0.5166666666666667\n",
      "iteration no 22: Loss: 0.8254519856983585, accuracy: 0.5166666666666667\n",
      "iteration no 23: Loss: 0.8215469272970858, accuracy: 0.5166666666666667\n",
      "iteration no 24: Loss: 0.8179203319501228, accuracy: 0.5166666666666667\n",
      "iteration no 25: Loss: 0.8145476011474612, accuracy: 0.5166666666666667\n",
      "iteration no 26: Loss: 0.8114067379499411, accuracy: 0.5166666666666667\n",
      "iteration no 27: Loss: 0.808478029340891, accuracy: 0.5166666666666667\n",
      "iteration no 28: Loss: 0.8057437719612028, accuracy: 0.5166666666666667\n",
      "iteration no 29: Loss: 0.8031880347757978, accuracy: 0.5166666666666667\n",
      "iteration no 30: Loss: 0.800796453238547, accuracy: 0.5166666666666667\n",
      "iteration no 31: Loss: 0.7985560503755337, accuracy: 0.52\n",
      "iteration no 32: Loss: 0.7964550809200115, accuracy: 0.5133333333333333\n",
      "iteration no 33: Loss: 0.7944828952294518, accuracy: 0.5133333333333333\n",
      "iteration no 34: Loss: 0.7926298202150792, accuracy: 0.5133333333333333\n",
      "iteration no 35: Loss: 0.7908870549334464, accuracy: 0.5133333333333333\n",
      "iteration no 36: Loss: 0.7892465788414417, accuracy: 0.5166666666666667\n",
      "iteration no 37: Loss: 0.7877010710119071, accuracy: 0.5166666666666667\n",
      "iteration no 38: Loss: 0.7862438388561077, accuracy: 0.5166666666666667\n",
      "iteration no 39: Loss: 0.784868755109376, accuracy: 0.5166666666666667\n",
      "iteration no 40: Loss: 0.7835702020137985, accuracy: 0.5166666666666667\n",
      "iteration no 41: Loss: 0.782343021782122, accuracy: 0.5166666666666667\n",
      "iteration no 42: Loss: 0.7811824725545691, accuracy: 0.5166666666666667\n",
      "iteration no 43: Loss: 0.7800841891686308, accuracy: 0.5166666666666667\n",
      "iteration no 44: Loss: 0.7790441481542068, accuracy: 0.5166666666666667\n",
      "iteration no 45: Loss: 0.7780586364452162, accuracy: 0.5166666666666667\n",
      "iteration no 46: Loss: 0.7771242233661543, accuracy: 0.5166666666666667\n",
      "iteration no 47: Loss: 0.7762377355097474, accuracy: 0.5166666666666667\n",
      "iteration no 48: Loss: 0.7753962341713869, accuracy: 0.5166666666666667\n",
      "iteration no 49: Loss: 0.7745969950486011, accuracy: 0.5166666666666667\n",
      "iteration no 50: Loss: 0.7738374899505243, accuracy: 0.5166666666666667\n",
      "iteration no 51: Loss: 0.7731153702939969, accuracy: 0.5166666666666667\n",
      "iteration no 52: Loss: 0.7724284521903293, accuracy: 0.5166666666666667\n",
      "iteration no 53: Loss: 0.7717747029504998, accuracy: 0.5133333333333333\n",
      "iteration no 54: Loss: 0.7711522288571692, accuracy: 0.5133333333333333\n",
      "iteration no 55: Loss: 0.7705592640698075, accuracy: 0.5133333333333333\n",
      "iteration no 56: Loss: 0.7699941605448405, accuracy: 0.5133333333333333\n",
      "iteration no 57: Loss: 0.7694553788663469, accuracy: 0.5133333333333333\n",
      "iteration no 58: Loss: 0.7689414798947383, accuracy: 0.5133333333333333\n",
      "iteration no 59: Loss: 0.7684511171512796, accuracy: 0.5133333333333333\n",
      "iteration no 60: Loss: 0.7679830298654478, accuracy: 0.5133333333333333\n",
      "iteration no 61: Loss: 0.7675360366201552, accuracy: 0.5133333333333333\n",
      "iteration no 62: Loss: 0.7671090295369265, accuracy: 0.5133333333333333\n",
      "iteration no 63: Loss: 0.7667009689493373, accuracy: 0.5133333333333333\n",
      "iteration no 64: Loss: 0.7663108785185194, accuracy: 0.5166666666666667\n",
      "iteration no 65: Loss: 0.7659378407493833, accuracy: 0.5166666666666667\n",
      "iteration no 66: Loss: 0.765580992870505, accuracy: 0.5166666666666667\n",
      "iteration no 67: Loss: 0.7652395230444253, accuracy: 0.5166666666666667\n",
      "iteration no 68: Loss: 0.7649126668784964, accuracy: 0.5166666666666667\n",
      "iteration no 69: Loss: 0.764599704209394, accuracy: 0.5166666666666667\n",
      "iteration no 70: Loss: 0.7642999561371012, accuracy: 0.5166666666666667\n",
      "iteration no 71: Loss: 0.7640127822865357, accuracy: 0.5166666666666667\n",
      "iteration no 72: Loss: 0.7637375782771219, accuracy: 0.5166666666666667\n",
      "iteration no 73: Loss: 0.763473773382499, accuracy: 0.5166666666666667\n",
      "iteration no 74: Loss: 0.7632208283642603, accuracy: 0.5166666666666667\n",
      "iteration no 75: Loss: 0.7629782334651244, accuracy: 0.5166666666666667\n",
      "iteration no 76: Loss: 0.7627455065483126, accuracy: 0.5166666666666667\n",
      "iteration no 77: Loss: 0.7625221913711208, accuracy: 0.5166666666666667\n",
      "iteration no 78: Loss: 0.7623078559817712, accuracy: 0.5166666666666667\n",
      "iteration no 79: Loss: 0.7621020912296224, accuracy: 0.5133333333333333\n",
      "iteration no 80: Loss: 0.7619045093796987, accuracy: 0.5133333333333333\n",
      "iteration no 81: Loss: 0.7617147428233061, accuracy: 0.5133333333333333\n",
      "iteration no 82: Loss: 0.7615324428772227, accuracy: 0.5133333333333333\n",
      "iteration no 83: Loss: 0.7613572786646042, accuracy: 0.5133333333333333\n",
      "iteration no 84: Loss: 0.7611889360713431, accuracy: 0.5133333333333333\n",
      "iteration no 85: Loss: 0.7610271167721402, accuracy: 0.5133333333333333\n",
      "iteration no 86: Loss: 0.7608715373210515, accuracy: 0.5133333333333333\n",
      "iteration no 87: Loss: 0.7607219283016949, accuracy: 0.5133333333333333\n",
      "iteration no 88: Loss: 0.7605780335327116, accuracy: 0.5133333333333333\n",
      "iteration no 89: Loss: 0.7604396093244339, accuracy: 0.5133333333333333\n",
      "iteration no 90: Loss: 0.7603064237830472, accuracy: 0.5133333333333333\n",
      "iteration no 91: Loss: 0.7601782561588238, accuracy: 0.5133333333333333\n",
      "iteration no 92: Loss: 0.7600548962352911, accuracy: 0.5133333333333333\n",
      "iteration no 93: Loss: 0.7599361437564379, accuracy: 0.5133333333333333\n",
      "iteration no 94: Loss: 0.759821807889295, accuracy: 0.5133333333333333\n",
      "iteration no 95: Loss: 0.7597117067194294, accuracy: 0.5133333333333333\n",
      "iteration no 96: Loss: 0.7596056667770891, accuracy: 0.5133333333333333\n",
      "iteration no 97: Loss: 0.7595035225918988, accuracy: 0.5133333333333333\n",
      "iteration no 98: Loss: 0.7594051162741767, accuracy: 0.5133333333333333\n",
      "iteration no 99: Loss: 0.7593102971210796, accuracy: 0.5133333333333333\n",
      "iteration no 100: Loss: 0.7592189212459257, accuracy: 0.5133333333333333\n",
      "iteration no 101: Loss: 0.7591308512291567, accuracy: 0.5133333333333333\n",
      "iteration no 102: Loss: 0.7590459557895253, accuracy: 0.5133333333333333\n",
      "iteration no 103: Loss: 0.758964109474188, accuracy: 0.5133333333333333\n",
      "iteration no 104: Loss: 0.7588851923664858, accuracy: 0.5133333333333333\n",
      "iteration no 105: Loss: 0.7588090898102745, accuracy: 0.5166666666666667\n",
      "iteration no 106: Loss: 0.7587356921497588, accuracy: 0.5166666666666667\n",
      "iteration no 107: Loss: 0.7586648944838463, accuracy: 0.5166666666666667\n",
      "iteration no 108: Loss: 0.758596596434115, accuracy: 0.5166666666666667\n",
      "iteration no 109: Loss: 0.7585307019255484, accuracy: 0.5166666666666667\n",
      "iteration no 110: Loss: 0.7584671189792486, accuracy: 0.5166666666666667\n",
      "iteration no 111: Loss: 0.7584057595163949, accuracy: 0.5166666666666667\n",
      "iteration no 112: Loss: 0.7583465391727655, accuracy: 0.5166666666666667\n",
      "iteration no 113: Loss: 0.7582893771231816, accuracy: 0.5166666666666667\n",
      "iteration no 114: Loss: 0.7582341959152816, accuracy: 0.5166666666666667\n",
      "iteration no 115: Loss: 0.7581809213120704, accuracy: 0.5166666666666667\n",
      "iteration no 116: Loss: 0.758129482142723, accuracy: 0.5166666666666667\n",
      "iteration no 117: Loss: 0.7580798101611612, accuracy: 0.5166666666666667\n",
      "iteration no 118: Loss: 0.7580318399119462, accuracy: 0.5166666666666667\n",
      "iteration no 119: Loss: 0.7579855086030693, accuracy: 0.5166666666666667\n",
      "iteration no 120: Loss: 0.7579407559852382, accuracy: 0.5166666666666667\n",
      "iteration no 121: Loss: 0.7578975242372925, accuracy: 0.5166666666666667\n",
      "iteration no 122: Loss: 0.7578557578573988, accuracy: 0.5166666666666667\n",
      "iteration no 123: Loss: 0.7578154035596987, accuracy: 0.5166666666666667\n",
      "iteration no 124: Loss: 0.7577764101761078, accuracy: 0.5166666666666667\n",
      "iteration no 125: Loss: 0.7577387285629757, accuracy: 0.5166666666666667\n",
      "iteration no 126: Loss: 0.7577023115123407, accuracy: 0.5166666666666667\n",
      "iteration no 127: Loss: 0.7576671136675248, accuracy: 0.5166666666666667\n",
      "iteration no 128: Loss: 0.757633091442835, accuracy: 0.5166666666666667\n",
      "iteration no 129: Loss: 0.7576002029471465, accuracy: 0.5166666666666667\n",
      "iteration no 130: Loss: 0.7575684079111573, accuracy: 0.5133333333333333\n",
      "iteration no 131: Loss: 0.7575376676181218, accuracy: 0.5133333333333333\n",
      "iteration no 132: Loss: 0.7575079448378735, accuracy: 0.5133333333333333\n",
      "iteration no 133: Loss: 0.7574792037639658, accuracy: 0.5133333333333333\n",
      "iteration no 134: Loss: 0.7574514099537665, accuracy: 0.5133333333333333\n",
      "iteration no 135: Loss: 0.7574245302713499, accuracy: 0.5133333333333333\n",
      "iteration no 136: Loss: 0.757398532833045, accuracy: 0.5133333333333333\n",
      "iteration no 137: Loss: 0.7573733869554973, accuracy: 0.5133333333333333\n",
      "iteration no 138: Loss: 0.7573490631061195, accuracy: 0.5133333333333333\n",
      "iteration no 139: Loss: 0.7573255328558072, accuracy: 0.5133333333333333\n",
      "iteration no 140: Loss: 0.7573027688338057, accuracy: 0.5133333333333333\n",
      "iteration no 141: Loss: 0.7572807446846173, accuracy: 0.5133333333333333\n",
      "iteration no 142: Loss: 0.7572594350268487, accuracy: 0.5133333333333333\n",
      "iteration no 143: Loss: 0.7572388154139017, accuracy: 0.5133333333333333\n",
      "iteration no 144: Loss: 0.7572188622964137, accuracy: 0.5133333333333333\n",
      "iteration no 145: Loss: 0.7571995529863635, accuracy: 0.5133333333333333\n",
      "iteration no 146: Loss: 0.757180865622761, accuracy: 0.5133333333333333\n",
      "iteration no 147: Loss: 0.7571627791388407, accuracy: 0.5133333333333333\n",
      "iteration no 148: Loss: 0.757145273230689, accuracy: 0.5133333333333333\n",
      "iteration no 149: Loss: 0.7571283283272358, accuracy: 0.5133333333333333\n",
      "iteration no 150: Loss: 0.7571119255615414, accuracy: 0.5133333333333333\n",
      "iteration no 151: Loss: 0.7570960467433216, accuracy: 0.5133333333333333\n",
      "iteration no 152: Loss: 0.7570806743326493, accuracy: 0.5133333333333333\n",
      "iteration no 153: Loss: 0.757065791414776, accuracy: 0.5133333333333333\n",
      "iteration no 154: Loss: 0.7570513816760248, accuracy: 0.5133333333333333\n",
      "iteration no 155: Loss: 0.7570374293806988, accuracy: 0.5133333333333333\n",
      "iteration no 156: Loss: 0.757023919348964, accuracy: 0.5133333333333333\n",
      "iteration no 157: Loss: 0.7570108369356545, accuracy: 0.5133333333333333\n",
      "iteration no 158: Loss: 0.7569981680099651, accuracy: 0.5133333333333333\n",
      "iteration no 159: Loss: 0.7569858989359837, accuracy: 0.5133333333333333\n",
      "iteration no 160: Loss: 0.7569740165540296, accuracy: 0.5133333333333333\n",
      "iteration no 161: Loss: 0.7569625081627592, accuracy: 0.5133333333333333\n",
      "iteration no 162: Loss: 0.7569513615020056, accuracy: 0.5133333333333333\n",
      "iteration no 163: Loss: 0.756940564736316, accuracy: 0.5133333333333333\n",
      "iteration no 164: Loss: 0.7569301064391608, accuracy: 0.5133333333333333\n",
      "iteration no 165: Loss: 0.7569199755777796, accuracy: 0.5133333333333333\n",
      "iteration no 166: Loss: 0.756910161498638, accuracy: 0.5133333333333333\n",
      "iteration no 167: Loss: 0.7569006539134694, accuracy: 0.5133333333333333\n",
      "iteration no 168: Loss: 0.7568914428858724, accuracy: 0.5133333333333333\n",
      "iteration no 169: Loss: 0.7568825188184434, accuracy: 0.5133333333333333\n",
      "iteration no 170: Loss: 0.7568738724404188, accuracy: 0.5133333333333333\n",
      "iteration no 171: Loss: 0.756865494795805, accuracy: 0.5133333333333333\n",
      "iteration no 172: Loss: 0.7568573772319772, accuracy: 0.5133333333333333\n",
      "iteration no 173: Loss: 0.7568495113887216, accuracy: 0.5133333333333333\n",
      "iteration no 174: Loss: 0.7568418891877089, accuracy: 0.5133333333333333\n",
      "iteration no 175: Loss: 0.756834502822376, accuracy: 0.5133333333333333\n",
      "iteration no 176: Loss: 0.7568273447481982, accuracy: 0.5133333333333333\n",
      "iteration no 177: Loss: 0.7568204076733408, accuracy: 0.5133333333333333\n",
      "iteration no 178: Loss: 0.7568136845496662, accuracy: 0.5133333333333333\n",
      "iteration no 179: Loss: 0.756807168564089, accuracy: 0.5133333333333333\n",
      "iteration no 180: Loss: 0.7568008531302595, accuracy: 0.5133333333333333\n",
      "iteration no 181: Loss: 0.7567947318805639, accuracy: 0.5133333333333333\n",
      "iteration no 182: Loss: 0.7567887986584299, accuracy: 0.5133333333333333\n",
      "iteration no 183: Loss: 0.75678304751092, accuracy: 0.5133333333333333\n",
      "iteration no 184: Loss: 0.7567774726816066, accuracy: 0.5133333333333333\n",
      "iteration no 185: Loss: 0.7567720686037132, accuracy: 0.5133333333333333\n",
      "iteration no 186: Loss: 0.7567668298935127, accuracy: 0.5133333333333333\n",
      "iteration no 187: Loss: 0.7567617513439726, accuracy: 0.5133333333333333\n",
      "iteration no 188: Loss: 0.7567568279186342, accuracy: 0.5133333333333333\n",
      "iteration no 189: Loss: 0.7567520547457223, accuracy: 0.5133333333333333\n",
      "iteration no 190: Loss: 0.7567474271124686, accuracy: 0.5133333333333333\n",
      "iteration no 191: Loss: 0.7567429404596475, accuracy: 0.5133333333333333\n",
      "iteration no 192: Loss: 0.7567385903763094, accuracy: 0.5133333333333333\n",
      "iteration no 193: Loss: 0.7567343725947101, accuracy: 0.5133333333333333\n",
      "iteration no 194: Loss: 0.7567302829854223, accuracy: 0.5133333333333333\n",
      "iteration no 195: Loss: 0.7567263175526263, accuracy: 0.5133333333333333\n",
      "iteration no 196: Loss: 0.7567224724295714, accuracy: 0.5133333333333333\n",
      "iteration no 197: Loss: 0.7567187438742011, accuracy: 0.5133333333333333\n",
      "iteration no 198: Loss: 0.7567151282649368, accuracy: 0.5133333333333333\n",
      "iteration no 199: Loss: 0.7567116220966116, accuracy: 0.5133333333333333\n",
      "iteration no 200: Loss: 0.7567082219765515, accuracy: 0.5166666666666667\n",
      "iteration no 1: Loss: 1.075916960458718, accuracy: 0.5166666666666667\n",
      "iteration no 2: Loss: 1.0477404189513237, accuracy: 0.5166666666666667\n",
      "iteration no 3: Loss: 1.0226436264899392, accuracy: 0.53\n",
      "iteration no 4: Loss: 1.000252768270575, accuracy: 0.5133333333333333\n",
      "iteration no 5: Loss: 0.9802375700783925, accuracy: 0.5133333333333333\n",
      "iteration no 6: Loss: 0.9623079813883303, accuracy: 0.5166666666666667\n",
      "iteration no 7: Loss: 0.9462104691525661, accuracy: 0.5166666666666667\n",
      "iteration no 8: Loss: 0.9317241396102011, accuracy: 0.5166666666666667\n",
      "iteration no 9: Loss: 0.9186569120124799, accuracy: 0.5166666666666667\n",
      "iteration no 10: Loss: 0.9068419065483344, accuracy: 0.5166666666666667\n",
      "iteration no 11: Loss: 0.8961341408679667, accuracy: 0.5133333333333333\n",
      "iteration no 12: Loss: 0.8864075759490891, accuracy: 0.5133333333333333\n",
      "iteration no 13: Loss: 0.8775525157775103, accuracy: 0.51\n",
      "iteration no 14: Loss: 0.8694733435284798, accuracy: 0.5133333333333333\n",
      "iteration no 15: Loss: 0.8620865655227163, accuracy: 0.5133333333333333\n",
      "iteration no 16: Loss: 0.8553191295904083, accuracy: 0.5133333333333333\n",
      "iteration no 17: Loss: 0.8491069839372163, accuracy: 0.5133333333333333\n",
      "iteration no 18: Loss: 0.8433938443553377, accuracy: 0.5133333333333333\n",
      "iteration no 19: Loss: 0.8381301405010486, accuracy: 0.5133333333333333\n",
      "iteration no 20: Loss: 0.8332721152635456, accuracy: 0.5133333333333333\n",
      "iteration no 21: Loss: 0.8287810545729637, accuracy: 0.5133333333333333\n",
      "iteration no 22: Loss: 0.824622628121454, accuracy: 0.5133333333333333\n",
      "iteration no 23: Loss: 0.8207663242982178, accuracy: 0.5133333333333333\n",
      "iteration no 24: Loss: 0.8171849651330123, accuracy: 0.5133333333333333\n",
      "iteration no 25: Loss: 0.8138542892060366, accuracy: 0.5133333333333333\n",
      "iteration no 26: Loss: 0.8107525923380493, accuracy: 0.5166666666666667\n",
      "iteration no 27: Loss: 0.807860417454533, accuracy: 0.5166666666666667\n",
      "iteration no 28: Loss: 0.8051602863556708, accuracy: 0.5166666666666667\n",
      "iteration no 29: Loss: 0.8026364672529854, accuracy: 0.5166666666666667\n",
      "iteration no 30: Loss: 0.800274772884142, accuracy: 0.5133333333333333\n",
      "iteration no 31: Loss: 0.7980623848168745, accuracy: 0.5133333333333333\n",
      "iteration no 32: Loss: 0.7959877002248955, accuracy: 0.5133333333333333\n",
      "iteration no 33: Loss: 0.7940401979833616, accuracy: 0.5166666666666667\n",
      "iteration no 34: Loss: 0.7922103214063171, accuracy: 0.5166666666666667\n",
      "iteration no 35: Loss: 0.7904893753480938, accuracy: 0.5166666666666667\n",
      "iteration no 36: Loss: 0.7888694357272353, accuracy: 0.5166666666666667\n",
      "iteration no 37: Loss: 0.7873432698153728, accuracy: 0.5133333333333333\n",
      "iteration no 38: Loss: 0.7859042658732331, accuracy: 0.5133333333333333\n",
      "iteration no 39: Loss: 0.7845463709187442, accuracy: 0.5133333333333333\n",
      "iteration no 40: Loss: 0.783264035584012, accuracy: 0.5133333333333333\n",
      "iteration no 41: Loss: 0.7820521651637455, accuracy: 0.5133333333333333\n",
      "iteration no 42: Loss: 0.7809060760816549, accuracy: 0.5133333333333333\n",
      "iteration no 43: Loss: 0.7798214571069088, accuracy: 0.5133333333333333\n",
      "iteration no 44: Loss: 0.7787943347427972, accuracy: 0.5133333333333333\n",
      "iteration no 45: Loss: 0.7778210422867232, accuracy: 0.5133333333333333\n",
      "iteration no 46: Loss: 0.7768981921265598, accuracy: 0.5133333333333333\n",
      "iteration no 47: Loss: 0.7760226508949579, accuracy: 0.5133333333333333\n",
      "iteration no 48: Loss: 0.7751915171517845, accuracy: 0.5133333333333333\n",
      "iteration no 49: Loss: 0.7744021013067109, accuracy: 0.5133333333333333\n",
      "iteration no 50: Loss: 0.7736519075300655, accuracy: 0.5133333333333333\n",
      "iteration no 51: Loss: 0.7729386174312441, accuracy: 0.5133333333333333\n",
      "iteration no 52: Loss: 0.7722600753109687, accuracy: 0.5133333333333333\n",
      "iteration no 53: Loss: 0.7716142748170931, accuracy: 0.5133333333333333\n",
      "iteration no 54: Loss: 0.7709993468539837, accuracy: 0.5133333333333333\n",
      "iteration no 55: Loss: 0.7704135486131958, accuracy: 0.5133333333333333\n",
      "iteration no 56: Loss: 0.7698552536085871, accuracy: 0.5133333333333333\n",
      "iteration no 57: Loss: 0.7693229426124679, accuracy: 0.5133333333333333\n",
      "iteration no 58: Loss: 0.7688151954011612, accuracy: 0.5133333333333333\n",
      "iteration no 59: Loss: 0.7683306832286524, accuracy: 0.5166666666666667\n",
      "iteration no 60: Loss: 0.7678681619560548, accuracy: 0.5166666666666667\n",
      "iteration no 61: Loss: 0.7674264657725577, accuracy: 0.5166666666666667\n",
      "iteration no 62: Loss: 0.7670045014505219, accuracy: 0.5166666666666667\n",
      "iteration no 63: Loss: 0.7666012430835407, accuracy: 0.5166666666666667\n",
      "iteration no 64: Loss: 0.7662157272617252, accuracy: 0.5166666666666667\n",
      "iteration no 65: Loss: 0.7658470486432775, accuracy: 0.5166666666666667\n",
      "iteration no 66: Loss: 0.7654943558856684, accuracy: 0.5166666666666667\n",
      "iteration no 67: Loss: 0.7651568479035011, accuracy: 0.5166666666666667\n",
      "iteration no 68: Loss: 0.7648337704234934, accuracy: 0.5166666666666667\n",
      "iteration no 69: Loss: 0.7645244128099786, accuracy: 0.5166666666666667\n",
      "iteration no 70: Loss: 0.7642281051369711, accuracy: 0.5166666666666667\n",
      "iteration no 71: Loss: 0.7639442154852005, accuracy: 0.5166666666666667\n",
      "iteration no 72: Loss: 0.7636721474446196, accuracy: 0.5166666666666667\n",
      "iteration no 73: Loss: 0.763411337804769, accuracy: 0.5166666666666667\n",
      "iteration no 74: Loss: 0.7631612544170596, accuracy: 0.5166666666666667\n",
      "iteration no 75: Loss: 0.7629213942145404, accuracy: 0.5166666666666667\n",
      "iteration no 76: Loss: 0.7626912813760649, accuracy: 0.5166666666666667\n",
      "iteration no 77: Loss: 0.7624704656229768, accuracy: 0.5133333333333333\n",
      "iteration no 78: Loss: 0.7622585206375296, accuracy: 0.5133333333333333\n",
      "iteration no 79: Loss: 0.7620550425932221, accuracy: 0.5133333333333333\n",
      "iteration no 80: Loss: 0.7618596487881215, accuracy: 0.5133333333333333\n",
      "iteration no 81: Loss: 0.7616719763730333, accuracy: 0.5133333333333333\n",
      "iteration no 82: Loss: 0.7614916811670941, accuracy: 0.5133333333333333\n",
      "iteration no 83: Loss: 0.7613184365540131, accuracy: 0.5133333333333333\n",
      "iteration no 84: Loss: 0.761151932452768, accuracy: 0.5133333333333333\n",
      "iteration no 85: Loss: 0.7609918743570944, accuracy: 0.5133333333333333\n",
      "iteration no 86: Loss: 0.7608379824385848, accuracy: 0.5133333333333333\n",
      "iteration no 87: Loss: 0.7606899907086491, accuracy: 0.5133333333333333\n",
      "iteration no 88: Loss: 0.7605476462349822, accuracy: 0.5133333333333333\n",
      "iteration no 89: Loss: 0.7604107084085451, accuracy: 0.5133333333333333\n",
      "iteration no 90: Loss: 0.7602789482573886, accuracy: 0.5133333333333333\n",
      "iteration no 91: Loss: 0.7601521478039482, accuracy: 0.5133333333333333\n",
      "iteration no 92: Loss: 0.7600300994627074, accuracy: 0.5133333333333333\n",
      "iteration no 93: Loss: 0.7599126054753751, accuracy: 0.5133333333333333\n",
      "iteration no 94: Loss: 0.7597994773809447, accuracy: 0.5133333333333333\n",
      "iteration no 95: Loss: 0.7596905355182114, accuracy: 0.5133333333333333\n",
      "iteration no 96: Loss: 0.7595856085585081, accuracy: 0.5133333333333333\n",
      "iteration no 97: Loss: 0.759484533066596, accuracy: 0.5133333333333333\n",
      "iteration no 98: Loss: 0.7593871530878, accuracy: 0.5133333333333333\n",
      "iteration no 99: Loss: 0.7592933197596253, accuracy: 0.5133333333333333\n",
      "iteration no 100: Loss: 0.7592028909462214, accuracy: 0.5133333333333333\n",
      "iteration no 101: Loss: 0.7591157308941833, accuracy: 0.5133333333333333\n",
      "iteration no 102: Loss: 0.7590317099082897, accuracy: 0.5166666666666667\n",
      "iteration no 103: Loss: 0.758950704045879, accuracy: 0.5166666666666667\n",
      "iteration no 104: Loss: 0.7588725948286622, accuracy: 0.5166666666666667\n",
      "iteration no 105: Loss: 0.7587972689708515, accuracy: 0.5166666666666667\n",
      "iteration no 106: Loss: 0.7587246181225702, accuracy: 0.5166666666666667\n",
      "iteration no 107: Loss: 0.7586545386275786, accuracy: 0.5166666666666667\n",
      "iteration no 108: Loss: 0.7585869312944177, accuracy: 0.5166666666666667\n",
      "iteration no 109: Loss: 0.7585217011801412, accuracy: 0.5166666666666667\n",
      "iteration no 110: Loss: 0.7584587573858521, accuracy: 0.5166666666666667\n",
      "iteration no 111: Loss: 0.7583980128633292, accuracy: 0.5166666666666667\n",
      "iteration no 112: Loss: 0.7583393842320615, accuracy: 0.5166666666666667\n",
      "iteration no 113: Loss: 0.758282791606069, accuracy: 0.5166666666666667\n",
      "iteration no 114: Loss: 0.7582281584299185, accuracy: 0.5166666666666667\n",
      "iteration no 115: Loss: 0.7581754113233904, accuracy: 0.5166666666666667\n",
      "iteration no 116: Loss: 0.7581244799342842, accuracy: 0.5166666666666667\n",
      "iteration no 117: Loss: 0.7580752967988852, accuracy: 0.5166666666666667\n",
      "iteration no 118: Loss: 0.758027797209646, accuracy: 0.5166666666666667\n",
      "iteration no 119: Loss: 0.7579819190896677, accuracy: 0.5166666666666667\n",
      "iteration no 120: Loss: 0.7579376028735856, accuracy: 0.5166666666666667\n",
      "iteration no 121: Loss: 0.7578947913944972, accuracy: 0.5166666666666667\n",
      "iteration no 122: Loss: 0.7578534297765901, accuracy: 0.5166666666666667\n",
      "iteration no 123: Loss: 0.7578134653331461, accuracy: 0.5166666666666667\n",
      "iteration no 124: Loss: 0.7577748474696239, accuracy: 0.5166666666666667\n",
      "iteration no 125: Loss: 0.7577375275915356, accuracy: 0.5166666666666667\n",
      "iteration no 126: Loss: 0.7577014590168528, accuracy: 0.5166666666666667\n",
      "iteration no 127: Loss: 0.7576665968926959, accuracy: 0.5166666666666667\n",
      "iteration no 128: Loss: 0.7576328981160697, accuracy: 0.5166666666666667\n",
      "iteration no 129: Loss: 0.7576003212584281, accuracy: 0.5166666666666667\n",
      "iteration no 130: Loss: 0.7575688264938618, accuracy: 0.5166666666666667\n",
      "iteration no 131: Loss: 0.7575383755307139, accuracy: 0.5166666666666667\n",
      "iteration no 132: Loss: 0.7575089315464428, accuracy: 0.5133333333333333\n",
      "iteration no 133: Loss: 0.757480459125559, accuracy: 0.5133333333333333\n",
      "iteration no 134: Loss: 0.7574529242004745, accuracy: 0.5133333333333333\n",
      "iteration no 135: Loss: 0.7574262939951157, accuracy: 0.5133333333333333\n",
      "iteration no 136: Loss: 0.7574005369711512, accuracy: 0.5133333333333333\n",
      "iteration no 137: Loss: 0.7573756227767036, accuracy: 0.5133333333333333\n",
      "iteration no 138: Loss: 0.7573515221974163, accuracy: 0.5133333333333333\n",
      "iteration no 139: Loss: 0.7573282071097557, accuracy: 0.5133333333333333\n",
      "iteration no 140: Loss: 0.7573056504364342, accuracy: 0.5133333333333333\n",
      "iteration no 141: Loss: 0.7572838261038489, accuracy: 0.5133333333333333\n",
      "iteration no 142: Loss: 0.7572627090014334, accuracy: 0.5133333333333333\n",
      "iteration no 143: Loss: 0.7572422749428284, accuracy: 0.5133333333333333\n",
      "iteration no 144: Loss: 0.757222500628779, accuracy: 0.5133333333333333\n",
      "iteration no 145: Loss: 0.7572033636116773, accuracy: 0.5133333333333333\n",
      "iteration no 146: Loss: 0.7571848422616648, accuracy: 0.5133333333333333\n",
      "iteration no 147: Loss: 0.7571669157342227, accuracy: 0.5133333333333333\n",
      "iteration no 148: Loss: 0.7571495639391745, accuracy: 0.5133333333333333\n",
      "iteration no 149: Loss: 0.7571327675110375, accuracy: 0.5133333333333333\n",
      "iteration no 150: Loss: 0.7571165077806503, accuracy: 0.5133333333333333\n",
      "iteration no 151: Loss: 0.7571007667480258, accuracy: 0.5133333333333333\n",
      "iteration no 152: Loss: 0.7570855270563626, accuracy: 0.5133333333333333\n",
      "iteration no 153: Loss: 0.7570707719671642, accuracy: 0.5133333333333333\n",
      "iteration no 154: Loss: 0.7570564853364152, accuracy: 0.5133333333333333\n",
      "iteration no 155: Loss: 0.7570426515917602, accuracy: 0.5133333333333333\n",
      "iteration no 156: Loss: 0.757029255710644, accuracy: 0.5133333333333333\n",
      "iteration no 157: Loss: 0.7570162831993645, accuracy: 0.5133333333333333\n",
      "iteration no 158: Loss: 0.7570037200730007, accuracy: 0.5133333333333333\n",
      "iteration no 159: Loss: 0.7569915528361686, accuracy: 0.5133333333333333\n",
      "iteration no 160: Loss: 0.7569797684645764, accuracy: 0.5133333333333333\n",
      "iteration no 161: Loss: 0.7569683543873349, accuracy: 0.5133333333333333\n",
      "iteration no 162: Loss: 0.7569572984699922, accuracy: 0.5133333333333333\n",
      "iteration no 163: Loss: 0.756946588998261, accuracy: 0.5133333333333333\n",
      "iteration no 164: Loss: 0.7569362146624066, accuracy: 0.5133333333333333\n",
      "iteration no 165: Loss: 0.7569261645422638, accuracy: 0.5133333333333333\n",
      "iteration no 166: Loss: 0.756916428092859, accuracy: 0.5133333333333333\n",
      "iteration no 167: Loss: 0.7569069951306094, accuracy: 0.5133333333333333\n",
      "iteration no 168: Loss: 0.7568978558200695, accuracy: 0.5133333333333333\n",
      "iteration no 169: Loss: 0.75688900066121, accuracy: 0.5133333333333333\n",
      "iteration no 170: Loss: 0.7568804204771967, accuracy: 0.5133333333333333\n",
      "iteration no 171: Loss: 0.7568721064026531, accuracy: 0.5133333333333333\n",
      "iteration no 172: Loss: 0.7568640498723848, accuracy: 0.5133333333333333\n",
      "iteration no 173: Loss: 0.7568562426105445, accuracy: 0.5133333333333333\n",
      "iteration no 174: Loss: 0.7568486766202213, accuracy: 0.5133333333333333\n",
      "iteration no 175: Loss: 0.7568413441734323, accuracy: 0.5133333333333333\n",
      "iteration no 176: Loss: 0.7568342378015047, accuracy: 0.5133333333333333\n",
      "iteration no 177: Loss: 0.7568273502858267, accuracy: 0.5133333333333333\n",
      "iteration no 178: Loss: 0.7568206746489547, accuracy: 0.5133333333333333\n",
      "iteration no 179: Loss: 0.7568142041460615, accuracy: 0.5133333333333333\n",
      "iteration no 180: Loss: 0.756807932256711, accuracy: 0.5133333333333333\n",
      "iteration no 181: Loss: 0.7568018526769449, accuracy: 0.5133333333333333\n",
      "iteration no 182: Loss: 0.7567959593116704, accuracy: 0.5133333333333333\n",
      "iteration no 183: Loss: 0.7567902462673356, accuracy: 0.5133333333333333\n",
      "iteration no 184: Loss: 0.7567847078448791, accuracy: 0.5133333333333333\n",
      "iteration no 185: Loss: 0.7567793385329455, accuracy: 0.5133333333333333\n",
      "iteration no 186: Loss: 0.7567741330013558, accuracy: 0.5133333333333333\n",
      "iteration no 187: Loss: 0.7567690860948171, accuracy: 0.5133333333333333\n",
      "iteration no 188: Loss: 0.7567641928268711, accuracy: 0.5133333333333333\n",
      "iteration no 189: Loss: 0.7567594483740636, accuracy: 0.5133333333333333\n",
      "iteration no 190: Loss: 0.7567548480703291, accuracy: 0.5133333333333333\n",
      "iteration no 191: Loss: 0.7567503874015843, accuracy: 0.5133333333333333\n",
      "iteration no 192: Loss: 0.7567460620005174, accuracy: 0.5133333333333333\n",
      "iteration no 193: Loss: 0.7567418676415701, accuracy: 0.5133333333333333\n",
      "iteration no 194: Loss: 0.7567378002361016, accuracy: 0.5133333333333333\n",
      "iteration no 195: Loss: 0.756733855827728, accuracy: 0.5133333333333333\n",
      "iteration no 196: Loss: 0.7567300305878311, accuracy: 0.5133333333333333\n",
      "iteration no 197: Loss: 0.7567263208112303, accuracy: 0.5133333333333333\n",
      "iteration no 198: Loss: 0.7567227229120091, accuracy: 0.5133333333333333\n",
      "iteration no 199: Loss: 0.7567192334194928, accuracy: 0.5166666666666667\n",
      "iteration no 200: Loss: 0.7567158489743685, accuracy: 0.5166666666666667\n",
      "iteration no 1: Loss: 1.1035172804957363, accuracy: 0.5333333333333333\n",
      "iteration no 2: Loss: 1.072137652667126, accuracy: 0.5266666666666666\n",
      "iteration no 3: Loss: 1.0442321152156233, accuracy: 0.5233333333333333\n",
      "iteration no 4: Loss: 1.0193777531199157, accuracy: 0.53\n",
      "iteration no 5: Loss: 0.9972041009360365, accuracy: 0.5366666666666666\n",
      "iteration no 6: Loss: 0.9773853320734783, accuracy: 0.5233333333333333\n",
      "iteration no 7: Loss: 0.9596352771983029, accuracy: 0.5166666666666667\n",
      "iteration no 8: Loss: 0.9437033558673011, accuracy: 0.5166666666666667\n",
      "iteration no 9: Loss: 0.9293707953772774, accuracy: 0.5166666666666667\n",
      "iteration no 10: Loss: 0.9164470138913723, accuracy: 0.5166666666666667\n",
      "iteration no 11: Loss: 0.9047662044925061, accuracy: 0.52\n",
      "iteration no 12: Loss: 0.8941841809182258, accuracy: 0.52\n",
      "iteration no 13: Loss: 0.8845755261591709, accuracy: 0.52\n",
      "iteration no 14: Loss: 0.8758310586189693, accuracy: 0.5166666666666667\n",
      "iteration no 15: Loss: 0.8678556093714006, accuracy: 0.5166666666666667\n",
      "iteration no 16: Loss: 0.8605660904816058, accuracy: 0.5166666666666667\n",
      "iteration no 17: Loss: 0.8538898272253261, accuracy: 0.5166666666666667\n",
      "iteration no 18: Loss: 0.8477631244465974, accuracy: 0.5166666666666667\n",
      "iteration no 19: Loss: 0.842130037553894, accuracy: 0.5166666666666667\n",
      "iteration no 20: Loss: 0.8369413205235771, accuracy: 0.5166666666666667\n",
      "iteration no 21: Loss: 0.8321535259268169, accuracy: 0.5166666666666667\n",
      "iteration no 22: Loss: 0.8277282349053142, accuracy: 0.5166666666666667\n",
      "iteration no 23: Loss: 0.8236313978922167, accuracy: 0.5166666666666667\n",
      "iteration no 24: Loss: 0.8198327695482787, accuracy: 0.5166666666666667\n",
      "iteration no 25: Loss: 0.8163054237869507, accuracy: 0.5166666666666667\n",
      "iteration no 26: Loss: 0.8130253368745328, accuracy: 0.5166666666666667\n",
      "iteration no 27: Loss: 0.8099710284199771, accuracy: 0.5166666666666667\n",
      "iteration no 28: Loss: 0.8071232516353497, accuracy: 0.5166666666666667\n",
      "iteration no 29: Loss: 0.8044647255803482, accuracy: 0.5166666666666667\n",
      "iteration no 30: Loss: 0.8019799032322414, accuracy: 0.5166666666666667\n",
      "iteration no 31: Loss: 0.7996547701744434, accuracy: 0.5166666666666667\n",
      "iteration no 32: Loss: 0.7974766694986254, accuracy: 0.5166666666666667\n",
      "iteration no 33: Loss: 0.7954341491897721, accuracy: 0.5166666666666667\n",
      "iteration no 34: Loss: 0.7935168288308776, accuracy: 0.5166666666666667\n",
      "iteration no 35: Loss: 0.7917152829411503, accuracy: 0.5166666666666667\n",
      "iteration no 36: Loss: 0.7900209386632244, accuracy: 0.5166666666666667\n",
      "iteration no 37: Loss: 0.7884259858531667, accuracy: 0.5166666666666667\n",
      "iteration no 38: Loss: 0.7869232979123734, accuracy: 0.5133333333333333\n",
      "iteration no 39: Loss: 0.7855063619413368, accuracy: 0.5133333333333333\n",
      "iteration no 40: Loss: 0.7841692169989776, accuracy: 0.5133333333333333\n",
      "iteration no 41: Loss: 0.7829063994237502, accuracy: 0.5133333333333333\n",
      "iteration no 42: Loss: 0.7817128943190894, accuracy: 0.5133333333333333\n",
      "iteration no 43: Loss: 0.7805840924301235, accuracy: 0.5133333333333333\n",
      "iteration no 44: Loss: 0.7795157517444485, accuracy: 0.5133333333333333\n",
      "iteration no 45: Loss: 0.7785039632400338, accuracy: 0.5133333333333333\n",
      "iteration no 46: Loss: 0.7775451202804504, accuracy: 0.5133333333333333\n",
      "iteration no 47: Loss: 0.7766358912236284, accuracy: 0.5133333333333333\n",
      "iteration no 48: Loss: 0.7757731948669409, accuracy: 0.5133333333333333\n",
      "iteration no 49: Loss: 0.7749541784000296, accuracy: 0.5133333333333333\n",
      "iteration no 50: Loss: 0.7741761975786231, accuracy: 0.5133333333333333\n",
      "iteration no 51: Loss: 0.7734367988686677, accuracy: 0.5133333333333333\n",
      "iteration no 52: Loss: 0.7727337033412318, accuracy: 0.5133333333333333\n",
      "iteration no 53: Loss: 0.7720647921256071, accuracy: 0.5133333333333333\n",
      "iteration no 54: Loss: 0.7714280932513669, accuracy: 0.5133333333333333\n",
      "iteration no 55: Loss: 0.7708217697304425, accuracy: 0.5133333333333333\n",
      "iteration no 56: Loss: 0.7702441087478907, accuracy: 0.5133333333333333\n",
      "iteration no 57: Loss: 0.7696935118454026, accuracy: 0.5166666666666667\n",
      "iteration no 58: Loss: 0.7691684859950036, accuracy: 0.5166666666666667\n",
      "iteration no 59: Loss: 0.7686676354721104, accuracy: 0.5166666666666667\n",
      "iteration no 60: Loss: 0.7681896544473646, accuracy: 0.5166666666666667\n",
      "iteration no 61: Loss: 0.7677333202256605, accuracy: 0.5166666666666667\n",
      "iteration no 62: Loss: 0.7672974870686756, accuracy: 0.5166666666666667\n",
      "iteration no 63: Loss: 0.7668810805441607, accuracy: 0.5166666666666667\n",
      "iteration no 64: Loss: 0.7664830923513634, accuracy: 0.5166666666666667\n",
      "iteration no 65: Loss: 0.7661025755773547, accuracy: 0.5166666666666667\n",
      "iteration no 66: Loss: 0.7657386403437937, accuracy: 0.5166666666666667\n",
      "iteration no 67: Loss: 0.7653904498078882, accuracy: 0.5166666666666667\n",
      "iteration no 68: Loss: 0.7650572164850352, accuracy: 0.5166666666666667\n",
      "iteration no 69: Loss: 0.7647381988639514, accuracy: 0.5166666666666667\n",
      "iteration no 70: Loss: 0.7644326982880361, accuracy: 0.5166666666666667\n",
      "iteration no 71: Loss: 0.7641400560793356, accuracy: 0.5166666666666667\n",
      "iteration no 72: Loss: 0.763859650883807, accuracy: 0.5166666666666667\n",
      "iteration no 73: Loss: 0.7635908962186585, accuracy: 0.5166666666666667\n",
      "iteration no 74: Loss: 0.7633332382044016, accuracy: 0.5166666666666667\n",
      "iteration no 75: Loss: 0.7630861534659094, accuracy: 0.5166666666666667\n",
      "iteration no 76: Loss: 0.7628491471882629, accuracy: 0.5166666666666667\n",
      "iteration no 77: Loss: 0.7626217513144948, accuracy: 0.5166666666666667\n",
      "iteration no 78: Loss: 0.7624035228735395, accuracy: 0.5166666666666667\n",
      "iteration no 79: Loss: 0.7621940424277655, accuracy: 0.5133333333333333\n",
      "iteration no 80: Loss: 0.7619929126304372, accuracy: 0.5133333333333333\n",
      "iteration no 81: Loss: 0.761799756884316, accuracy: 0.5133333333333333\n",
      "iteration no 82: Loss: 0.7616142180933929, accuracy: 0.5133333333333333\n",
      "iteration no 83: Loss: 0.7614359575004591, accuracy: 0.5133333333333333\n",
      "iteration no 84: Loss: 0.7612646536038438, accuracy: 0.5133333333333333\n",
      "iteration no 85: Loss: 0.7611000011472403, accuracy: 0.5133333333333333\n",
      "iteration no 86: Loss: 0.7609417101770503, accuracy: 0.5133333333333333\n",
      "iteration no 87: Loss: 0.7607895051621553, accuracy: 0.5133333333333333\n",
      "iteration no 88: Loss: 0.7606431241714509, accuracy: 0.5133333333333333\n",
      "iteration no 89: Loss: 0.7605023181048651, accuracy: 0.5133333333333333\n",
      "iteration no 90: Loss: 0.7603668499739403, accuracy: 0.5133333333333333\n",
      "iteration no 91: Loss: 0.7602364942283725, accuracy: 0.5133333333333333\n",
      "iteration no 92: Loss: 0.760111036125199, accuracy: 0.5133333333333333\n",
      "iteration no 93: Loss: 0.7599902711375899, accuracy: 0.5133333333333333\n",
      "iteration no 94: Loss: 0.7598740044004366, accuracy: 0.5133333333333333\n",
      "iteration no 95: Loss: 0.759762050190162, accuracy: 0.5133333333333333\n",
      "iteration no 96: Loss: 0.7596542314363643, accuracy: 0.5133333333333333\n",
      "iteration no 97: Loss: 0.7595503792631086, accuracy: 0.5133333333333333\n",
      "iteration no 98: Loss: 0.7594503325578278, accuracy: 0.5133333333333333\n",
      "iteration no 99: Loss: 0.7593539375659719, accuracy: 0.5133333333333333\n",
      "iteration no 100: Loss: 0.7592610475096677, accuracy: 0.5133333333333333\n",
      "iteration no 101: Loss: 0.7591715222287889, accuracy: 0.5133333333333333\n",
      "iteration no 102: Loss: 0.7590852278429556, accuracy: 0.5133333333333333\n",
      "iteration no 103: Loss: 0.7590020364330898, accuracy: 0.5166666666666667\n",
      "iteration no 104: Loss: 0.758921825741253, accuracy: 0.5166666666666667\n",
      "iteration no 105: Loss: 0.758844478887586, accuracy: 0.5166666666666667\n",
      "iteration no 106: Loss: 0.7587698841032569, accuracy: 0.5166666666666667\n",
      "iteration no 107: Loss: 0.7586979344783972, accuracy: 0.5166666666666667\n",
      "iteration no 108: Loss: 0.7586285277240812, accuracy: 0.5166666666666667\n",
      "iteration no 109: Loss: 0.7585615659474731, accuracy: 0.5166666666666667\n",
      "iteration no 110: Loss: 0.7584969554393177, accuracy: 0.5166666666666667\n",
      "iteration no 111: Loss: 0.7584346064730193, accuracy: 0.5166666666666667\n",
      "iteration no 112: Loss: 0.7583744331145968, accuracy: 0.5166666666666667\n",
      "iteration no 113: Loss: 0.7583163530428548, accuracy: 0.5166666666666667\n",
      "iteration no 114: Loss: 0.7582602873791553, accuracy: 0.5166666666666667\n",
      "iteration no 115: Loss: 0.7582061605262138, accuracy: 0.5166666666666667\n",
      "iteration no 116: Loss: 0.7581539000153861, accuracy: 0.5166666666666667\n",
      "iteration no 117: Loss: 0.7581034363619409, accuracy: 0.5166666666666667\n",
      "iteration no 118: Loss: 0.7580547029278534, accuracy: 0.5166666666666667\n",
      "iteration no 119: Loss: 0.7580076357916817, accuracy: 0.5166666666666667\n",
      "iteration no 120: Loss: 0.7579621736251146, accuracy: 0.5166666666666667\n",
      "iteration no 121: Loss: 0.7579182575758114, accuracy: 0.5166666666666667\n",
      "iteration no 122: Loss: 0.75787583115617, accuracy: 0.5166666666666667\n",
      "iteration no 123: Loss: 0.7578348401376941, accuracy: 0.5166666666666667\n",
      "iteration no 124: Loss: 0.7577952324506393, accuracy: 0.5166666666666667\n",
      "iteration no 125: Loss: 0.7577569580886443, accuracy: 0.5166666666666667\n",
      "iteration no 126: Loss: 0.7577199690180729, accuracy: 0.5166666666666667\n",
      "iteration no 127: Loss: 0.7576842190918037, accuracy: 0.5166666666666667\n",
      "iteration no 128: Loss: 0.7576496639672262, accuracy: 0.5166666666666667\n",
      "iteration no 129: Loss: 0.7576162610282136, accuracy: 0.5133333333333333\n",
      "iteration no 130: Loss: 0.7575839693108558, accuracy: 0.5133333333333333\n",
      "iteration no 131: Loss: 0.7575527494327536, accuracy: 0.5133333333333333\n",
      "iteration no 132: Loss: 0.75752256352568, accuracy: 0.5133333333333333\n",
      "iteration no 133: Loss: 0.7574933751714327, accuracy: 0.5133333333333333\n",
      "iteration no 134: Loss: 0.7574651493407077, accuracy: 0.5133333333333333\n",
      "iteration no 135: Loss: 0.7574378523348383, accuracy: 0.5133333333333333\n",
      "iteration no 136: Loss: 0.7574114517302443, accuracy: 0.5133333333333333\n",
      "iteration no 137: Loss: 0.7573859163254589, accuracy: 0.5133333333333333\n",
      "iteration no 138: Loss: 0.7573612160905934, accuracy: 0.5133333333333333\n",
      "iteration no 139: Loss: 0.7573373221191183, accuracy: 0.5133333333333333\n",
      "iteration no 140: Loss: 0.7573142065818428, accuracy: 0.5133333333333333\n",
      "iteration no 141: Loss: 0.7572918426829817, accuracy: 0.5133333333333333\n",
      "iteration no 142: Loss: 0.7572702046182006, accuracy: 0.5133333333333333\n",
      "iteration no 143: Loss: 0.7572492675345485, accuracy: 0.5133333333333333\n",
      "iteration no 144: Loss: 0.757229007492173, accuracy: 0.5133333333333333\n",
      "iteration no 145: Loss: 0.7572094014277423, accuracy: 0.5133333333333333\n",
      "iteration no 146: Loss: 0.7571904271194771, accuracy: 0.5133333333333333\n",
      "iteration no 147: Loss: 0.7571720631537258, accuracy: 0.5133333333333333\n",
      "iteration no 148: Loss: 0.7571542888929971, accuracy: 0.5133333333333333\n",
      "iteration no 149: Loss: 0.7571370844453874, accuracy: 0.5133333333333333\n",
      "iteration no 150: Loss: 0.7571204306353327, accuracy: 0.5133333333333333\n",
      "iteration no 151: Loss: 0.7571043089756201, accuracy: 0.5133333333333333\n",
      "iteration no 152: Loss: 0.757088701640601, accuracy: 0.5133333333333333\n",
      "iteration no 153: Loss: 0.7570735914405495, accuracy: 0.5133333333333333\n",
      "iteration no 154: Loss: 0.7570589617971095, accuracy: 0.5133333333333333\n",
      "iteration no 155: Loss: 0.7570447967197818, accuracy: 0.5133333333333333\n",
      "iteration no 156: Loss: 0.7570310807834006, accuracy: 0.5133333333333333\n",
      "iteration no 157: Loss: 0.7570177991065569, accuracy: 0.5133333333333333\n",
      "iteration no 158: Loss: 0.7570049373309186, accuracy: 0.5133333333333333\n",
      "iteration no 159: Loss: 0.7569924816014147, accuracy: 0.5133333333333333\n",
      "iteration no 160: Loss: 0.7569804185472366, accuracy: 0.5133333333333333\n",
      "iteration no 161: Loss: 0.7569687352636226, accuracy: 0.5133333333333333\n",
      "iteration no 162: Loss: 0.7569574192943908, accuracy: 0.5133333333333333\n",
      "iteration no 163: Loss: 0.7569464586151835, accuracy: 0.5133333333333333\n",
      "iteration no 164: Loss: 0.7569358416173961, accuracy: 0.5133333333333333\n",
      "iteration no 165: Loss: 0.7569255570927547, accuracy: 0.5133333333333333\n",
      "iteration no 166: Loss: 0.7569155942185161, accuracy: 0.5133333333333333\n",
      "iteration no 167: Loss: 0.7569059425432634, accuracy: 0.5133333333333333\n",
      "iteration no 168: Loss: 0.75689659197327, accuracy: 0.5133333333333333\n",
      "iteration no 169: Loss: 0.756887532759406, accuracy: 0.5133333333333333\n",
      "iteration no 170: Loss: 0.7568787554845647, accuracy: 0.5133333333333333\n",
      "iteration no 171: Loss: 0.7568702510515882, accuracy: 0.5133333333333333\n",
      "iteration no 172: Loss: 0.7568620106716646, accuracy: 0.5133333333333333\n",
      "iteration no 173: Loss: 0.7568540258531854, accuracy: 0.5133333333333333\n",
      "iteration no 174: Loss: 0.7568462883910339, accuracy: 0.5133333333333333\n",
      "iteration no 175: Loss: 0.7568387903562942, accuracy: 0.5133333333333333\n",
      "iteration no 176: Loss: 0.7568315240863576, accuracy: 0.5133333333333333\n",
      "iteration no 177: Loss: 0.7568244821754118, accuracy: 0.5133333333333333\n",
      "iteration no 178: Loss: 0.7568176574652968, accuracy: 0.5133333333333333\n",
      "iteration no 179: Loss: 0.7568110430367111, accuracy: 0.5133333333333333\n",
      "iteration no 180: Loss: 0.7568046322007551, accuracy: 0.5133333333333333\n",
      "iteration no 181: Loss: 0.756798418490796, accuracy: 0.5133333333333333\n",
      "iteration no 182: Loss: 0.7567923956546411, accuracy: 0.5133333333333333\n",
      "iteration no 183: Loss: 0.756786557647009, accuracy: 0.5133333333333333\n",
      "iteration no 184: Loss: 0.7567808986222829, accuracy: 0.5133333333333333\n",
      "iteration no 185: Loss: 0.7567754129275381, accuracy: 0.5133333333333333\n",
      "iteration no 186: Loss: 0.756770095095828, accuracy: 0.5133333333333333\n",
      "iteration no 187: Loss: 0.7567649398397241, accuracy: 0.5133333333333333\n",
      "iteration no 188: Loss: 0.7567599420450937, accuracy: 0.5133333333333333\n",
      "iteration no 189: Loss: 0.7567550967651095, accuracy: 0.5133333333333333\n",
      "iteration no 190: Loss: 0.7567503992144814, accuracy: 0.5133333333333333\n",
      "iteration no 191: Loss: 0.7567458447638988, accuracy: 0.5133333333333333\n",
      "iteration no 192: Loss: 0.7567414289346802, accuracy: 0.5133333333333333\n",
      "iteration no 193: Loss: 0.7567371473936152, accuracy: 0.5133333333333333\n",
      "iteration no 194: Loss: 0.7567329959479975, accuracy: 0.5133333333333333\n",
      "iteration no 195: Loss: 0.7567289705408388, accuracy: 0.5133333333333333\n",
      "iteration no 196: Loss: 0.7567250672462534, accuracy: 0.5133333333333333\n",
      "iteration no 197: Loss: 0.7567212822650141, accuracy: 0.5133333333333333\n",
      "iteration no 198: Loss: 0.7567176119202651, accuracy: 0.5133333333333333\n",
      "iteration no 199: Loss: 0.7567140526533905, accuracy: 0.5133333333333333\n",
      "iteration no 200: Loss: 0.7567106010200315, accuracy: 0.5133333333333333\n",
      "iteration no 1: Loss: 1.0990389651694996, accuracy: 0.48\n",
      "iteration no 2: Loss: 1.0682954582758224, accuracy: 0.49\n",
      "iteration no 3: Loss: 1.0409560909464495, accuracy: 0.49333333333333335\n",
      "iteration no 4: Loss: 1.0166049452043655, accuracy: 0.5066666666666667\n",
      "iteration no 5: Loss: 0.9948755075779294, accuracy: 0.5233333333333333\n",
      "iteration no 6: Loss: 0.9754457044258443, accuracy: 0.5233333333333333\n",
      "iteration no 7: Loss: 0.9580336366551306, accuracy: 0.52\n",
      "iteration no 8: Loss: 0.9423934472116081, accuracy: 0.5166666666666667\n",
      "iteration no 9: Loss: 0.928311270037605, accuracy: 0.5166666666666667\n",
      "iteration no 10: Loss: 0.9156013547108235, accuracy: 0.5166666666666667\n",
      "iteration no 11: Loss: 0.9041024657401614, accuracy: 0.5166666666666667\n",
      "iteration no 12: Loss: 0.893674619386807, accuracy: 0.5166666666666667\n",
      "iteration no 13: Loss: 0.8841961826163895, accuracy: 0.5166666666666667\n",
      "iteration no 14: Loss: 0.8755613301482797, accuracy: 0.51\n",
      "iteration no 15: Loss: 0.8676778377122746, accuracy: 0.5133333333333333\n",
      "iteration no 16: Loss: 0.8604651803301603, accuracy: 0.5166666666666667\n",
      "iteration no 17: Loss: 0.8538529009987267, accuracy: 0.5166666666666667\n",
      "iteration no 18: Loss: 0.8477792153170984, accuracy: 0.5166666666666667\n",
      "iteration no 19: Loss: 0.8421898197736672, accuracy: 0.5166666666666667\n",
      "iteration no 20: Loss: 0.8370368745390904, accuracy: 0.5133333333333333\n",
      "iteration no 21: Loss: 0.8322781350606172, accuracy: 0.5133333333333333\n",
      "iteration no 22: Loss: 0.8278762101522343, accuracy: 0.5133333333333333\n",
      "iteration no 23: Loss: 0.8237979274325092, accuracy: 0.5133333333333333\n",
      "iteration no 24: Loss: 0.8200137897919477, accuracy: 0.5133333333333333\n",
      "iteration no 25: Loss: 0.8164975090506806, accuracy: 0.5133333333333333\n",
      "iteration no 26: Loss: 0.8132256051060083, accuracy: 0.5133333333333333\n",
      "iteration no 27: Loss: 0.8101770606955405, accuracy: 0.5133333333333333\n",
      "iteration no 28: Loss: 0.8073330234500752, accuracy: 0.5166666666666667\n",
      "iteration no 29: Loss: 0.8046765482170871, accuracy: 0.5166666666666667\n",
      "iteration no 30: Loss: 0.8021923737350738, accuracy: 0.5166666666666667\n",
      "iteration no 31: Loss: 0.7998667286622464, accuracy: 0.5166666666666667\n",
      "iteration no 32: Loss: 0.7976871627376331, accuracy: 0.5166666666666667\n",
      "iteration no 33: Loss: 0.7956423995023685, accuracy: 0.5166666666666667\n",
      "iteration no 34: Loss: 0.7937222075540362, accuracy: 0.52\n",
      "iteration no 35: Loss: 0.791917287764567, accuracy: 0.52\n",
      "iteration no 36: Loss: 0.7902191742767861, accuracy: 0.52\n",
      "iteration no 37: Loss: 0.7886201474182857, accuracy: 0.5166666666666667\n",
      "iteration no 38: Loss: 0.7871131569439412, accuracy: 0.5166666666666667\n",
      "iteration no 39: Loss: 0.7856917542484794, accuracy: 0.5166666666666667\n",
      "iteration no 40: Loss: 0.784350032384964, accuracy: 0.5166666666666667\n",
      "iteration no 41: Loss: 0.7830825728897503, accuracy: 0.5166666666666667\n",
      "iteration no 42: Loss: 0.7818843985541313, accuracy: 0.5166666666666667\n",
      "iteration no 43: Loss: 0.7807509314016025, accuracy: 0.5133333333333333\n",
      "iteration no 44: Loss: 0.7796779552307317, accuracy: 0.5133333333333333\n",
      "iteration no 45: Loss: 0.7786615821698376, accuracy: 0.5133333333333333\n",
      "iteration no 46: Loss: 0.777698222763338, accuracy: 0.5133333333333333\n",
      "iteration no 47: Loss: 0.7767845591727248, accuracy: 0.5133333333333333\n",
      "iteration no 48: Loss: 0.7759175211292354, accuracy: 0.5133333333333333\n",
      "iteration no 49: Loss: 0.7750942643217938, accuracy: 0.5133333333333333\n",
      "iteration no 50: Loss: 0.7743121509438544, accuracy: 0.5133333333333333\n",
      "iteration no 51: Loss: 0.7735687321573248, accuracy: 0.5133333333333333\n",
      "iteration no 52: Loss: 0.7728617322616015, accuracy: 0.5133333333333333\n",
      "iteration no 53: Loss: 0.7721890343816071, accuracy: 0.5133333333333333\n",
      "iteration no 54: Loss: 0.7715486675111408, accuracy: 0.5133333333333333\n",
      "iteration no 55: Loss: 0.7709387947673304, accuracy: 0.5133333333333333\n",
      "iteration no 56: Loss: 0.7703577027289372, accuracy: 0.5133333333333333\n",
      "iteration no 57: Loss: 0.7698037917460425, accuracy: 0.5133333333333333\n",
      "iteration no 58: Loss: 0.7692755671215596, accuracy: 0.5133333333333333\n",
      "iteration no 59: Loss: 0.7687716310763063, accuracy: 0.5133333333333333\n",
      "iteration no 60: Loss: 0.7682906754192644, accuracy: 0.5133333333333333\n",
      "iteration no 61: Loss: 0.7678314748533445, accuracy: 0.5166666666666667\n",
      "iteration no 62: Loss: 0.7673928808545942, accuracy: 0.5166666666666667\n",
      "iteration no 63: Loss: 0.7669738160695178, accuracy: 0.5166666666666667\n",
      "iteration no 64: Loss: 0.7665732691810853, accuracy: 0.5166666666666667\n",
      "iteration no 65: Loss: 0.76619029019925, accuracy: 0.5166666666666667\n",
      "iteration no 66: Loss: 0.7658239861364058, accuracy: 0.5166666666666667\n",
      "iteration no 67: Loss: 0.7654735170323161, accuracy: 0.5166666666666667\n",
      "iteration no 68: Loss: 0.765138092296673, accuracy: 0.5166666666666667\n",
      "iteration no 69: Loss: 0.7648169673406635, accuracy: 0.5166666666666667\n",
      "iteration no 70: Loss: 0.7645094404717955, accuracy: 0.5166666666666667\n",
      "iteration no 71: Loss: 0.7642148500287713, accuracy: 0.5166666666666667\n",
      "iteration no 72: Loss: 0.7639325717354845, accuracy: 0.5166666666666667\n",
      "iteration no 73: Loss: 0.7636620162552323, accuracy: 0.5166666666666667\n",
      "iteration no 74: Loss: 0.7634026269280615, accuracy: 0.5166666666666667\n",
      "iteration no 75: Loss: 0.7631538776757706, accuracy: 0.5166666666666667\n",
      "iteration no 76: Loss: 0.7629152710605659, accuracy: 0.5166666666666667\n",
      "iteration no 77: Loss: 0.7626863364846497, accuracy: 0.5166666666666667\n",
      "iteration no 78: Loss: 0.7624666285192088, accuracy: 0.5166666666666667\n",
      "iteration no 79: Loss: 0.7622557253523051, accuracy: 0.5166666666666667\n",
      "iteration no 80: Loss: 0.7620532273461322, accuracy: 0.5133333333333333\n",
      "iteration no 81: Loss: 0.7618587556949425, accuracy: 0.5133333333333333\n",
      "iteration no 82: Loss: 0.7616719511757257, accuracy: 0.5133333333333333\n",
      "iteration no 83: Loss: 0.7614924729844078, accuracy: 0.5133333333333333\n",
      "iteration no 84: Loss: 0.7613199976509709, accuracy: 0.5133333333333333\n",
      "iteration no 85: Loss: 0.7611542180274609, accuracy: 0.5133333333333333\n",
      "iteration no 86: Loss: 0.7609948423433589, accuracy: 0.5133333333333333\n",
      "iteration no 87: Loss: 0.7608415933232674, accuracy: 0.5133333333333333\n",
      "iteration no 88: Loss: 0.7606942073622714, accuracy: 0.5133333333333333\n",
      "iteration no 89: Loss: 0.7605524337547334, accuracy: 0.5133333333333333\n",
      "iteration no 90: Loss: 0.7604160339726136, accuracy: 0.5133333333333333\n",
      "iteration no 91: Loss: 0.7602847809897381, accuracy: 0.5133333333333333\n",
      "iteration no 92: Loss: 0.7601584586487176, accuracy: 0.5133333333333333\n",
      "iteration no 93: Loss: 0.7600368610674837, accuracy: 0.5133333333333333\n",
      "iteration no 94: Loss: 0.7599197920826533, accuracy: 0.5133333333333333\n",
      "iteration no 95: Loss: 0.7598070647271471, accuracy: 0.5133333333333333\n",
      "iteration no 96: Loss: 0.7596985007396895, accuracy: 0.5133333333333333\n",
      "iteration no 97: Loss: 0.7595939301040004, accuracy: 0.5133333333333333\n",
      "iteration no 98: Loss: 0.7594931906156559, accuracy: 0.5133333333333333\n",
      "iteration no 99: Loss: 0.7593961274747503, accuracy: 0.5133333333333333\n",
      "iteration no 100: Loss: 0.7593025929026299, accuracy: 0.5133333333333333\n",
      "iteration no 101: Loss: 0.7592124457810979, accuracy: 0.5133333333333333\n",
      "iteration no 102: Loss: 0.7591255513126127, accuracy: 0.5133333333333333\n",
      "iteration no 103: Loss: 0.759041780700103, accuracy: 0.5133333333333333\n",
      "iteration no 104: Loss: 0.7589610108451287, accuracy: 0.5166666666666667\n",
      "iteration no 105: Loss: 0.7588831240632075, accuracy: 0.5166666666666667\n",
      "iteration no 106: Loss: 0.7588080078152092, accuracy: 0.5166666666666667\n",
      "iteration no 107: Loss: 0.7587355544538005, accuracy: 0.5166666666666667\n",
      "iteration no 108: Loss: 0.7586656609839938, accuracy: 0.5166666666666667\n",
      "iteration no 109: Loss: 0.7585982288369173, accuracy: 0.5166666666666667\n",
      "iteration no 110: Loss: 0.7585331636559913, accuracy: 0.5166666666666667\n",
      "iteration no 111: Loss: 0.7584703750947438, accuracy: 0.5166666666666667\n",
      "iteration no 112: Loss: 0.7584097766255574, accuracy: 0.5166666666666667\n",
      "iteration no 113: Loss: 0.7583512853586829, accuracy: 0.5166666666666667\n",
      "iteration no 114: Loss: 0.7582948218709051, accuracy: 0.5166666666666667\n",
      "iteration no 115: Loss: 0.7582403100432791, accuracy: 0.5166666666666667\n",
      "iteration no 116: Loss: 0.7581876769074061, accuracy: 0.5166666666666667\n",
      "iteration no 117: Loss: 0.7581368524997375, accuracy: 0.5166666666666667\n",
      "iteration no 118: Loss: 0.7580877697234449, accuracy: 0.5166666666666667\n",
      "iteration no 119: Loss: 0.7580403642174126, accuracy: 0.5166666666666667\n",
      "iteration no 120: Loss: 0.7579945742319422, accuracy: 0.5166666666666667\n",
      "iteration no 121: Loss: 0.757950340510786, accuracy: 0.5166666666666667\n",
      "iteration no 122: Loss: 0.7579076061791473, accuracy: 0.5166666666666667\n",
      "iteration no 123: Loss: 0.7578663166373121, accuracy: 0.5166666666666667\n",
      "iteration no 124: Loss: 0.757826419459593, accuracy: 0.5166666666666667\n",
      "iteration no 125: Loss: 0.7577878642982928, accuracy: 0.5166666666666667\n",
      "iteration no 126: Loss: 0.7577506027924069, accuracy: 0.5166666666666667\n",
      "iteration no 127: Loss: 0.757714588480803, accuracy: 0.5166666666666667\n",
      "iteration no 128: Loss: 0.757679776719635, accuracy: 0.5166666666666667\n",
      "iteration no 129: Loss: 0.7576461246037615, accuracy: 0.5166666666666667\n",
      "iteration no 130: Loss: 0.7576135908919467, accuracy: 0.5166666666666667\n",
      "iteration no 131: Loss: 0.7575821359356495, accuracy: 0.5133333333333333\n",
      "iteration no 132: Loss: 0.7575517216112025, accuracy: 0.5133333333333333\n",
      "iteration no 133: Loss: 0.757522311255202, accuracy: 0.5133333333333333\n",
      "iteration no 134: Loss: 0.7574938696029434, accuracy: 0.5133333333333333\n",
      "iteration no 135: Loss: 0.7574663627297381, accuracy: 0.5133333333333333\n",
      "iteration no 136: Loss: 0.7574397579949633, accuracy: 0.5133333333333333\n",
      "iteration no 137: Loss: 0.7574140239887056, accuracy: 0.5133333333333333\n",
      "iteration no 138: Loss: 0.7573891304808598, accuracy: 0.5133333333333333\n",
      "iteration no 139: Loss: 0.7573650483725615, accuracy: 0.5133333333333333\n",
      "iteration no 140: Loss: 0.7573417496498339, accuracy: 0.5133333333333333\n",
      "iteration no 141: Loss: 0.7573192073393348, accuracy: 0.5133333333333333\n",
      "iteration no 142: Loss: 0.7572973954661005, accuracy: 0.5133333333333333\n",
      "iteration no 143: Loss: 0.7572762890131848, accuracy: 0.5133333333333333\n",
      "iteration no 144: Loss: 0.7572558638831004, accuracy: 0.5133333333333333\n",
      "iteration no 145: Loss: 0.7572360968609732, accuracy: 0.5133333333333333\n",
      "iteration no 146: Loss: 0.7572169655793224, accuracy: 0.5133333333333333\n",
      "iteration no 147: Loss: 0.7571984484843938, accuracy: 0.5133333333333333\n",
      "iteration no 148: Loss: 0.7571805248039607, accuracy: 0.5133333333333333\n",
      "iteration no 149: Loss: 0.7571631745165315, accuracy: 0.5133333333333333\n",
      "iteration no 150: Loss: 0.7571463783218887, accuracy: 0.5133333333333333\n",
      "iteration no 151: Loss: 0.7571301176129, accuracy: 0.5133333333333333\n",
      "iteration no 152: Loss: 0.7571143744485376, accuracy: 0.5133333333333333\n",
      "iteration no 153: Loss: 0.7570991315280525, accuracy: 0.5133333333333333\n",
      "iteration no 154: Loss: 0.7570843721662449, accuracy: 0.5133333333333333\n",
      "iteration no 155: Loss: 0.7570700802697821, accuracy: 0.5133333333333333\n",
      "iteration no 156: Loss: 0.7570562403145139, accuracy: 0.5133333333333333\n",
      "iteration no 157: Loss: 0.7570428373237409, accuracy: 0.5133333333333333\n",
      "iteration no 158: Loss: 0.7570298568473878, accuracy: 0.5133333333333333\n",
      "iteration no 159: Loss: 0.7570172849420445, accuracy: 0.5133333333333333\n",
      "iteration no 160: Loss: 0.7570051081518326, accuracy: 0.5133333333333333\n",
      "iteration no 161: Loss: 0.7569933134900614, accuracy: 0.5133333333333333\n",
      "iteration no 162: Loss: 0.7569818884216339, accuracy: 0.5133333333333333\n",
      "iteration no 163: Loss: 0.7569708208461761, accuracy: 0.5133333333333333\n",
      "iteration no 164: Loss: 0.7569600990818478, accuracy: 0.5133333333333333\n",
      "iteration no 165: Loss: 0.7569497118498139, accuracy: 0.5133333333333333\n",
      "iteration no 166: Loss: 0.7569396482593396, accuracy: 0.5133333333333333\n",
      "iteration no 167: Loss: 0.7569298977934855, accuracy: 0.5133333333333333\n",
      "iteration no 168: Loss: 0.7569204502953754, accuracy: 0.5133333333333333\n",
      "iteration no 169: Loss: 0.7569112959550109, accuracy: 0.5133333333333333\n",
      "iteration no 170: Loss: 0.756902425296611, accuracy: 0.5133333333333333\n",
      "iteration no 171: Loss: 0.7568938291664508, accuracy: 0.5133333333333333\n",
      "iteration no 172: Loss: 0.756885498721181, accuracy: 0.5133333333333333\n",
      "iteration no 173: Loss: 0.7568774254166042, accuracy: 0.5133333333333333\n",
      "iteration no 174: Loss: 0.7568696009968918, accuracy: 0.5133333333333333\n",
      "iteration no 175: Loss: 0.7568620174842196, accuracy: 0.5133333333333333\n",
      "iteration no 176: Loss: 0.756854667168807, accuracy: 0.5133333333333333\n",
      "iteration no 177: Loss: 0.7568475425993397, accuracy: 0.5133333333333333\n",
      "iteration no 178: Loss: 0.7568406365737628, accuracy: 0.5133333333333333\n",
      "iteration no 179: Loss: 0.7568339421304267, accuracy: 0.5133333333333333\n",
      "iteration no 180: Loss: 0.7568274525395722, accuracy: 0.5133333333333333\n",
      "iteration no 181: Loss: 0.7568211612951397, accuracy: 0.5133333333333333\n",
      "iteration no 182: Loss: 0.7568150621068893, accuracy: 0.5133333333333333\n",
      "iteration no 183: Loss: 0.7568091488928206, accuracy: 0.5133333333333333\n",
      "iteration no 184: Loss: 0.756803415771876, accuracy: 0.5133333333333333\n",
      "iteration no 185: Loss: 0.7567978570569216, accuracy: 0.5133333333333333\n",
      "iteration no 186: Loss: 0.7567924672479872, accuracy: 0.5133333333333333\n",
      "iteration no 187: Loss: 0.7567872410257636, accuracy: 0.5133333333333333\n",
      "iteration no 188: Loss: 0.7567821732453366, accuracy: 0.5133333333333333\n",
      "iteration no 189: Loss: 0.7567772589301582, accuracy: 0.5133333333333333\n",
      "iteration no 190: Loss: 0.756772493266238, accuracy: 0.5133333333333333\n",
      "iteration no 191: Loss: 0.7567678715965499, accuracy: 0.5133333333333333\n",
      "iteration no 192: Loss: 0.7567633894156437, accuracy: 0.5133333333333333\n",
      "iteration no 193: Loss: 0.7567590423644551, accuracy: 0.5133333333333333\n",
      "iteration no 194: Loss: 0.7567548262253043, accuracy: 0.5133333333333333\n",
      "iteration no 195: Loss: 0.7567507369170775, accuracy: 0.5133333333333333\n",
      "iteration no 196: Loss: 0.756746770490583, accuracy: 0.5133333333333333\n",
      "iteration no 197: Loss: 0.7567429231240752, accuracy: 0.5133333333333333\n",
      "iteration no 198: Loss: 0.7567391911189412, accuracy: 0.5133333333333333\n",
      "iteration no 199: Loss: 0.756735570895541, accuracy: 0.5133333333333333\n",
      "iteration no 200: Loss: 0.7567320589891989, accuracy: 0.5133333333333333\n",
      "iteration no 1: Loss: 1.098333394054288, accuracy: 0.5233333333333333\n",
      "iteration no 2: Loss: 1.0676635081868366, accuracy: 0.5166666666666667\n",
      "iteration no 3: Loss: 1.040366980716308, accuracy: 0.5166666666666667\n",
      "iteration no 4: Loss: 1.016039014667309, accuracy: 0.5133333333333333\n",
      "iteration no 5: Loss: 0.9943201296293288, accuracy: 0.5166666666666667\n",
      "iteration no 6: Loss: 0.9748930008792122, accuracy: 0.52\n",
      "iteration no 7: Loss: 0.9574790518612712, accuracy: 0.51\n",
      "iteration no 8: Loss: 0.94183476776467, accuracy: 0.5066666666666667\n",
      "iteration no 9: Loss: 0.9277479098522567, accuracy: 0.5066666666666667\n",
      "iteration no 10: Loss: 0.9150338208382145, accuracy: 0.5066666666666667\n",
      "iteration no 11: Loss: 0.9035319590777181, accuracy: 0.51\n",
      "iteration no 12: Loss: 0.8931027402694098, accuracy: 0.51\n",
      "iteration no 13: Loss: 0.8836247185499638, accuracy: 0.51\n",
      "iteration no 14: Loss: 0.87499210726139, accuracy: 0.51\n",
      "iteration no 15: Loss: 0.8671126208068187, accuracy: 0.51\n",
      "iteration no 16: Loss: 0.8599056092692823, accuracy: 0.51\n",
      "iteration no 17: Loss: 0.853300453680544, accuracy: 0.51\n",
      "iteration no 18: Loss: 0.8472351896392872, accuracy: 0.5133333333333333\n",
      "iteration no 19: Loss: 0.8416553287902259, accuracy: 0.5133333333333333\n",
      "iteration no 20: Loss: 0.8365128504644747, accuracy: 0.5133333333333333\n",
      "iteration no 21: Loss: 0.8317653389245668, accuracy: 0.5133333333333333\n",
      "iteration no 22: Loss: 0.827375244796691, accuracy: 0.5133333333333333\n",
      "iteration no 23: Loss: 0.8233092522169048, accuracy: 0.5133333333333333\n",
      "iteration no 24: Loss: 0.8195377358780154, accuracy: 0.5166666666666667\n",
      "iteration no 25: Loss: 0.8160342945101934, accuracy: 0.5166666666666667\n",
      "iteration no 26: Loss: 0.8127753493654234, accuracy: 0.5166666666666667\n",
      "iteration no 27: Loss: 0.8097397980252319, accuracy: 0.5166666666666667\n",
      "iteration no 28: Loss: 0.8069087153421987, accuracy: 0.5133333333333333\n",
      "iteration no 29: Loss: 0.8042650945901502, accuracy: 0.5133333333333333\n",
      "iteration no 30: Loss: 0.8017936229664193, accuracy: 0.51\n",
      "iteration no 31: Loss: 0.7994804864905118, accuracy: 0.5133333333333333\n",
      "iteration no 32: Loss: 0.7973132001022076, accuracy: 0.5133333333333333\n",
      "iteration no 33: Loss: 0.7952804594006829, accuracy: 0.5133333333333333\n",
      "iteration no 34: Loss: 0.7933720110036275, accuracy: 0.5133333333333333\n",
      "iteration no 35: Loss: 0.7915785389577928, accuracy: 0.5133333333333333\n",
      "iteration no 36: Loss: 0.7898915650136025, accuracy: 0.5133333333333333\n",
      "iteration no 37: Loss: 0.7883033608979559, accuracy: 0.5133333333333333\n",
      "iteration no 38: Loss: 0.7868068709907993, accuracy: 0.5133333333333333\n",
      "iteration no 39: Loss: 0.785395644040545, accuracy: 0.5133333333333333\n",
      "iteration no 40: Loss: 0.7840637727477455, accuracy: 0.5133333333333333\n",
      "iteration no 41: Loss: 0.7828058402112235, accuracy: 0.5133333333333333\n",
      "iteration no 42: Loss: 0.7816168723708367, accuracy: 0.5133333333333333\n",
      "iteration no 43: Loss: 0.7804922957001675, accuracy: 0.5133333333333333\n",
      "iteration no 44: Loss: 0.779427899503932, accuracy: 0.5133333333333333\n",
      "iteration no 45: Loss: 0.778419802261594, accuracy: 0.5133333333333333\n",
      "iteration no 46: Loss: 0.7774644215327993, accuracy: 0.5133333333333333\n",
      "iteration no 47: Loss: 0.7765584470037795, accuracy: 0.5133333333333333\n",
      "iteration no 48: Loss: 0.7756988163084146, accuracy: 0.5133333333333333\n",
      "iteration no 49: Loss: 0.7748826933045319, accuracy: 0.5133333333333333\n",
      "iteration no 50: Loss: 0.7741074485264291, accuracy: 0.5133333333333333\n",
      "iteration no 51: Loss: 0.7733706415694778, accuracy: 0.5133333333333333\n",
      "iteration no 52: Loss: 0.7726700051928015, accuracy: 0.5133333333333333\n",
      "iteration no 53: Loss: 0.7720034309521376, accuracy: 0.5133333333333333\n",
      "iteration no 54: Loss: 0.7713689561976355, accuracy: 0.5133333333333333\n",
      "iteration no 55: Loss: 0.7707647522910283, accuracy: 0.5133333333333333\n",
      "iteration no 56: Loss: 0.7701891139137466, accuracy: 0.5133333333333333\n",
      "iteration no 57: Loss: 0.7696404493524814, accuracy: 0.5133333333333333\n",
      "iteration no 58: Loss: 0.7691172716617485, accuracy: 0.5166666666666667\n",
      "iteration no 59: Loss: 0.7686181906144213, accuracy: 0.5166666666666667\n",
      "iteration no 60: Loss: 0.7681419053611933, accuracy: 0.5166666666666667\n",
      "iteration no 61: Loss: 0.7676871977287079, accuracy: 0.5166666666666667\n",
      "iteration no 62: Loss: 0.7672529260938032, accuracy: 0.5166666666666667\n",
      "iteration no 63: Loss: 0.7668380197781032, accuracy: 0.5166666666666667\n",
      "iteration no 64: Loss: 0.7664414739131727, accuracy: 0.5166666666666667\n",
      "iteration no 65: Loss: 0.7660623447317293, accuracy: 0.5166666666666667\n",
      "iteration no 66: Loss: 0.765699745245077, accuracy: 0.5166666666666667\n",
      "iteration no 67: Loss: 0.7653528412710571, accuracy: 0.5166666666666667\n",
      "iteration no 68: Loss: 0.7650208477804735, accuracy: 0.5166666666666667\n",
      "iteration no 69: Loss: 0.7647030255332034, accuracy: 0.5166666666666667\n",
      "iteration no 70: Loss: 0.7643986779780924, accuracy: 0.5166666666666667\n",
      "iteration no 71: Loss: 0.7641071483933066, accuracy: 0.5166666666666667\n",
      "iteration no 72: Loss: 0.7638278172461063, accuracy: 0.5166666666666667\n",
      "iteration no 73: Loss: 0.7635600997530472, accuracy: 0.5166666666666667\n",
      "iteration no 74: Loss: 0.7633034436234497, accuracy: 0.5166666666666667\n",
      "iteration no 75: Loss: 0.7630573269706041, accuracy: 0.5166666666666667\n",
      "iteration no 76: Loss: 0.7628212563766442, accuracy: 0.5166666666666667\n",
      "iteration no 77: Loss: 0.7625947650983373, accuracy: 0.5133333333333333\n",
      "iteration no 78: Loss: 0.7623774114022128, accuracy: 0.5133333333333333\n",
      "iteration no 79: Loss: 0.7621687770185096, accuracy: 0.5133333333333333\n",
      "iteration no 80: Loss: 0.7619684657043772, accuracy: 0.5133333333333333\n",
      "iteration no 81: Loss: 0.7617761019076192, accuracy: 0.5133333333333333\n",
      "iteration no 82: Loss: 0.761591329523046, accuracy: 0.5133333333333333\n",
      "iteration no 83: Loss: 0.761413810734189, accuracy: 0.5133333333333333\n",
      "iteration no 84: Loss: 0.7612432249337793, accuracy: 0.5133333333333333\n",
      "iteration no 85: Loss: 0.7610792677169389, accuracy: 0.5133333333333333\n",
      "iteration no 86: Loss: 0.7609216499415687, accuracy: 0.5133333333333333\n",
      "iteration no 87: Loss: 0.760770096850873, accuracy: 0.5133333333333333\n",
      "iteration no 88: Loss: 0.7606243472533872, accuracy: 0.5133333333333333\n",
      "iteration no 89: Loss: 0.7604841527562628, accuracy: 0.5133333333333333\n",
      "iteration no 90: Loss: 0.7603492770479094, accuracy: 0.5133333333333333\n",
      "iteration no 91: Loss: 0.760219495226411, accuracy: 0.5133333333333333\n",
      "iteration no 92: Loss: 0.7600945931704282, accuracy: 0.5133333333333333\n",
      "iteration no 93: Loss: 0.7599743669495566, accuracy: 0.5133333333333333\n",
      "iteration no 94: Loss: 0.7598586222713538, accuracy: 0.5133333333333333\n",
      "iteration no 95: Loss: 0.7597471739624669, accuracy: 0.5133333333333333\n",
      "iteration no 96: Loss: 0.7596398454814918, accuracy: 0.5133333333333333\n",
      "iteration no 97: Loss: 0.7595364684613803, accuracy: 0.5133333333333333\n",
      "iteration no 98: Loss: 0.7594368822793784, accuracy: 0.5133333333333333\n",
      "iteration no 99: Loss: 0.759340933652628, accuracy: 0.5133333333333333\n",
      "iteration no 100: Loss: 0.7592484762577142, accuracy: 0.5133333333333333\n",
      "iteration no 101: Loss: 0.7591593703725588, accuracy: 0.5133333333333333\n",
      "iteration no 102: Loss: 0.7590734825391855, accuracy: 0.5133333333333333\n",
      "iteration no 103: Loss: 0.7589906852459897, accuracy: 0.5166666666666667\n",
      "iteration no 104: Loss: 0.7589108566282438, accuracy: 0.5166666666666667\n",
      "iteration no 105: Loss: 0.7588338801856607, accuracy: 0.5166666666666667\n",
      "iteration no 106: Loss: 0.7587596445159277, accuracy: 0.5166666666666667\n",
      "iteration no 107: Loss: 0.7586880430631914, accuracy: 0.5166666666666667\n",
      "iteration no 108: Loss: 0.7586189738805558, accuracy: 0.5166666666666667\n",
      "iteration no 109: Loss: 0.758552339405713, accuracy: 0.5166666666666667\n",
      "iteration no 110: Loss: 0.7584880462488978, accuracy: 0.5166666666666667\n",
      "iteration no 111: Loss: 0.7584260049923967, accuracy: 0.5166666666666667\n",
      "iteration no 112: Loss: 0.7583661300009168, accuracy: 0.5166666666666667\n",
      "iteration no 113: Loss: 0.7583083392421451, accuracy: 0.5166666666666667\n",
      "iteration no 114: Loss: 0.75825255411689, accuracy: 0.5166666666666667\n",
      "iteration no 115: Loss: 0.7581986992982278, accuracy: 0.5166666666666667\n",
      "iteration no 116: Loss: 0.7581467025791246, accuracy: 0.5166666666666667\n",
      "iteration no 117: Loss: 0.758096494728026, accuracy: 0.5166666666666667\n",
      "iteration no 118: Loss: 0.7580480093519546, accuracy: 0.5166666666666667\n",
      "iteration no 119: Loss: 0.7580011827666757, accuracy: 0.5166666666666667\n",
      "iteration no 120: Loss: 0.757955953873521, accuracy: 0.5166666666666667\n",
      "iteration no 121: Loss: 0.757912264042493, accuracy: 0.5166666666666667\n",
      "iteration no 122: Loss: 0.7578700570012872, accuracy: 0.5166666666666667\n",
      "iteration no 123: Loss: 0.7578292787298997, accuracy: 0.5166666666666667\n",
      "iteration no 124: Loss: 0.7577898773605066, accuracy: 0.5166666666666667\n",
      "iteration no 125: Loss: 0.7577518030823187, accuracy: 0.5166666666666667\n",
      "iteration no 126: Loss: 0.7577150080511357, accuracy: 0.5166666666666667\n",
      "iteration no 127: Loss: 0.7576794463033424, accuracy: 0.5166666666666667\n",
      "iteration no 128: Loss: 0.7576450736741028, accuracy: 0.5166666666666667\n",
      "iteration no 129: Loss: 0.757611847719521, accuracy: 0.5166666666666667\n",
      "iteration no 130: Loss: 0.7575797276425587, accuracy: 0.5166666666666667\n",
      "iteration no 131: Loss: 0.757548674222503, accuracy: 0.5166666666666667\n",
      "iteration no 132: Loss: 0.7575186497477998, accuracy: 0.5133333333333333\n",
      "iteration no 133: Loss: 0.7574896179520673, accuracy: 0.5133333333333333\n",
      "iteration no 134: Loss: 0.7574615439531274, accuracy: 0.5133333333333333\n",
      "iteration no 135: Loss: 0.7574343941948963, accuracy: 0.5133333333333333\n",
      "iteration no 136: Loss: 0.7574081363919801, accuracy: 0.5133333333333333\n",
      "iteration no 137: Loss: 0.7573827394768428, accuracy: 0.5133333333333333\n",
      "iteration no 138: Loss: 0.7573581735494059, accuracy: 0.5133333333333333\n",
      "iteration no 139: Loss: 0.757334409828963, accuracy: 0.5133333333333333\n",
      "iteration no 140: Loss: 0.7573114206082845, accuracy: 0.5133333333333333\n",
      "iteration no 141: Loss: 0.7572891792098074, accuracy: 0.5133333333333333\n",
      "iteration no 142: Loss: 0.7572676599438002, accuracy: 0.5133333333333333\n",
      "iteration no 143: Loss: 0.7572468380684066, accuracy: 0.5133333333333333\n",
      "iteration no 144: Loss: 0.757226689751476, accuracy: 0.5133333333333333\n",
      "iteration no 145: Loss: 0.7572071920340872, accuracy: 0.5133333333333333\n",
      "iteration no 146: Loss: 0.7571883227956882, accuracy: 0.5133333333333333\n",
      "iteration no 147: Loss: 0.7571700607207676, accuracy: 0.5133333333333333\n",
      "iteration no 148: Loss: 0.7571523852669868, accuracy: 0.5133333333333333\n",
      "iteration no 149: Loss: 0.7571352766346989, accuracy: 0.5133333333333333\n",
      "iteration no 150: Loss: 0.757118715737791, accuracy: 0.5133333333333333\n",
      "iteration no 151: Loss: 0.7571026841757841, accuracy: 0.5133333333333333\n",
      "iteration no 152: Loss: 0.7570871642071305, accuracy: 0.5133333333333333\n",
      "iteration no 153: Loss: 0.7570721387236535, accuracy: 0.5133333333333333\n",
      "iteration no 154: Loss: 0.7570575912260746, accuracy: 0.5133333333333333\n",
      "iteration no 155: Loss: 0.7570435058005767, accuracy: 0.5133333333333333\n",
      "iteration no 156: Loss: 0.7570298670963548, accuracy: 0.5133333333333333\n",
      "iteration no 157: Loss: 0.7570166603041109, accuracy: 0.5133333333333333\n",
      "iteration no 158: Loss: 0.7570038711354449, accuracy: 0.5133333333333333\n",
      "iteration no 159: Loss: 0.7569914858031049, accuracy: 0.5133333333333333\n",
      "iteration no 160: Loss: 0.7569794910020535, accuracy: 0.5133333333333333\n",
      "iteration no 161: Loss: 0.7569678738913166, accuracy: 0.5133333333333333\n",
      "iteration no 162: Loss: 0.7569566220765758, accuracy: 0.5133333333333333\n",
      "iteration no 163: Loss: 0.756945723593474, accuracy: 0.5133333333333333\n",
      "iteration no 164: Loss: 0.7569351668915998, accuracy: 0.5133333333333333\n",
      "iteration no 165: Loss: 0.7569249408191207, accuracy: 0.5133333333333333\n",
      "iteration no 166: Loss: 0.7569150346080379, accuracy: 0.5133333333333333\n",
      "iteration no 167: Loss: 0.7569054378600336, accuracy: 0.5133333333333333\n",
      "iteration no 168: Loss: 0.7568961405328849, accuracy: 0.5133333333333333\n",
      "iteration no 169: Loss: 0.7568871329274187, accuracy: 0.5133333333333333\n",
      "iteration no 170: Loss: 0.7568784056749873, accuracy: 0.5133333333333333\n",
      "iteration no 171: Loss: 0.7568699497254374, accuracy: 0.5133333333333333\n",
      "iteration no 172: Loss: 0.7568617563355539, accuracy: 0.5133333333333333\n",
      "iteration no 173: Loss: 0.7568538170579582, accuracy: 0.5133333333333333\n",
      "iteration no 174: Loss: 0.7568461237304404, accuracy: 0.5133333333333333\n",
      "iteration no 175: Loss: 0.7568386684657067, accuracy: 0.5133333333333333\n",
      "iteration no 176: Loss: 0.7568314436415262, accuracy: 0.5133333333333333\n",
      "iteration no 177: Loss: 0.7568244418912575, accuracy: 0.5133333333333333\n",
      "iteration no 178: Loss: 0.7568176560947412, accuracy: 0.5133333333333333\n",
      "iteration no 179: Loss: 0.756811079369542, accuracy: 0.5133333333333333\n",
      "iteration no 180: Loss: 0.7568047050625262, accuracy: 0.5133333333333333\n",
      "iteration no 181: Loss: 0.7567985267417598, accuracy: 0.5133333333333333\n",
      "iteration no 182: Loss: 0.7567925381887154, accuracy: 0.5133333333333333\n",
      "iteration no 183: Loss: 0.756786733390773, accuracy: 0.5133333333333333\n",
      "iteration no 184: Loss: 0.7567811065340052, accuracy: 0.5133333333333333\n",
      "iteration no 185: Loss: 0.7567756519962325, accuracy: 0.5133333333333333\n",
      "iteration no 186: Loss: 0.7567703643403401, accuracy: 0.5133333333333333\n",
      "iteration no 187: Loss: 0.7567652383078429, accuracy: 0.5133333333333333\n",
      "iteration no 188: Loss: 0.7567602688126922, accuracy: 0.5133333333333333\n",
      "iteration no 189: Loss: 0.7567554509353107, accuracy: 0.5133333333333333\n",
      "iteration no 190: Loss: 0.7567507799168482, accuracy: 0.5133333333333333\n",
      "iteration no 191: Loss: 0.7567462511536505, accuracy: 0.5133333333333333\n",
      "iteration no 192: Loss: 0.7567418601919301, accuracy: 0.5133333333333333\n",
      "iteration no 193: Loss: 0.7567376027226337, accuracy: 0.5133333333333333\n",
      "iteration no 194: Loss: 0.756733474576496, accuracy: 0.5133333333333333\n",
      "iteration no 195: Loss: 0.7567294717192742, accuracy: 0.5133333333333333\n",
      "iteration no 196: Loss: 0.7567255902471561, accuracy: 0.5133333333333333\n",
      "iteration no 197: Loss: 0.7567218263823338, accuracy: 0.5133333333333333\n",
      "iteration no 198: Loss: 0.7567181764687378, accuracy: 0.5133333333333333\n",
      "iteration no 199: Loss: 0.7567146369679235, accuracy: 0.5133333333333333\n",
      "iteration no 200: Loss: 0.7567112044551068, accuracy: 0.5166666666666667\n",
      "iteration no 1: Loss: 1.1059345682815005, accuracy: 0.5033333333333333\n",
      "iteration no 2: Loss: 1.0742718652109855, accuracy: 0.5266666666666666\n",
      "iteration no 3: Loss: 1.0461309918418573, accuracy: 0.5066666666666667\n",
      "iteration no 4: Loss: 1.0210828790297972, accuracy: 0.5133333333333333\n",
      "iteration no 5: Loss: 0.9987495603330508, accuracy: 0.52\n",
      "iteration no 6: Loss: 0.9787981959368748, accuracy: 0.53\n",
      "iteration no 7: Loss: 0.9609367075363645, accuracy: 0.5233333333333333\n",
      "iteration no 8: Loss: 0.9449097917140216, accuracy: 0.5233333333333333\n",
      "iteration no 9: Loss: 0.930495011652657, accuracy: 0.5233333333333333\n",
      "iteration no 10: Loss: 0.9174989928620871, accuracy: 0.52\n",
      "iteration no 11: Loss: 0.9057538224144623, accuracy: 0.5166666666666667\n",
      "iteration no 12: Loss: 0.8951137352234665, accuracy: 0.5166666666666667\n",
      "iteration no 13: Loss: 0.8854521335839162, accuracy: 0.5166666666666667\n",
      "iteration no 14: Loss: 0.8766589525340514, accuracy: 0.5166666666666667\n",
      "iteration no 15: Loss: 0.8686383600912349, accuracy: 0.5166666666666667\n",
      "iteration no 16: Loss: 0.8613067675630305, accuracy: 0.5166666666666667\n",
      "iteration no 17: Loss: 0.8545911185172577, accuracy: 0.5166666666666667\n",
      "iteration no 18: Loss: 0.8484274231411106, accuracy: 0.5166666666666667\n",
      "iteration no 19: Loss: 0.8427595057074421, accuracy: 0.5166666666666667\n",
      "iteration no 20: Loss: 0.8375379353662459, accuracy: 0.5166666666666667\n",
      "iteration no 21: Loss: 0.8327191136408698, accuracy: 0.5166666666666667\n",
      "iteration no 22: Loss: 0.8282644953224316, accuracy: 0.5166666666666667\n",
      "iteration no 23: Loss: 0.824139922638947, accuracy: 0.5166666666666667\n",
      "iteration no 24: Loss: 0.8203150554863919, accuracy: 0.5166666666666667\n",
      "iteration no 25: Loss: 0.8167628830909888, accuracy: 0.5166666666666667\n",
      "iteration no 26: Loss: 0.8134593047177628, accuracy: 0.5166666666666667\n",
      "iteration no 27: Loss: 0.8103827689679952, accuracy: 0.5166666666666667\n",
      "iteration no 28: Loss: 0.8075139628479412, accuracy: 0.5166666666666667\n",
      "iteration no 29: Loss: 0.8048355431777073, accuracy: 0.5166666666666667\n",
      "iteration no 30: Loss: 0.8023319040769148, accuracy: 0.52\n",
      "iteration no 31: Loss: 0.7999889752448025, accuracy: 0.52\n",
      "iteration no 32: Loss: 0.7977940465754525, accuracy: 0.52\n",
      "iteration no 33: Loss: 0.7957356153388698, accuracy: 0.52\n",
      "iteration no 34: Loss: 0.793803252737239, accuracy: 0.52\n",
      "iteration no 35: Loss: 0.791987487131044, accuracy: 0.5166666666666667\n",
      "iteration no 36: Loss: 0.7902797016372546, accuracy: 0.5166666666666667\n",
      "iteration no 37: Loss: 0.7886720441443111, accuracy: 0.5166666666666667\n",
      "iteration no 38: Loss: 0.7871573480769459, accuracy: 0.5166666666666667\n",
      "iteration no 39: Loss: 0.7857290624869069, accuracy: 0.5166666666666667\n",
      "iteration no 40: Loss: 0.7843811902508345, accuracy: 0.5166666666666667\n",
      "iteration no 41: Loss: 0.7831082333300969, accuracy: 0.5166666666666667\n",
      "iteration no 42: Loss: 0.7819051441944274, accuracy: 0.5166666666666667\n",
      "iteration no 43: Loss: 0.7807672826360376, accuracy: 0.5166666666666667\n",
      "iteration no 44: Loss: 0.7796903773070282, accuracy: 0.5166666666666667\n",
      "iteration no 45: Loss: 0.7786704914033779, accuracy: 0.5166666666666667\n",
      "iteration no 46: Loss: 0.7777039919960043, accuracy: 0.5166666666666667\n",
      "iteration no 47: Loss: 0.7767875225754428, accuracy: 0.5166666666666667\n",
      "iteration no 48: Loss: 0.7759179784332946, accuracy: 0.5133333333333333\n",
      "iteration no 49: Loss: 0.7750924845521805, accuracy: 0.5133333333333333\n",
      "iteration no 50: Loss: 0.7743083757177597, accuracy: 0.5133333333333333\n",
      "iteration no 51: Loss: 0.773563178602386, accuracy: 0.5133333333333333\n",
      "iteration no 52: Loss: 0.7728545956010913, accuracy: 0.5133333333333333\n",
      "iteration no 53: Loss: 0.7721804902274914, accuracy: 0.5133333333333333\n",
      "iteration no 54: Loss: 0.7715388739005264, accuracy: 0.5133333333333333\n",
      "iteration no 55: Loss: 0.7709278939731915, accuracy: 0.5133333333333333\n",
      "iteration no 56: Loss: 0.7703458228720158, accuracy: 0.5133333333333333\n",
      "iteration no 57: Loss: 0.7697910482313829, accuracy: 0.5133333333333333\n",
      "iteration no 58: Loss: 0.7692620639201603, accuracy: 0.5133333333333333\n",
      "iteration no 59: Loss: 0.7687574618698123, accuracy: 0.5133333333333333\n",
      "iteration no 60: Loss: 0.7682759246233957, accuracy: 0.5133333333333333\n",
      "iteration no 61: Loss: 0.767816218533824, accuracy: 0.5133333333333333\n",
      "iteration no 62: Loss: 0.7673771875476713, accuracy: 0.5166666666666667\n",
      "iteration no 63: Loss: 0.7669577475177142, accuracy: 0.5166666666666667\n",
      "iteration no 64: Loss: 0.7665568809935326, accuracy: 0.5166666666666667\n",
      "iteration no 65: Loss: 0.7661736324448694, accuracy: 0.5166666666666667\n",
      "iteration no 66: Loss: 0.7658071038772201, accuracy: 0.5166666666666667\n",
      "iteration no 67: Loss: 0.7654564508033329, accuracy: 0.5166666666666667\n",
      "iteration no 68: Loss: 0.7651208785380385, accuracy: 0.5166666666666667\n",
      "iteration no 69: Loss: 0.7647996387871367, accuracy: 0.5166666666666667\n",
      "iteration no 70: Loss: 0.7644920265040189, accuracy: 0.5166666666666667\n",
      "iteration no 71: Loss: 0.7641973769903149, accuracy: 0.5166666666666667\n",
      "iteration no 72: Loss: 0.7639150632191939, accuracy: 0.5166666666666667\n",
      "iteration no 73: Loss: 0.7636444933620302, accuracy: 0.5166666666666667\n",
      "iteration no 74: Loss: 0.7633851085009946, accuracy: 0.5166666666666667\n",
      "iteration no 75: Loss: 0.7631363805118084, accuracy: 0.5166666666666667\n",
      "iteration no 76: Loss: 0.7628978101023692, accuracy: 0.5166666666666667\n",
      "iteration no 77: Loss: 0.7626689249943086, accuracy: 0.5166666666666667\n",
      "iteration no 78: Loss: 0.7624492782357223, accuracy: 0.5166666666666667\n",
      "iteration no 79: Loss: 0.7622384466344001, accuracy: 0.5133333333333333\n",
      "iteration no 80: Loss: 0.7620360293018477, accuracy: 0.5133333333333333\n",
      "iteration no 81: Loss: 0.7618416462992572, accuracy: 0.5133333333333333\n",
      "iteration no 82: Loss: 0.761654937377378, accuracy: 0.5133333333333333\n",
      "iteration no 83: Loss: 0.7614755608029397, accuracy: 0.5133333333333333\n",
      "iteration no 84: Loss: 0.7613031922649222, accuracy: 0.5133333333333333\n",
      "iteration no 85: Loss: 0.7611375238545477, accuracy: 0.5133333333333333\n",
      "iteration no 86: Loss: 0.7609782631133881, accuracy: 0.5133333333333333\n",
      "iteration no 87: Loss: 0.7608251321444611, accuracy: 0.5133333333333333\n",
      "iteration no 88: Loss: 0.7606778667816161, accuracy: 0.5133333333333333\n",
      "iteration no 89: Loss: 0.7605362158128988, accuracy: 0.5133333333333333\n",
      "iteration no 90: Loss: 0.7603999402539422, accuracy: 0.5133333333333333\n",
      "iteration no 91: Loss: 0.7602688126677541, accuracy: 0.5133333333333333\n",
      "iteration no 92: Loss: 0.7601426165275599, accuracy: 0.5133333333333333\n",
      "iteration no 93: Loss: 0.7600211456196322, accuracy: 0.5133333333333333\n",
      "iteration no 94: Loss: 0.7599042034832815, accuracy: 0.5133333333333333\n",
      "iteration no 95: Loss: 0.7597916028854015, accuracy: 0.5133333333333333\n",
      "iteration no 96: Loss: 0.7596831653271713, accuracy: 0.5133333333333333\n",
      "iteration no 97: Loss: 0.7595787205806945, accuracy: 0.5133333333333333\n",
      "iteration no 98: Loss: 0.7594781062535373, accuracy: 0.5133333333333333\n",
      "iteration no 99: Loss: 0.7593811673792674, accuracy: 0.5133333333333333\n",
      "iteration no 100: Loss: 0.7592877560322544, accuracy: 0.5133333333333333\n",
      "iteration no 101: Loss: 0.7591977309651099, accuracy: 0.5133333333333333\n",
      "iteration no 102: Loss: 0.7591109572672725, accuracy: 0.5133333333333333\n",
      "iteration no 103: Loss: 0.7590273060433529, accuracy: 0.5133333333333333\n",
      "iteration no 104: Loss: 0.7589466541099502, accuracy: 0.5166666666666667\n",
      "iteration no 105: Loss: 0.7588688837097517, accuracy: 0.5166666666666667\n",
      "iteration no 106: Loss: 0.7587938822418049, accuracy: 0.5166666666666667\n",
      "iteration no 107: Loss: 0.758721542006938, accuracy: 0.5166666666666667\n",
      "iteration no 108: Loss: 0.7586517599673713, accuracy: 0.5166666666666667\n",
      "iteration no 109: Loss: 0.7585844375196327, accuracy: 0.5166666666666667\n",
      "iteration no 110: Loss: 0.75851948027995, accuracy: 0.5166666666666667\n",
      "iteration no 111: Loss: 0.7584567978813518, accuracy: 0.5166666666666667\n",
      "iteration no 112: Loss: 0.75839630378176, accuracy: 0.5166666666666667\n",
      "iteration no 113: Loss: 0.7583379150824072, accuracy: 0.5166666666666667\n",
      "iteration no 114: Loss: 0.7582815523559536, accuracy: 0.5166666666666667\n",
      "iteration no 115: Loss: 0.7582271394837267, accuracy: 0.5166666666666667\n",
      "iteration no 116: Loss: 0.7581746035015371, accuracy: 0.5166666666666667\n",
      "iteration no 117: Loss: 0.7581238744535692, accuracy: 0.5166666666666667\n",
      "iteration no 118: Loss: 0.7580748852538678, accuracy: 0.5166666666666667\n",
      "iteration no 119: Loss: 0.7580275715549838, accuracy: 0.5166666666666667\n",
      "iteration no 120: Loss: 0.7579818716233595, accuracy: 0.5166666666666667\n",
      "iteration no 121: Loss: 0.757937726221074, accuracy: 0.5166666666666667\n",
      "iteration no 122: Loss: 0.757895078493577, accuracy: 0.5166666666666667\n",
      "iteration no 123: Loss: 0.7578538738630796, accuracy: 0.5166666666666667\n",
      "iteration no 124: Loss: 0.7578140599272785, accuracy: 0.5166666666666667\n",
      "iteration no 125: Loss: 0.7577755863631193, accuracy: 0.5166666666666667\n",
      "iteration no 126: Loss: 0.757738404835315, accuracy: 0.5166666666666667\n",
      "iteration no 127: Loss: 0.7577024689093612, accuracy: 0.5166666666666667\n",
      "iteration no 128: Loss: 0.7576677339687984, accuracy: 0.5166666666666667\n",
      "iteration no 129: Loss: 0.7576341571364923, accuracy: 0.5133333333333333\n",
      "iteration no 130: Loss: 0.7576016971997118, accuracy: 0.5133333333333333\n",
      "iteration no 131: Loss: 0.7575703145388051, accuracy: 0.5133333333333333\n",
      "iteration no 132: Loss: 0.7575399710592747, accuracy: 0.5133333333333333\n",
      "iteration no 133: Loss: 0.7575106301270781, accuracy: 0.5133333333333333\n",
      "iteration no 134: Loss: 0.7574822565069764, accuracy: 0.5133333333333333\n",
      "iteration no 135: Loss: 0.7574548163037791, accuracy: 0.5133333333333333\n",
      "iteration no 136: Loss: 0.7574282769063251, accuracy: 0.5133333333333333\n",
      "iteration no 137: Loss: 0.7574026069340649, accuracy: 0.5133333333333333\n",
      "iteration no 138: Loss: 0.757377776186106, accuracy: 0.5133333333333333\n",
      "iteration no 139: Loss: 0.7573537555925972, accuracy: 0.5133333333333333\n",
      "iteration no 140: Loss: 0.7573305171683317, accuracy: 0.5133333333333333\n",
      "iteration no 141: Loss: 0.7573080339684557, accuracy: 0.5133333333333333\n",
      "iteration no 142: Loss: 0.7572862800461789, accuracy: 0.5133333333333333\n",
      "iteration no 143: Loss: 0.7572652304123833, accuracy: 0.5133333333333333\n",
      "iteration no 144: Loss: 0.7572448609970395, accuracy: 0.5133333333333333\n",
      "iteration no 145: Loss: 0.7572251486123365, accuracy: 0.5133333333333333\n",
      "iteration no 146: Loss: 0.7572060709174447, accuracy: 0.5133333333333333\n",
      "iteration no 147: Loss: 0.7571876063848302, accuracy: 0.5133333333333333\n",
      "iteration no 148: Loss: 0.7571697342680436, accuracy: 0.5133333333333333\n",
      "iteration no 149: Loss: 0.7571524345709142, accuracy: 0.5133333333333333\n",
      "iteration no 150: Loss: 0.7571356880180808, accuracy: 0.5133333333333333\n",
      "iteration no 151: Loss: 0.7571194760267942, accuracy: 0.5133333333333333\n",
      "iteration no 152: Loss: 0.7571037806799312, accuracy: 0.5133333333333333\n",
      "iteration no 153: Loss: 0.7570885847001644, accuracy: 0.5133333333333333\n",
      "iteration no 154: Loss: 0.7570738714252299, accuracy: 0.5133333333333333\n",
      "iteration no 155: Loss: 0.7570596247842433, accuracy: 0.5133333333333333\n",
      "iteration no 156: Loss: 0.7570458292750164, accuracy: 0.5133333333333333\n",
      "iteration no 157: Loss: 0.7570324699423234, accuracy: 0.5133333333333333\n",
      "iteration no 158: Loss: 0.7570195323570792, accuracy: 0.5133333333333333\n",
      "iteration no 159: Loss: 0.757007002596381, accuracy: 0.5133333333333333\n",
      "iteration no 160: Loss: 0.7569948672243793, accuracy: 0.5133333333333333\n",
      "iteration no 161: Loss: 0.7569831132739361, accuracy: 0.5133333333333333\n",
      "iteration no 162: Loss: 0.7569717282290387, accuracy: 0.5133333333333333\n",
      "iteration no 163: Loss: 0.7569607000079313, accuracy: 0.5133333333333333\n",
      "iteration no 164: Loss: 0.7569500169469335, accuracy: 0.5133333333333333\n",
      "iteration no 165: Loss: 0.756939667784917, accuracy: 0.5133333333333333\n",
      "iteration no 166: Loss: 0.7569296416484065, accuracy: 0.5133333333333333\n",
      "iteration no 167: Loss: 0.7569199280372835, accuracy: 0.5133333333333333\n",
      "iteration no 168: Loss: 0.7569105168110593, accuracy: 0.5133333333333333\n",
      "iteration no 169: Loss: 0.7569013981756981, accuracy: 0.5133333333333333\n",
      "iteration no 170: Loss: 0.7568925626709637, accuracy: 0.5133333333333333\n",
      "iteration no 171: Loss: 0.7568840011582657, accuracy: 0.5133333333333333\n",
      "iteration no 172: Loss: 0.7568757048089874, accuracy: 0.5133333333333333\n",
      "iteration no 173: Loss: 0.7568676650932692, accuracy: 0.5133333333333333\n",
      "iteration no 174: Loss: 0.7568598737692341, accuracy: 0.5133333333333333\n",
      "iteration no 175: Loss: 0.756852322872631, accuracy: 0.5133333333333333\n",
      "iteration no 176: Loss: 0.7568450047068819, accuracy: 0.5133333333333333\n",
      "iteration no 177: Loss: 0.7568379118335122, accuracy: 0.5133333333333333\n",
      "iteration no 178: Loss: 0.7568310370629533, accuracy: 0.5133333333333333\n",
      "iteration no 179: Loss: 0.7568243734456949, accuracy: 0.5133333333333333\n",
      "iteration no 180: Loss: 0.7568179142637788, accuracy: 0.5133333333333333\n",
      "iteration no 181: Loss: 0.7568116530226154, accuracy: 0.5133333333333333\n",
      "iteration no 182: Loss: 0.7568055834431133, accuracy: 0.5133333333333333\n",
      "iteration no 183: Loss: 0.7567996994541039, accuracy: 0.5133333333333333\n",
      "iteration no 184: Loss: 0.7567939951850557, accuracy: 0.5133333333333333\n",
      "iteration no 185: Loss: 0.7567884649590602, accuracy: 0.5133333333333333\n",
      "iteration no 186: Loss: 0.7567831032860812, accuracy: 0.5133333333333333\n",
      "iteration no 187: Loss: 0.7567779048564576, accuracy: 0.5133333333333333\n",
      "iteration no 188: Loss: 0.7567728645346465, accuracy: 0.5133333333333333\n",
      "iteration no 189: Loss: 0.7567679773532008, accuracy: 0.5133333333333333\n",
      "iteration no 190: Loss: 0.7567632385069668, accuracy: 0.5133333333333333\n",
      "iteration no 191: Loss: 0.7567586433474993, accuracy: 0.5133333333333333\n",
      "iteration no 192: Loss: 0.756754187377679, accuracy: 0.5133333333333333\n",
      "iteration no 193: Loss: 0.7567498662465293, accuracy: 0.5133333333333333\n",
      "iteration no 194: Loss: 0.7567456757442217, accuracy: 0.5133333333333333\n",
      "iteration no 195: Loss: 0.7567416117972636, accuracy: 0.5133333333333333\n",
      "iteration no 196: Loss: 0.7567376704638602, accuracy: 0.5133333333333333\n",
      "iteration no 197: Loss: 0.7567338479294454, accuracy: 0.5133333333333333\n",
      "iteration no 198: Loss: 0.7567301405023741, accuracy: 0.5133333333333333\n",
      "iteration no 199: Loss: 0.7567265446097677, accuracy: 0.5133333333333333\n",
      "iteration no 200: Loss: 0.7567230567935121, accuracy: 0.5133333333333333\n",
      "iteration no 1: Loss: 1.1095784160140154, accuracy: 0.30333333333333334\n",
      "iteration no 2: Loss: 1.0778685590725212, accuracy: 0.45666666666666667\n",
      "iteration no 3: Loss: 1.0496612471092852, accuracy: 0.47333333333333333\n",
      "iteration no 4: Loss: 1.0245326466175504, accuracy: 0.4766666666666667\n",
      "iteration no 5: Loss: 1.0021067322405077, accuracy: 0.49\n",
      "iteration no 6: Loss: 0.9820522278431199, accuracy: 0.5066666666666667\n",
      "iteration no 7: Loss: 0.9640789460060427, accuracy: 0.5066666666666667\n",
      "iteration no 8: Loss: 0.9479337474551156, accuracy: 0.5066666666666667\n",
      "iteration no 9: Loss: 0.9333964076033537, accuracy: 0.5133333333333333\n",
      "iteration no 10: Loss: 0.9202756187382022, accuracy: 0.5133333333333333\n",
      "iteration no 11: Loss: 0.9084052733249071, accuracy: 0.5133333333333333\n",
      "iteration no 12: Loss: 0.8976411029382528, accuracy: 0.5133333333333333\n",
      "iteration no 13: Loss: 0.8878576970017524, accuracy: 0.5133333333333333\n",
      "iteration no 14: Loss: 0.8789458938193575, accuracy: 0.5166666666666667\n",
      "iteration no 15: Loss: 0.8708105185809155, accuracy: 0.5166666666666667\n",
      "iteration no 16: Loss: 0.863368434557565, accuracy: 0.5166666666666667\n",
      "iteration no 17: Loss: 0.8565468710680058, accuracy: 0.5166666666666667\n",
      "iteration no 18: Loss: 0.8502819925508122, accuracy: 0.5133333333333333\n",
      "iteration no 19: Loss: 0.844517675629956, accuracy: 0.5133333333333333\n",
      "iteration no 20: Loss: 0.8392044644217918, accuracy: 0.5133333333333333\n",
      "iteration no 21: Loss: 0.8342986779163463, accuracy: 0.5133333333333333\n",
      "iteration no 22: Loss: 0.8297616467464277, accuracy: 0.5133333333333333\n",
      "iteration no 23: Loss: 0.825559059867556, accuracy: 0.5166666666666667\n",
      "iteration no 24: Loss: 0.8216604045386974, accuracy: 0.5133333333333333\n",
      "iteration no 25: Loss: 0.8180384855024346, accuracy: 0.5133333333333333\n",
      "iteration no 26: Loss: 0.8146690114278637, accuracy: 0.5133333333333333\n",
      "iteration no 27: Loss: 0.8115302385295325, accuracy: 0.5133333333333333\n",
      "iteration no 28: Loss: 0.8086026628464212, accuracy: 0.5133333333333333\n",
      "iteration no 29: Loss: 0.8058687539925024, accuracy: 0.5133333333333333\n",
      "iteration no 30: Loss: 0.803312724309088, accuracy: 0.5133333333333333\n",
      "iteration no 31: Loss: 0.8009203282901022, accuracy: 0.5133333333333333\n",
      "iteration no 32: Loss: 0.7986786879420641, accuracy: 0.5133333333333333\n",
      "iteration no 33: Loss: 0.7965761404047004, accuracy: 0.5133333333333333\n",
      "iteration no 34: Loss: 0.7946021047160695, accuracy: 0.5133333333333333\n",
      "iteration no 35: Loss: 0.7927469650751093, accuracy: 0.5133333333333333\n",
      "iteration no 36: Loss: 0.791001968349138, accuracy: 0.5133333333333333\n",
      "iteration no 37: Loss: 0.7893591339062078, accuracy: 0.5133333333333333\n",
      "iteration no 38: Loss: 0.7878111741325315, accuracy: 0.5133333333333333\n",
      "iteration no 39: Loss: 0.7863514242319675, accuracy: 0.5133333333333333\n",
      "iteration no 40: Loss: 0.7849737801048216, accuracy: 0.5133333333333333\n",
      "iteration no 41: Loss: 0.7836726432729368, accuracy: 0.5133333333333333\n",
      "iteration no 42: Loss: 0.7824428719620883, accuracy: 0.5133333333333333\n",
      "iteration no 43: Loss: 0.7812797375751819, accuracy: 0.5133333333333333\n",
      "iteration no 44: Loss: 0.7801788858940937, accuracy: 0.5133333333333333\n",
      "iteration no 45: Loss: 0.7791363024370281, accuracy: 0.5133333333333333\n",
      "iteration no 46: Loss: 0.7781482814744044, accuracy: 0.5133333333333333\n",
      "iteration no 47: Loss: 0.7772113982714866, accuracy: 0.5133333333333333\n",
      "iteration no 48: Loss: 0.7763224841819373, accuracy: 0.5133333333333333\n",
      "iteration no 49: Loss: 0.7754786042645834, accuracy: 0.5133333333333333\n",
      "iteration no 50: Loss: 0.7746770371371184, accuracy: 0.5133333333333333\n",
      "iteration no 51: Loss: 0.7739152568162306, accuracy: 0.5133333333333333\n",
      "iteration no 52: Loss: 0.7731909163245434, accuracy: 0.5133333333333333\n",
      "iteration no 53: Loss: 0.7725018328715322, accuracy: 0.5133333333333333\n",
      "iteration no 54: Loss: 0.7718459744387958, accuracy: 0.5133333333333333\n",
      "iteration no 55: Loss: 0.7712214476202379, accuracy: 0.5133333333333333\n",
      "iteration no 56: Loss: 0.7706264865852828, accuracy: 0.5133333333333333\n",
      "iteration no 57: Loss: 0.7700594430485636, accuracy: 0.5133333333333333\n",
      "iteration no 58: Loss: 0.7695187771428957, accuracy: 0.5133333333333333\n",
      "iteration no 59: Loss: 0.7690030491040556, accuracy: 0.5133333333333333\n",
      "iteration no 60: Loss: 0.7685109116861337, accuracy: 0.5133333333333333\n",
      "iteration no 61: Loss: 0.7680411032352337, accuracy: 0.5133333333333333\n",
      "iteration no 62: Loss: 0.7675924413571968, accuracy: 0.5166666666666667\n",
      "iteration no 63: Loss: 0.7671638171219973, accuracy: 0.5166666666666667\n",
      "iteration no 64: Loss: 0.7667541897535867, accuracy: 0.5166666666666667\n",
      "iteration no 65: Loss: 0.7663625817593938, accuracy: 0.5166666666666667\n",
      "iteration no 66: Loss: 0.7659880744584738, accuracy: 0.5166666666666667\n",
      "iteration no 67: Loss: 0.7656298038715428, accuracy: 0.5166666666666667\n",
      "iteration no 68: Loss: 0.7652869569399006, accuracy: 0.5166666666666667\n",
      "iteration no 69: Loss: 0.7649587680435815, accuracy: 0.5166666666666667\n",
      "iteration no 70: Loss: 0.7646445157920457, accuracy: 0.5166666666666667\n",
      "iteration no 71: Loss: 0.7643435200633646, accuracy: 0.5166666666666667\n",
      "iteration no 72: Loss: 0.7640551392702101, accuracy: 0.5166666666666667\n",
      "iteration no 73: Loss: 0.7637787678330692, accuracy: 0.5166666666666667\n",
      "iteration no 74: Loss: 0.7635138338429699, accuracy: 0.5166666666666667\n",
      "iteration no 75: Loss: 0.7632597968977008, accuracy: 0.5166666666666667\n",
      "iteration no 76: Loss: 0.7630161460970026, accuracy: 0.5166666666666667\n",
      "iteration no 77: Loss: 0.7627823981835667, accuracy: 0.5166666666666667\n",
      "iteration no 78: Loss: 0.762558095817885, accuracy: 0.5166666666666667\n",
      "iteration no 79: Loss: 0.7623428059760878, accuracy: 0.5166666666666667\n",
      "iteration no 80: Loss: 0.7621361184608847, accuracy: 0.5133333333333333\n",
      "iteration no 81: Loss: 0.7619376445166103, accuracy: 0.5133333333333333\n",
      "iteration no 82: Loss: 0.7617470155401689, accuracy: 0.5133333333333333\n",
      "iteration no 83: Loss: 0.7615638818803966, accuracy: 0.5133333333333333\n",
      "iteration no 84: Loss: 0.7613879117190047, accuracy: 0.5133333333333333\n",
      "iteration no 85: Loss: 0.7612187900268593, accuracy: 0.5133333333333333\n",
      "iteration no 86: Loss: 0.76105621758988, accuracy: 0.5133333333333333\n",
      "iteration no 87: Loss: 0.7608999100993294, accuracy: 0.5133333333333333\n",
      "iteration no 88: Loss: 0.7607495973016958, accuracy: 0.5133333333333333\n",
      "iteration no 89: Loss: 0.7606050222037744, accuracy: 0.5133333333333333\n",
      "iteration no 90: Loss: 0.7604659403289084, accuracy: 0.5133333333333333\n",
      "iteration no 91: Loss: 0.7603321190206836, accuracy: 0.5133333333333333\n",
      "iteration no 92: Loss: 0.7602033367906692, accuracy: 0.5133333333333333\n",
      "iteration no 93: Loss: 0.7600793827070654, accuracy: 0.5133333333333333\n",
      "iteration no 94: Loss: 0.7599600558213723, accuracy: 0.5133333333333333\n",
      "iteration no 95: Loss: 0.7598451646304194, accuracy: 0.5133333333333333\n",
      "iteration no 96: Loss: 0.7597345265713008, accuracy: 0.5133333333333333\n",
      "iteration no 97: Loss: 0.7596279675469527, accuracy: 0.5133333333333333\n",
      "iteration no 98: Loss: 0.7595253214802826, accuracy: 0.5133333333333333\n",
      "iteration no 99: Loss: 0.7594264298949164, accuracy: 0.5133333333333333\n",
      "iteration no 100: Loss: 0.7593311415207795, accuracy: 0.5133333333333333\n",
      "iteration no 101: Loss: 0.7592393119228571, accuracy: 0.5133333333333333\n",
      "iteration no 102: Loss: 0.7591508031516018, accuracy: 0.5133333333333333\n",
      "iteration no 103: Loss: 0.7590654834135723, accuracy: 0.5133333333333333\n",
      "iteration no 104: Loss: 0.7589832267609867, accuracy: 0.5166666666666667\n",
      "iteration no 105: Loss: 0.7589039127989728, accuracy: 0.5166666666666667\n",
      "iteration no 106: Loss: 0.7588274264093783, accuracy: 0.5166666666666667\n",
      "iteration no 107: Loss: 0.7587536574900942, accuracy: 0.5166666666666667\n",
      "iteration no 108: Loss: 0.7586825007089115, accuracy: 0.5166666666666667\n",
      "iteration no 109: Loss: 0.7586138552710018, accuracy: 0.5166666666666667\n",
      "iteration no 110: Loss: 0.7585476246991755, accuracy: 0.5166666666666667\n",
      "iteration no 111: Loss: 0.7584837166261338, accuracy: 0.5166666666666667\n",
      "iteration no 112: Loss: 0.7584220425979747, accuracy: 0.5166666666666667\n",
      "iteration no 113: Loss: 0.7583625178882765, accuracy: 0.5166666666666667\n",
      "iteration no 114: Loss: 0.7583050613221142, accuracy: 0.5166666666666667\n",
      "iteration no 115: Loss: 0.7582495951094198, accuracy: 0.5166666666666667\n",
      "iteration no 116: Loss: 0.7581960446871283, accuracy: 0.5166666666666667\n",
      "iteration no 117: Loss: 0.7581443385695902, accuracy: 0.5166666666666667\n",
      "iteration no 118: Loss: 0.7580944082067688, accuracy: 0.5166666666666667\n",
      "iteration no 119: Loss: 0.7580461878497657, accuracy: 0.5166666666666667\n",
      "iteration no 120: Loss: 0.7579996144232546, accuracy: 0.5166666666666667\n",
      "iteration no 121: Loss: 0.7579546274044214, accuracy: 0.5166666666666667\n",
      "iteration no 122: Loss: 0.7579111687080469, accuracy: 0.5166666666666667\n",
      "iteration no 123: Loss: 0.7578691825773763, accuracy: 0.5166666666666667\n",
      "iteration no 124: Loss: 0.7578286154804551, accuracy: 0.5166666666666667\n",
      "iteration no 125: Loss: 0.7577894160116244, accuracy: 0.5166666666666667\n",
      "iteration no 126: Loss: 0.7577515347978871, accuracy: 0.5166666666666667\n",
      "iteration no 127: Loss: 0.7577149244098785, accuracy: 0.5166666666666667\n",
      "iteration no 128: Loss: 0.757679539277188, accuracy: 0.5166666666666667\n",
      "iteration no 129: Loss: 0.7576453356077948, accuracy: 0.5166666666666667\n",
      "iteration no 130: Loss: 0.7576122713113934, accuracy: 0.5166666666666667\n",
      "iteration no 131: Loss: 0.7575803059264028, accuracy: 0.5166666666666667\n",
      "iteration no 132: Loss: 0.7575494005504576, accuracy: 0.5166666666666667\n",
      "iteration no 133: Loss: 0.7575195177742019, accuracy: 0.5166666666666667\n",
      "iteration no 134: Loss: 0.7574906216182055, accuracy: 0.5166666666666667\n",
      "iteration no 135: Loss: 0.757462677472842, accuracy: 0.5133333333333333\n",
      "iteration no 136: Loss: 0.7574356520409744, accuracy: 0.5133333333333333\n",
      "iteration no 137: Loss: 0.757409513283299, accuracy: 0.5133333333333333\n",
      "iteration no 138: Loss: 0.7573842303662148, accuracy: 0.5133333333333333\n",
      "iteration no 139: Loss: 0.7573597736120865, accuracy: 0.5133333333333333\n",
      "iteration no 140: Loss: 0.7573361144517781, accuracy: 0.5133333333333333\n",
      "iteration no 141: Loss: 0.7573132253793448, accuracy: 0.5133333333333333\n",
      "iteration no 142: Loss: 0.7572910799087713, accuracy: 0.5133333333333333\n",
      "iteration no 143: Loss: 0.7572696525326579, accuracy: 0.5133333333333333\n",
      "iteration no 144: Loss: 0.7572489186827505, accuracy: 0.5133333333333333\n",
      "iteration no 145: Loss: 0.7572288546922321, accuracy: 0.5133333333333333\n",
      "iteration no 146: Loss: 0.757209437759681, accuracy: 0.5133333333333333\n",
      "iteration no 147: Loss: 0.7571906459146184, accuracy: 0.5133333333333333\n",
      "iteration no 148: Loss: 0.7571724579845659, accuracy: 0.5133333333333333\n",
      "iteration no 149: Loss: 0.7571548535635421, accuracy: 0.5133333333333333\n",
      "iteration no 150: Loss: 0.7571378129819241, accuracy: 0.5133333333333333\n",
      "iteration no 151: Loss: 0.7571213172776149, accuracy: 0.5133333333333333\n",
      "iteration no 152: Loss: 0.7571053481684485, accuracy: 0.5133333333333333\n",
      "iteration no 153: Loss: 0.7570898880257766, accuracy: 0.5133333333333333\n",
      "iteration no 154: Loss: 0.7570749198491818, accuracy: 0.5133333333333333\n",
      "iteration no 155: Loss: 0.7570604272422624, accuracy: 0.5133333333333333\n",
      "iteration no 156: Loss: 0.7570463943894391, accuracy: 0.5133333333333333\n",
      "iteration no 157: Loss: 0.7570328060337381, accuracy: 0.5133333333333333\n",
      "iteration no 158: Loss: 0.757019647455502, accuracy: 0.5133333333333333\n",
      "iteration no 159: Loss: 0.7570069044519888, accuracy: 0.5133333333333333\n",
      "iteration no 160: Loss: 0.7569945633178159, accuracy: 0.5133333333333333\n",
      "iteration no 161: Loss: 0.7569826108262131, accuracy: 0.5133333333333333\n",
      "iteration no 162: Loss: 0.7569710342110448, accuracy: 0.5133333333333333\n",
      "iteration no 163: Loss: 0.7569598211495691, accuracy: 0.5133333333333333\n",
      "iteration no 164: Loss: 0.7569489597458995, accuracy: 0.5133333333333333\n",
      "iteration no 165: Loss: 0.7569384385151392, accuracy: 0.5133333333333333\n",
      "iteration no 166: Loss: 0.7569282463681541, accuracy: 0.5133333333333333\n",
      "iteration no 167: Loss: 0.7569183725969625, accuracy: 0.5133333333333333\n",
      "iteration no 168: Loss: 0.7569088068607076, accuracy: 0.5133333333333333\n",
      "iteration no 169: Loss: 0.756899539172192, accuracy: 0.5133333333333333\n",
      "iteration no 170: Loss: 0.7568905598849484, accuracy: 0.5133333333333333\n",
      "iteration no 171: Loss: 0.7568818596808206, accuracy: 0.5133333333333333\n",
      "iteration no 172: Loss: 0.7568734295580362, accuracy: 0.5133333333333333\n",
      "iteration no 173: Loss: 0.756865260819749, accuracy: 0.5133333333333333\n",
      "iteration no 174: Loss: 0.7568573450630283, accuracy: 0.5133333333333333\n",
      "iteration no 175: Loss: 0.7568496741682793, accuracy: 0.5133333333333333\n",
      "iteration no 176: Loss: 0.7568422402890751, accuracy: 0.5133333333333333\n",
      "iteration no 177: Loss: 0.7568350358423804, accuracy: 0.5133333333333333\n",
      "iteration no 178: Loss: 0.7568280534991565, accuracy: 0.5133333333333333\n",
      "iteration no 179: Loss: 0.7568212861753227, accuracy: 0.5133333333333333\n",
      "iteration no 180: Loss: 0.7568147270230685, accuracy: 0.5133333333333333\n",
      "iteration no 181: Loss: 0.7568083694224944, accuracy: 0.5133333333333333\n",
      "iteration no 182: Loss: 0.7568022069735731, accuracy: 0.5133333333333333\n",
      "iteration no 183: Loss: 0.7567962334884138, accuracy: 0.5133333333333333\n",
      "iteration no 184: Loss: 0.7567904429838189, accuracy: 0.5133333333333333\n",
      "iteration no 185: Loss: 0.7567848296741233, accuracy: 0.5133333333333333\n",
      "iteration no 186: Loss: 0.7567793879642991, accuracy: 0.5133333333333333\n",
      "iteration no 187: Loss: 0.7567741124433212, accuracy: 0.5133333333333333\n",
      "iteration no 188: Loss: 0.7567689978777777, accuracy: 0.5133333333333333\n",
      "iteration no 189: Loss: 0.7567640392057211, accuracy: 0.5133333333333333\n",
      "iteration no 190: Loss: 0.7567592315307434, accuracy: 0.5133333333333333\n",
      "iteration no 191: Loss: 0.7567545701162735, accuracy: 0.5133333333333333\n",
      "iteration no 192: Loss: 0.7567500503800816, accuracy: 0.5133333333333333\n",
      "iteration no 193: Loss: 0.7567456678889883, accuracy: 0.5133333333333333\n",
      "iteration no 194: Loss: 0.7567414183537644, accuracy: 0.5133333333333333\n",
      "iteration no 195: Loss: 0.7567372976242185, accuracy: 0.5133333333333333\n",
      "iteration no 196: Loss: 0.7567333016844632, accuracy: 0.5133333333333333\n",
      "iteration no 197: Loss: 0.756729426648352, accuracy: 0.5133333333333333\n",
      "iteration no 198: Loss: 0.7567256687550821, accuracy: 0.5133333333333333\n",
      "iteration no 199: Loss: 0.7567220243649553, accuracy: 0.5133333333333333\n",
      "iteration no 200: Loss: 0.7567184899552908, accuracy: 0.5133333333333333\n",
      "iteration no 1: Loss: 1.1066185957666967, accuracy: 0.5466666666666666\n",
      "iteration no 2: Loss: 1.0749283189684453, accuracy: 0.5133333333333333\n",
      "iteration no 3: Loss: 1.0467516246527606, accuracy: 0.5266666666666666\n",
      "iteration no 4: Loss: 1.0216574191007128, accuracy: 0.5233333333333333\n",
      "iteration no 5: Loss: 0.9992708465940001, accuracy: 0.5266666666666666\n",
      "iteration no 6: Loss: 0.979263084301027, accuracy: 0.5233333333333333\n",
      "iteration no 7: Loss: 0.9613454472589189, accuracy: 0.5166666666666667\n",
      "iteration no 8: Loss: 0.9452650311232675, accuracy: 0.51\n",
      "iteration no 9: Loss: 0.9308008776880267, accuracy: 0.52\n",
      "iteration no 10: Loss: 0.9177603750366383, accuracy: 0.52\n",
      "iteration no 11: Loss: 0.9059758662797857, accuracy: 0.5233333333333333\n",
      "iteration no 12: Loss: 0.8953015088279463, accuracy: 0.5233333333333333\n",
      "iteration no 13: Loss: 0.8856104234412393, accuracy: 0.5233333333333333\n",
      "iteration no 14: Loss: 0.8767921510311498, accuracy: 0.5166666666666667\n",
      "iteration no 15: Loss: 0.8687504147059294, accuracy: 0.5166666666666667\n",
      "iteration no 16: Loss: 0.8614011702055341, accuracy: 0.52\n",
      "iteration no 17: Loss: 0.8546709197207243, accuracy: 0.52\n",
      "iteration no 18: Loss: 0.848495260640962, accuracy: 0.5166666666666667\n",
      "iteration no 19: Loss: 0.8428176404216242, accuracy: 0.5166666666666667\n",
      "iteration no 20: Loss: 0.837588290229173, accuracy: 0.5166666666666667\n",
      "iteration no 21: Loss: 0.8327633124272708, accuracy: 0.5166666666666667\n",
      "iteration no 22: Loss: 0.8283038997400819, accuracy: 0.5166666666666667\n",
      "iteration no 23: Loss: 0.8241756667327005, accuracy: 0.5166666666666667\n",
      "iteration no 24: Loss: 0.8203480768965598, accuracy: 0.5166666666666667\n",
      "iteration no 25: Loss: 0.8167939510296373, accuracy: 0.5166666666666667\n",
      "iteration no 26: Loss: 0.8134890447248093, accuracy: 0.5166666666666667\n",
      "iteration no 27: Loss: 0.8104116846252931, accuracy: 0.5166666666666667\n",
      "iteration no 28: Loss: 0.8075424546916273, accuracy: 0.5166666666666667\n",
      "iteration no 29: Loss: 0.8048639250759041, accuracy: 0.5166666666666667\n",
      "iteration no 30: Loss: 0.8023604173443962, accuracy: 0.5166666666666667\n",
      "iteration no 31: Loss: 0.8000178007571594, accuracy: 0.5166666666666667\n",
      "iteration no 32: Loss: 0.7978233151284411, accuracy: 0.5166666666666667\n",
      "iteration no 33: Loss: 0.7957654164778416, accuracy: 0.5166666666666667\n",
      "iteration no 34: Loss: 0.7938336422592916, accuracy: 0.5166666666666667\n",
      "iteration no 35: Loss: 0.7920184934403494, accuracy: 0.5166666666666667\n",
      "iteration no 36: Loss: 0.7903113311128434, accuracy: 0.5166666666666667\n",
      "iteration no 37: Loss: 0.7887042856599307, accuracy: 0.5166666666666667\n",
      "iteration no 38: Loss: 0.7871901767947156, accuracy: 0.5166666666666667\n",
      "iteration no 39: Loss: 0.7857624430304201, accuracy: 0.5166666666666667\n",
      "iteration no 40: Loss: 0.7844150793490983, accuracy: 0.5166666666666667\n",
      "iteration no 41: Loss: 0.7831425820111203, accuracy: 0.5166666666666667\n",
      "iteration no 42: Loss: 0.7819398995962707, accuracy: 0.5166666666666667\n",
      "iteration no 43: Loss: 0.7808023894935404, accuracy: 0.5133333333333333\n",
      "iteration no 44: Loss: 0.7797257791641167, accuracy: 0.5133333333333333\n",
      "iteration no 45: Loss: 0.7787061315936523, accuracy: 0.5133333333333333\n",
      "iteration no 46: Loss: 0.7777398144280976, accuracy: 0.5133333333333333\n",
      "iteration no 47: Loss: 0.7768234723542959, accuracy: 0.5133333333333333\n",
      "iteration no 48: Loss: 0.7759540023438916, accuracy: 0.5133333333333333\n",
      "iteration no 49: Loss: 0.7751285314283499, accuracy: 0.5133333333333333\n",
      "iteration no 50: Loss: 0.774344396715262, accuracy: 0.5133333333333333\n",
      "iteration no 51: Loss: 0.7735991273926103, accuracy: 0.5133333333333333\n",
      "iteration no 52: Loss: 0.7728904284992101, accuracy: 0.5133333333333333\n",
      "iteration no 53: Loss: 0.7722161662668032, accuracy: 0.5133333333333333\n",
      "iteration no 54: Loss: 0.7715743548629062, accuracy: 0.5133333333333333\n",
      "iteration no 55: Loss: 0.7709631443840232, accuracy: 0.5166666666666667\n",
      "iteration no 56: Loss: 0.7703808099666699, accuracy: 0.5166666666666667\n",
      "iteration no 57: Loss: 0.7698257418991685, accuracy: 0.5166666666666667\n",
      "iteration no 58: Loss: 0.7692964366307387, accuracy: 0.5166666666666667\n",
      "iteration no 59: Loss: 0.768791488586231, accuracy: 0.5166666666666667\n",
      "iteration no 60: Loss: 0.7683095827052202, accuracy: 0.5166666666666667\n",
      "iteration no 61: Loss: 0.7678494876332516, accuracy: 0.5166666666666667\n",
      "iteration no 62: Loss: 0.7674100495010108, accuracy: 0.5166666666666667\n",
      "iteration no 63: Loss: 0.7669901862341983, accuracy: 0.5166666666666667\n",
      "iteration no 64: Loss: 0.7665888823430644, accuracy: 0.5166666666666667\n",
      "iteration no 65: Loss: 0.7662051841460064, accuracy: 0.5166666666666667\n",
      "iteration no 66: Loss: 0.7658381953864403, accuracy: 0.5166666666666667\n",
      "iteration no 67: Loss: 0.7654870732064171, accuracy: 0.5166666666666667\n",
      "iteration no 68: Loss: 0.7651510244442151, accuracy: 0.5166666666666667\n",
      "iteration no 69: Loss: 0.7648293022264909, accuracy: 0.5166666666666667\n",
      "iteration no 70: Loss: 0.764521202828535, accuracy: 0.5166666666666667\n",
      "iteration no 71: Loss: 0.7642260627788199, accuracy: 0.5166666666666667\n",
      "iteration no 72: Loss: 0.7639432561863816, accuracy: 0.5166666666666667\n",
      "iteration no 73: Loss: 0.7636721922716705, accuracy: 0.5166666666666667\n",
      "iteration no 74: Loss: 0.7634123130833782, accuracy: 0.5166666666666667\n",
      "iteration no 75: Loss: 0.7631630913854223, accuracy: 0.5166666666666667\n",
      "iteration no 76: Loss: 0.7629240286997682, accuracy: 0.5166666666666667\n",
      "iteration no 77: Loss: 0.7626946534921073, accuracy: 0.5166666666666667\n",
      "iteration no 78: Loss: 0.7624745194886162, accuracy: 0.5166666666666667\n",
      "iteration no 79: Loss: 0.7622632041131001, accuracy: 0.5166666666666667\n",
      "iteration no 80: Loss: 0.7620603070347984, accuracy: 0.5133333333333333\n",
      "iteration no 81: Loss: 0.7618654488180024, accuracy: 0.5133333333333333\n",
      "iteration no 82: Loss: 0.7616782696654266, accuracy: 0.5133333333333333\n",
      "iteration no 83: Loss: 0.7614984282479808, accuracy: 0.5133333333333333\n",
      "iteration no 84: Loss: 0.7613256006142377, accuracy: 0.5133333333333333\n",
      "iteration no 85: Loss: 0.7611594791734687, accuracy: 0.5133333333333333\n",
      "iteration no 86: Loss: 0.7609997717466429, accuracy: 0.5133333333333333\n",
      "iteration no 87: Loss: 0.7608462006802643, accuracy: 0.5133333333333333\n",
      "iteration no 88: Loss: 0.7606985020183524, accuracy: 0.5133333333333333\n",
      "iteration no 89: Loss: 0.7605564247282575, accuracy: 0.5133333333333333\n",
      "iteration no 90: Loss: 0.7604197299763636, accuracy: 0.5133333333333333\n",
      "iteration no 91: Loss: 0.7602881904500546, accuracy: 0.5133333333333333\n",
      "iteration no 92: Loss: 0.7601615897226046, accuracy: 0.5133333333333333\n",
      "iteration no 93: Loss: 0.7600397216579337, accuracy: 0.5133333333333333\n",
      "iteration no 94: Loss: 0.7599223898524045, accuracy: 0.5133333333333333\n",
      "iteration no 95: Loss: 0.7598094071110604, accuracy: 0.5133333333333333\n",
      "iteration no 96: Loss: 0.7597005949559128, accuracy: 0.5133333333333333\n",
      "iteration no 97: Loss: 0.7595957831640656, accuracy: 0.5133333333333333\n",
      "iteration no 98: Loss: 0.7594948093336364, accuracy: 0.5133333333333333\n",
      "iteration no 99: Loss: 0.7593975184755909, accuracy: 0.5133333333333333\n",
      "iteration no 100: Loss: 0.7593037626297505, accuracy: 0.5133333333333333\n",
      "iteration no 101: Loss: 0.7592134005033555, accuracy: 0.5133333333333333\n",
      "iteration no 102: Loss: 0.7591262971306982, accuracy: 0.5166666666666667\n",
      "iteration no 103: Loss: 0.7590423235524422, accuracy: 0.5166666666666667\n",
      "iteration no 104: Loss: 0.7589613565133426, accuracy: 0.5166666666666667\n",
      "iteration no 105: Loss: 0.7588832781771874, accuracy: 0.5166666666666667\n",
      "iteration no 106: Loss: 0.7588079758578473, accuracy: 0.5166666666666667\n",
      "iteration no 107: Loss: 0.7587353417654201, accuracy: 0.5166666666666667\n",
      "iteration no 108: Loss: 0.758665272766511, accuracy: 0.5166666666666667\n",
      "iteration no 109: Loss: 0.7585976701577645, accuracy: 0.5166666666666667\n",
      "iteration no 110: Loss: 0.7585324394518306, accuracy: 0.5166666666666667\n",
      "iteration no 111: Loss: 0.7584694901749922, accuracy: 0.5166666666666667\n",
      "iteration no 112: Loss: 0.7584087356757441, accuracy: 0.5166666666666667\n",
      "iteration no 113: Loss: 0.7583500929436581, accuracy: 0.5166666666666667\n",
      "iteration no 114: Loss: 0.7582934824379116, accuracy: 0.5166666666666667\n",
      "iteration no 115: Loss: 0.7582388279249065, accuracy: 0.5166666666666667\n",
      "iteration no 116: Loss: 0.7581860563244323, accuracy: 0.5166666666666667\n",
      "iteration no 117: Loss: 0.7581350975638727, accuracy: 0.5166666666666667\n",
      "iteration no 118: Loss: 0.7580858844399855, accuracy: 0.5166666666666667\n",
      "iteration no 119: Loss: 0.7580383524878118, accuracy: 0.5166666666666667\n",
      "iteration no 120: Loss: 0.757992439856305, accuracy: 0.5166666666666667\n",
      "iteration no 121: Loss: 0.7579480871902939, accuracy: 0.5166666666666667\n",
      "iteration no 122: Loss: 0.7579052375184185, accuracy: 0.5166666666666667\n",
      "iteration no 123: Loss: 0.7578638361466998, accuracy: 0.5166666666666667\n",
      "iteration no 124: Loss: 0.7578238305574287, accuracy: 0.5166666666666667\n",
      "iteration no 125: Loss: 0.7577851703130742, accuracy: 0.5166666666666667\n",
      "iteration no 126: Loss: 0.7577478069649359, accuracy: 0.5166666666666667\n",
      "iteration no 127: Loss: 0.757711693966275, accuracy: 0.5166666666666667\n",
      "iteration no 128: Loss: 0.7576767865896835, accuracy: 0.5133333333333333\n",
      "iteration no 129: Loss: 0.7576430418484553, accuracy: 0.5133333333333333\n",
      "iteration no 130: Loss: 0.7576104184217485, accuracy: 0.5133333333333333\n",
      "iteration no 131: Loss: 0.7575788765833305, accuracy: 0.5133333333333333\n",
      "iteration no 132: Loss: 0.7575483781337176, accuracy: 0.5133333333333333\n",
      "iteration no 133: Loss: 0.7575188863355272, accuracy: 0.5133333333333333\n",
      "iteration no 134: Loss: 0.7574903658518743, accuracy: 0.5133333333333333\n",
      "iteration no 135: Loss: 0.7574627826876515, accuracy: 0.5133333333333333\n",
      "iteration no 136: Loss: 0.757436104133544, accuracy: 0.5133333333333333\n",
      "iteration no 137: Loss: 0.7574102987126357, accuracy: 0.5133333333333333\n",
      "iteration no 138: Loss: 0.7573853361294743, accuracy: 0.5133333333333333\n",
      "iteration no 139: Loss: 0.7573611872214705, accuracy: 0.5133333333333333\n",
      "iteration no 140: Loss: 0.7573378239125095, accuracy: 0.5133333333333333\n",
      "iteration no 141: Loss: 0.7573152191686661, accuracy: 0.5133333333333333\n",
      "iteration no 142: Loss: 0.7572933469559149, accuracy: 0.5133333333333333\n",
      "iteration no 143: Loss: 0.7572721821997386, accuracy: 0.5133333333333333\n",
      "iteration no 144: Loss: 0.7572517007465368, accuracy: 0.5133333333333333\n",
      "iteration no 145: Loss: 0.7572318793267488, accuracy: 0.5133333333333333\n",
      "iteration no 146: Loss: 0.7572126955196042, accuracy: 0.5133333333333333\n",
      "iteration no 147: Loss: 0.7571941277194235, accuracy: 0.5133333333333333\n",
      "iteration no 148: Loss: 0.7571761551033921, accuracy: 0.5133333333333333\n",
      "iteration no 149: Loss: 0.7571587576007361, accuracy: 0.5133333333333333\n",
      "iteration no 150: Loss: 0.7571419158632331, accuracy: 0.5133333333333333\n",
      "iteration no 151: Loss: 0.7571256112369941, accuracy: 0.5133333333333333\n",
      "iteration no 152: Loss: 0.7571098257354558, accuracy: 0.5133333333333333\n",
      "iteration no 153: Loss: 0.7570945420135256, accuracy: 0.5133333333333333\n",
      "iteration no 154: Loss: 0.7570797433428251, accuracy: 0.5133333333333333\n",
      "iteration no 155: Loss: 0.7570654135879817, accuracy: 0.5133333333333333\n",
      "iteration no 156: Loss: 0.7570515371839174, accuracy: 0.5133333333333333\n",
      "iteration no 157: Loss: 0.757038099114089, accuracy: 0.5133333333333333\n",
      "iteration no 158: Loss: 0.7570250848896389, accuracy: 0.5133333333333333\n",
      "iteration no 159: Loss: 0.757012480529409, accuracy: 0.5133333333333333\n",
      "iteration no 160: Loss: 0.7570002725407823, accuracy: 0.5133333333333333\n",
      "iteration no 161: Loss: 0.7569884479013153, accuracy: 0.5133333333333333\n",
      "iteration no 162: Loss: 0.7569769940411203, accuracy: 0.5133333333333333\n",
      "iteration no 163: Loss: 0.7569658988259695, accuracy: 0.5133333333333333\n",
      "iteration no 164: Loss: 0.7569551505410855, accuracy: 0.5133333333333333\n",
      "iteration no 165: Loss: 0.7569447378755879, accuracy: 0.5133333333333333\n",
      "iteration no 166: Loss: 0.7569346499075668, accuracy: 0.5133333333333333\n",
      "iteration no 167: Loss: 0.7569248760897561, accuracy: 0.5133333333333333\n",
      "iteration no 168: Loss: 0.7569154062357804, accuracy: 0.5133333333333333\n",
      "iteration no 169: Loss: 0.7569062305069487, accuracy: 0.5133333333333333\n",
      "iteration no 170: Loss: 0.7568973393995732, accuracy: 0.5133333333333333\n",
      "iteration no 171: Loss: 0.7568887237327877, accuracy: 0.5133333333333333\n",
      "iteration no 172: Loss: 0.7568803746368483, accuracy: 0.5133333333333333\n",
      "iteration no 173: Loss: 0.7568722835418906, accuracy: 0.5133333333333333\n",
      "iteration no 174: Loss: 0.7568644421671263, accuracy: 0.5133333333333333\n",
      "iteration no 175: Loss: 0.7568568425104625, accuracy: 0.5133333333333333\n",
      "iteration no 176: Loss: 0.756849476838521, accuracy: 0.5133333333333333\n",
      "iteration no 177: Loss: 0.756842337677045, accuracy: 0.5133333333333333\n",
      "iteration no 178: Loss: 0.756835417801674, accuracy: 0.5133333333333333\n",
      "iteration no 179: Loss: 0.7568287102290738, accuracy: 0.5133333333333333\n",
      "iteration no 180: Loss: 0.7568222082084042, accuracy: 0.5133333333333333\n",
      "iteration no 181: Loss: 0.7568159052131131, accuracy: 0.5133333333333333\n",
      "iteration no 182: Loss: 0.7568097949330406, accuracy: 0.5133333333333333\n",
      "iteration no 183: Loss: 0.7568038712668227, accuracy: 0.5133333333333333\n",
      "iteration no 184: Loss: 0.7567981283145818, accuracy: 0.5133333333333333\n",
      "iteration no 185: Loss: 0.7567925603708907, accuracy: 0.5133333333333333\n",
      "iteration no 186: Loss: 0.7567871619180004, accuracy: 0.5133333333333333\n",
      "iteration no 187: Loss: 0.7567819276193216, accuracy: 0.5133333333333333\n",
      "iteration no 188: Loss: 0.756776852313147, accuracy: 0.5133333333333333\n",
      "iteration no 189: Loss: 0.7567719310066084, accuracy: 0.5133333333333333\n",
      "iteration no 190: Loss: 0.7567671588698551, accuracy: 0.5133333333333333\n",
      "iteration no 191: Loss: 0.7567625312304482, accuracy: 0.5133333333333333\n",
      "iteration no 192: Loss: 0.7567580435679608, accuracy: 0.5133333333333333\n",
      "iteration no 193: Loss: 0.7567536915087747, accuracy: 0.5133333333333333\n",
      "iteration no 194: Loss: 0.7567494708210684, accuracy: 0.5133333333333333\n",
      "iteration no 195: Loss: 0.7567453774099873, accuracy: 0.5133333333333333\n",
      "iteration no 196: Loss: 0.7567414073129879, accuracy: 0.5133333333333333\n",
      "iteration no 197: Loss: 0.7567375566953521, accuracy: 0.5133333333333333\n",
      "iteration no 198: Loss: 0.7567338218458628, accuracy: 0.5133333333333333\n",
      "iteration no 199: Loss: 0.7567301991726338, accuracy: 0.5133333333333333\n",
      "iteration no 200: Loss: 0.7567266851990914, accuracy: 0.5133333333333333\n",
      "iteration no 1: Loss: 1.0936063886244813, accuracy: 0.5133333333333333\n",
      "iteration no 2: Loss: 1.0636257811065017, accuracy: 0.5066666666666667\n",
      "iteration no 3: Loss: 1.0369312820531809, accuracy: 0.49666666666666665\n",
      "iteration no 4: Loss: 1.0131240429979627, accuracy: 0.5066666666666667\n",
      "iteration no 5: Loss: 0.9918533118304953, accuracy: 0.51\n",
      "iteration no 6: Loss: 0.9728109467608091, accuracy: 0.51\n",
      "iteration no 7: Loss: 0.9557270120271879, accuracy: 0.51\n",
      "iteration no 8: Loss: 0.9403657243850136, accuracy: 0.51\n",
      "iteration no 9: Loss: 0.9265215902407637, accuracy: 0.5066666666666667\n",
      "iteration no 10: Loss: 0.9140157597067767, accuracy: 0.5133333333333333\n",
      "iteration no 11: Loss: 0.9026926572888607, accuracy: 0.5133333333333333\n",
      "iteration no 12: Loss: 0.892416931745301, accuracy: 0.5133333333333333\n",
      "iteration no 13: Loss: 0.8830707411100672, accuracy: 0.51\n",
      "iteration no 14: Loss: 0.8745513669693825, accuracy: 0.5133333333333333\n",
      "iteration no 15: Loss: 0.866769137741138, accuracy: 0.51\n",
      "iteration no 16: Loss: 0.8596456329757485, accuracy: 0.51\n",
      "iteration no 17: Loss: 0.8531121377094747, accuracy: 0.5066666666666667\n",
      "iteration no 18: Loss: 0.847108315944058, accuracy: 0.51\n",
      "iteration no 19: Loss: 0.8415810741139806, accuracy: 0.51\n",
      "iteration no 20: Loss: 0.8364835880635669, accuracy: 0.51\n",
      "iteration no 21: Loss: 0.8317744700401533, accuracy: 0.51\n",
      "iteration no 22: Loss: 0.8274170551910938, accuracy: 0.51\n",
      "iteration no 23: Loss: 0.8233787898535166, accuracy: 0.51\n",
      "iteration no 24: Loss: 0.8196307064613498, accuracy: 0.51\n",
      "iteration no 25: Loss: 0.8161469721348689, accuracy: 0.51\n",
      "iteration no 26: Loss: 0.8129044999663926, accuracy: 0.5133333333333333\n",
      "iteration no 27: Loss: 0.8098826136911634, accuracy: 0.5133333333333333\n",
      "iteration no 28: Loss: 0.8070627578620625, accuracy: 0.5133333333333333\n",
      "iteration no 29: Loss: 0.8044282468602141, accuracy: 0.5133333333333333\n",
      "iteration no 30: Loss: 0.8019640470997123, accuracy: 0.5133333333333333\n",
      "iteration no 31: Loss: 0.799656587650525, accuracy: 0.5133333333333333\n",
      "iteration no 32: Loss: 0.7974935952331588, accuracy: 0.5133333333333333\n",
      "iteration no 33: Loss: 0.7954639501529468, accuracy: 0.5166666666666667\n",
      "iteration no 34: Loss: 0.793557560259029, accuracy: 0.5166666666666667\n",
      "iteration no 35: Loss: 0.7917652504486954, accuracy: 0.5166666666666667\n",
      "iteration no 36: Loss: 0.7900786656049338, accuracy: 0.5166666666666667\n",
      "iteration no 37: Loss: 0.7884901851647468, accuracy: 0.5166666666666667\n",
      "iteration no 38: Loss: 0.7869928477774408, accuracy: 0.5166666666666667\n",
      "iteration no 39: Loss: 0.7855802847333421, accuracy: 0.5166666666666667\n",
      "iteration no 40: Loss: 0.7842466610308153, accuracy: 0.5166666666666667\n",
      "iteration no 41: Loss: 0.7829866231084386, accuracy: 0.5166666666666667\n",
      "iteration no 42: Loss: 0.781795252404279, accuracy: 0.5133333333333333\n",
      "iteration no 43: Loss: 0.780668024019196, accuracy: 0.5133333333333333\n",
      "iteration no 44: Loss: 0.7796007698591313, accuracy: 0.5133333333333333\n",
      "iteration no 45: Loss: 0.7785896457150879, accuracy: 0.5133333333333333\n",
      "iteration no 46: Loss: 0.7776311018111401, accuracy: 0.5133333333333333\n",
      "iteration no 47: Loss: 0.7767218564122434, accuracy: 0.5133333333333333\n",
      "iteration no 48: Loss: 0.7758588721363511, accuracy: 0.5133333333333333\n",
      "iteration no 49: Loss: 0.7750393346607164, accuracy: 0.5133333333333333\n",
      "iteration no 50: Loss: 0.7742606335513698, accuracy: 0.5133333333333333\n",
      "iteration no 51: Loss: 0.7735203449785103, accuracy: 0.5133333333333333\n",
      "iteration no 52: Loss: 0.7728162161097623, accuracy: 0.5133333333333333\n",
      "iteration no 53: Loss: 0.7721461509985348, accuracy: 0.5133333333333333\n",
      "iteration no 54: Loss: 0.7715081978066851, accuracy: 0.5133333333333333\n",
      "iteration no 55: Loss: 0.7709005372197713, accuracy: 0.5166666666666667\n",
      "iteration no 56: Loss: 0.7703214719297983, accuracy: 0.5166666666666667\n",
      "iteration no 57: Loss: 0.7697694170748718, accuracy: 0.5166666666666667\n",
      "iteration no 58: Loss: 0.7692428915378262, accuracy: 0.5166666666666667\n",
      "iteration no 59: Loss: 0.768740510016993, accuracy: 0.5166666666666667\n",
      "iteration no 60: Loss: 0.768260975791984, accuracy: 0.5166666666666667\n",
      "iteration no 61: Loss: 0.7678030741158972, accuracy: 0.5166666666666667\n",
      "iteration no 62: Loss: 0.7673656661728506, accuracy: 0.5166666666666667\n",
      "iteration no 63: Loss: 0.7669476835463565, accuracy: 0.5166666666666667\n",
      "iteration no 64: Loss: 0.7665481231498645, accuracy: 0.5166666666666667\n",
      "iteration no 65: Loss: 0.7661660425759536, accuracy: 0.5166666666666667\n",
      "iteration no 66: Loss: 0.7658005558251935, accuracy: 0.5166666666666667\n",
      "iteration no 67: Loss: 0.7654508293797271, accuracy: 0.5166666666666667\n",
      "iteration no 68: Loss: 0.7651160785901975, accuracy: 0.5166666666666667\n",
      "iteration no 69: Loss: 0.7647955643478127, accuracy: 0.5166666666666667\n",
      "iteration no 70: Loss: 0.764488590016165, accuracy: 0.5166666666666667\n",
      "iteration no 71: Loss: 0.7641944985999302, accuracy: 0.5166666666666667\n",
      "iteration no 72: Loss: 0.7639126701298111, accuracy: 0.5166666666666667\n",
      "iteration no 73: Loss: 0.7636425192450919, accuracy: 0.5166666666666667\n",
      "iteration no 74: Loss: 0.7633834929569491, accuracy: 0.5166666666666667\n",
      "iteration no 75: Loss: 0.7631350685772691, accuracy: 0.5166666666666667\n",
      "iteration no 76: Loss: 0.7628967517991506, accuracy: 0.5166666666666667\n",
      "iteration no 77: Loss: 0.7626680749165566, accuracy: 0.5166666666666667\n",
      "iteration no 78: Loss: 0.7624485951717299, accuracy: 0.5166666666666667\n",
      "iteration no 79: Loss: 0.7622378932200262, accuracy: 0.5166666666666667\n",
      "iteration no 80: Loss: 0.7620355717027523, accuracy: 0.5166666666666667\n",
      "iteration no 81: Loss: 0.7618412539194316, accuracy: 0.5133333333333333\n",
      "iteration no 82: Loss: 0.7616545825916823, accuracy: 0.5133333333333333\n",
      "iteration no 83: Loss: 0.761475218711578, accuracy: 0.5133333333333333\n",
      "iteration no 84: Loss: 0.7613028404679727, accuracy: 0.5133333333333333\n",
      "iteration no 85: Loss: 0.7611371422448396, accuracy: 0.5133333333333333\n",
      "iteration no 86: Loss: 0.7609778336861738, accuracy: 0.5133333333333333\n",
      "iteration no 87: Loss: 0.7608246388224693, accuracy: 0.5133333333333333\n",
      "iteration no 88: Loss: 0.7606772952541999, accuracy: 0.5133333333333333\n",
      "iteration no 89: Loss: 0.7605355533881074, accuracy: 0.5133333333333333\n",
      "iteration no 90: Loss: 0.7603991757224474, accuracy: 0.5133333333333333\n",
      "iteration no 91: Loss: 0.7602679361776564, accuracy: 0.5133333333333333\n",
      "iteration no 92: Loss: 0.7601416194691842, accuracy: 0.5133333333333333\n",
      "iteration no 93: Loss: 0.7600200205195006, accuracy: 0.5133333333333333\n",
      "iteration no 94: Loss: 0.7599029439065182, accuracy: 0.5133333333333333\n",
      "iteration no 95: Loss: 0.7597902033458918, accuracy: 0.5133333333333333\n",
      "iteration no 96: Loss: 0.75968162120485, accuracy: 0.5133333333333333\n",
      "iteration no 97: Loss: 0.7595770280453964, accuracy: 0.5133333333333333\n",
      "iteration no 98: Loss: 0.7594762621948837, accuracy: 0.5133333333333333\n",
      "iteration no 99: Loss: 0.7593791693421146, accuracy: 0.5133333333333333\n",
      "iteration no 100: Loss: 0.759285602157259, accuracy: 0.5133333333333333\n",
      "iteration no 101: Loss: 0.759195419934014, accuracy: 0.5166666666666667\n",
      "iteration no 102: Loss: 0.7591084882525324, accuracy: 0.5166666666666667\n",
      "iteration no 103: Loss: 0.7590246786617771, accuracy: 0.5166666666666667\n",
      "iteration no 104: Loss: 0.7589438683800287, accuracy: 0.5166666666666667\n",
      "iteration no 105: Loss: 0.758865940012393, accuracy: 0.5166666666666667\n",
      "iteration no 106: Loss: 0.758790781284215, accuracy: 0.5166666666666667\n",
      "iteration no 107: Loss: 0.7587182847893971, accuracy: 0.5166666666666667\n",
      "iteration no 108: Loss: 0.7586483477526859, accuracy: 0.5166666666666667\n",
      "iteration no 109: Loss: 0.7585808718050557, accuracy: 0.5166666666666667\n",
      "iteration no 110: Loss: 0.7585157627713791, accuracy: 0.5166666666666667\n",
      "iteration no 111: Loss: 0.7584529304696302, accuracy: 0.5166666666666667\n",
      "iteration no 112: Loss: 0.7583922885209199, accuracy: 0.5166666666666667\n",
      "iteration no 113: Loss: 0.7583337541697037, accuracy: 0.5166666666666667\n",
      "iteration no 114: Loss: 0.7582772481135556, accuracy: 0.5166666666666667\n",
      "iteration no 115: Loss: 0.7582226943419346, accuracy: 0.5166666666666667\n",
      "iteration no 116: Loss: 0.7581700199834127, accuracy: 0.5166666666666667\n",
      "iteration no 117: Loss: 0.7581191551608674, accuracy: 0.5166666666666667\n",
      "iteration no 118: Loss: 0.7580700328541723, accuracy: 0.5166666666666667\n",
      "iteration no 119: Loss: 0.7580225887699528, accuracy: 0.5166666666666667\n",
      "iteration no 120: Loss: 0.757976761217999, accuracy: 0.5166666666666667\n",
      "iteration no 121: Loss: 0.7579324909939543, accuracy: 0.5166666666666667\n",
      "iteration no 122: Loss: 0.7578897212679274, accuracy: 0.5166666666666667\n",
      "iteration no 123: Loss: 0.7578483974786869, accuracy: 0.5166666666666667\n",
      "iteration no 124: Loss: 0.7578084672331318, accuracy: 0.5166666666666667\n",
      "iteration no 125: Loss: 0.7577698802107399, accuracy: 0.5166666666666667\n",
      "iteration no 126: Loss: 0.7577325880727215, accuracy: 0.5166666666666667\n",
      "iteration no 127: Loss: 0.7576965443756177, accuracy: 0.5166666666666667\n",
      "iteration no 128: Loss: 0.7576617044891027, accuracy: 0.5166666666666667\n",
      "iteration no 129: Loss: 0.7576280255177592, accuracy: 0.5166666666666667\n",
      "iteration no 130: Loss: 0.7575954662266153, accuracy: 0.5166666666666667\n",
      "iteration no 131: Loss: 0.7575639869702407, accuracy: 0.5166666666666667\n",
      "iteration no 132: Loss: 0.7575335496252108, accuracy: 0.5133333333333333\n",
      "iteration no 133: Loss: 0.7575041175257627, accuracy: 0.5133333333333333\n",
      "iteration no 134: Loss: 0.757475655402475, accuracy: 0.5133333333333333\n",
      "iteration no 135: Loss: 0.7574481293238103, accuracy: 0.5133333333333333\n",
      "iteration no 136: Loss: 0.7574215066403773, accuracy: 0.5133333333333333\n",
      "iteration no 137: Loss: 0.757395755931765, accuracy: 0.5133333333333333\n",
      "iteration no 138: Loss: 0.7573708469558229, accuracy: 0.5133333333333333\n",
      "iteration no 139: Loss: 0.757346750600259, accuracy: 0.5133333333333333\n",
      "iteration no 140: Loss: 0.7573234388364382, accuracy: 0.5133333333333333\n",
      "iteration no 141: Loss: 0.7573008846752711, accuracy: 0.5133333333333333\n",
      "iteration no 142: Loss: 0.7572790621250882, accuracy: 0.5133333333333333\n",
      "iteration no 143: Loss: 0.7572579461513982, accuracy: 0.5133333333333333\n",
      "iteration no 144: Loss: 0.7572375126384406, accuracy: 0.5133333333333333\n",
      "iteration no 145: Loss: 0.7572177383524403, accuracy: 0.5133333333333333\n",
      "iteration no 146: Loss: 0.7571986009064835, accuracy: 0.5133333333333333\n",
      "iteration no 147: Loss: 0.7571800787269335, accuracy: 0.5133333333333333\n",
      "iteration no 148: Loss: 0.7571621510213136, accuracy: 0.5133333333333333\n",
      "iteration no 149: Loss: 0.7571447977475855, accuracy: 0.5133333333333333\n",
      "iteration no 150: Loss: 0.7571279995847544, accuracy: 0.5133333333333333\n",
      "iteration no 151: Loss: 0.7571117379047412, accuracy: 0.5133333333333333\n",
      "iteration no 152: Loss: 0.7570959947454569, accuracy: 0.5133333333333333\n",
      "iteration no 153: Loss: 0.7570807527850273, accuracy: 0.5133333333333333\n",
      "iteration no 154: Loss: 0.7570659953171094, accuracy: 0.5133333333333333\n",
      "iteration no 155: Loss: 0.7570517062272513, accuracy: 0.5133333333333333\n",
      "iteration no 156: Loss: 0.7570378699702462, accuracy: 0.5133333333333333\n",
      "iteration no 157: Loss: 0.7570244715484328, accuracy: 0.5133333333333333\n",
      "iteration no 158: Loss: 0.7570114964909022, accuracy: 0.5133333333333333\n",
      "iteration no 159: Loss: 0.7569989308335658, accuracy: 0.5133333333333333\n",
      "iteration no 160: Loss: 0.7569867611000475, accuracy: 0.5133333333333333\n",
      "iteration no 161: Loss: 0.7569749742833602, accuracy: 0.5133333333333333\n",
      "iteration no 162: Loss: 0.7569635578283356, accuracy: 0.5133333333333333\n",
      "iteration no 163: Loss: 0.7569524996147668, accuracy: 0.5133333333333333\n",
      "iteration no 164: Loss: 0.7569417879412397, accuracy: 0.5133333333333333\n",
      "iteration no 165: Loss: 0.7569314115096133, accuracy: 0.5133333333333333\n",
      "iteration no 166: Loss: 0.7569213594101312, accuracy: 0.5133333333333333\n",
      "iteration no 167: Loss: 0.7569116211071238, accuracy: 0.5133333333333333\n",
      "iteration no 168: Loss: 0.7569021864252872, accuracy: 0.5133333333333333\n",
      "iteration no 169: Loss: 0.7568930455365048, accuracy: 0.5133333333333333\n",
      "iteration no 170: Loss: 0.7568841889471923, accuracy: 0.5133333333333333\n",
      "iteration no 171: Loss: 0.756875607486143, accuracy: 0.5133333333333333\n",
      "iteration no 172: Loss: 0.7568672922928502, accuracy: 0.5133333333333333\n",
      "iteration no 173: Loss: 0.7568592348062869, accuracy: 0.5133333333333333\n",
      "iteration no 174: Loss: 0.7568514267541259, accuracy: 0.5133333333333333\n",
      "iteration no 175: Loss: 0.7568438601423749, accuracy: 0.5133333333333333\n",
      "iteration no 176: Loss: 0.7568365272454175, accuracy: 0.5133333333333333\n",
      "iteration no 177: Loss: 0.7568294205964359, accuracy: 0.5133333333333333\n",
      "iteration no 178: Loss: 0.7568225329782026, accuracy: 0.5133333333333333\n",
      "iteration no 179: Loss: 0.7568158574142266, accuracy: 0.5133333333333333\n",
      "iteration no 180: Loss: 0.756809387160235, accuracy: 0.5133333333333333\n",
      "iteration no 181: Loss: 0.7568031156959802, accuracy: 0.5133333333333333\n",
      "iteration no 182: Loss: 0.7567970367173593, accuracy: 0.5133333333333333\n",
      "iteration no 183: Loss: 0.7567911441288281, accuracy: 0.5133333333333333\n",
      "iteration no 184: Loss: 0.7567854320361034, accuracy: 0.5133333333333333\n",
      "iteration no 185: Loss: 0.7567798947391389, accuracy: 0.5133333333333333\n",
      "iteration no 186: Loss: 0.7567745267253626, accuracy: 0.5133333333333333\n",
      "iteration no 187: Loss: 0.7567693226631684, accuracy: 0.5133333333333333\n",
      "iteration no 188: Loss: 0.7567642773956474, accuracy: 0.5133333333333333\n",
      "iteration no 189: Loss: 0.7567593859345535, accuracy: 0.5133333333333333\n",
      "iteration no 190: Loss: 0.7567546434544902, accuracy: 0.5133333333333333\n",
      "iteration no 191: Loss: 0.7567500452873136, accuracy: 0.5133333333333333\n",
      "iteration no 192: Loss: 0.7567455869167379, accuracy: 0.5133333333333333\n",
      "iteration no 193: Loss: 0.7567412639731411, accuracy: 0.5133333333333333\n",
      "iteration no 194: Loss: 0.7567370722285587, accuracy: 0.5133333333333333\n",
      "iteration no 195: Loss: 0.7567330075918592, accuracy: 0.5133333333333333\n",
      "iteration no 196: Loss: 0.7567290661040962, accuracy: 0.5133333333333333\n",
      "iteration no 197: Loss: 0.756725243934027, accuracy: 0.5133333333333333\n",
      "iteration no 198: Loss: 0.7567215373737931, accuracy: 0.5133333333333333\n",
      "iteration no 199: Loss: 0.7567179428347566, accuracy: 0.5133333333333333\n",
      "iteration no 200: Loss: 0.756714456843485, accuracy: 0.5133333333333333\n",
      "iteration no 1: Loss: 1.0938323132769407, accuracy: 0.48333333333333334\n",
      "iteration no 2: Loss: 1.0635296107110077, accuracy: 0.51\n",
      "iteration no 3: Loss: 1.0365726403850177, accuracy: 0.5066666666666667\n",
      "iteration no 4: Loss: 1.0125583761309656, accuracy: 0.5066666666666667\n",
      "iteration no 5: Loss: 0.9911287196384946, accuracy: 0.5133333333333333\n",
      "iteration no 6: Loss: 0.971967576052239, accuracy: 0.5133333333333333\n",
      "iteration no 7: Loss: 0.9547975808203871, accuracy: 0.5166666666666667\n",
      "iteration no 8: Loss: 0.939376473318805, accuracy: 0.5166666666666667\n",
      "iteration no 9: Loss: 0.9254933235601368, accuracy: 0.5233333333333333\n",
      "iteration no 10: Loss: 0.9129648234552131, accuracy: 0.5233333333333333\n",
      "iteration no 11: Loss: 0.9016317952879656, accuracy: 0.52\n",
      "iteration no 12: Loss: 0.8913560054858505, accuracy: 0.5166666666666667\n",
      "iteration no 13: Loss: 0.8820173207394241, accuracy: 0.5166666666666667\n",
      "iteration no 14: Loss: 0.8735112090510425, accuracy: 0.5166666666666667\n",
      "iteration no 15: Loss: 0.8657465676564751, accuracy: 0.5166666666666667\n",
      "iteration no 16: Loss: 0.8586438490557866, accuracy: 0.5166666666666667\n",
      "iteration no 17: Loss: 0.8521334521676469, accuracy: 0.5166666666666667\n",
      "iteration no 18: Loss: 0.8461543453178125, accuracy: 0.5166666666666667\n",
      "iteration no 19: Loss: 0.8406528896406756, accuracy: 0.5166666666666667\n",
      "iteration no 20: Loss: 0.8355818343950809, accuracy: 0.5166666666666667\n",
      "iteration no 21: Loss: 0.8308994589947942, accuracy: 0.5166666666666667\n",
      "iteration no 22: Loss: 0.8265688398428571, accuracy: 0.5133333333333333\n",
      "iteration no 23: Loss: 0.8225572231332675, accuracy: 0.5133333333333333\n",
      "iteration no 24: Loss: 0.8188354875496539, accuracy: 0.5133333333333333\n",
      "iteration no 25: Loss: 0.815377683220261, accuracy: 0.5166666666666667\n",
      "iteration no 26: Loss: 0.812160635388708, accuracy: 0.5133333333333333\n",
      "iteration no 27: Loss: 0.8091636030557555, accuracy: 0.5133333333333333\n",
      "iteration no 28: Loss: 0.8063679843716461, accuracy: 0.5133333333333333\n",
      "iteration no 29: Loss: 0.8037570618460581, accuracy: 0.5133333333333333\n",
      "iteration no 30: Loss: 0.8013157815267129, accuracy: 0.5133333333333333\n",
      "iteration no 31: Loss: 0.7990305612084984, accuracy: 0.5166666666666667\n",
      "iteration no 32: Loss: 0.7968891234995437, accuracy: 0.5166666666666667\n",
      "iteration no 33: Loss: 0.7948803502122392, accuracy: 0.5166666666666667\n",
      "iteration no 34: Loss: 0.7929941550856658, accuracy: 0.5166666666666667\n",
      "iteration no 35: Loss: 0.7912213722981037, accuracy: 0.5166666666666667\n",
      "iteration no 36: Loss: 0.7895536586084239, accuracy: 0.5166666666666667\n",
      "iteration no 37: Loss: 0.7879834072850703, accuracy: 0.5166666666666667\n",
      "iteration no 38: Loss: 0.786503672250949, accuracy: 0.5166666666666667\n",
      "iteration no 39: Loss: 0.7851081011000812, accuracy: 0.5166666666666667\n",
      "iteration no 40: Loss: 0.7837908758342529, accuracy: 0.5166666666666667\n",
      "iteration no 41: Loss: 0.7825466603307858, accuracy: 0.5166666666666667\n",
      "iteration no 42: Loss: 0.7813705536907625, accuracy: 0.5166666666666667\n",
      "iteration no 43: Loss: 0.7802580487344792, accuracy: 0.5166666666666667\n",
      "iteration no 44: Loss: 0.779204995010913, accuracy: 0.5166666666666667\n",
      "iteration no 45: Loss: 0.7782075657732904, accuracy: 0.5166666666666667\n",
      "iteration no 46: Loss: 0.7772622284457607, accuracy: 0.5166666666666667\n",
      "iteration no 47: Loss: 0.7763657181685982, accuracy: 0.5166666666666667\n",
      "iteration no 48: Loss: 0.7755150140629218, accuracy: 0.5166666666666667\n",
      "iteration no 49: Loss: 0.7747073179019399, accuracy: 0.5133333333333333\n",
      "iteration no 50: Loss: 0.7739400349153747, accuracy: 0.5133333333333333\n",
      "iteration no 51: Loss: 0.7732107564878965, accuracy: 0.5133333333333333\n",
      "iteration no 52: Loss: 0.7725172445419582, accuracy: 0.5133333333333333\n",
      "iteration no 53: Loss: 0.7718574174209968, accuracy: 0.5133333333333333\n",
      "iteration no 54: Loss: 0.7712293371111519, accuracy: 0.5133333333333333\n",
      "iteration no 55: Loss: 0.7706311976589372, accuracy: 0.5133333333333333\n",
      "iteration no 56: Loss: 0.7700613146590647, accuracy: 0.5133333333333333\n",
      "iteration no 57: Loss: 0.7695181157012609, accuracy: 0.5133333333333333\n",
      "iteration no 58: Loss: 0.7690001316776726, accuracy: 0.5133333333333333\n",
      "iteration no 59: Loss: 0.768505988863641, accuracy: 0.5133333333333333\n",
      "iteration no 60: Loss: 0.7680344016944038, accuracy: 0.5133333333333333\n",
      "iteration no 61: Loss: 0.7675841661688746, accuracy: 0.5133333333333333\n",
      "iteration no 62: Loss: 0.7671541538191963, accuracy: 0.5166666666666667\n",
      "iteration no 63: Loss: 0.7667433061914062, accuracy: 0.5166666666666667\n",
      "iteration no 64: Loss: 0.7663506297884085, accuracy: 0.5166666666666667\n",
      "iteration no 65: Loss: 0.7659751914316177, accuracy: 0.5166666666666667\n",
      "iteration no 66: Loss: 0.7656161140022046, accuracy: 0.5166666666666667\n",
      "iteration no 67: Loss: 0.7652725725269286, accuracy: 0.5166666666666667\n",
      "iteration no 68: Loss: 0.7649437905771171, accuracy: 0.5166666666666667\n",
      "iteration no 69: Loss: 0.7646290369525506, accuracy: 0.5166666666666667\n",
      "iteration no 70: Loss: 0.7643276226248256, accuracy: 0.5166666666666667\n",
      "iteration no 71: Loss: 0.7640388979173061, accuracy: 0.5166666666666667\n",
      "iteration no 72: Loss: 0.7637622499010035, accuracy: 0.5166666666666667\n",
      "iteration no 73: Loss: 0.7634970999877406, accuracy: 0.5166666666666667\n",
      "iteration no 74: Loss: 0.763242901703739, accuracy: 0.5166666666666667\n",
      "iteration no 75: Loss: 0.7629991386283742, accuracy: 0.5166666666666667\n",
      "iteration no 76: Loss: 0.7627653224842784, accuracy: 0.5166666666666667\n",
      "iteration no 77: Loss: 0.7625409913662529, accuracy: 0.5133333333333333\n",
      "iteration no 78: Loss: 0.7623257080976122, accuracy: 0.5133333333333333\n",
      "iteration no 79: Loss: 0.7621190587036195, accuracy: 0.5133333333333333\n",
      "iteration no 80: Loss: 0.7619206509925999, accuracy: 0.5133333333333333\n",
      "iteration no 81: Loss: 0.7617301132361715, accuracy: 0.5133333333333333\n",
      "iteration no 82: Loss: 0.7615470929407817, accuracy: 0.5133333333333333\n",
      "iteration no 83: Loss: 0.761371255703425, accuracy: 0.5133333333333333\n",
      "iteration no 84: Loss: 0.7612022841450424, accuracy: 0.5133333333333333\n",
      "iteration no 85: Loss: 0.7610398769156494, accuracy: 0.5133333333333333\n",
      "iteration no 86: Loss: 0.7608837477657613, accuracy: 0.5133333333333333\n",
      "iteration no 87: Loss: 0.7607336246791312, accuracy: 0.5133333333333333\n",
      "iteration no 88: Loss: 0.7605892490622396, accuracy: 0.5133333333333333\n",
      "iteration no 89: Loss: 0.7604503749863513, accuracy: 0.5133333333333333\n",
      "iteration no 90: Loss: 0.7603167684782945, accuracy: 0.5133333333333333\n",
      "iteration no 91: Loss: 0.7601882068564394, accuracy: 0.5133333333333333\n",
      "iteration no 92: Loss: 0.760064478108624, accuracy: 0.5133333333333333\n",
      "iteration no 93: Loss: 0.7599453803090511, accuracy: 0.5133333333333333\n",
      "iteration no 94: Loss: 0.7598307210713998, accuracy: 0.5133333333333333\n",
      "iteration no 95: Loss: 0.7597203170356231, accuracy: 0.5133333333333333\n",
      "iteration no 96: Loss: 0.7596139933860979, accuracy: 0.5133333333333333\n",
      "iteration no 97: Loss: 0.7595115833989654, accuracy: 0.5133333333333333\n",
      "iteration no 98: Loss: 0.7594129280166789, accuracy: 0.5133333333333333\n",
      "iteration no 99: Loss: 0.7593178754479162, accuracy: 0.5133333333333333\n",
      "iteration no 100: Loss: 0.759226280791155, accuracy: 0.5133333333333333\n",
      "iteration no 101: Loss: 0.7591380056803393, accuracy: 0.5133333333333333\n",
      "iteration no 102: Loss: 0.7590529179511801, accuracy: 0.5133333333333333\n",
      "iteration no 103: Loss: 0.7589708913267368, accuracy: 0.5133333333333333\n",
      "iteration no 104: Loss: 0.7588918051210288, accuracy: 0.5166666666666667\n",
      "iteration no 105: Loss: 0.7588155439595189, accuracy: 0.5166666666666667\n",
      "iteration no 106: Loss: 0.758741997515384, accuracy: 0.5166666666666667\n",
      "iteration no 107: Loss: 0.758671060260577, accuracy: 0.5166666666666667\n",
      "iteration no 108: Loss: 0.7586026312307458, accuracy: 0.5166666666666667\n",
      "iteration no 109: Loss: 0.7585366138031447, accuracy: 0.5166666666666667\n",
      "iteration no 110: Loss: 0.7584729154867329, accuracy: 0.5166666666666667\n",
      "iteration no 111: Loss: 0.7584114477237086, accuracy: 0.5166666666666667\n",
      "iteration no 112: Loss: 0.7583521257017817, accuracy: 0.5166666666666667\n",
      "iteration no 113: Loss: 0.7582948681765332, accuracy: 0.5166666666666667\n",
      "iteration no 114: Loss: 0.7582395973032532, accuracy: 0.5166666666666667\n",
      "iteration no 115: Loss: 0.7581862384776911, accuracy: 0.5166666666666667\n",
      "iteration no 116: Loss: 0.7581347201851902, accuracy: 0.5166666666666667\n",
      "iteration no 117: Loss: 0.7580849738577111, accuracy: 0.5166666666666667\n",
      "iteration no 118: Loss: 0.7580369337382824, accuracy: 0.5166666666666667\n",
      "iteration no 119: Loss: 0.7579905367524469, accuracy: 0.5166666666666667\n",
      "iteration no 120: Loss: 0.7579457223862996, accuracy: 0.5166666666666667\n",
      "iteration no 121: Loss: 0.7579024325707403, accuracy: 0.5166666666666667\n",
      "iteration no 122: Loss: 0.7578606115715852, accuracy: 0.5166666666666667\n",
      "iteration no 123: Loss: 0.7578202058852066, accuracy: 0.5166666666666667\n",
      "iteration no 124: Loss: 0.7577811641393909, accuracy: 0.5166666666666667\n",
      "iteration no 125: Loss: 0.7577434369991225, accuracy: 0.5166666666666667\n",
      "iteration no 126: Loss: 0.7577069770770205, accuracy: 0.5166666666666667\n",
      "iteration no 127: Loss: 0.7576717388481722, accuracy: 0.5166666666666667\n",
      "iteration no 128: Loss: 0.757637678569122, accuracy: 0.5166666666666667\n",
      "iteration no 129: Loss: 0.7576047542007901, accuracy: 0.5166666666666667\n",
      "iteration no 130: Loss: 0.7575729253351077, accuracy: 0.5133333333333333\n",
      "iteration no 131: Loss: 0.7575421531251697, accuracy: 0.5133333333333333\n",
      "iteration no 132: Loss: 0.7575124002187177, accuracy: 0.5133333333333333\n",
      "iteration no 133: Loss: 0.7574836306947739, accuracy: 0.5133333333333333\n",
      "iteration no 134: Loss: 0.757455810003263, accuracy: 0.5133333333333333\n",
      "iteration no 135: Loss: 0.757428904907462, accuracy: 0.5133333333333333\n",
      "iteration no 136: Loss: 0.7574028834291341, accuracy: 0.5133333333333333\n",
      "iteration no 137: Loss: 0.757377714796204, accuracy: 0.5133333333333333\n",
      "iteration no 138: Loss: 0.7573533693928464, accuracy: 0.5133333333333333\n",
      "iteration no 139: Loss: 0.757329818711863, accuracy: 0.5133333333333333\n",
      "iteration no 140: Loss: 0.7573070353092308, accuracy: 0.5133333333333333\n",
      "iteration no 141: Loss: 0.7572849927607124, accuracy: 0.5133333333333333\n",
      "iteration no 142: Loss: 0.7572636656204257, accuracy: 0.5133333333333333\n",
      "iteration no 143: Loss: 0.7572430293812733, accuracy: 0.5133333333333333\n",
      "iteration no 144: Loss: 0.7572230604371376, accuracy: 0.5133333333333333\n",
      "iteration no 145: Loss: 0.757203736046759, accuracy: 0.5133333333333333\n",
      "iteration no 146: Loss: 0.7571850342992095, accuracy: 0.5133333333333333\n",
      "iteration no 147: Loss: 0.7571669340808852, accuracy: 0.5133333333333333\n",
      "iteration no 148: Loss: 0.7571494150439453, accuracy: 0.5133333333333333\n",
      "iteration no 149: Loss: 0.7571324575761249, accuracy: 0.5133333333333333\n",
      "iteration no 150: Loss: 0.7571160427718576, accuracy: 0.5133333333333333\n",
      "iteration no 151: Loss: 0.7571001524046429, accuracy: 0.5133333333333333\n",
      "iteration no 152: Loss: 0.7570847689006022, accuracy: 0.5133333333333333\n",
      "iteration no 153: Loss: 0.7570698753131635, accuracy: 0.5133333333333333\n",
      "iteration no 154: Loss: 0.7570554552988239, accuracy: 0.5133333333333333\n",
      "iteration no 155: Loss: 0.7570414930939388, accuracy: 0.5133333333333333\n",
      "iteration no 156: Loss: 0.7570279734924885, accuracy: 0.5133333333333333\n",
      "iteration no 157: Loss: 0.7570148818247798, accuracy: 0.5133333333333333\n",
      "iteration no 158: Loss: 0.7570022039370357, accuracy: 0.5133333333333333\n",
      "iteration no 159: Loss: 0.7569899261718344, accuracy: 0.5133333333333333\n",
      "iteration no 160: Loss: 0.7569780353493599, accuracy: 0.5133333333333333\n",
      "iteration no 161: Loss: 0.7569665187494233, accuracy: 0.5133333333333333\n",
      "iteration no 162: Loss: 0.7569553640942246, accuracy: 0.5133333333333333\n",
      "iteration no 163: Loss: 0.7569445595318174, accuracy: 0.5133333333333333\n",
      "iteration no 164: Loss: 0.7569340936202481, accuracy: 0.5133333333333333\n",
      "iteration no 165: Loss: 0.7569239553123371, accuracy: 0.5133333333333333\n",
      "iteration no 166: Loss: 0.7569141339410758, accuracy: 0.5133333333333333\n",
      "iteration no 167: Loss: 0.7569046192056087, accuracy: 0.5133333333333333\n",
      "iteration no 168: Loss: 0.7568954011577792, accuracy: 0.5133333333333333\n",
      "iteration no 169: Loss: 0.7568864701892106, accuracy: 0.5133333333333333\n",
      "iteration no 170: Loss: 0.7568778170189003, accuracy: 0.5133333333333333\n",
      "iteration no 171: Loss: 0.7568694326813066, accuracy: 0.5133333333333333\n",
      "iteration no 172: Loss: 0.7568613085149019, accuracy: 0.5133333333333333\n",
      "iteration no 173: Loss: 0.7568534361511797, accuracy: 0.5133333333333333\n",
      "iteration no 174: Loss: 0.7568458075040877, accuracy: 0.5133333333333333\n",
      "iteration no 175: Loss: 0.7568384147598755, accuracy: 0.5133333333333333\n",
      "iteration no 176: Loss: 0.7568312503673341, accuracy: 0.5133333333333333\n",
      "iteration no 177: Loss: 0.7568243070284157, accuracy: 0.5133333333333333\n",
      "iteration no 178: Loss: 0.7568175776892129, accuracy: 0.5133333333333333\n",
      "iteration no 179: Loss: 0.7568110555312846, accuracy: 0.5133333333333333\n",
      "iteration no 180: Loss: 0.7568047339633154, accuracy: 0.5133333333333333\n",
      "iteration no 181: Loss: 0.7567986066130914, accuracy: 0.5133333333333333\n",
      "iteration no 182: Loss: 0.7567926673197811, accuracy: 0.5133333333333333\n",
      "iteration no 183: Loss: 0.7567869101265097, accuracy: 0.5133333333333333\n",
      "iteration no 184: Loss: 0.7567813292732126, accuracy: 0.5133333333333333\n",
      "iteration no 185: Loss: 0.756775919189758, accuracy: 0.5133333333333333\n",
      "iteration no 186: Loss: 0.7567706744893272, accuracy: 0.5133333333333333\n",
      "iteration no 187: Loss: 0.7567655899620431, accuracy: 0.5133333333333333\n",
      "iteration no 188: Loss: 0.7567606605688341, accuracy: 0.5133333333333333\n",
      "iteration no 189: Loss: 0.7567558814355276, accuracy: 0.5133333333333333\n",
      "iteration no 190: Loss: 0.7567512478471612, accuracy: 0.5133333333333333\n",
      "iteration no 191: Loss: 0.7567467552425042, accuracy: 0.5133333333333333\n",
      "iteration no 192: Loss: 0.7567423992087791, accuracy: 0.5133333333333333\n",
      "iteration no 193: Loss: 0.7567381754765785, accuracy: 0.5133333333333333\n",
      "iteration no 194: Loss: 0.7567340799149673, accuracy: 0.5133333333333333\n",
      "iteration no 195: Loss: 0.7567301085267615, accuracy: 0.5133333333333333\n",
      "iteration no 196: Loss: 0.7567262574439811, accuracy: 0.5133333333333333\n",
      "iteration no 197: Loss: 0.756722522923466, accuracy: 0.5133333333333333\n",
      "iteration no 198: Loss: 0.7567189013426506, accuracy: 0.5133333333333333\n",
      "iteration no 199: Loss: 0.7567153891954904, accuracy: 0.5133333333333333\n",
      "iteration no 200: Loss: 0.7567119830885355, accuracy: 0.5166666666666667\n",
      "iteration no 1: Loss: 1.0897887841631024, accuracy: 0.4866666666666667\n",
      "iteration no 2: Loss: 1.0601449946520456, accuracy: 0.5166666666666667\n",
      "iteration no 3: Loss: 1.0337595722870725, accuracy: 0.51\n",
      "iteration no 4: Loss: 1.0102362303124302, accuracy: 0.5133333333333333\n",
      "iteration no 5: Loss: 0.989225067174621, accuracy: 0.5133333333333333\n",
      "iteration no 6: Loss: 0.9704186514501466, accuracy: 0.5133333333333333\n",
      "iteration no 7: Loss: 0.9535480763975377, accuracy: 0.5133333333333333\n",
      "iteration no 8: Loss: 0.9383789335520103, accuracy: 0.5133333333333333\n",
      "iteration no 9: Loss: 0.9247073414809521, accuracy: 0.5133333333333333\n",
      "iteration no 10: Loss: 0.91235617453506, accuracy: 0.5133333333333333\n",
      "iteration no 11: Loss: 0.901171591411839, accuracy: 0.5133333333333333\n",
      "iteration no 12: Loss: 0.8910199139511592, accuracy: 0.5133333333333333\n",
      "iteration no 13: Loss: 0.8817848684444688, accuracy: 0.5133333333333333\n",
      "iteration no 14: Loss: 0.8733651768945898, accuracy: 0.5133333333333333\n",
      "iteration no 15: Loss: 0.8656724716459023, accuracy: 0.5133333333333333\n",
      "iteration no 16: Loss: 0.8586295003641374, accuracy: 0.51\n",
      "iteration no 17: Loss: 0.8521685867175288, accuracy: 0.5133333333333333\n",
      "iteration no 18: Loss: 0.8462303132887969, accuracy: 0.5133333333333333\n",
      "iteration no 19: Loss: 0.8407623958922965, accuracy: 0.5133333333333333\n",
      "iteration no 20: Loss: 0.8357187217443394, accuracy: 0.5133333333333333\n",
      "iteration no 21: Loss: 0.8310585273410767, accuracy: 0.5133333333333333\n",
      "iteration no 22: Loss: 0.8267456951627632, accuracy: 0.5133333333333333\n",
      "iteration no 23: Loss: 0.8227481513089975, accuracy: 0.5133333333333333\n",
      "iteration no 24: Loss: 0.8190373488222219, accuracy: 0.5133333333333333\n",
      "iteration no 25: Loss: 0.8155878237690696, accuracy: 0.5133333333333333\n",
      "iteration no 26: Loss: 0.812376813138994, accuracy: 0.5133333333333333\n",
      "iteration no 27: Loss: 0.8093839253169237, accuracy: 0.5133333333333333\n",
      "iteration no 28: Loss: 0.8065908553257912, accuracy: 0.5166666666666667\n",
      "iteration no 29: Loss: 0.8039811382499997, accuracy: 0.5166666666666667\n",
      "iteration no 30: Loss: 0.80153993527436, accuracy: 0.5166666666666667\n",
      "iteration no 31: Loss: 0.7992538476336511, accuracy: 0.5166666666666667\n",
      "iteration no 32: Loss: 0.797110754491106, accuracy: 0.5166666666666667\n",
      "iteration no 33: Loss: 0.7950996713716234, accuracy: 0.5166666666666667\n",
      "iteration no 34: Loss: 0.7932106262860652, accuracy: 0.5133333333333333\n",
      "iteration no 35: Loss: 0.7914345511123241, accuracy: 0.5133333333333333\n",
      "iteration no 36: Loss: 0.7897631861602514, accuracy: 0.5133333333333333\n",
      "iteration no 37: Loss: 0.7881889961521047, accuracy: 0.5133333333333333\n",
      "iteration no 38: Loss: 0.786705096107201, accuracy: 0.5133333333333333\n",
      "iteration no 39: Loss: 0.7853051858367002, accuracy: 0.5133333333333333\n",
      "iteration no 40: Loss: 0.7839834919383242, accuracy: 0.5133333333333333\n",
      "iteration no 41: Loss: 0.7827347163367578, accuracy: 0.5133333333333333\n",
      "iteration no 42: Loss: 0.7815539905479173, accuracy: 0.5133333333333333\n",
      "iteration no 43: Loss: 0.7804368349579869, accuracy: 0.5133333333333333\n",
      "iteration no 44: Loss: 0.779379122504194, accuracy: 0.5133333333333333\n",
      "iteration no 45: Loss: 0.778377046226357, accuracy: 0.5133333333333333\n",
      "iteration no 46: Loss: 0.7774270902284401, accuracy: 0.5133333333333333\n",
      "iteration no 47: Loss: 0.7765260036495412, accuracy: 0.5133333333333333\n",
      "iteration no 48: Loss: 0.7756707772954059, accuracy: 0.5133333333333333\n",
      "iteration no 49: Loss: 0.7748586226260439, accuracy: 0.5133333333333333\n",
      "iteration no 50: Loss: 0.7740869528333327, accuracy: 0.5133333333333333\n",
      "iteration no 51: Loss: 0.7733533657755977, accuracy: 0.5133333333333333\n",
      "iteration no 52: Loss: 0.7726556285647739, accuracy: 0.5133333333333333\n",
      "iteration no 53: Loss: 0.7719916636265673, accuracy: 0.5133333333333333\n",
      "iteration no 54: Loss: 0.7713595360755654, accuracy: 0.5133333333333333\n",
      "iteration no 55: Loss: 0.7707574422659584, accuracy: 0.5133333333333333\n",
      "iteration no 56: Loss: 0.770183699394864, accuracy: 0.5133333333333333\n",
      "iteration no 57: Loss: 0.76963673604945, accuracy: 0.5133333333333333\n",
      "iteration no 58: Loss: 0.7691150836015084, accuracy: 0.5133333333333333\n",
      "iteration no 59: Loss: 0.7686173683640006, accuracy: 0.5166666666666667\n",
      "iteration no 60: Loss: 0.7681423044336505, accuracy: 0.5166666666666667\n",
      "iteration no 61: Loss: 0.7676886871520261, accuracy: 0.5166666666666667\n",
      "iteration no 62: Loss: 0.7672553871249376, accuracy: 0.5166666666666667\n",
      "iteration no 63: Loss: 0.766841344746451, accuracy: 0.5166666666666667\n",
      "iteration no 64: Loss: 0.766445565179558, accuracy: 0.5166666666666667\n",
      "iteration no 65: Loss: 0.7660671137505869, accuracy: 0.5166666666666667\n",
      "iteration no 66: Loss: 0.7657051117189223, accuracy: 0.5166666666666667\n",
      "iteration no 67: Loss: 0.7653587323875576, accuracy: 0.5166666666666667\n",
      "iteration no 68: Loss: 0.7650271975235249, accuracy: 0.5166666666666667\n",
      "iteration no 69: Loss: 0.7647097740603646, accuracy: 0.5166666666666667\n",
      "iteration no 70: Loss: 0.7644057710575767, accuracy: 0.5166666666666667\n",
      "iteration no 71: Loss: 0.7641145368944725, accuracy: 0.5166666666666667\n",
      "iteration no 72: Loss: 0.7638354566780435, accuracy: 0.5166666666666667\n",
      "iteration no 73: Loss: 0.7635679498464409, accuracy: 0.5166666666666667\n",
      "iteration no 74: Loss: 0.7633114679514132, accuracy: 0.5166666666666667\n",
      "iteration no 75: Loss: 0.7630654926046353, accuracy: 0.5166666666666667\n",
      "iteration no 76: Loss: 0.762829533574258, accuracy: 0.5166666666666667\n",
      "iteration no 77: Loss: 0.7626031270192921, accuracy: 0.5166666666666667\n",
      "iteration no 78: Loss: 0.7623858338505647, accuracy: 0.5166666666666667\n",
      "iteration no 79: Loss: 0.7621772382080145, accuracy: 0.5133333333333333\n",
      "iteration no 80: Loss: 0.761976946045014, accuracy: 0.5133333333333333\n",
      "iteration no 81: Loss: 0.7617845838112329, accuracy: 0.5133333333333333\n",
      "iteration no 82: Loss: 0.7615997972263102, accuracy: 0.5133333333333333\n",
      "iteration no 83: Loss: 0.7614222501372705, accuracy: 0.5133333333333333\n",
      "iteration no 84: Loss: 0.7612516234532436, accuracy: 0.5133333333333333\n",
      "iteration no 85: Loss: 0.7610876141515829, accuracy: 0.5133333333333333\n",
      "iteration no 86: Loss: 0.7609299343499935, accuracy: 0.5133333333333333\n",
      "iteration no 87: Loss: 0.7607783104397277, accuracy: 0.5133333333333333\n",
      "iteration no 88: Loss: 0.7606324822753147, accuracy: 0.5133333333333333\n",
      "iteration no 89: Loss: 0.7604922024166737, accuracy: 0.5133333333333333\n",
      "iteration no 90: Loss: 0.7603572354197925, accuracy: 0.5133333333333333\n",
      "iteration no 91: Loss: 0.7602273571724651, accuracy: 0.5133333333333333\n",
      "iteration no 92: Loss: 0.760102354271866, accuracy: 0.5133333333333333\n",
      "iteration no 93: Loss: 0.7599820234409922, accuracy: 0.5133333333333333\n",
      "iteration no 94: Loss: 0.7598661709812403, accuracy: 0.5133333333333333\n",
      "iteration no 95: Loss: 0.759754612258601, accuracy: 0.5133333333333333\n",
      "iteration no 96: Loss: 0.7596471712211446, accuracy: 0.5133333333333333\n",
      "iteration no 97: Loss: 0.7595436799456545, accuracy: 0.5133333333333333\n",
      "iteration no 98: Loss: 0.7594439782114296, accuracy: 0.5133333333333333\n",
      "iteration no 99: Loss: 0.7593479130994195, accuracy: 0.5133333333333333\n",
      "iteration no 100: Loss: 0.759255338615001, accuracy: 0.5133333333333333\n",
      "iteration no 101: Loss: 0.7591661153328325, accuracy: 0.5133333333333333\n",
      "iteration no 102: Loss: 0.7590801100623248, accuracy: 0.5133333333333333\n",
      "iteration no 103: Loss: 0.7589971955323918, accuracy: 0.5166666666666667\n",
      "iteration no 104: Loss: 0.7589172500942263, accuracy: 0.5166666666666667\n",
      "iteration no 105: Loss: 0.7588401574409487, accuracy: 0.5166666666666667\n",
      "iteration no 106: Loss: 0.7587658063430461, accuracy: 0.5166666666666667\n",
      "iteration no 107: Loss: 0.7586940903986098, accuracy: 0.5166666666666667\n",
      "iteration no 108: Loss: 0.7586249077974385, accuracy: 0.5166666666666667\n",
      "iteration no 109: Loss: 0.7585581610981426, accuracy: 0.5166666666666667\n",
      "iteration no 110: Loss: 0.7584937570174494, accuracy: 0.5166666666666667\n",
      "iteration no 111: Loss: 0.7584316062309553, accuracy: 0.5166666666666667\n",
      "iteration no 112: Loss: 0.7583716231846324, accuracy: 0.5166666666666667\n",
      "iteration no 113: Loss: 0.7583137259164342, accuracy: 0.5166666666666667\n",
      "iteration no 114: Loss: 0.7582578358873985, accuracy: 0.5166666666666667\n",
      "iteration no 115: Loss: 0.7582038778216778, accuracy: 0.5166666666666667\n",
      "iteration no 116: Loss: 0.7581517795549694, accuracy: 0.5166666666666667\n",
      "iteration no 117: Loss: 0.7581014718908525, accuracy: 0.5166666666666667\n",
      "iteration no 118: Loss: 0.7580528884645694, accuracy: 0.5166666666666667\n",
      "iteration no 119: Loss: 0.7580059656138195, accuracy: 0.5166666666666667\n",
      "iteration no 120: Loss: 0.7579606422561627, accuracy: 0.5166666666666667\n",
      "iteration no 121: Loss: 0.7579168597726527, accuracy: 0.5166666666666667\n",
      "iteration no 122: Loss: 0.7578745618973491, accuracy: 0.5166666666666667\n",
      "iteration no 123: Loss: 0.7578336946123728, accuracy: 0.5166666666666667\n",
      "iteration no 124: Loss: 0.7577942060481971, accuracy: 0.5166666666666667\n",
      "iteration no 125: Loss: 0.7577560463888804, accuracy: 0.5166666666666667\n",
      "iteration no 126: Loss: 0.757719167781969, accuracy: 0.5166666666666667\n",
      "iteration no 127: Loss: 0.7576835242528118, accuracy: 0.5166666666666667\n",
      "iteration no 128: Loss: 0.7576490716230474, accuracy: 0.5166666666666667\n",
      "iteration no 129: Loss: 0.7576157674330338, accuracy: 0.5166666666666667\n",
      "iteration no 130: Loss: 0.7575835708680136, accuracy: 0.5166666666666667\n",
      "iteration no 131: Loss: 0.7575524426878072, accuracy: 0.5166666666666667\n",
      "iteration no 132: Loss: 0.7575223451598533, accuracy: 0.5133333333333333\n",
      "iteration no 133: Loss: 0.7574932419954139, accuracy: 0.5133333333333333\n",
      "iteration no 134: Loss: 0.7574650982887796, accuracy: 0.5133333333333333\n",
      "iteration no 135: Loss: 0.7574378804593189, accuracy: 0.5133333333333333\n",
      "iteration no 136: Loss: 0.7574115561962209, accuracy: 0.5133333333333333\n",
      "iteration no 137: Loss: 0.7573860944057952, accuracy: 0.5133333333333333\n",
      "iteration no 138: Loss: 0.757361465161195, accuracy: 0.5133333333333333\n",
      "iteration no 139: Loss: 0.7573376396544416, accuracy: 0.5133333333333333\n",
      "iteration no 140: Loss: 0.7573145901506324, accuracy: 0.5133333333333333\n",
      "iteration no 141: Loss: 0.7572922899442209, accuracy: 0.5133333333333333\n",
      "iteration no 142: Loss: 0.7572707133172676, accuracy: 0.5133333333333333\n",
      "iteration no 143: Loss: 0.7572498354995589, accuracy: 0.5133333333333333\n",
      "iteration no 144: Loss: 0.7572296326305081, accuracy: 0.5133333333333333\n",
      "iteration no 145: Loss: 0.7572100817227412, accuracy: 0.5133333333333333\n",
      "iteration no 146: Loss: 0.757191160627294, accuracy: 0.5133333333333333\n",
      "iteration no 147: Loss: 0.7571728480003344, accuracy: 0.5133333333333333\n",
      "iteration no 148: Loss: 0.7571551232713409, accuracy: 0.5133333333333333\n",
      "iteration no 149: Loss: 0.7571379666126647, accuracy: 0.5133333333333333\n",
      "iteration no 150: Loss: 0.7571213589104067, accuracy: 0.5133333333333333\n",
      "iteration no 151: Loss: 0.7571052817365526, accuracy: 0.5133333333333333\n",
      "iteration no 152: Loss: 0.7570897173222989, accuracy: 0.5133333333333333\n",
      "iteration no 153: Loss: 0.7570746485325203, accuracy: 0.5133333333333333\n",
      "iteration no 154: Loss: 0.757060058841318, accuracy: 0.5133333333333333\n",
      "iteration no 155: Loss: 0.7570459323086054, accuracy: 0.5133333333333333\n",
      "iteration no 156: Loss: 0.7570322535576752, accuracy: 0.5133333333333333\n",
      "iteration no 157: Loss: 0.7570190077537103, accuracy: 0.5133333333333333\n",
      "iteration no 158: Loss: 0.7570061805831882, accuracy: 0.5133333333333333\n",
      "iteration no 159: Loss: 0.7569937582341414, accuracy: 0.5133333333333333\n",
      "iteration no 160: Loss: 0.7569817273772341, accuracy: 0.5133333333333333\n",
      "iteration no 161: Loss: 0.7569700751476186, accuracy: 0.5133333333333333\n",
      "iteration no 162: Loss: 0.756958789127533, accuracy: 0.5133333333333333\n",
      "iteration no 163: Loss: 0.7569478573296131, accuracy: 0.5133333333333333\n",
      "iteration no 164: Loss: 0.7569372681808784, accuracy: 0.5133333333333333\n",
      "iteration no 165: Loss: 0.7569270105073691, accuracy: 0.5133333333333333\n",
      "iteration no 166: Loss: 0.7569170735194016, accuracy: 0.5133333333333333\n",
      "iteration no 167: Loss: 0.7569074467974161, accuracy: 0.5133333333333333\n",
      "iteration no 168: Loss: 0.7568981202783895, accuracy: 0.5133333333333333\n",
      "iteration no 169: Loss: 0.7568890842427903, accuracy: 0.5133333333333333\n",
      "iteration no 170: Loss: 0.7568803293020503, accuracy: 0.5133333333333333\n",
      "iteration no 171: Loss: 0.7568718463865319, accuracy: 0.5133333333333333\n",
      "iteration no 172: Loss: 0.7568636267339686, accuracy: 0.5133333333333333\n",
      "iteration no 173: Loss: 0.7568556618783596, accuracy: 0.5133333333333333\n",
      "iteration no 174: Loss: 0.7568479436392971, accuracy: 0.5133333333333333\n",
      "iteration no 175: Loss: 0.7568404641117098, accuracy: 0.5133333333333333\n",
      "iteration no 176: Loss: 0.7568332156560037, accuracy: 0.5133333333333333\n",
      "iteration no 177: Loss: 0.7568261908885844, accuracy: 0.5133333333333333\n",
      "iteration no 178: Loss: 0.7568193826727431, accuracy: 0.5133333333333333\n",
      "iteration no 179: Loss: 0.7568127841098932, accuracy: 0.5133333333333333\n",
      "iteration no 180: Loss: 0.7568063885311413, accuracy: 0.5133333333333333\n",
      "iteration no 181: Loss: 0.7568001894891806, accuracy: 0.5133333333333333\n",
      "iteration no 182: Loss: 0.7567941807504887, accuracy: 0.5133333333333333\n",
      "iteration no 183: Loss: 0.756788356287824, accuracy: 0.5133333333333333\n",
      "iteration no 184: Loss: 0.7567827102730036, accuracy: 0.5133333333333333\n",
      "iteration no 185: Loss: 0.7567772370699516, accuracy: 0.5133333333333333\n",
      "iteration no 186: Loss: 0.7567719312280092, accuracy: 0.5133333333333333\n",
      "iteration no 187: Loss: 0.7567667874754939, accuracy: 0.5133333333333333\n",
      "iteration no 188: Loss: 0.756761800713497, accuracy: 0.5133333333333333\n",
      "iteration no 189: Loss: 0.7567569660099144, accuracy: 0.5133333333333333\n",
      "iteration no 190: Loss: 0.7567522785936938, accuracy: 0.5133333333333333\n",
      "iteration no 191: Loss: 0.7567477338492968, accuracy: 0.5133333333333333\n",
      "iteration no 192: Loss: 0.7567433273113645, accuracy: 0.5133333333333333\n",
      "iteration no 193: Loss: 0.7567390546595755, accuracy: 0.5133333333333333\n",
      "iteration no 194: Loss: 0.7567349117136949, accuracy: 0.5133333333333333\n",
      "iteration no 195: Loss: 0.756730894428802, accuracy: 0.5133333333333333\n",
      "iteration no 196: Loss: 0.7567269988906912, accuracy: 0.5133333333333333\n",
      "iteration no 197: Loss: 0.7567232213114393, accuracy: 0.5133333333333333\n",
      "iteration no 198: Loss: 0.7567195580251332, accuracy: 0.5133333333333333\n",
      "iteration no 199: Loss: 0.7567160054837504, accuracy: 0.5133333333333333\n",
      "iteration no 200: Loss: 0.756712560253188, accuracy: 0.5166666666666667\n",
      "iteration no 1: Loss: 1.1016774148744886, accuracy: 0.5333333333333333\n",
      "iteration no 2: Loss: 1.070754299850772, accuracy: 0.55\n",
      "iteration no 3: Loss: 1.0432192288742537, accuracy: 0.53\n",
      "iteration no 4: Loss: 1.0186675376446863, accuracy: 0.5233333333333333\n",
      "iteration no 5: Loss: 0.9967400476956954, accuracy: 0.5233333333333333\n",
      "iteration no 6: Loss: 0.9771195494429495, accuracy: 0.5166666666666667\n",
      "iteration no 7: Loss: 0.9595272927664132, accuracy: 0.51\n",
      "iteration no 8: Loss: 0.9437193135849273, accuracy: 0.5066666666666667\n",
      "iteration no 9: Loss: 0.9294827054986051, accuracy: 0.51\n",
      "iteration no 10: Loss: 0.916631992484127, accuracy: 0.5066666666666667\n",
      "iteration no 11: Loss: 0.9050057249886954, accuracy: 0.5133333333333333\n",
      "iteration no 12: Loss: 0.894463372351216, accuracy: 0.5133333333333333\n",
      "iteration no 13: Loss: 0.8848825423930732, accuracy: 0.5133333333333333\n",
      "iteration no 14: Loss: 0.8761565295638497, accuracy: 0.5133333333333333\n",
      "iteration no 15: Loss: 0.8681921749555366, accuracy: 0.5133333333333333\n",
      "iteration no 16: Loss: 0.8609080118769057, accuracy: 0.51\n",
      "iteration no 17: Loss: 0.8542326667034323, accuracy: 0.51\n",
      "iteration no 18: Loss: 0.8481034842277066, accuracy: 0.51\n",
      "iteration no 19: Loss: 0.8424653482295097, accuracy: 0.51\n",
      "iteration no 20: Loss: 0.8372696704856925, accuracy: 0.5133333333333333\n",
      "iteration no 21: Loss: 0.8324735243420823, accuracy: 0.51\n",
      "iteration no 22: Loss: 0.8280389019168384, accuracy: 0.5133333333333333\n",
      "iteration no 23: Loss: 0.8239320768010953, accuracy: 0.5133333333333333\n",
      "iteration no 24: Loss: 0.8201230566719947, accuracy: 0.5133333333333333\n",
      "iteration no 25: Loss: 0.8165851124985204, accuracy: 0.5133333333333333\n",
      "iteration no 26: Loss: 0.8132943729994705, accuracy: 0.5133333333333333\n",
      "iteration no 27: Loss: 0.8102294747214404, accuracy: 0.5133333333333333\n",
      "iteration no 28: Loss: 0.8073712595677639, accuracy: 0.5133333333333333\n",
      "iteration no 29: Loss: 0.8047025128551072, accuracy: 0.5133333333333333\n",
      "iteration no 30: Loss: 0.8022077360309522, accuracy: 0.5166666666666667\n",
      "iteration no 31: Loss: 0.7998729490789127, accuracy: 0.5166666666666667\n",
      "iteration no 32: Loss: 0.7976855183935838, accuracy: 0.5166666666666667\n",
      "iteration no 33: Loss: 0.7956340065434291, accuracy: 0.5166666666666667\n",
      "iteration no 34: Loss: 0.7937080408773659, accuracy: 0.5166666666666667\n",
      "iteration no 35: Loss: 0.79189819838384, accuracy: 0.5166666666666667\n",
      "iteration no 36: Loss: 0.7901959045936432, accuracy: 0.5133333333333333\n",
      "iteration no 37: Loss: 0.7885933446407812, accuracy: 0.5133333333333333\n",
      "iteration no 38: Loss: 0.7870833848688561, accuracy: 0.5133333333333333\n",
      "iteration no 39: Loss: 0.785659503601657, accuracy: 0.5133333333333333\n",
      "iteration no 40: Loss: 0.7843157298926575, accuracy: 0.5133333333333333\n",
      "iteration no 41: Loss: 0.7830465892344989, accuracy: 0.5133333333333333\n",
      "iteration no 42: Loss: 0.7818470553509915, accuracy: 0.5133333333333333\n",
      "iteration no 43: Loss: 0.7807125073146056, accuracy: 0.5133333333333333\n",
      "iteration no 44: Loss: 0.7796386913351591, accuracy: 0.5133333333333333\n",
      "iteration no 45: Loss: 0.7786216866531674, accuracy: 0.5133333333333333\n",
      "iteration no 46: Loss: 0.7776578750464372, accuracy: 0.5133333333333333\n",
      "iteration no 47: Loss: 0.7767439135228664, accuracy: 0.5133333333333333\n",
      "iteration no 48: Loss: 0.7758767098277101, accuracy: 0.5133333333333333\n",
      "iteration no 49: Loss: 0.7750534004411384, accuracy: 0.5133333333333333\n",
      "iteration no 50: Loss: 0.7742713307828992, accuracy: 0.5133333333333333\n",
      "iteration no 51: Loss: 0.7735280373762825, accuracy: 0.5133333333333333\n",
      "iteration no 52: Loss: 0.772821231754179, accuracy: 0.5133333333333333\n",
      "iteration no 53: Loss: 0.7721487859165274, accuracy: 0.5166666666666667\n",
      "iteration no 54: Loss: 0.7715087191714345, accuracy: 0.5166666666666667\n",
      "iteration no 55: Loss: 0.7708991862122458, accuracy: 0.5166666666666667\n",
      "iteration no 56: Loss: 0.7703184663002262, accuracy: 0.5166666666666667\n",
      "iteration no 57: Loss: 0.7697649534376935, accuracy: 0.5166666666666667\n",
      "iteration no 58: Loss: 0.7692371474296842, accuracy: 0.5166666666666667\n",
      "iteration no 59: Loss: 0.7687336457438273, accuracy: 0.5166666666666667\n",
      "iteration no 60: Loss: 0.7682531360882486, accuracy: 0.5166666666666667\n",
      "iteration no 61: Loss: 0.7677943896362386, accuracy: 0.5166666666666667\n",
      "iteration no 62: Loss: 0.7673562548342482, accuracy: 0.5166666666666667\n",
      "iteration no 63: Loss: 0.7669376517366603, accuracy: 0.5166666666666667\n",
      "iteration no 64: Loss: 0.7665375668168655, accuracy: 0.5166666666666667\n",
      "iteration no 65: Loss: 0.7661550482095233, accuracy: 0.5166666666666667\n",
      "iteration no 66: Loss: 0.7657892013436327, accuracy: 0.5166666666666667\n",
      "iteration no 67: Loss: 0.7654391849302287, accuracy: 0.5166666666666667\n",
      "iteration no 68: Loss: 0.7651042072722358, accuracy: 0.5166666666666667\n",
      "iteration no 69: Loss: 0.7647835228673148, accuracy: 0.5166666666666667\n",
      "iteration no 70: Loss: 0.7644764292774612, accuracy: 0.5166666666666667\n",
      "iteration no 71: Loss: 0.7641822642417349, accuracy: 0.5166666666666667\n",
      "iteration no 72: Loss: 0.7639004030108149, accuracy: 0.5166666666666667\n",
      "iteration no 73: Loss: 0.7636302558841526, accuracy: 0.5166666666666667\n",
      "iteration no 74: Loss: 0.7633712659323498, accuracy: 0.5166666666666667\n",
      "iteration no 75: Loss: 0.7631229068890387, accuracy: 0.5166666666666667\n",
      "iteration no 76: Loss: 0.7628846811980334, accuracy: 0.5166666666666667\n",
      "iteration no 77: Loss: 0.7626561182028445, accuracy: 0.5166666666666667\n",
      "iteration no 78: Loss: 0.7624367724668426, accuracy: 0.5133333333333333\n",
      "iteration no 79: Loss: 0.7622262222134372, accuracy: 0.5133333333333333\n",
      "iteration no 80: Loss: 0.7620240678765867, accuracy: 0.5133333333333333\n",
      "iteration no 81: Loss: 0.7618299307528389, accuracy: 0.5133333333333333\n",
      "iteration no 82: Loss: 0.7616434517468716, accuracy: 0.5133333333333333\n",
      "iteration no 83: Loss: 0.7614642902032195, accuracy: 0.5133333333333333\n",
      "iteration no 84: Loss: 0.7612921228175021, accuracy: 0.5133333333333333\n",
      "iteration no 85: Loss: 0.761126642621051, accuracy: 0.5133333333333333\n",
      "iteration no 86: Loss: 0.7609675580333533, accuracy: 0.5133333333333333\n",
      "iteration no 87: Loss: 0.7608145919772021, accuracy: 0.5133333333333333\n",
      "iteration no 88: Loss: 0.7606674810518725, accuracy: 0.5133333333333333\n",
      "iteration no 89: Loss: 0.7605259747600325, accuracy: 0.5133333333333333\n",
      "iteration no 90: Loss: 0.7603898347844503, accuracy: 0.5133333333333333\n",
      "iteration no 91: Loss: 0.7602588343108828, accuracy: 0.5133333333333333\n",
      "iteration no 92: Loss: 0.7601327573938177, accuracy: 0.5133333333333333\n",
      "iteration no 93: Loss: 0.7600113983620164, accuracy: 0.5133333333333333\n",
      "iteration no 94: Loss: 0.7598945612610413, accuracy: 0.5133333333333333\n",
      "iteration no 95: Loss: 0.7597820593301725, accuracy: 0.5133333333333333\n",
      "iteration no 96: Loss: 0.759673714511329, accuracy: 0.5133333333333333\n",
      "iteration no 97: Loss: 0.759569356987783, accuracy: 0.5133333333333333\n",
      "iteration no 98: Loss: 0.7594688247506365, accuracy: 0.5133333333333333\n",
      "iteration no 99: Loss: 0.7593719631911758, accuracy: 0.5133333333333333\n",
      "iteration no 100: Loss: 0.7592786247173682, accuracy: 0.5133333333333333\n",
      "iteration no 101: Loss: 0.7591886683928871, accuracy: 0.5166666666666667\n",
      "iteration no 102: Loss: 0.7591019595971825, accuracy: 0.5166666666666667\n",
      "iteration no 103: Loss: 0.7590183697052114, accuracy: 0.5166666666666667\n",
      "iteration no 104: Loss: 0.7589377757855521, accuracy: 0.5166666666666667\n",
      "iteration no 105: Loss: 0.7588600603157151, accuracy: 0.5166666666666667\n",
      "iteration no 106: Loss: 0.7587851109135505, accuracy: 0.5166666666666667\n",
      "iteration no 107: Loss: 0.7587128200837271, accuracy: 0.5166666666666667\n",
      "iteration no 108: Loss: 0.7586430849783357, accuracy: 0.5166666666666667\n",
      "iteration no 109: Loss: 0.75857580717073, accuracy: 0.5166666666666667\n",
      "iteration no 110: Loss: 0.7585108924417855, accuracy: 0.5166666666666667\n",
      "iteration no 111: Loss: 0.7584482505778097, accuracy: 0.5166666666666667\n",
      "iteration no 112: Loss: 0.7583877951793926, accuracy: 0.5166666666666667\n",
      "iteration no 113: Loss: 0.7583294434805286, accuracy: 0.5166666666666667\n",
      "iteration no 114: Loss: 0.7582731161773975, accuracy: 0.5166666666666667\n",
      "iteration no 115: Loss: 0.7582187372662206, accuracy: 0.5166666666666667\n",
      "iteration no 116: Loss: 0.7581662338896551, accuracy: 0.5166666666666667\n",
      "iteration no 117: Loss: 0.7581155361912232, accuracy: 0.5166666666666667\n",
      "iteration no 118: Loss: 0.7580665771773045, accuracy: 0.5166666666666667\n",
      "iteration no 119: Loss: 0.7580192925862514, accuracy: 0.5166666666666667\n",
      "iteration no 120: Loss: 0.7579736207642179, accuracy: 0.5166666666666667\n",
      "iteration no 121: Loss: 0.7579295025473127, accuracy: 0.5166666666666667\n",
      "iteration no 122: Loss: 0.7578868811497207, accuracy: 0.5166666666666667\n",
      "iteration no 123: Loss: 0.7578457020574513, accuracy: 0.5166666666666667\n",
      "iteration no 124: Loss: 0.7578059129273991, accuracy: 0.5166666666666667\n",
      "iteration no 125: Loss: 0.7577674634914202, accuracy: 0.5166666666666667\n",
      "iteration no 126: Loss: 0.7577303054651442, accuracy: 0.5166666666666667\n",
      "iteration no 127: Loss: 0.757694392461262, accuracy: 0.5166666666666667\n",
      "iteration no 128: Loss: 0.7576596799070446, accuracy: 0.5166666666666667\n",
      "iteration no 129: Loss: 0.7576261249658611, accuracy: 0.5166666666666667\n",
      "iteration no 130: Loss: 0.75759368646248, accuracy: 0.5166666666666667\n",
      "iteration no 131: Loss: 0.757562324811953, accuracy: 0.5166666666666667\n",
      "iteration no 132: Loss: 0.7575320019518844, accuracy: 0.5166666666666667\n",
      "iteration no 133: Loss: 0.7575026812779126, accuracy: 0.5133333333333333\n",
      "iteration no 134: Loss: 0.7574743275822293, accuracy: 0.5133333333333333\n",
      "iteration no 135: Loss: 0.7574469069949799, accuracy: 0.5133333333333333\n",
      "iteration no 136: Loss: 0.7574203869283938, accuracy: 0.5133333333333333\n",
      "iteration no 137: Loss: 0.7573947360235029, accuracy: 0.5133333333333333\n",
      "iteration no 138: Loss: 0.7573699240993167, accuracy: 0.5133333333333333\n",
      "iteration no 139: Loss: 0.7573459221043249, accuracy: 0.5133333333333333\n",
      "iteration no 140: Loss: 0.7573227020702124, accuracy: 0.5133333333333333\n",
      "iteration no 141: Loss: 0.7573002370676724, accuracy: 0.5133333333333333\n",
      "iteration no 142: Loss: 0.7572785011642131, accuracy: 0.5133333333333333\n",
      "iteration no 143: Loss: 0.7572574693838591, accuracy: 0.5133333333333333\n",
      "iteration no 144: Loss: 0.7572371176686485, accuracy: 0.5133333333333333\n",
      "iteration no 145: Loss: 0.7572174228418448, accuracy: 0.5133333333333333\n",
      "iteration no 146: Loss: 0.757198362572771, accuracy: 0.5133333333333333\n",
      "iteration no 147: Loss: 0.7571799153431928, accuracy: 0.5133333333333333\n",
      "iteration no 148: Loss: 0.7571620604151713, accuracy: 0.5133333333333333\n",
      "iteration no 149: Loss: 0.757144777800315, accuracy: 0.5133333333333333\n",
      "iteration no 150: Loss: 0.7571280482303677, accuracy: 0.5133333333333333\n",
      "iteration no 151: Loss: 0.7571118531290603, accuracy: 0.5133333333333333\n",
      "iteration no 152: Loss: 0.7570961745851738, accuracy: 0.5133333333333333\n",
      "iteration no 153: Loss: 0.7570809953267531, accuracy: 0.5133333333333333\n",
      "iteration no 154: Loss: 0.7570662986964156, accuracy: 0.5133333333333333\n",
      "iteration no 155: Loss: 0.7570520686277064, accuracy: 0.5133333333333333\n",
      "iteration no 156: Loss: 0.7570382896224497, accuracy: 0.5133333333333333\n",
      "iteration no 157: Loss: 0.757024946729049, accuracy: 0.5133333333333333\n",
      "iteration no 158: Loss: 0.7570120255216952, accuracy: 0.5133333333333333\n",
      "iteration no 159: Loss: 0.7569995120804383, accuracy: 0.5133333333333333\n",
      "iteration no 160: Loss: 0.7569873929720828, accuracy: 0.5133333333333333\n",
      "iteration no 161: Loss: 0.7569756552318733, accuracy: 0.5133333333333333\n",
      "iteration no 162: Loss: 0.7569642863459282, accuracy: 0.5133333333333333\n",
      "iteration no 163: Loss: 0.7569532742343941, accuracy: 0.5133333333333333\n",
      "iteration no 164: Loss: 0.7569426072352844, accuracy: 0.5133333333333333\n",
      "iteration no 165: Loss: 0.7569322740889725, accuracy: 0.5133333333333333\n",
      "iteration no 166: Loss: 0.7569222639233114, accuracy: 0.5133333333333333\n",
      "iteration no 167: Loss: 0.7569125662393512, accuracy: 0.5133333333333333\n",
      "iteration no 168: Loss: 0.7569031708976282, accuracy: 0.5133333333333333\n",
      "iteration no 169: Loss: 0.7568940681049999, accuracy: 0.5133333333333333\n",
      "iteration no 170: Loss: 0.7568852484020052, accuracy: 0.5133333333333333\n",
      "iteration no 171: Loss: 0.7568767026507214, accuracy: 0.5133333333333333\n",
      "iteration no 172: Loss: 0.7568684220231016, accuracy: 0.5133333333333333\n",
      "iteration no 173: Loss: 0.7568603979897701, accuracy: 0.5133333333333333\n",
      "iteration no 174: Loss: 0.7568526223092548, accuracy: 0.5133333333333333\n",
      "iteration no 175: Loss: 0.756845087017639, accuracy: 0.5133333333333333\n",
      "iteration no 176: Loss: 0.7568377844186164, accuracy: 0.5133333333333333\n",
      "iteration no 177: Loss: 0.7568307070739297, accuracy: 0.5133333333333333\n",
      "iteration no 178: Loss: 0.7568238477941764, accuracy: 0.5133333333333333\n",
      "iteration no 179: Loss: 0.75681719962997, accuracy: 0.5133333333333333\n",
      "iteration no 180: Loss: 0.7568107558634372, accuracy: 0.5133333333333333\n",
      "iteration no 181: Loss: 0.7568045100000407, accuracy: 0.5133333333333333\n",
      "iteration no 182: Loss: 0.7567984557607109, accuracy: 0.5133333333333333\n",
      "iteration no 183: Loss: 0.7567925870742787, accuracy: 0.5133333333333333\n",
      "iteration no 184: Loss: 0.7567868980701888, accuracy: 0.5133333333333333\n",
      "iteration no 185: Loss: 0.7567813830714923, accuracy: 0.5133333333333333\n",
      "iteration no 186: Loss: 0.7567760365880977, accuracy: 0.5133333333333333\n",
      "iteration no 187: Loss: 0.7567708533102765, accuracy: 0.5133333333333333\n",
      "iteration no 188: Loss: 0.7567658281024098, accuracy: 0.5133333333333333\n",
      "iteration no 189: Loss: 0.756760955996966, accuracy: 0.5133333333333333\n",
      "iteration no 190: Loss: 0.7567562321887036, accuracy: 0.5133333333333333\n",
      "iteration no 191: Loss: 0.756751652029085, accuracy: 0.5133333333333333\n",
      "iteration no 192: Loss: 0.7567472110208977, accuracy: 0.5133333333333333\n",
      "iteration no 193: Loss: 0.7567429048130717, accuracy: 0.5133333333333333\n",
      "iteration no 194: Loss: 0.756738729195686, accuracy: 0.5133333333333333\n",
      "iteration no 195: Loss: 0.7567346800951581, accuracy: 0.5133333333333333\n",
      "iteration no 196: Loss: 0.7567307535696068, accuracy: 0.5133333333333333\n",
      "iteration no 197: Loss: 0.7567269458043837, accuracy: 0.5133333333333333\n",
      "iteration no 198: Loss: 0.756723253107766, accuracy: 0.5133333333333333\n",
      "iteration no 199: Loss: 0.7567196719068043, accuracy: 0.5133333333333333\n",
      "iteration no 200: Loss: 0.7567161987433186, accuracy: 0.5166666666666667\n",
      "iteration no 1: Loss: 1.0997854653831263, accuracy: 0.44333333333333336\n",
      "iteration no 2: Loss: 1.0687887234403577, accuracy: 0.45666666666666667\n",
      "iteration no 3: Loss: 1.0412226017835577, accuracy: 0.49333333333333335\n",
      "iteration no 4: Loss: 1.0166773506974163, accuracy: 0.5066666666666667\n",
      "iteration no 5: Loss: 0.9947862459050418, accuracy: 0.51\n",
      "iteration no 6: Loss: 0.9752243586576751, accuracy: 0.5033333333333333\n",
      "iteration no 7: Loss: 0.9577060471673627, accuracy: 0.5033333333333333\n",
      "iteration no 8: Loss: 0.9419816349269715, accuracy: 0.5033333333333333\n",
      "iteration no 9: Loss: 0.927833714498333, accuracy: 0.51\n",
      "iteration no 10: Loss: 0.9150734058210961, accuracy: 0.5166666666666667\n",
      "iteration no 11: Loss: 0.9035367825434291, accuracy: 0.52\n",
      "iteration no 12: Loss: 0.8930815862355679, accuracy: 0.52\n",
      "iteration no 13: Loss: 0.8835842819075698, accuracy: 0.52\n",
      "iteration no 14: Loss: 0.8749374653936116, accuracy: 0.52\n",
      "iteration no 15: Loss: 0.867047607925687, accuracy: 0.52\n",
      "iteration no 16: Loss: 0.8598331100189597, accuracy: 0.5166666666666667\n",
      "iteration no 17: Loss: 0.8532226312768291, accuracy: 0.5166666666666667\n",
      "iteration no 18: Loss: 0.8471536618136132, accuracy: 0.5166666666666667\n",
      "iteration no 19: Loss: 0.8415713026385238, accuracy: 0.5133333333333333\n",
      "iteration no 20: Loss: 0.8364272252518588, accuracy: 0.5133333333333333\n",
      "iteration no 21: Loss: 0.831678784092218, accuracy: 0.5133333333333333\n",
      "iteration no 22: Loss: 0.8272882588953883, accuracy: 0.5133333333333333\n",
      "iteration no 23: Loss: 0.8232222072441674, accuracy: 0.5133333333333333\n",
      "iteration no 24: Loss: 0.8194509104933219, accuracy: 0.5133333333333333\n",
      "iteration no 25: Loss: 0.815947898808537, accuracy: 0.5133333333333333\n",
      "iteration no 26: Loss: 0.8126895432668417, accuracy: 0.5133333333333333\n",
      "iteration no 27: Loss: 0.8096547048536876, accuracy: 0.5133333333333333\n",
      "iteration no 28: Loss: 0.8068244317928118, accuracy: 0.5133333333333333\n",
      "iteration no 29: Loss: 0.8041816979957042, accuracy: 0.5166666666666667\n",
      "iteration no 30: Loss: 0.8017111765532329, accuracy: 0.5166666666666667\n",
      "iteration no 31: Loss: 0.7993990431449854, accuracy: 0.5166666666666667\n",
      "iteration no 32: Loss: 0.7972328050407014, accuracy: 0.5166666666666667\n",
      "iteration no 33: Loss: 0.795201152037525, accuracy: 0.5166666666666667\n",
      "iteration no 34: Loss: 0.7932938262377942, accuracy: 0.5133333333333333\n",
      "iteration no 35: Loss: 0.7915015080425577, accuracy: 0.5166666666666667\n",
      "iteration no 36: Loss: 0.7898157161309516, accuracy: 0.5166666666666667\n",
      "iteration no 37: Loss: 0.7882287195275272, accuracy: 0.5166666666666667\n",
      "iteration no 38: Loss: 0.7867334601390157, accuracy: 0.5166666666666667\n",
      "iteration no 39: Loss: 0.7853234843775596, accuracy: 0.5166666666666667\n",
      "iteration no 40: Loss: 0.7839928826863365, accuracy: 0.5166666666666667\n",
      "iteration no 41: Loss: 0.7827362359517578, accuracy: 0.5166666666666667\n",
      "iteration no 42: Loss: 0.7815485679290192, accuracy: 0.5166666666666667\n",
      "iteration no 43: Loss: 0.780425302928848, accuracy: 0.5166666666666667\n",
      "iteration no 44: Loss: 0.7793622281162936, accuracy: 0.5166666666666667\n",
      "iteration no 45: Loss: 0.7783554598601851, accuracy: 0.5166666666666667\n",
      "iteration no 46: Loss: 0.7774014136468572, accuracy: 0.5166666666666667\n",
      "iteration no 47: Loss: 0.7764967771358763, accuracy: 0.5166666666666667\n",
      "iteration no 48: Loss: 0.7756384859904989, accuracy: 0.5166666666666667\n",
      "iteration no 49: Loss: 0.7748237021628166, accuracy: 0.5166666666666667\n",
      "iteration no 50: Loss: 0.774049794354184, accuracy: 0.5166666666666667\n",
      "iteration no 51: Loss: 0.7733143204065747, accuracy: 0.5166666666666667\n",
      "iteration no 52: Loss: 0.7726150114107649, accuracy: 0.5166666666666667\n",
      "iteration no 53: Loss: 0.7719497573434458, accuracy: 0.5166666666666667\n",
      "iteration no 54: Loss: 0.7713165940680615, accuracy: 0.5166666666666667\n",
      "iteration no 55: Loss: 0.7707136915538937, accuracy: 0.5166666666666667\n",
      "iteration no 56: Loss: 0.7701393431850613, accuracy: 0.5166666666666667\n",
      "iteration no 57: Loss: 0.7695919560460578, accuracy: 0.5133333333333333\n",
      "iteration no 58: Loss: 0.7690700420834917, accuracy: 0.5133333333333333\n",
      "iteration no 59: Loss: 0.7685722100551111, accuracy: 0.5133333333333333\n",
      "iteration no 60: Loss: 0.7680971581871824, accuracy: 0.5133333333333333\n",
      "iteration no 61: Loss: 0.767643667470061, accuracy: 0.5133333333333333\n",
      "iteration no 62: Loss: 0.7672105955294922, accuracy: 0.5133333333333333\n",
      "iteration no 63: Loss: 0.7667968710179596, accuracy: 0.5133333333333333\n",
      "iteration no 64: Loss: 0.7664014884763677, accuracy: 0.5133333333333333\n",
      "iteration no 65: Loss: 0.766023503621623, accuracy: 0.5166666666666667\n",
      "iteration no 66: Loss: 0.7656620290203284, accuracy: 0.5166666666666667\n",
      "iteration no 67: Loss: 0.7653162301129393, accuracy: 0.5166666666666667\n",
      "iteration no 68: Loss: 0.7649853215563793, accuracy: 0.5166666666666667\n",
      "iteration no 69: Loss: 0.76466856385636, accuracy: 0.5166666666666667\n",
      "iteration no 70: Loss: 0.7643652602635366, accuracy: 0.5166666666666667\n",
      "iteration no 71: Loss: 0.7640747539101922, accuracy: 0.5166666666666667\n",
      "iteration no 72: Loss: 0.7637964251664361, accuracy: 0.5166666666666667\n",
      "iteration no 73: Loss: 0.7635296891969396, accuracy: 0.5166666666666667\n",
      "iteration no 74: Loss: 0.7632739937010595, accuracy: 0.5166666666666667\n",
      "iteration no 75: Loss: 0.7630288168208254, accuracy: 0.5166666666666667\n",
      "iteration no 76: Loss: 0.7627936652027324, accuracy: 0.5133333333333333\n",
      "iteration no 77: Loss: 0.7625680722005908, accuracy: 0.5133333333333333\n",
      "iteration no 78: Loss: 0.7623515962078525, accuracy: 0.5133333333333333\n",
      "iteration no 79: Loss: 0.7621438191089033, accuracy: 0.5133333333333333\n",
      "iteration no 80: Loss: 0.7619443448397488, accuracy: 0.5133333333333333\n",
      "iteration no 81: Loss: 0.7617527980493809, accuracy: 0.5133333333333333\n",
      "iteration no 82: Loss: 0.7615688228538943, accuracy: 0.5133333333333333\n",
      "iteration no 83: Loss: 0.7613920816760976, accuracy: 0.5133333333333333\n",
      "iteration no 84: Loss: 0.7612222541640172, accuracy: 0.5133333333333333\n",
      "iteration no 85: Loss: 0.7610590361822421, accuracy: 0.5133333333333333\n",
      "iteration no 86: Loss: 0.7609021388705859, accuracy: 0.5133333333333333\n",
      "iteration no 87: Loss: 0.7607512877650003, accuracy: 0.5133333333333333\n",
      "iteration no 88: Loss: 0.7606062219761055, accuracy: 0.5133333333333333\n",
      "iteration no 89: Loss: 0.7604666934210822, accuracy: 0.5133333333333333\n",
      "iteration no 90: Loss: 0.7603324661050221, accuracy: 0.5133333333333333\n",
      "iteration no 91: Loss: 0.760203315448149, accuracy: 0.5133333333333333\n",
      "iteration no 92: Loss: 0.7600790276556165, accuracy: 0.5133333333333333\n",
      "iteration no 93: Loss: 0.7599593991268483, accuracy: 0.5133333333333333\n",
      "iteration no 94: Loss: 0.7598442359016274, accuracy: 0.5133333333333333\n",
      "iteration no 95: Loss: 0.7597333531403625, accuracy: 0.5133333333333333\n",
      "iteration no 96: Loss: 0.7596265746361589, accuracy: 0.5133333333333333\n",
      "iteration no 97: Loss: 0.7595237323565043, accuracy: 0.5133333333333333\n",
      "iteration no 98: Loss: 0.7594246660125478, accuracy: 0.5133333333333333\n",
      "iteration no 99: Loss: 0.759329222654105, accuracy: 0.5133333333333333\n",
      "iteration no 100: Loss: 0.7592372562886603, accuracy: 0.5133333333333333\n",
      "iteration no 101: Loss: 0.75914862752277, accuracy: 0.5133333333333333\n",
      "iteration no 102: Loss: 0.7590632032243848, accuracy: 0.5133333333333333\n",
      "iteration no 103: Loss: 0.7589808562047207, accuracy: 0.5133333333333333\n",
      "iteration no 104: Loss: 0.758901464918409, accuracy: 0.5133333333333333\n",
      "iteration no 105: Loss: 0.758824913180744, accuracy: 0.5166666666666667\n",
      "iteration no 106: Loss: 0.758751089900934, accuracy: 0.5166666666666667\n",
      "iteration no 107: Loss: 0.7586798888303404, accuracy: 0.5166666666666667\n",
      "iteration no 108: Loss: 0.7586112083247596, accuracy: 0.5166666666666667\n",
      "iteration no 109: Loss: 0.7585449511198668, accuracy: 0.5166666666666667\n",
      "iteration no 110: Loss: 0.7584810241190071, accuracy: 0.5166666666666667\n",
      "iteration no 111: Loss: 0.7584193381925729, accuracy: 0.5166666666666667\n",
      "iteration no 112: Loss: 0.758359807988255, accuracy: 0.5166666666666667\n",
      "iteration no 113: Loss: 0.7583023517515098, accuracy: 0.5166666666666667\n",
      "iteration no 114: Loss: 0.7582468911556276, accuracy: 0.5166666666666667\n",
      "iteration no 115: Loss: 0.7581933511408231, accuracy: 0.5166666666666667\n",
      "iteration no 116: Loss: 0.7581416597618145, accuracy: 0.5166666666666667\n",
      "iteration no 117: Loss: 0.7580917480433895, accuracy: 0.5166666666666667\n",
      "iteration no 118: Loss: 0.7580435498434893, accuracy: 0.5166666666666667\n",
      "iteration no 119: Loss: 0.7579970017233735, accuracy: 0.5166666666666667\n",
      "iteration no 120: Loss: 0.7579520428244554, accuracy: 0.5166666666666667\n",
      "iteration no 121: Loss: 0.7579086147514257, accuracy: 0.5166666666666667\n",
      "iteration no 122: Loss: 0.7578666614613052, accuracy: 0.5166666666666667\n",
      "iteration no 123: Loss: 0.7578261291580897, accuracy: 0.5166666666666667\n",
      "iteration no 124: Loss: 0.7577869661926749, accuracy: 0.5166666666666667\n",
      "iteration no 125: Loss: 0.7577491229677612, accuracy: 0.5166666666666667\n",
      "iteration no 126: Loss: 0.7577125518474669, accuracy: 0.5166666666666667\n",
      "iteration no 127: Loss: 0.7576772070713863, accuracy: 0.5166666666666667\n",
      "iteration no 128: Loss: 0.7576430446728499, accuracy: 0.5166666666666667\n",
      "iteration no 129: Loss: 0.757610022401158, accuracy: 0.5166666666666667\n",
      "iteration no 130: Loss: 0.7575780996475715, accuracy: 0.5166666666666667\n",
      "iteration no 131: Loss: 0.7575472373748579, accuracy: 0.5133333333333333\n",
      "iteration no 132: Loss: 0.7575173980502038, accuracy: 0.5133333333333333\n",
      "iteration no 133: Loss: 0.7574885455813113, accuracy: 0.5133333333333333\n",
      "iteration no 134: Loss: 0.7574606452555144, accuracy: 0.5133333333333333\n",
      "iteration no 135: Loss: 0.7574336636817527, accuracy: 0.5133333333333333\n",
      "iteration no 136: Loss: 0.7574075687352558, accuracy: 0.5133333333333333\n",
      "iteration no 137: Loss: 0.7573823295047974, accuracy: 0.5133333333333333\n",
      "iteration no 138: Loss: 0.7573579162423837, accuracy: 0.5133333333333333\n",
      "iteration no 139: Loss: 0.7573343003152557, accuracy: 0.5133333333333333\n",
      "iteration no 140: Loss: 0.7573114541600835, accuracy: 0.5133333333333333\n",
      "iteration no 141: Loss: 0.7572893512392431, accuracy: 0.5133333333333333\n",
      "iteration no 142: Loss: 0.7572679659990703, accuracy: 0.5133333333333333\n",
      "iteration no 143: Loss: 0.7572472738299931, accuracy: 0.5133333333333333\n",
      "iteration no 144: Loss: 0.7572272510284482, accuracy: 0.5133333333333333\n",
      "iteration no 145: Loss: 0.7572078747604921, accuracy: 0.5133333333333333\n",
      "iteration no 146: Loss: 0.7571891230270266, accuracy: 0.5133333333333333\n",
      "iteration no 147: Loss: 0.7571709746305547, accuracy: 0.5133333333333333\n",
      "iteration no 148: Loss: 0.7571534091433973, accuracy: 0.5133333333333333\n",
      "iteration no 149: Loss: 0.7571364068772958, accuracy: 0.5133333333333333\n",
      "iteration no 150: Loss: 0.757119948854335, accuracy: 0.5133333333333333\n",
      "iteration no 151: Loss: 0.7571040167791239, accuracy: 0.5133333333333333\n",
      "iteration no 152: Loss: 0.7570885930121731, accuracy: 0.5133333333333333\n",
      "iteration no 153: Loss: 0.7570736605444127, accuracy: 0.5133333333333333\n",
      "iteration no 154: Loss: 0.7570592029727952, accuracy: 0.5133333333333333\n",
      "iteration no 155: Loss: 0.7570452044769344, accuracy: 0.5133333333333333\n",
      "iteration no 156: Loss: 0.757031649796732, accuracy: 0.5133333333333333\n",
      "iteration no 157: Loss: 0.7570185242109425, accuracy: 0.5133333333333333\n",
      "iteration no 158: Loss: 0.7570058135166373, accuracy: 0.5133333333333333\n",
      "iteration no 159: Loss: 0.756993504009523, accuracy: 0.5133333333333333\n",
      "iteration no 160: Loss: 0.7569815824650779, accuracy: 0.5133333333333333\n",
      "iteration no 161: Loss: 0.7569700361204665, accuracy: 0.5133333333333333\n",
      "iteration no 162: Loss: 0.7569588526571974, accuracy: 0.5133333333333333\n",
      "iteration no 163: Loss: 0.7569480201844937, accuracy: 0.5133333333333333\n",
      "iteration no 164: Loss: 0.7569375272233384, accuracy: 0.5133333333333333\n",
      "iteration no 165: Loss: 0.7569273626911704, accuracy: 0.5133333333333333\n",
      "iteration no 166: Loss: 0.7569175158871981, accuracy: 0.5133333333333333\n",
      "iteration no 167: Loss: 0.7569079764783051, accuracy: 0.5133333333333333\n",
      "iteration no 168: Loss: 0.75689873448552, accuracy: 0.5133333333333333\n",
      "iteration no 169: Loss: 0.7568897802710296, accuracy: 0.5133333333333333\n",
      "iteration no 170: Loss: 0.7568811045257048, accuracy: 0.5133333333333333\n",
      "iteration no 171: Loss: 0.7568726982571254, accuracy: 0.5133333333333333\n",
      "iteration no 172: Loss: 0.7568645527780737, accuracy: 0.5133333333333333\n",
      "iteration no 173: Loss: 0.756856659695484, accuracy: 0.5133333333333333\n",
      "iteration no 174: Loss: 0.7568490108998224, accuracy: 0.5133333333333333\n",
      "iteration no 175: Loss: 0.756841598554883, accuracy: 0.5133333333333333\n",
      "iteration no 176: Loss: 0.7568344150879798, accuracy: 0.5133333333333333\n",
      "iteration no 177: Loss: 0.7568274531805197, accuracy: 0.5133333333333333\n",
      "iteration no 178: Loss: 0.7568207057589377, accuracy: 0.5133333333333333\n",
      "iteration no 179: Loss: 0.7568141659859823, accuracy: 0.5133333333333333\n",
      "iteration no 180: Loss: 0.7568078272523344, accuracy: 0.5133333333333333\n",
      "iteration no 181: Loss: 0.7568016831685465, accuracy: 0.5133333333333333\n",
      "iteration no 182: Loss: 0.7567957275572875, accuracy: 0.5133333333333333\n",
      "iteration no 183: Loss: 0.7567899544458828, accuracy: 0.5133333333333333\n",
      "iteration no 184: Loss: 0.7567843580591367, accuracy: 0.5133333333333333\n",
      "iteration no 185: Loss: 0.7567789328124234, accuracy: 0.5133333333333333\n",
      "iteration no 186: Loss: 0.7567736733050385, accuracy: 0.5133333333333333\n",
      "iteration no 187: Loss: 0.7567685743137998, accuracy: 0.5133333333333333\n",
      "iteration no 188: Loss: 0.7567636307868857, accuracy: 0.5133333333333333\n",
      "iteration no 189: Loss: 0.7567588378379022, accuracy: 0.5133333333333333\n",
      "iteration no 190: Loss: 0.7567541907401713, accuracy: 0.5133333333333333\n",
      "iteration no 191: Loss: 0.7567496849212293, accuracy: 0.5133333333333333\n",
      "iteration no 192: Loss: 0.7567453159575275, accuracy: 0.5133333333333333\n",
      "iteration no 193: Loss: 0.7567410795693278, accuracy: 0.5133333333333333\n",
      "iteration no 194: Loss: 0.7567369716157849, accuracy: 0.5133333333333333\n",
      "iteration no 195: Loss: 0.7567329880902084, accuracy: 0.5133333333333333\n",
      "iteration no 196: Loss: 0.7567291251154955, accuracy: 0.5133333333333333\n",
      "iteration no 197: Loss: 0.756725378939732, accuracy: 0.5133333333333333\n",
      "iteration no 198: Loss: 0.7567217459319494, accuracy: 0.5133333333333333\n",
      "iteration no 199: Loss: 0.7567182225780373, accuracy: 0.5166666666666667\n",
      "iteration no 200: Loss: 0.7567148054768011, accuracy: 0.5166666666666667\n",
      "iteration no 1: Loss: 1.1052284123551952, accuracy: 0.47\n",
      "iteration no 2: Loss: 1.0737938172198385, accuracy: 0.5033333333333333\n",
      "iteration no 3: Loss: 1.045853242749604, accuracy: 0.5066666666666667\n",
      "iteration no 4: Loss: 1.0209756716824714, accuracy: 0.5066666666666667\n",
      "iteration no 5: Loss: 0.9987844783204192, accuracy: 0.5033333333333333\n",
      "iteration no 6: Loss: 0.9789495895330341, accuracy: 0.5033333333333333\n",
      "iteration no 7: Loss: 0.9611821598941958, accuracy: 0.5066666666666667\n",
      "iteration no 8: Loss: 0.9452301035745458, accuracy: 0.5066666666666667\n",
      "iteration no 9: Loss: 0.9308739596698657, accuracy: 0.5066666666666667\n",
      "iteration no 10: Loss: 0.9179229973959425, accuracy: 0.5166666666666667\n",
      "iteration no 11: Loss: 0.9062115958935947, accuracy: 0.5133333333333333\n",
      "iteration no 12: Loss: 0.895595946627977, accuracy: 0.5133333333333333\n",
      "iteration no 13: Loss: 0.885951105056519, accuracy: 0.5133333333333333\n",
      "iteration no 14: Loss: 0.8771683935339849, accuracy: 0.5166666666666667\n",
      "iteration no 15: Loss: 0.8691531389678196, accuracy: 0.5133333333333333\n",
      "iteration no 16: Loss: 0.8618227177425586, accuracy: 0.5133333333333333\n",
      "iteration no 17: Loss: 0.8551048754015332, accuracy: 0.5133333333333333\n",
      "iteration no 18: Loss: 0.8489362875757167, accuracy: 0.5133333333333333\n",
      "iteration no 19: Loss: 0.8432613300713582, accuracy: 0.5133333333333333\n",
      "iteration no 20: Loss: 0.8380310287235665, accuracy: 0.5133333333333333\n",
      "iteration no 21: Loss: 0.8332021628493089, accuracy: 0.5133333333333333\n",
      "iteration no 22: Loss: 0.8287364994437902, accuracy: 0.5133333333333333\n",
      "iteration no 23: Loss: 0.8246001384111283, accuracy: 0.5133333333333333\n",
      "iteration no 24: Loss: 0.8207629519819747, accuracy: 0.5133333333333333\n",
      "iteration no 25: Loss: 0.817198104001499, accuracy: 0.5133333333333333\n",
      "iteration no 26: Loss: 0.8138816369684712, accuracy: 0.5133333333333333\n",
      "iteration no 27: Loss: 0.8107921165904559, accuracy: 0.5133333333333333\n",
      "iteration no 28: Loss: 0.8079103252224122, accuracy: 0.5166666666666667\n",
      "iteration no 29: Loss: 0.8052189969107084, accuracy: 0.5166666666666667\n",
      "iteration no 30: Loss: 0.8027025879056422, accuracy: 0.5166666666666667\n",
      "iteration no 31: Loss: 0.8003470774644548, accuracy: 0.5166666666666667\n",
      "iteration no 32: Loss: 0.7981397945715454, accuracy: 0.5166666666666667\n",
      "iteration no 33: Loss: 0.7960692668775642, accuracy: 0.52\n",
      "iteration no 34: Loss: 0.7941250887252562, accuracy: 0.52\n",
      "iteration no 35: Loss: 0.7922978056051021, accuracy: 0.52\n",
      "iteration no 36: Loss: 0.7905788127829438, accuracy: 0.52\n",
      "iteration no 37: Loss: 0.7889602661774402, accuracy: 0.52\n",
      "iteration no 38: Loss: 0.7874350038478385, accuracy: 0.52\n",
      "iteration no 39: Loss: 0.7859964766909178, accuracy: 0.52\n",
      "iteration no 40: Loss: 0.7846386871473104, accuracy: 0.5166666666666667\n",
      "iteration no 41: Loss: 0.7833561348877814, accuracy: 0.5133333333333333\n",
      "iteration no 42: Loss: 0.7821437685944754, accuracy: 0.5133333333333333\n",
      "iteration no 43: Loss: 0.7809969430747935, accuracy: 0.5133333333333333\n",
      "iteration no 44: Loss: 0.7799113810499132, accuracy: 0.5133333333333333\n",
      "iteration no 45: Loss: 0.7788831390489293, accuracy: 0.5133333333333333\n",
      "iteration no 46: Loss: 0.7779085769155688, accuracy: 0.5133333333333333\n",
      "iteration no 47: Loss: 0.776984330499445, accuracy: 0.5133333333333333\n",
      "iteration no 48: Loss: 0.7761072871595555, accuracy: 0.5133333333333333\n",
      "iteration no 49: Loss: 0.7752745637556019, accuracy: 0.5133333333333333\n",
      "iteration no 50: Loss: 0.7744834868439195, accuracy: 0.5133333333333333\n",
      "iteration no 51: Loss: 0.7737315748303156, accuracy: 0.5133333333333333\n",
      "iteration no 52: Loss: 0.7730165218628092, accuracy: 0.5133333333333333\n",
      "iteration no 53: Loss: 0.7723361832738079, accuracy: 0.5133333333333333\n",
      "iteration no 54: Loss: 0.7716885624042871, accuracy: 0.5133333333333333\n",
      "iteration no 55: Loss: 0.7710717986625104, accuracy: 0.5133333333333333\n",
      "iteration no 56: Loss: 0.7704841566872325, accuracy: 0.5133333333333333\n",
      "iteration no 57: Loss: 0.7699240165004755, accuracy: 0.5133333333333333\n",
      "iteration no 58: Loss: 0.7693898645481956, accuracy: 0.5133333333333333\n",
      "iteration no 59: Loss: 0.7688802855387249, accuracy: 0.5133333333333333\n",
      "iteration no 60: Loss: 0.7683939549990051, accuracy: 0.5133333333333333\n",
      "iteration no 61: Loss: 0.76792963247751, accuracy: 0.5166666666666667\n",
      "iteration no 62: Loss: 0.7674861553305661, accuracy: 0.5166666666666667\n",
      "iteration no 63: Loss: 0.7670624330356433, accuracy: 0.5166666666666667\n",
      "iteration no 64: Loss: 0.7666574419812489, accuracy: 0.5166666666666667\n",
      "iteration no 65: Loss: 0.7662702206883931, accuracy: 0.5166666666666667\n",
      "iteration no 66: Loss: 0.7658998654233204, accuracy: 0.5166666666666667\n",
      "iteration no 67: Loss: 0.765545526165383, accuracy: 0.5166666666666667\n",
      "iteration no 68: Loss: 0.7652064028976303, accuracy: 0.5166666666666667\n",
      "iteration no 69: Loss: 0.7648817421909831, accuracy: 0.5166666666666667\n",
      "iteration no 70: Loss: 0.7645708340557822, accuracy: 0.5166666666666667\n",
      "iteration no 71: Loss: 0.7642730090370988, accuracy: 0.5166666666666667\n",
      "iteration no 72: Loss: 0.7639876355325164, accuracy: 0.5166666666666667\n",
      "iteration no 73: Loss: 0.7637141173131589, accuracy: 0.5166666666666667\n",
      "iteration no 74: Loss: 0.7634518912305874, accuracy: 0.5166666666666667\n",
      "iteration no 75: Loss: 0.7632004250938431, accuracy: 0.5166666666666667\n",
      "iteration no 76: Loss: 0.7629592157023903, accuracy: 0.5166666666666667\n",
      "iteration no 77: Loss: 0.762727787022042, accuracy: 0.5166666666666667\n",
      "iteration no 78: Loss: 0.7625056884921424, accuracy: 0.5166666666666667\n",
      "iteration no 79: Loss: 0.7622924934533505, accuracy: 0.5166666666666667\n",
      "iteration no 80: Loss: 0.7620877976863291, accuracy: 0.5166666666666667\n",
      "iteration no 81: Loss: 0.7618912180525159, accuracy: 0.5166666666666667\n",
      "iteration no 82: Loss: 0.7617023912289317, accuracy: 0.5133333333333333\n",
      "iteration no 83: Loss: 0.7615209725296866, accuracy: 0.5133333333333333\n",
      "iteration no 84: Loss: 0.7613466348074851, accuracy: 0.5133333333333333\n",
      "iteration no 85: Loss: 0.7611790674290039, accuracy: 0.5133333333333333\n",
      "iteration no 86: Loss: 0.7610179753185417, accuracy: 0.5133333333333333\n",
      "iteration no 87: Loss: 0.7608630780648131, accuracy: 0.5133333333333333\n",
      "iteration no 88: Loss: 0.7607141090861831, accuracy: 0.5133333333333333\n",
      "iteration no 89: Loss: 0.7605708148500366, accuracy: 0.5133333333333333\n",
      "iteration no 90: Loss: 0.7604329541423226, accuracy: 0.5133333333333333\n",
      "iteration no 91: Loss: 0.7603002973836396, accuracy: 0.5133333333333333\n",
      "iteration no 92: Loss: 0.7601726259885251, accuracy: 0.5133333333333333\n",
      "iteration no 93: Loss: 0.760049731764868, accuracy: 0.5133333333333333\n",
      "iteration no 94: Loss: 0.75993141635062, accuracy: 0.5133333333333333\n",
      "iteration no 95: Loss: 0.7598174906851939, accuracy: 0.5133333333333333\n",
      "iteration no 96: Loss: 0.7597077745131452, accuracy: 0.5133333333333333\n",
      "iteration no 97: Loss: 0.7596020959179184, accuracy: 0.5133333333333333\n",
      "iteration no 98: Loss: 0.7595002908836057, accuracy: 0.5133333333333333\n",
      "iteration no 99: Loss: 0.7594022028828282, accuracy: 0.5133333333333333\n",
      "iteration no 100: Loss: 0.7593076824889846, accuracy: 0.5133333333333333\n",
      "iteration no 101: Loss: 0.7592165870112487, accuracy: 0.5133333333333333\n",
      "iteration no 102: Loss: 0.7591287801508145, accuracy: 0.5133333333333333\n",
      "iteration no 103: Loss: 0.7590441316769988, accuracy: 0.5133333333333333\n",
      "iteration no 104: Loss: 0.7589625171219114, accuracy: 0.5166666666666667\n",
      "iteration no 105: Loss: 0.7588838174924969, accuracy: 0.5166666666666667\n",
      "iteration no 106: Loss: 0.7588079189988385, accuracy: 0.5166666666666667\n",
      "iteration no 107: Loss: 0.7587347127976922, accuracy: 0.5166666666666667\n",
      "iteration no 108: Loss: 0.7586640947502926, accuracy: 0.5166666666666667\n",
      "iteration no 109: Loss: 0.758595965193538, accuracy: 0.5166666666666667\n",
      "iteration no 110: Loss: 0.7585302287237272, accuracy: 0.5166666666666667\n",
      "iteration no 111: Loss: 0.7584667939920741, accuracy: 0.5166666666666667\n",
      "iteration no 112: Loss: 0.7584055735112812, accuracy: 0.5166666666666667\n",
      "iteration no 113: Loss: 0.758346483472502, accuracy: 0.5166666666666667\n",
      "iteration no 114: Loss: 0.7582894435720646, accuracy: 0.5166666666666667\n",
      "iteration no 115: Loss: 0.7582343768473768, accuracy: 0.5166666666666667\n",
      "iteration no 116: Loss: 0.7581812095214644, accuracy: 0.5166666666666667\n",
      "iteration no 117: Loss: 0.758129870855634, accuracy: 0.5166666666666667\n",
      "iteration no 118: Loss: 0.7580802930097902, accuracy: 0.5166666666666667\n",
      "iteration no 119: Loss: 0.7580324109099537, accuracy: 0.5166666666666667\n",
      "iteration no 120: Loss: 0.7579861621225712, accuracy: 0.5166666666666667\n",
      "iteration no 121: Loss: 0.757941486735227, accuracy: 0.5166666666666667\n",
      "iteration no 122: Loss: 0.7578983272433881, accuracy: 0.5166666666666667\n",
      "iteration no 123: Loss: 0.7578566284428458, accuracy: 0.5166666666666667\n",
      "iteration no 124: Loss: 0.7578163373275314, accuracy: 0.5166666666666667\n",
      "iteration no 125: Loss: 0.7577774029924063, accuracy: 0.5166666666666667\n",
      "iteration no 126: Loss: 0.757739776541146, accuracy: 0.5166666666666667\n",
      "iteration no 127: Loss: 0.7577034109983527, accuracy: 0.5166666666666667\n",
      "iteration no 128: Loss: 0.7576682612260491, accuracy: 0.5166666666666667\n",
      "iteration no 129: Loss: 0.7576342838442212, accuracy: 0.5166666666666667\n",
      "iteration no 130: Loss: 0.7576014371551891, accuracy: 0.5133333333333333\n",
      "iteration no 131: Loss: 0.7575696810716038, accuracy: 0.5133333333333333\n",
      "iteration no 132: Loss: 0.7575389770478715, accuracy: 0.5133333333333333\n",
      "iteration no 133: Loss: 0.7575092880148286, accuracy: 0.5133333333333333\n",
      "iteration no 134: Loss: 0.7574805783174918, accuracy: 0.5133333333333333\n",
      "iteration no 135: Loss: 0.7574528136557254, accuracy: 0.5133333333333333\n",
      "iteration no 136: Loss: 0.7574259610276705, accuracy: 0.5133333333333333\n",
      "iteration no 137: Loss: 0.7573999886757966, accuracy: 0.5133333333333333\n",
      "iteration no 138: Loss: 0.7573748660354372, accuracy: 0.5133333333333333\n",
      "iteration no 139: Loss: 0.757350563685685, accuracy: 0.5133333333333333\n",
      "iteration no 140: Loss: 0.7573270533025237, accuracy: 0.5133333333333333\n",
      "iteration no 141: Loss: 0.7573043076140872, accuracy: 0.5133333333333333\n",
      "iteration no 142: Loss: 0.7572823003579341, accuracy: 0.5133333333333333\n",
      "iteration no 143: Loss: 0.7572610062402418, accuracy: 0.5133333333333333\n",
      "iteration no 144: Loss: 0.7572404008968207, accuracy: 0.5133333333333333\n",
      "iteration no 145: Loss: 0.7572204608558599, accuracy: 0.5133333333333333\n",
      "iteration no 146: Loss: 0.7572011635023205, accuracy: 0.5133333333333333\n",
      "iteration no 147: Loss: 0.7571824870438936, accuracy: 0.5133333333333333\n",
      "iteration no 148: Loss: 0.7571644104784473, accuracy: 0.5133333333333333\n",
      "iteration no 149: Loss: 0.7571469135628935, accuracy: 0.5133333333333333\n",
      "iteration no 150: Loss: 0.757129976783401, accuracy: 0.5133333333333333\n",
      "iteration no 151: Loss: 0.7571135813268943, accuracy: 0.5133333333333333\n",
      "iteration no 152: Loss: 0.757097709053777, accuracy: 0.5133333333333333\n",
      "iteration no 153: Loss: 0.7570823424718177, accuracy: 0.5133333333333333\n",
      "iteration no 154: Loss: 0.7570674647111494, accuracy: 0.5133333333333333\n",
      "iteration no 155: Loss: 0.7570530595003242, accuracy: 0.5133333333333333\n",
      "iteration no 156: Loss: 0.7570391111433795, accuracy: 0.5133333333333333\n",
      "iteration no 157: Loss: 0.7570256044978639, accuracy: 0.5133333333333333\n",
      "iteration no 158: Loss: 0.757012524953783, accuracy: 0.5133333333333333\n",
      "iteration no 159: Loss: 0.7569998584134174, accuracy: 0.5133333333333333\n",
      "iteration no 160: Loss: 0.7569875912719796, accuracy: 0.5133333333333333\n",
      "iteration no 161: Loss: 0.7569757103990653, accuracy: 0.5133333333333333\n",
      "iteration no 162: Loss: 0.7569642031208673, accuracy: 0.5133333333333333\n",
      "iteration no 163: Loss: 0.7569530572031156, accuracy: 0.5133333333333333\n",
      "iteration no 164: Loss: 0.7569422608347117, accuracy: 0.5133333333333333\n",
      "iteration no 165: Loss: 0.7569318026120264, accuracy: 0.5133333333333333\n",
      "iteration no 166: Loss: 0.7569216715238306, accuracy: 0.5133333333333333\n",
      "iteration no 167: Loss: 0.7569118569368327, accuracy: 0.5133333333333333\n",
      "iteration no 168: Loss: 0.756902348581794, accuracy: 0.5133333333333333\n",
      "iteration no 169: Loss: 0.7568931365401976, accuracy: 0.5133333333333333\n",
      "iteration no 170: Loss: 0.7568842112314478, accuracy: 0.5133333333333333\n",
      "iteration no 171: Loss: 0.7568755634005742, accuracy: 0.5133333333333333\n",
      "iteration no 172: Loss: 0.7568671841064222, accuracy: 0.5133333333333333\n",
      "iteration no 173: Loss: 0.7568590647103051, accuracy: 0.5133333333333333\n",
      "iteration no 174: Loss: 0.7568511968651009, accuracy: 0.5133333333333333\n",
      "iteration no 175: Loss: 0.7568435725047745, accuracy: 0.5133333333333333\n",
      "iteration no 176: Loss: 0.7568361838343052, accuracy: 0.5133333333333333\n",
      "iteration no 177: Loss: 0.7568290233200039, accuracy: 0.5133333333333333\n",
      "iteration no 178: Loss: 0.7568220836802048, accuracy: 0.5133333333333333\n",
      "iteration no 179: Loss: 0.7568153578763125, accuracy: 0.5133333333333333\n",
      "iteration no 180: Loss: 0.756808839104193, accuracy: 0.5133333333333333\n",
      "iteration no 181: Loss: 0.756802520785892, accuracy: 0.5133333333333333\n",
      "iteration no 182: Loss: 0.7567963965616684, accuracy: 0.5133333333333333\n",
      "iteration no 183: Loss: 0.7567904602823288, accuracy: 0.5133333333333333\n",
      "iteration no 184: Loss: 0.7567847060018513, accuracy: 0.5133333333333333\n",
      "iteration no 185: Loss: 0.7567791279702879, accuracy: 0.5133333333333333\n",
      "iteration no 186: Loss: 0.7567737206269298, accuracy: 0.5133333333333333\n",
      "iteration no 187: Loss: 0.7567684785937315, accuracy: 0.5133333333333333\n",
      "iteration no 188: Loss: 0.7567633966689782, accuracy: 0.5133333333333333\n",
      "iteration no 189: Loss: 0.7567584698211863, accuracy: 0.5133333333333333\n",
      "iteration no 190: Loss: 0.7567536931832336, accuracy: 0.5133333333333333\n",
      "iteration no 191: Loss: 0.7567490620467024, accuracy: 0.5133333333333333\n",
      "iteration no 192: Loss: 0.7567445718564312, accuracy: 0.5133333333333333\n",
      "iteration no 193: Loss: 0.7567402182052684, accuracy: 0.5133333333333333\n",
      "iteration no 194: Loss: 0.7567359968290137, accuracy: 0.5133333333333333\n",
      "iteration no 195: Loss: 0.756731903601548, accuracy: 0.5133333333333333\n",
      "iteration no 196: Loss: 0.7567279345301361, accuracy: 0.5133333333333333\n",
      "iteration no 197: Loss: 0.7567240857509029, accuracy: 0.5133333333333333\n",
      "iteration no 198: Loss: 0.7567203535244701, accuracy: 0.5133333333333333\n",
      "iteration no 199: Loss: 0.7567167342317526, accuracy: 0.5133333333333333\n",
      "iteration no 200: Loss: 0.756713224369903, accuracy: 0.5133333333333333\n",
      "iteration no 1: Loss: 1.1153699969320647, accuracy: 0.4033333333333333\n",
      "iteration no 2: Loss: 1.082915783514436, accuracy: 0.5\n",
      "iteration no 3: Loss: 1.0540452379153702, accuracy: 0.51\n",
      "iteration no 4: Loss: 1.0283298123221567, accuracy: 0.5166666666666667\n",
      "iteration no 5: Loss: 1.005387931940779, accuracy: 0.5066666666666667\n",
      "iteration no 6: Loss: 0.9848820939451703, accuracy: 0.5066666666666667\n",
      "iteration no 7: Loss: 0.9665155922976875, accuracy: 0.5133333333333333\n",
      "iteration no 8: Loss: 0.9500288334233347, accuracy: 0.51\n",
      "iteration no 9: Loss: 0.9351954564063821, accuracy: 0.51\n",
      "iteration no 10: Loss: 0.9218184860796346, accuracy: 0.51\n",
      "iteration no 11: Loss: 0.9097266873568226, accuracy: 0.51\n",
      "iteration no 12: Loss: 0.8987712205634736, accuracy: 0.51\n",
      "iteration no 13: Loss: 0.8888226426093737, accuracy: 0.51\n",
      "iteration no 14: Loss: 0.879768261377752, accuracy: 0.51\n",
      "iteration no 15: Loss: 0.8715098279546855, accuracy: 0.51\n",
      "iteration no 16: Loss: 0.8639615391795485, accuracy: 0.51\n",
      "iteration no 17: Loss: 0.857048317826361, accuracy: 0.51\n",
      "iteration no 18: Loss: 0.8507043368136316, accuracy: 0.51\n",
      "iteration no 19: Loss: 0.8448717553362242, accuracy: 0.51\n",
      "iteration no 20: Loss: 0.8394996375270644, accuracy: 0.51\n",
      "iteration no 21: Loss: 0.8345430274605264, accuracy: 0.51\n",
      "iteration no 22: Loss: 0.8299621575788086, accuracy: 0.51\n",
      "iteration no 23: Loss: 0.8257217707264118, accuracy: 0.51\n",
      "iteration no 24: Loss: 0.8217905388034372, accuracy: 0.51\n",
      "iteration no 25: Loss: 0.8181405635536323, accuracy: 0.51\n",
      "iteration no 26: Loss: 0.8147469471856809, accuracy: 0.51\n",
      "iteration no 27: Loss: 0.8115874224051361, accuracy: 0.51\n",
      "iteration no 28: Loss: 0.8086420330386304, accuracy: 0.5133333333333333\n",
      "iteration no 29: Loss: 0.8058928577939193, accuracy: 0.5133333333333333\n",
      "iteration no 30: Loss: 0.80332377085111, accuracy: 0.5133333333333333\n",
      "iteration no 31: Loss: 0.8009202339520126, accuracy: 0.5133333333333333\n",
      "iteration no 32: Loss: 0.7986691154729243, accuracy: 0.5133333333333333\n",
      "iteration no 33: Loss: 0.7965585326548926, accuracy: 0.5133333333333333\n",
      "iteration no 34: Loss: 0.7945777137450881, accuracy: 0.51\n",
      "iteration no 35: Loss: 0.7927168772907233, accuracy: 0.5133333333333333\n",
      "iteration no 36: Loss: 0.7909671262378165, accuracy: 0.5133333333333333\n",
      "iteration no 37: Loss: 0.7893203548334367, accuracy: 0.5133333333333333\n",
      "iteration no 38: Loss: 0.7877691666223519, accuracy: 0.5133333333333333\n",
      "iteration no 39: Loss: 0.7863068020759854, accuracy: 0.5133333333333333\n",
      "iteration no 40: Loss: 0.7849270746005929, accuracy: 0.5133333333333333\n",
      "iteration no 41: Loss: 0.7836243138487038, accuracy: 0.5133333333333333\n",
      "iteration no 42: Loss: 0.7823933154082388, accuracy: 0.5133333333333333\n",
      "iteration no 43: Loss: 0.7812292960715782, accuracy: 0.5133333333333333\n",
      "iteration no 44: Loss: 0.7801278539957567, accuracy: 0.5133333333333333\n",
      "iteration no 45: Loss: 0.7790849331578946, accuracy: 0.5133333333333333\n",
      "iteration no 46: Loss: 0.7780967915894096, accuracy: 0.5133333333333333\n",
      "iteration no 47: Loss: 0.7771599729405687, accuracy: 0.5133333333333333\n",
      "iteration no 48: Loss: 0.7762712809853013, accuracy: 0.5133333333333333\n",
      "iteration no 49: Loss: 0.7754277567263387, accuracy: 0.5133333333333333\n",
      "iteration no 50: Loss: 0.774626657803921, accuracy: 0.5133333333333333\n",
      "iteration no 51: Loss: 0.7738654399485551, accuracy: 0.5133333333333333\n",
      "iteration no 52: Loss: 0.7731417402504708, accuracy: 0.5133333333333333\n",
      "iteration no 53: Loss: 0.7724533620462843, accuracy: 0.5133333333333333\n",
      "iteration no 54: Loss: 0.7717982612475072, accuracy: 0.5133333333333333\n",
      "iteration no 55: Loss: 0.7711745339565182, accuracy: 0.5133333333333333\n",
      "iteration no 56: Loss: 0.7705804052338504, accuracy: 0.5133333333333333\n",
      "iteration no 57: Loss: 0.7700142188965525, accuracy: 0.5133333333333333\n",
      "iteration no 58: Loss: 0.7694744282412453, accuracy: 0.5166666666666667\n",
      "iteration no 59: Loss: 0.7689595875976392, accuracy: 0.5166666666666667\n",
      "iteration no 60: Loss: 0.7684683446288901, accuracy: 0.5166666666666667\n",
      "iteration no 61: Loss: 0.7679994333044972, accuracy: 0.5166666666666667\n",
      "iteration no 62: Loss: 0.7675516674796204, accuracy: 0.5166666666666667\n",
      "iteration no 63: Loss: 0.7671239350219022, accuracy: 0.5166666666666667\n",
      "iteration no 64: Loss: 0.7667151924332146, accuracy: 0.5166666666666667\n",
      "iteration no 65: Loss: 0.7663244599193562, accuracy: 0.5166666666666667\n",
      "iteration no 66: Loss: 0.7659508168656587, accuracy: 0.5166666666666667\n",
      "iteration no 67: Loss: 0.7655933976808501, accuracy: 0.5166666666666667\n",
      "iteration no 68: Loss: 0.7652513879753864, accuracy: 0.5166666666666667\n",
      "iteration no 69: Loss: 0.7649240210439114, accuracy: 0.5166666666666667\n",
      "iteration no 70: Loss: 0.764610574624559, accuracy: 0.5166666666666667\n",
      "iteration no 71: Loss: 0.7643103679105292, accuracy: 0.5166666666666667\n",
      "iteration no 72: Loss: 0.7640227587917932, accuracy: 0.5166666666666667\n",
      "iteration no 73: Loss: 0.7637471413069382, accuracy: 0.5166666666666667\n",
      "iteration no 74: Loss: 0.7634829432871031, accuracy: 0.5166666666666667\n",
      "iteration no 75: Loss: 0.763229624175664, accuracy: 0.5166666666666667\n",
      "iteration no 76: Loss: 0.7629866730088871, accuracy: 0.5166666666666667\n",
      "iteration no 77: Loss: 0.762753606544145, accuracy: 0.5166666666666667\n",
      "iteration no 78: Loss: 0.7625299675235285, accuracy: 0.5166666666666667\n",
      "iteration no 79: Loss: 0.7623153230618129, accuracy: 0.5133333333333333\n",
      "iteration no 80: Loss: 0.7621092631487282, accuracy: 0.5133333333333333\n",
      "iteration no 81: Loss: 0.7619113992563938, accuracy: 0.5133333333333333\n",
      "iteration no 82: Loss: 0.7617213630435917, accuracy: 0.5133333333333333\n",
      "iteration no 83: Loss: 0.761538805149278, accuracy: 0.5133333333333333\n",
      "iteration no 84: Loss: 0.7613633940684064, accuracy: 0.5133333333333333\n",
      "iteration no 85: Loss: 0.7611948151037297, accuracy: 0.5133333333333333\n",
      "iteration no 86: Loss: 0.7610327693877899, accuracy: 0.5133333333333333\n",
      "iteration no 87: Loss: 0.760876972969797, accuracy: 0.5133333333333333\n",
      "iteration no 88: Loss: 0.760727155962544, accuracy: 0.5133333333333333\n",
      "iteration no 89: Loss: 0.7605830617449101, accuracy: 0.5133333333333333\n",
      "iteration no 90: Loss: 0.7604444462158676, accuracy: 0.5133333333333333\n",
      "iteration no 91: Loss: 0.760311077096246, accuracy: 0.5133333333333333\n",
      "iteration no 92: Loss: 0.7601827332748088, accuracy: 0.5133333333333333\n",
      "iteration no 93: Loss: 0.7600592041954727, accuracy: 0.5133333333333333\n",
      "iteration no 94: Loss: 0.7599402892827574, accuracy: 0.5133333333333333\n",
      "iteration no 95: Loss: 0.7598257974027756, accuracy: 0.5133333333333333\n",
      "iteration no 96: Loss: 0.7597155463572934, accuracy: 0.5133333333333333\n",
      "iteration no 97: Loss: 0.7596093624085704, accuracy: 0.5133333333333333\n",
      "iteration no 98: Loss: 0.7595070798328782, accuracy: 0.5133333333333333\n",
      "iteration no 99: Loss: 0.7594085405007455, accuracy: 0.5133333333333333\n",
      "iteration no 100: Loss: 0.7593135934821312, accuracy: 0.5133333333333333\n",
      "iteration no 101: Loss: 0.7592220946748613, accuracy: 0.5133333333333333\n",
      "iteration no 102: Loss: 0.7591339064547863, accuracy: 0.5133333333333333\n",
      "iteration no 103: Loss: 0.7590488973462325, accuracy: 0.5166666666666667\n",
      "iteration no 104: Loss: 0.7589669417114253, accuracy: 0.5166666666666667\n",
      "iteration no 105: Loss: 0.7588879194576528, accuracy: 0.5166666666666667\n",
      "iteration no 106: Loss: 0.758811715761038, accuracy: 0.5166666666666667\n",
      "iteration no 107: Loss: 0.7587382208058548, accuracy: 0.5166666666666667\n",
      "iteration no 108: Loss: 0.7586673295384099, accuracy: 0.5166666666666667\n",
      "iteration no 109: Loss: 0.7585989414345744, accuracy: 0.5166666666666667\n",
      "iteration no 110: Loss: 0.7585329602801185, accuracy: 0.5166666666666667\n",
      "iteration no 111: Loss: 0.7584692939630553, accuracy: 0.5166666666666667\n",
      "iteration no 112: Loss: 0.7584078542772563, accuracy: 0.5166666666666667\n",
      "iteration no 113: Loss: 0.7583485567366592, accuracy: 0.5166666666666667\n",
      "iteration no 114: Loss: 0.7582913203994172, accuracy: 0.5166666666666667\n",
      "iteration no 115: Loss: 0.758236067701403, accuracy: 0.5166666666666667\n",
      "iteration no 116: Loss: 0.7581827242985044, accuracy: 0.5166666666666667\n",
      "iteration no 117: Loss: 0.7581312189171943, accuracy: 0.5166666666666667\n",
      "iteration no 118: Loss: 0.7580814832128856, accuracy: 0.5166666666666667\n",
      "iteration no 119: Loss: 0.7580334516356231, accuracy: 0.5166666666666667\n",
      "iteration no 120: Loss: 0.7579870613026788, accuracy: 0.5166666666666667\n",
      "iteration no 121: Loss: 0.757942251877662, accuracy: 0.5166666666666667\n",
      "iteration no 122: Loss: 0.7578989654557652, accuracy: 0.5166666666666667\n",
      "iteration no 123: Loss: 0.7578571464548016, accuracy: 0.5166666666666667\n",
      "iteration no 124: Loss: 0.7578167415117062, accuracy: 0.5166666666666667\n",
      "iteration no 125: Loss: 0.7577776993841929, accuracy: 0.5166666666666667\n",
      "iteration no 126: Loss: 0.7577399708572844, accuracy: 0.5166666666666667\n",
      "iteration no 127: Loss: 0.7577035086544414, accuracy: 0.5166666666666667\n",
      "iteration no 128: Loss: 0.7576682673530416, accuracy: 0.5166666666666667\n",
      "iteration no 129: Loss: 0.7576342033039681, accuracy: 0.5166666666666667\n",
      "iteration no 130: Loss: 0.7576012745550885, accuracy: 0.5166666666666667\n",
      "iteration no 131: Loss: 0.7575694407784094, accuracy: 0.5166666666666667\n",
      "iteration no 132: Loss: 0.757538663200714, accuracy: 0.5166666666666667\n",
      "iteration no 133: Loss: 0.7575089045374963, accuracy: 0.5166666666666667\n",
      "iteration no 134: Loss: 0.7574801289300153, accuracy: 0.5133333333333333\n",
      "iteration no 135: Loss: 0.7574523018853073, accuracy: 0.5133333333333333\n",
      "iteration no 136: Loss: 0.7574253902190023, accuracy: 0.5133333333333333\n",
      "iteration no 137: Loss: 0.7573993620007956, accuracy: 0.5133333333333333\n",
      "iteration no 138: Loss: 0.7573741865024399, accuracy: 0.5133333333333333\n",
      "iteration no 139: Loss: 0.7573498341481286, accuracy: 0.5133333333333333\n",
      "iteration no 140: Loss: 0.7573262764671471, accuracy: 0.5133333333333333\n",
      "iteration no 141: Loss: 0.7573034860486765, accuracy: 0.5133333333333333\n",
      "iteration no 142: Loss: 0.7572814364986428, accuracy: 0.5133333333333333\n",
      "iteration no 143: Loss: 0.7572601023985082, accuracy: 0.5133333333333333\n",
      "iteration no 144: Loss: 0.7572394592659061, accuracy: 0.5133333333333333\n",
      "iteration no 145: Loss: 0.7572194835170307, accuracy: 0.5133333333333333\n",
      "iteration no 146: Loss: 0.757200152430693, accuracy: 0.5133333333333333\n",
      "iteration no 147: Loss: 0.7571814441139618, accuracy: 0.5133333333333333\n",
      "iteration no 148: Loss: 0.7571633374693119, accuracy: 0.5133333333333333\n",
      "iteration no 149: Loss: 0.7571458121632079, accuracy: 0.5133333333333333\n",
      "iteration no 150: Loss: 0.7571288485960523, accuracy: 0.5133333333333333\n",
      "iteration no 151: Loss: 0.7571124278734329, accuracy: 0.5133333333333333\n",
      "iteration no 152: Loss: 0.7570965317786078, accuracy: 0.5133333333333333\n",
      "iteration no 153: Loss: 0.7570811427461699, accuracy: 0.5133333333333333\n",
      "iteration no 154: Loss: 0.7570662438368333, accuracy: 0.5133333333333333\n",
      "iteration no 155: Loss: 0.7570518187132895, accuracy: 0.5133333333333333\n",
      "iteration no 156: Loss: 0.7570378516170846, accuracy: 0.5133333333333333\n",
      "iteration no 157: Loss: 0.7570243273464679, accuracy: 0.5133333333333333\n",
      "iteration no 158: Loss: 0.7570112312351683, accuracy: 0.5133333333333333\n",
      "iteration no 159: Loss: 0.7569985491320558, accuracy: 0.5133333333333333\n",
      "iteration no 160: Loss: 0.7569862673816454, accuracy: 0.5133333333333333\n",
      "iteration no 161: Loss: 0.7569743728054081, accuracy: 0.5133333333333333\n",
      "iteration no 162: Loss: 0.7569628526838509, accuracy: 0.5133333333333333\n",
      "iteration no 163: Loss: 0.7569516947393278, accuracy: 0.5133333333333333\n",
      "iteration no 164: Loss: 0.7569408871195566, accuracy: 0.5133333333333333\n",
      "iteration no 165: Loss: 0.7569304183818019, accuracy: 0.5133333333333333\n",
      "iteration no 166: Loss: 0.7569202774776979, accuracy: 0.5133333333333333\n",
      "iteration no 167: Loss: 0.7569104537386855, accuracy: 0.5133333333333333\n",
      "iteration no 168: Loss: 0.7569009368620297, accuracy: 0.5133333333333333\n",
      "iteration no 169: Loss: 0.7568917168973991, accuracy: 0.5133333333333333\n",
      "iteration no 170: Loss: 0.756882784233978, accuracy: 0.5133333333333333\n",
      "iteration no 171: Loss: 0.7568741295880894, accuracy: 0.5133333333333333\n",
      "iteration no 172: Loss: 0.7568657439913075, accuracy: 0.5133333333333333\n",
      "iteration no 173: Loss: 0.7568576187790369, accuracy: 0.5133333333333333\n",
      "iteration no 174: Loss: 0.7568497455795409, accuracy: 0.5133333333333333\n",
      "iteration no 175: Loss: 0.7568421163033958, accuracy: 0.5133333333333333\n",
      "iteration no 176: Loss: 0.7568347231333584, accuracy: 0.5133333333333333\n",
      "iteration no 177: Loss: 0.756827558514623, accuracy: 0.5133333333333333\n",
      "iteration no 178: Loss: 0.7568206151454575, accuracy: 0.5133333333333333\n",
      "iteration no 179: Loss: 0.7568138859681967, accuracy: 0.5133333333333333\n",
      "iteration no 180: Loss: 0.7568073641605844, accuracy: 0.5133333333333333\n",
      "iteration no 181: Loss: 0.7568010431274435, accuracy: 0.5133333333333333\n",
      "iteration no 182: Loss: 0.7567949164926631, accuracy: 0.5133333333333333\n",
      "iteration no 183: Loss: 0.756788978091492, accuracy: 0.5133333333333333\n",
      "iteration no 184: Loss: 0.7567832219631215, accuracy: 0.5133333333333333\n",
      "iteration no 185: Loss: 0.7567776423435473, accuracy: 0.5133333333333333\n",
      "iteration no 186: Loss: 0.756772233658701, accuracy: 0.5133333333333333\n",
      "iteration no 187: Loss: 0.756766990517838, accuracy: 0.5133333333333333\n",
      "iteration no 188: Loss: 0.7567619077071717, accuracy: 0.5133333333333333\n",
      "iteration no 189: Loss: 0.756756980183744, accuracy: 0.5133333333333333\n",
      "iteration no 190: Loss: 0.7567522030695244, accuracy: 0.5133333333333333\n",
      "iteration no 191: Loss: 0.7567475716457259, accuracy: 0.5133333333333333\n",
      "iteration no 192: Loss: 0.7567430813473301, accuracy: 0.5133333333333333\n",
      "iteration no 193: Loss: 0.7567387277578147, accuracy: 0.5133333333333333\n",
      "iteration no 194: Loss: 0.7567345066040715, accuracy: 0.5133333333333333\n",
      "iteration no 195: Loss: 0.7567304137515134, accuracy: 0.5133333333333333\n",
      "iteration no 196: Loss: 0.7567264451993551, accuracy: 0.5133333333333333\n",
      "iteration no 197: Loss: 0.7567225970760684, accuracy: 0.5133333333333333\n",
      "iteration no 198: Loss: 0.7567188656350008, accuracy: 0.5133333333333333\n",
      "iteration no 199: Loss: 0.756715247250151, accuracy: 0.5133333333333333\n",
      "iteration no 200: Loss: 0.756711738412098, accuracy: 0.5133333333333333\n",
      "iteration no 1: Loss: 1.0917085254637875, accuracy: 0.47\n",
      "iteration no 2: Loss: 1.0617471796492495, accuracy: 0.49\n",
      "iteration no 3: Loss: 1.0351069171941503, accuracy: 0.49333333333333335\n",
      "iteration no 4: Loss: 1.0113765393917027, accuracy: 0.5\n",
      "iteration no 5: Loss: 0.9901968186642377, accuracy: 0.5066666666666667\n",
      "iteration no 6: Loss: 0.9712534329068383, accuracy: 0.51\n",
      "iteration no 7: Loss: 0.9542718486308821, accuracy: 0.51\n",
      "iteration no 8: Loss: 0.9390128933826958, accuracy: 0.5133333333333333\n",
      "iteration no 9: Loss: 0.9252686459487438, accuracy: 0.51\n",
      "iteration no 10: Loss: 0.9128585950944301, accuracy: 0.51\n",
      "iteration no 11: Loss: 0.9016261040744157, accuracy: 0.5066666666666667\n",
      "iteration no 12: Loss: 0.8914352186454775, accuracy: 0.5166666666666667\n",
      "iteration no 13: Loss: 0.8821678338071498, accuracy: 0.5133333333333333\n",
      "iteration no 14: Loss: 0.8737212123101825, accuracy: 0.5133333333333333\n",
      "iteration no 15: Loss: 0.8660058325854861, accuracy: 0.5133333333333333\n",
      "iteration no 16: Loss: 0.8589435352974741, accuracy: 0.5166666666666667\n",
      "iteration no 17: Loss: 0.8524659345097279, accuracy: 0.5166666666666667\n",
      "iteration no 18: Loss: 0.8465130596293101, accuracy: 0.5166666666666667\n",
      "iteration no 19: Loss: 0.8410321964204448, accuracy: 0.5166666666666667\n",
      "iteration no 20: Loss: 0.835976898452485, accuracy: 0.5166666666666667\n",
      "iteration no 21: Loss: 0.8313061437436379, accuracy: 0.5166666666666667\n",
      "iteration no 22: Loss: 0.8269836147150752, accuracy: 0.5166666666666667\n",
      "iteration no 23: Loss: 0.8229770826862198, accuracy: 0.5133333333333333\n",
      "iteration no 24: Loss: 0.819257880934089, accuracy: 0.5166666666666667\n",
      "iteration no 25: Loss: 0.8158004527833443, accuracy: 0.5133333333333333\n",
      "iteration no 26: Loss: 0.8125819632995098, accuracy: 0.5133333333333333\n",
      "iteration no 27: Loss: 0.8095819649533634, accuracy: 0.5133333333333333\n",
      "iteration no 28: Loss: 0.8067821091446502, accuracy: 0.5166666666666667\n",
      "iteration no 29: Loss: 0.8041658967541202, accuracy: 0.5166666666666667\n",
      "iteration no 30: Loss: 0.8017184619688698, accuracy: 0.5166666666666667\n",
      "iteration no 31: Loss: 0.7994263845282008, accuracy: 0.5166666666666667\n",
      "iteration no 32: Loss: 0.7972775262931067, accuracy: 0.5166666666666667\n",
      "iteration no 33: Loss: 0.7952608886756757, accuracy: 0.5166666666666667\n",
      "iteration no 34: Loss: 0.7933664879952883, accuracy: 0.52\n",
      "iteration no 35: Loss: 0.7915852462734329, accuracy: 0.52\n",
      "iteration no 36: Loss: 0.7899088953524925, accuracy: 0.52\n",
      "iteration no 37: Loss: 0.7883298925378559, accuracy: 0.52\n",
      "iteration no 38: Loss: 0.7868413462270462, accuracy: 0.52\n",
      "iteration no 39: Loss: 0.7854369502124549, accuracy: 0.52\n",
      "iteration no 40: Loss: 0.7841109255325621, accuracy: 0.52\n",
      "iteration no 41: Loss: 0.7828579689058552, accuracy: 0.5166666666666667\n",
      "iteration no 42: Loss: 0.7816732069167422, accuracy: 0.5166666666666667\n",
      "iteration no 43: Loss: 0.7805521552375145, accuracy: 0.5166666666666667\n",
      "iteration no 44: Loss: 0.7794906822680606, accuracy: 0.5166666666666667\n",
      "iteration no 45: Loss: 0.7784849766583272, accuracy: 0.5166666666666667\n",
      "iteration no 46: Loss: 0.7775315182496702, accuracy: 0.5133333333333333\n",
      "iteration no 47: Loss: 0.7766270520321529, accuracy: 0.5133333333333333\n",
      "iteration no 48: Loss: 0.7757685647671029, accuracy: 0.5133333333333333\n",
      "iteration no 49: Loss: 0.7749532639691316, accuracy: 0.5133333333333333\n",
      "iteration no 50: Loss: 0.7741785589804909, accuracy: 0.5133333333333333\n",
      "iteration no 51: Loss: 0.7734420439039847, accuracy: 0.5133333333333333\n",
      "iteration no 52: Loss: 0.7727414821894796, accuracy: 0.5133333333333333\n",
      "iteration no 53: Loss: 0.7720747926940186, accuracy: 0.5133333333333333\n",
      "iteration no 54: Loss: 0.7714400370571863, accuracy: 0.5133333333333333\n",
      "iteration no 55: Loss: 0.770835408252184, accuracy: 0.5133333333333333\n",
      "iteration no 56: Loss: 0.7702592201894507, accuracy: 0.5133333333333333\n",
      "iteration no 57: Loss: 0.7697098982639446, accuracy: 0.5133333333333333\n",
      "iteration no 58: Loss: 0.7691859707496663, accuracy: 0.5133333333333333\n",
      "iteration no 59: Loss: 0.7686860609559208, accuracy: 0.5133333333333333\n",
      "iteration no 60: Loss: 0.7682088800693778, accuracy: 0.5133333333333333\n",
      "iteration no 61: Loss: 0.7677532206143761, accuracy: 0.5133333333333333\n",
      "iteration no 62: Loss: 0.7673179504713067, accuracy: 0.5166666666666667\n",
      "iteration no 63: Loss: 0.7669020073993937, accuracy: 0.5166666666666667\n",
      "iteration no 64: Loss: 0.7665043940159333, accuracy: 0.5166666666666667\n",
      "iteration no 65: Loss: 0.7661241731890974, accuracy: 0.5166666666666667\n",
      "iteration no 66: Loss: 0.7657604638058947, accuracy: 0.5166666666666667\n",
      "iteration no 67: Loss: 0.7654124368808346, accuracy: 0.5166666666666667\n",
      "iteration no 68: Loss: 0.7650793119743614, accuracy: 0.5166666666666667\n",
      "iteration no 69: Loss: 0.7647603538932434, accuracy: 0.5166666666666667\n",
      "iteration no 70: Loss: 0.7644548696478756, accuracy: 0.5166666666666667\n",
      "iteration no 71: Loss: 0.7641622056439353, accuracy: 0.5166666666666667\n",
      "iteration no 72: Loss: 0.7638817450880226, accuracy: 0.5166666666666667\n",
      "iteration no 73: Loss: 0.7636129055888903, accuracy: 0.5166666666666667\n",
      "iteration no 74: Loss: 0.7633551369376338, accuracy: 0.5166666666666667\n",
      "iteration no 75: Loss: 0.7631079190517694, accuracy: 0.5166666666666667\n",
      "iteration no 76: Loss: 0.7628707600695578, accuracy: 0.5166666666666667\n",
      "iteration no 77: Loss: 0.7626431945821792, accuracy: 0.5166666666666667\n",
      "iteration no 78: Loss: 0.7624247819925166, accuracy: 0.5166666666666667\n",
      "iteration no 79: Loss: 0.7622151049903136, accuracy: 0.5166666666666667\n",
      "iteration no 80: Loss: 0.7620137681344035, accuracy: 0.5166666666666667\n",
      "iteration no 81: Loss: 0.7618203965335235, accuracy: 0.5133333333333333\n",
      "iteration no 82: Loss: 0.76163463461799, accuracy: 0.5133333333333333\n",
      "iteration no 83: Loss: 0.7614561449951704, accuracy: 0.5133333333333333\n",
      "iteration no 84: Loss: 0.761284607382311, accuracy: 0.5133333333333333\n",
      "iteration no 85: Loss: 0.7611197176108208, accuracy: 0.5133333333333333\n",
      "iteration no 86: Loss: 0.7609611866966226, accuracy: 0.5133333333333333\n",
      "iteration no 87: Loss: 0.7608087399716261, accuracy: 0.5133333333333333\n",
      "iteration no 88: Loss: 0.760662116271795, accuracy: 0.5133333333333333\n",
      "iteration no 89: Loss: 0.7605210671776528, accuracy: 0.5133333333333333\n",
      "iteration no 90: Loss: 0.7603853563034114, accuracy: 0.5133333333333333\n",
      "iteration no 91: Loss: 0.7602547586312141, accuracy: 0.5133333333333333\n",
      "iteration no 92: Loss: 0.7601290598872705, accuracy: 0.5133333333333333\n",
      "iteration no 93: Loss: 0.7600080559569132, accuracy: 0.5133333333333333\n",
      "iteration no 94: Loss: 0.7598915523358405, accuracy: 0.5133333333333333\n",
      "iteration no 95: Loss: 0.75977936361503, accuracy: 0.5133333333333333\n",
      "iteration no 96: Loss: 0.7596713129969919, accuracy: 0.5133333333333333\n",
      "iteration no 97: Loss: 0.7595672318412202, accuracy: 0.5133333333333333\n",
      "iteration no 98: Loss: 0.7594669592368568, accuracy: 0.5133333333333333\n",
      "iteration no 99: Loss: 0.7593703416007376, accuracy: 0.5133333333333333\n",
      "iteration no 100: Loss: 0.7592772322991233, accuracy: 0.5133333333333333\n",
      "iteration no 101: Loss: 0.7591874912915467, accuracy: 0.5133333333333333\n",
      "iteration no 102: Loss: 0.7591009847953244, accuracy: 0.5133333333333333\n",
      "iteration no 103: Loss: 0.7590175849693812, accuracy: 0.5133333333333333\n",
      "iteration no 104: Loss: 0.7589371696161432, accuracy: 0.5166666666666667\n",
      "iteration no 105: Loss: 0.7588596219003356, accuracy: 0.5166666666666667\n",
      "iteration no 106: Loss: 0.7587848300836104, accuracy: 0.5166666666666667\n",
      "iteration no 107: Loss: 0.7587126872740044, accuracy: 0.5166666666666667\n",
      "iteration no 108: Loss: 0.7586430911892946, accuracy: 0.5166666666666667\n",
      "iteration no 109: Loss: 0.758575943933388, accuracy: 0.5166666666666667\n",
      "iteration no 110: Loss: 0.7585111517849403, accuracy: 0.5166666666666667\n",
      "iteration no 111: Loss: 0.7584486249974521, accuracy: 0.5166666666666667\n",
      "iteration no 112: Loss: 0.7583882776101452, accuracy: 0.5166666666666667\n",
      "iteration no 113: Loss: 0.7583300272689661, accuracy: 0.5166666666666667\n",
      "iteration no 114: Loss: 0.7582737950571107, accuracy: 0.5166666666666667\n",
      "iteration no 115: Loss: 0.7582195053345009, accuracy: 0.5166666666666667\n",
      "iteration no 116: Loss: 0.7581670855856842, accuracy: 0.5166666666666667\n",
      "iteration no 117: Loss: 0.7581164662756639, accuracy: 0.5166666666666667\n",
      "iteration no 118: Loss: 0.7580675807131908, accuracy: 0.5166666666666667\n",
      "iteration no 119: Loss: 0.7580203649210926, accuracy: 0.5166666666666667\n",
      "iteration no 120: Loss: 0.7579747575132282, accuracy: 0.5166666666666667\n",
      "iteration no 121: Loss: 0.7579306995776941, accuracy: 0.5166666666666667\n",
      "iteration no 122: Loss: 0.7578881345659236, accuracy: 0.5166666666666667\n",
      "iteration no 123: Loss: 0.7578470081873492, accuracy: 0.5166666666666667\n",
      "iteration no 124: Loss: 0.7578072683093151, accuracy: 0.5166666666666667\n",
      "iteration no 125: Loss: 0.7577688648619484, accuracy: 0.5166666666666667\n",
      "iteration no 126: Loss: 0.7577317497477141, accuracy: 0.5166666666666667\n",
      "iteration no 127: Loss: 0.7576958767553967, accuracy: 0.5166666666666667\n",
      "iteration no 128: Loss: 0.7576612014782675, accuracy: 0.5166666666666667\n",
      "iteration no 129: Loss: 0.7576276812362083, accuracy: 0.5133333333333333\n",
      "iteration no 130: Loss: 0.7575952750015812, accuracy: 0.5133333333333333\n",
      "iteration no 131: Loss: 0.7575639433286411, accuracy: 0.5133333333333333\n",
      "iteration no 132: Loss: 0.757533648286304, accuracy: 0.5133333333333333\n",
      "iteration no 133: Loss: 0.7575043533940927, accuracy: 0.5133333333333333\n",
      "iteration no 134: Loss: 0.7574760235610931, accuracy: 0.5133333333333333\n",
      "iteration no 135: Loss: 0.757448625027762, accuracy: 0.5133333333333333\n",
      "iteration no 136: Loss: 0.7574221253104416, accuracy: 0.5133333333333333\n",
      "iteration no 137: Loss: 0.7573964931484374, accuracy: 0.5133333333333333\n",
      "iteration no 138: Loss: 0.7573716984535304, accuracy: 0.5133333333333333\n",
      "iteration no 139: Loss: 0.757347712261796, accuracy: 0.5133333333333333\n",
      "iteration no 140: Loss: 0.7573245066876167, accuracy: 0.5133333333333333\n",
      "iteration no 141: Loss: 0.7573020548797756, accuracy: 0.5133333333333333\n",
      "iteration no 142: Loss: 0.7572803309795236, accuracy: 0.5133333333333333\n",
      "iteration no 143: Loss: 0.7572593100805276, accuracy: 0.5133333333333333\n",
      "iteration no 144: Loss: 0.7572389681906015, accuracy: 0.5133333333333333\n",
      "iteration no 145: Loss: 0.7572192821951339, accuracy: 0.5133333333333333\n",
      "iteration no 146: Loss: 0.7572002298221285, accuracy: 0.5133333333333333\n",
      "iteration no 147: Loss: 0.7571817896087805, accuracy: 0.5133333333333333\n",
      "iteration no 148: Loss: 0.7571639408695097, accuracy: 0.5133333333333333\n",
      "iteration no 149: Loss: 0.7571466636653863, accuracy: 0.5133333333333333\n",
      "iteration no 150: Loss: 0.7571299387748767, accuracy: 0.5133333333333333\n",
      "iteration no 151: Loss: 0.7571137476658495, accuracy: 0.5133333333333333\n",
      "iteration no 152: Loss: 0.7570980724687806, accuracy: 0.5133333333333333\n",
      "iteration no 153: Loss: 0.757082895951102, accuracy: 0.5133333333333333\n",
      "iteration no 154: Loss: 0.7570682014926373, accuracy: 0.5133333333333333\n",
      "iteration no 155: Loss: 0.7570539730620772, accuracy: 0.5133333333333333\n",
      "iteration no 156: Loss: 0.7570401951944435, accuracy: 0.5133333333333333\n",
      "iteration no 157: Loss: 0.7570268529694965, accuracy: 0.5133333333333333\n",
      "iteration no 158: Loss: 0.7570139319910428, accuracy: 0.5133333333333333\n",
      "iteration no 159: Loss: 0.7570014183671028, accuracy: 0.5133333333333333\n",
      "iteration no 160: Loss: 0.7569892986908956, accuracy: 0.5133333333333333\n",
      "iteration no 161: Loss: 0.7569775600226087, accuracy: 0.5133333333333333\n",
      "iteration no 162: Loss: 0.7569661898719127, accuracy: 0.5133333333333333\n",
      "iteration no 163: Loss: 0.75695517618119, accuracy: 0.5133333333333333\n",
      "iteration no 164: Loss: 0.7569445073094455, accuracy: 0.5133333333333333\n",
      "iteration no 165: Loss: 0.7569341720168666, accuracy: 0.5133333333333333\n",
      "iteration no 166: Loss: 0.756924159450007, accuracy: 0.5133333333333333\n",
      "iteration no 167: Loss: 0.7569144591275642, accuracy: 0.5133333333333333\n",
      "iteration no 168: Loss: 0.7569050609267247, accuracy: 0.5133333333333333\n",
      "iteration no 169: Loss: 0.756895955070055, accuracy: 0.5133333333333333\n",
      "iteration no 170: Loss: 0.7568871321129097, accuracy: 0.5133333333333333\n",
      "iteration no 171: Loss: 0.7568785829313381, accuracy: 0.5133333333333333\n",
      "iteration no 172: Loss: 0.7568702987104683, accuracy: 0.5133333333333333\n",
      "iteration no 173: Loss: 0.7568622709333427, accuracy: 0.5133333333333333\n",
      "iteration no 174: Loss: 0.7568544913701932, accuracy: 0.5133333333333333\n",
      "iteration no 175: Loss: 0.7568469520681315, accuracy: 0.5133333333333333\n",
      "iteration no 176: Loss: 0.7568396453412396, accuracy: 0.5133333333333333\n",
      "iteration no 177: Loss: 0.7568325637610428, accuracy: 0.5133333333333333\n",
      "iteration no 178: Loss: 0.7568257001473494, accuracy: 0.5133333333333333\n",
      "iteration no 179: Loss: 0.7568190475594414, accuracy: 0.5133333333333333\n",
      "iteration no 180: Loss: 0.7568125992876021, accuracy: 0.5133333333333333\n",
      "iteration no 181: Loss: 0.7568063488449651, accuracy: 0.5133333333333333\n",
      "iteration no 182: Loss: 0.7568002899596745, accuracy: 0.5133333333333333\n",
      "iteration no 183: Loss: 0.7567944165673391, accuracy: 0.5133333333333333\n",
      "iteration no 184: Loss: 0.756788722803773, accuracy: 0.5133333333333333\n",
      "iteration no 185: Loss: 0.7567832029980073, accuracy: 0.5133333333333333\n",
      "iteration no 186: Loss: 0.7567778516655644, accuracy: 0.5133333333333333\n",
      "iteration no 187: Loss: 0.7567726635019821, accuracy: 0.5133333333333333\n",
      "iteration no 188: Loss: 0.75676763337658, accuracy: 0.5133333333333333\n",
      "iteration no 189: Loss: 0.7567627563264544, accuracy: 0.5133333333333333\n",
      "iteration no 190: Loss: 0.7567580275506984, accuracy: 0.5133333333333333\n",
      "iteration no 191: Loss: 0.7567534424048321, accuracy: 0.5133333333333333\n",
      "iteration no 192: Loss: 0.7567489963954381, accuracy: 0.5133333333333333\n",
      "iteration no 193: Loss: 0.7567446851749938, accuracy: 0.5133333333333333\n",
      "iteration no 194: Loss: 0.7567405045368927, accuracy: 0.5133333333333333\n",
      "iteration no 195: Loss: 0.7567364504106441, accuracy: 0.5133333333333333\n",
      "iteration no 196: Loss: 0.756732518857251, accuracy: 0.5133333333333333\n",
      "iteration no 197: Loss: 0.7567287060647514, accuracy: 0.5133333333333333\n",
      "iteration no 198: Loss: 0.7567250083439231, accuracy: 0.5133333333333333\n",
      "iteration no 199: Loss: 0.7567214221241408, accuracy: 0.5133333333333333\n",
      "iteration no 200: Loss: 0.7567179439493836, accuracy: 0.5133333333333333\n",
      "iteration no 1: Loss: 1.0939813475980076, accuracy: 0.4866666666666667\n",
      "iteration no 2: Loss: 1.063990339980778, accuracy: 0.49666666666666665\n",
      "iteration no 3: Loss: 1.0372862146039177, accuracy: 0.4866666666666667\n",
      "iteration no 4: Loss: 1.0134713905732602, accuracy: 0.5033333333333333\n",
      "iteration no 5: Loss: 0.9921942168968326, accuracy: 0.51\n",
      "iteration no 6: Loss: 0.9731453995507867, accuracy: 0.51\n",
      "iteration no 7: Loss: 0.956054181078246, accuracy: 0.5066666666666667\n",
      "iteration no 8: Loss: 0.9406843601986276, accuracy: 0.5066666666666667\n",
      "iteration no 9: Loss: 0.9268303408171081, accuracy: 0.5033333333333333\n",
      "iteration no 10: Loss: 0.9143133731143271, accuracy: 0.51\n",
      "iteration no 11: Loss: 0.9029780911132047, accuracy: 0.51\n",
      "iteration no 12: Loss: 0.8926893976519761, accuracy: 0.51\n",
      "iteration no 13: Loss: 0.8833297088448339, accuracy: 0.51\n",
      "iteration no 14: Loss: 0.8747965455106682, accuracy: 0.51\n",
      "iteration no 15: Loss: 0.8670004453656089, accuracy: 0.51\n",
      "iteration no 16: Loss: 0.8598631635247529, accuracy: 0.51\n",
      "iteration no 17: Loss: 0.8533161272737058, accuracy: 0.51\n",
      "iteration no 18: Loss: 0.8472991121955198, accuracy: 0.51\n",
      "iteration no 19: Loss: 0.8417591092856453, accuracy: 0.51\n",
      "iteration no 20: Loss: 0.8366493558527343, accuracy: 0.51\n",
      "iteration no 21: Loss: 0.8319285063091825, accuracy: 0.51\n",
      "iteration no 22: Loss: 0.8275599221358616, accuracy: 0.51\n",
      "iteration no 23: Loss: 0.823511063225544, accuracy: 0.51\n",
      "iteration no 24: Loss: 0.8197529654133656, accuracy: 0.51\n",
      "iteration no 25: Loss: 0.8162597912802163, accuracy: 0.51\n",
      "iteration no 26: Loss: 0.8130084432812, accuracy: 0.51\n",
      "iteration no 27: Loss: 0.809978229933472, accuracy: 0.51\n",
      "iteration no 28: Loss: 0.8071505772278736, accuracy: 0.51\n",
      "iteration no 29: Loss: 0.8045087786393876, accuracy: 0.51\n",
      "iteration no 30: Loss: 0.8020377781332837, accuracy: 0.5133333333333333\n",
      "iteration no 31: Loss: 0.7997239814248, accuracy: 0.5133333333333333\n",
      "iteration no 32: Loss: 0.7975550914749474, accuracy: 0.5133333333333333\n",
      "iteration no 33: Loss: 0.7955199648148638, accuracy: 0.5133333333333333\n",
      "iteration no 34: Loss: 0.7936084858043738, accuracy: 0.5133333333333333\n",
      "iteration no 35: Loss: 0.791811456362581, accuracy: 0.5133333333333333\n",
      "iteration no 36: Loss: 0.7901204990725021, accuracy: 0.5133333333333333\n",
      "iteration no 37: Loss: 0.7885279718690053, accuracy: 0.5133333333333333\n",
      "iteration no 38: Loss: 0.7870268927788263, accuracy: 0.5133333333333333\n",
      "iteration no 39: Loss: 0.7856108734009628, accuracy: 0.5133333333333333\n",
      "iteration no 40: Loss: 0.7842740600017062, accuracy: 0.5133333333333333\n",
      "iteration no 41: Loss: 0.7830110812563564, accuracy: 0.5133333333333333\n",
      "iteration no 42: Loss: 0.7818170018037724, accuracy: 0.5133333333333333\n",
      "iteration no 43: Loss: 0.7806872808940851, accuracy: 0.5133333333333333\n",
      "iteration no 44: Loss: 0.779617735507272, accuracy: 0.5133333333333333\n",
      "iteration no 45: Loss: 0.7786045074034869, accuracy: 0.5133333333333333\n",
      "iteration no 46: Loss: 0.7776440336372502, accuracy: 0.5133333333333333\n",
      "iteration no 47: Loss: 0.7767330201286596, accuracy: 0.5133333333333333\n",
      "iteration no 48: Loss: 0.7758684179372309, accuracy: 0.5133333333333333\n",
      "iteration no 49: Loss: 0.7750474019291168, accuracy: 0.5133333333333333\n",
      "iteration no 50: Loss: 0.7742673515673636, accuracy: 0.5133333333333333\n",
      "iteration no 51: Loss: 0.7735258335884609, accuracy: 0.5133333333333333\n",
      "iteration no 52: Loss: 0.7728205863575313, accuracy: 0.5133333333333333\n",
      "iteration no 53: Loss: 0.7721495057196851, accuracy: 0.5133333333333333\n",
      "iteration no 54: Loss: 0.7715106321869498, accuracy: 0.5133333333333333\n",
      "iteration no 55: Loss: 0.7709021393192068, accuracy: 0.5133333333333333\n",
      "iteration no 56: Loss: 0.7703223231741353, accuracy: 0.5133333333333333\n",
      "iteration no 57: Loss: 0.769769592715623, accuracy: 0.5133333333333333\n",
      "iteration no 58: Loss: 0.7692424610827412, accuracy: 0.5166666666666667\n",
      "iteration no 59: Loss: 0.7687395376324406, accuracy: 0.5166666666666667\n",
      "iteration no 60: Loss: 0.7682595206788236, accuracy: 0.5166666666666667\n",
      "iteration no 61: Loss: 0.7678011908603628, accuracy: 0.5166666666666667\n",
      "iteration no 62: Loss: 0.7673634050739278, accuracy: 0.5166666666666667\n",
      "iteration no 63: Loss: 0.7669450909210733, accuracy: 0.5166666666666667\n",
      "iteration no 64: Loss: 0.7665452416178652, accuracy: 0.5166666666666667\n",
      "iteration no 65: Loss: 0.7661629113246556, accuracy: 0.5166666666666667\n",
      "iteration no 66: Loss: 0.7657972108567691, accuracy: 0.5166666666666667\n",
      "iteration no 67: Loss: 0.7654473037410832, accuracy: 0.5166666666666667\n",
      "iteration no 68: Loss: 0.7651124025870659, accuracy: 0.5166666666666667\n",
      "iteration no 69: Loss: 0.7647917657440011, accuracy: 0.5166666666666667\n",
      "iteration no 70: Loss: 0.7644846942189526, accuracy: 0.5166666666666667\n",
      "iteration no 71: Loss: 0.7641905288325398, accuracy: 0.5166666666666667\n",
      "iteration no 72: Loss: 0.7639086475918282, accuracy: 0.5166666666666667\n",
      "iteration no 73: Loss: 0.7636384632616485, accuracy: 0.5166666666666667\n",
      "iteration no 74: Loss: 0.763379421117437, accuracy: 0.5166666666666667\n",
      "iteration no 75: Loss: 0.7631309968642993, accuracy: 0.5166666666666667\n",
      "iteration no 76: Loss: 0.7628926947084304, accuracy: 0.5166666666666667\n",
      "iteration no 77: Loss: 0.7626640455683061, accuracy: 0.5166666666666667\n",
      "iteration no 78: Loss: 0.7624446054142259, accuracy: 0.5166666666666667\n",
      "iteration no 79: Loss: 0.7622339537258173, accuracy: 0.5166666666666667\n",
      "iteration no 80: Loss: 0.7620316920580513, accuracy: 0.5133333333333333\n",
      "iteration no 81: Loss: 0.7618374427071637, accuracy: 0.5133333333333333\n",
      "iteration no 82: Loss: 0.761650847468626, accuracy: 0.5133333333333333\n",
      "iteration no 83: Loss: 0.7614715664800126, accuracy: 0.5133333333333333\n",
      "iteration no 84: Loss: 0.761299277142216, accuracy: 0.5133333333333333\n",
      "iteration no 85: Loss: 0.7611336731130314, accuracy: 0.5133333333333333\n",
      "iteration no 86: Loss: 0.7609744633676369, accuracy: 0.5133333333333333\n",
      "iteration no 87: Loss: 0.7608213713209593, accuracy: 0.5133333333333333\n",
      "iteration no 88: Loss: 0.7606741340073294, accuracy: 0.5133333333333333\n",
      "iteration no 89: Loss: 0.7605325013132131, accuracy: 0.5133333333333333\n",
      "iteration no 90: Loss: 0.7603962352591492, accuracy: 0.5133333333333333\n",
      "iteration no 91: Loss: 0.7602651093273406, accuracy: 0.5133333333333333\n",
      "iteration no 92: Loss: 0.7601389078316259, accuracy: 0.5133333333333333\n",
      "iteration no 93: Loss: 0.7600174253268253, accuracy: 0.5133333333333333\n",
      "iteration no 94: Loss: 0.7599004660546909, accuracy: 0.5133333333333333\n",
      "iteration no 95: Loss: 0.7597878434239055, accuracy: 0.5133333333333333\n",
      "iteration no 96: Loss: 0.759679379521775, accuracy: 0.5133333333333333\n",
      "iteration no 97: Loss: 0.7595749046554425, accuracy: 0.5133333333333333\n",
      "iteration no 98: Loss: 0.7594742569206125, accuracy: 0.5133333333333333\n",
      "iteration no 99: Loss: 0.759377281795933, accuracy: 0.5133333333333333\n",
      "iteration no 100: Loss: 0.7592838317613154, accuracy: 0.5133333333333333\n",
      "iteration no 101: Loss: 0.759193765938607, accuracy: 0.5133333333333333\n",
      "iteration no 102: Loss: 0.7591069497531415, accuracy: 0.5166666666666667\n",
      "iteration no 103: Loss: 0.7590232546148062, accuracy: 0.5166666666666667\n",
      "iteration no 104: Loss: 0.7589425576173585, accuracy: 0.5166666666666667\n",
      "iteration no 105: Loss: 0.7588647412548236, accuracy: 0.5166666666666667\n",
      "iteration no 106: Loss: 0.7587896931538772, accuracy: 0.5166666666666667\n",
      "iteration no 107: Loss: 0.758717305821207, accuracy: 0.5166666666666667\n",
      "iteration no 108: Loss: 0.758647476404908, accuracy: 0.5166666666666667\n",
      "iteration no 109: Loss: 0.7585801064690398, accuracy: 0.5166666666666667\n",
      "iteration no 110: Loss: 0.7585151017805284, accuracy: 0.5166666666666667\n",
      "iteration no 111: Loss: 0.7584523721076557, accuracy: 0.5166666666666667\n",
      "iteration no 112: Loss: 0.7583918310294325, accuracy: 0.5166666666666667\n",
      "iteration no 113: Loss: 0.7583333957551899, accuracy: 0.5166666666666667\n",
      "iteration no 114: Loss: 0.7582769869537814, accuracy: 0.5166666666666667\n",
      "iteration no 115: Loss: 0.7582225285918199, accuracy: 0.5166666666666667\n",
      "iteration no 116: Loss: 0.7581699477804125, accuracy: 0.5166666666666667\n",
      "iteration no 117: Loss: 0.758119174629895, accuracy: 0.5166666666666667\n",
      "iteration no 118: Loss: 0.7580701421121003, accuracy: 0.5166666666666667\n",
      "iteration no 119: Loss: 0.7580227859297188, accuracy: 0.5166666666666667\n",
      "iteration no 120: Loss: 0.7579770443923484, accuracy: 0.5166666666666667\n",
      "iteration no 121: Loss: 0.757932858298843, accuracy: 0.5166666666666667\n",
      "iteration no 122: Loss: 0.7578901708256132, accuracy: 0.5166666666666667\n",
      "iteration no 123: Loss: 0.757848927420528, accuracy: 0.5166666666666667\n",
      "iteration no 124: Loss: 0.7578090757021186, accuracy: 0.5166666666666667\n",
      "iteration no 125: Loss: 0.7577705653637772, accuracy: 0.5166666666666667\n",
      "iteration no 126: Loss: 0.7577333480826792, accuracy: 0.5166666666666667\n",
      "iteration no 127: Loss: 0.7576973774331697, accuracy: 0.5166666666666667\n",
      "iteration no 128: Loss: 0.7576626088043674, accuracy: 0.5166666666666667\n",
      "iteration no 129: Loss: 0.7576289993217594, accuracy: 0.5166666666666667\n",
      "iteration no 130: Loss: 0.7575965077725687, accuracy: 0.5166666666666667\n",
      "iteration no 131: Loss: 0.757565094534695, accuracy: 0.5166666666666667\n",
      "iteration no 132: Loss: 0.7575347215090363, accuracy: 0.5166666666666667\n",
      "iteration no 133: Loss: 0.7575053520550121, accuracy: 0.5166666666666667\n",
      "iteration no 134: Loss: 0.7574769509291218, accuracy: 0.5133333333333333\n",
      "iteration no 135: Loss: 0.7574494842263757, accuracy: 0.5133333333333333\n",
      "iteration no 136: Loss: 0.7574229193244523, accuracy: 0.5133333333333333\n",
      "iteration no 137: Loss: 0.7573972248304401, accuracy: 0.5133333333333333\n",
      "iteration no 138: Loss: 0.757372370530029, accuracy: 0.5133333333333333\n",
      "iteration no 139: Loss: 0.7573483273390296, accuracy: 0.5133333333333333\n",
      "iteration no 140: Loss: 0.7573250672570985, accuracy: 0.5133333333333333\n",
      "iteration no 141: Loss: 0.7573025633235598, accuracy: 0.5133333333333333\n",
      "iteration no 142: Loss: 0.7572807895752186, accuracy: 0.5133333333333333\n",
      "iteration no 143: Loss: 0.757259721006064, accuracy: 0.5133333333333333\n",
      "iteration no 144: Loss: 0.7572393335287703, accuracy: 0.5133333333333333\n",
      "iteration no 145: Loss: 0.7572196039379058, accuracy: 0.5133333333333333\n",
      "iteration no 146: Loss: 0.7572005098747675, accuracy: 0.5133333333333333\n",
      "iteration no 147: Loss: 0.7571820297937586, accuracy: 0.5133333333333333\n",
      "iteration no 148: Loss: 0.7571641429302371, accuracy: 0.5133333333333333\n",
      "iteration no 149: Loss: 0.7571468292697634, accuracy: 0.5133333333333333\n",
      "iteration no 150: Loss: 0.7571300695186789, accuracy: 0.5133333333333333\n",
      "iteration no 151: Loss: 0.7571138450759517, accuracy: 0.5133333333333333\n",
      "iteration no 152: Loss: 0.7570981380062313, accuracy: 0.5133333333333333\n",
      "iteration no 153: Loss: 0.7570829310140518, accuracy: 0.5133333333333333\n",
      "iteration no 154: Loss: 0.7570682074191327, accuracy: 0.5133333333333333\n",
      "iteration no 155: Loss: 0.7570539511327229, accuracy: 0.5133333333333333\n",
      "iteration no 156: Loss: 0.7570401466349417, accuracy: 0.5133333333333333\n",
      "iteration no 157: Loss: 0.7570267789530684, accuracy: 0.5133333333333333\n",
      "iteration no 158: Loss: 0.7570138336407398, accuracy: 0.5133333333333333\n",
      "iteration no 159: Loss: 0.7570012967580092, accuracy: 0.5133333333333333\n",
      "iteration no 160: Loss: 0.7569891548522328, accuracy: 0.5133333333333333\n",
      "iteration no 161: Loss: 0.7569773949397407, accuracy: 0.5133333333333333\n",
      "iteration no 162: Loss: 0.7569660044882622, accuracy: 0.5133333333333333\n",
      "iteration no 163: Loss: 0.7569549714000668, accuracy: 0.5133333333333333\n",
      "iteration no 164: Loss: 0.7569442839957908, accuracy: 0.5133333333333333\n",
      "iteration no 165: Loss: 0.7569339309989204, accuracy: 0.5133333333333333\n",
      "iteration no 166: Loss: 0.7569239015208986, accuracy: 0.5133333333333333\n",
      "iteration no 167: Loss: 0.756914185046832, accuracy: 0.5133333333333333\n",
      "iteration no 168: Loss: 0.7569047714217678, accuracy: 0.5133333333333333\n",
      "iteration no 169: Loss: 0.7568956508375193, accuracy: 0.5133333333333333\n",
      "iteration no 170: Loss: 0.7568868138200121, accuracy: 0.5133333333333333\n",
      "iteration no 171: Loss: 0.756878251217133, accuracy: 0.5133333333333333\n",
      "iteration no 172: Loss: 0.7568699541870548, accuracy: 0.5133333333333333\n",
      "iteration no 173: Loss: 0.7568619141870221, accuracy: 0.5133333333333333\n",
      "iteration no 174: Loss: 0.7568541229625719, accuracy: 0.5133333333333333\n",
      "iteration no 175: Loss: 0.7568465725371774, accuracy: 0.5133333333333333\n",
      "iteration no 176: Loss: 0.7568392552022906, accuracy: 0.5133333333333333\n",
      "iteration no 177: Loss: 0.7568321635077717, accuracy: 0.5133333333333333\n",
      "iteration no 178: Loss: 0.756825290252686, accuracy: 0.5133333333333333\n",
      "iteration no 179: Loss: 0.7568186284764546, accuracy: 0.5133333333333333\n",
      "iteration no 180: Loss: 0.7568121714503426, accuracy: 0.5133333333333333\n",
      "iteration no 181: Loss: 0.7568059126692726, accuracy: 0.5133333333333333\n",
      "iteration no 182: Loss: 0.7567998458439483, accuracy: 0.5133333333333333\n",
      "iteration no 183: Loss: 0.756793964893277, accuracy: 0.5133333333333333\n",
      "iteration no 184: Loss: 0.7567882639370758, accuracy: 0.5133333333333333\n",
      "iteration no 185: Loss: 0.7567827372890551, accuracy: 0.5133333333333333\n",
      "iteration no 186: Loss: 0.7567773794500617, accuracy: 0.5133333333333333\n",
      "iteration no 187: Loss: 0.7567721851015768, accuracy: 0.5133333333333333\n",
      "iteration no 188: Loss: 0.7567671490994538, accuracy: 0.5133333333333333\n",
      "iteration no 189: Loss: 0.7567622664678898, accuracy: 0.5133333333333333\n",
      "iteration no 190: Loss: 0.7567575323936198, accuracy: 0.5133333333333333\n",
      "iteration no 191: Loss: 0.7567529422203243, accuracy: 0.5133333333333333\n",
      "iteration no 192: Loss: 0.7567484914432424, accuracy: 0.5133333333333333\n",
      "iteration no 193: Loss: 0.756744175703984, accuracy: 0.5133333333333333\n",
      "iteration no 194: Loss: 0.756739990785528, accuracy: 0.5133333333333333\n",
      "iteration no 195: Loss: 0.7567359326074065, accuracy: 0.5133333333333333\n",
      "iteration no 196: Loss: 0.7567319972210601, accuracy: 0.5133333333333333\n",
      "iteration no 197: Loss: 0.7567281808053649, accuracy: 0.5133333333333333\n",
      "iteration no 198: Loss: 0.7567244796623182, accuracy: 0.5133333333333333\n",
      "iteration no 199: Loss: 0.7567208902128804, accuracy: 0.5133333333333333\n",
      "iteration no 200: Loss: 0.7567174089929664, accuracy: 0.5133333333333333\n",
      "iteration no 1: Loss: 1.1114829130046504, accuracy: 0.38666666666666666\n",
      "iteration no 2: Loss: 1.079449388912237, accuracy: 0.51\n",
      "iteration no 3: Loss: 1.0509687600093927, accuracy: 0.5066666666666667\n",
      "iteration no 4: Loss: 1.0255972962045332, accuracy: 0.5166666666666667\n",
      "iteration no 5: Loss: 1.0029554594115502, accuracy: 0.5066666666666667\n",
      "iteration no 6: Loss: 0.9827125219348671, accuracy: 0.5033333333333333\n",
      "iteration no 7: Loss: 0.9645786100321779, accuracy: 0.5066666666666667\n",
      "iteration no 8: Loss: 0.9482996482175121, accuracy: 0.5066666666666667\n",
      "iteration no 9: Loss: 0.9336533747714449, accuracy: 0.5066666666666667\n",
      "iteration no 10: Loss: 0.92044577993777, accuracy: 0.5\n",
      "iteration no 11: Loss: 0.9085077934539965, accuracy: 0.5066666666666667\n",
      "iteration no 12: Loss: 0.897692211348371, accuracy: 0.5066666666666667\n",
      "iteration no 13: Loss: 0.8878708888301753, accuracy: 0.5066666666666667\n",
      "iteration no 14: Loss: 0.8789322192916937, accuracy: 0.5133333333333333\n",
      "iteration no 15: Loss: 0.8707789028089946, accuracy: 0.5133333333333333\n",
      "iteration no 16: Loss: 0.8633259930354242, accuracy: 0.5133333333333333\n",
      "iteration no 17: Loss: 0.8564992019101956, accuracy: 0.5133333333333333\n",
      "iteration no 18: Loss: 0.8502334367571566, accuracy: 0.51\n",
      "iteration no 19: Loss: 0.844471542868078, accuracy: 0.5133333333333333\n",
      "iteration no 20: Loss: 0.839163225327421, accuracy: 0.5133333333333333\n",
      "iteration no 21: Loss: 0.8342641257055279, accuracy: 0.5133333333333333\n",
      "iteration no 22: Loss: 0.8297350316865368, accuracy: 0.5133333333333333\n",
      "iteration no 23: Loss: 0.8255412003047569, accuracy: 0.5166666666666667\n",
      "iteration no 24: Loss: 0.8216517780045648, accuracy: 0.5166666666666667\n",
      "iteration no 25: Loss: 0.8180393030901224, accuracy: 0.5133333333333333\n",
      "iteration no 26: Loss: 0.8146792782372934, accuracy: 0.5166666666666667\n",
      "iteration no 27: Loss: 0.8115498025869241, accuracy: 0.5133333333333333\n",
      "iteration no 28: Loss: 0.8086312545349632, accuracy: 0.5133333333333333\n",
      "iteration no 29: Loss: 0.805906017701104, accuracy: 0.5133333333333333\n",
      "iteration no 30: Loss: 0.8033582437190184, accuracy: 0.5166666666666667\n",
      "iteration no 31: Loss: 0.8009736464739758, accuracy: 0.5166666666666667\n",
      "iteration no 32: Loss: 0.7987393232427047, accuracy: 0.5166666666666667\n",
      "iteration no 33: Loss: 0.7966435988884862, accuracy: 0.5166666666666667\n",
      "iteration no 34: Loss: 0.7946758898518177, accuracy: 0.5166666666666667\n",
      "iteration no 35: Loss: 0.792826585170967, accuracy: 0.5166666666666667\n",
      "iteration no 36: Loss: 0.7910869421823304, accuracy: 0.5166666666666667\n",
      "iteration no 37: Loss: 0.7894489949003368, accuracy: 0.5166666666666667\n",
      "iteration no 38: Loss: 0.7879054733714336, accuracy: 0.5166666666666667\n",
      "iteration no 39: Loss: 0.7864497325453691, accuracy: 0.5166666666666667\n",
      "iteration no 40: Loss: 0.7850756894170733, accuracy: 0.5166666666666667\n",
      "iteration no 41: Loss: 0.7837777673701807, accuracy: 0.5166666666666667\n",
      "iteration no 42: Loss: 0.7825508468038783, accuracy: 0.5166666666666667\n",
      "iteration no 43: Loss: 0.7813902212526193, accuracy: 0.5166666666666667\n",
      "iteration no 44: Loss: 0.7802915583169902, accuracy: 0.5166666666666667\n",
      "iteration no 45: Loss: 0.7792508648166623, accuracy: 0.5166666666666667\n",
      "iteration no 46: Loss: 0.7782644556554162, accuracy: 0.5166666666666667\n",
      "iteration no 47: Loss: 0.7773289259558558, accuracy: 0.5166666666666667\n",
      "iteration no 48: Loss: 0.7764411260793385, accuracy: 0.5166666666666667\n",
      "iteration no 49: Loss: 0.7755981391963735, accuracy: 0.5166666666666667\n",
      "iteration no 50: Loss: 0.7747972611154856, accuracy: 0.5133333333333333\n",
      "iteration no 51: Loss: 0.7740359821153713, accuracy: 0.5133333333333333\n",
      "iteration no 52: Loss: 0.7733119705569627, accuracy: 0.5133333333333333\n",
      "iteration no 53: Loss: 0.7726230580794974, accuracy: 0.5133333333333333\n",
      "iteration no 54: Loss: 0.7719672262084982, accuracy: 0.5133333333333333\n",
      "iteration no 55: Loss: 0.7713425942242237, accuracy: 0.5166666666666667\n",
      "iteration no 56: Loss: 0.7707474081571118, accuracy: 0.5166666666666667\n",
      "iteration no 57: Loss: 0.7701800307923693, accuracy: 0.5166666666666667\n",
      "iteration no 58: Loss: 0.7696389325795008, accuracy: 0.5166666666666667\n",
      "iteration no 59: Loss: 0.7691226833544893, accuracy: 0.5166666666666667\n",
      "iteration no 60: Loss: 0.7686299447927625, accuracy: 0.5166666666666667\n",
      "iteration no 61: Loss: 0.7681594635202265, accuracy: 0.5166666666666667\n",
      "iteration no 62: Loss: 0.7677100648176649, accuracy: 0.5166666666666667\n",
      "iteration no 63: Loss: 0.7672806468608685, accuracy: 0.5166666666666667\n",
      "iteration no 64: Loss: 0.7668701754450628, accuracy: 0.5166666666666667\n",
      "iteration no 65: Loss: 0.7664776791476926, accuracy: 0.5166666666666667\n",
      "iteration no 66: Loss: 0.7661022448884537, accuracy: 0.5166666666666667\n",
      "iteration no 67: Loss: 0.7657430138497536, accuracy: 0.5166666666666667\n",
      "iteration no 68: Loss: 0.7653991777245679, accuracy: 0.5166666666666667\n",
      "iteration no 69: Loss: 0.7650699752620268, accuracy: 0.5166666666666667\n",
      "iteration no 70: Loss: 0.7647546890840615, accuracy: 0.5166666666666667\n",
      "iteration no 71: Loss: 0.7644526427490826, accuracy: 0.5166666666666667\n",
      "iteration no 72: Loss: 0.764163198041049, accuracy: 0.5166666666666667\n",
      "iteration no 73: Loss: 0.7638857524643824, accuracy: 0.5166666666666667\n",
      "iteration no 74: Loss: 0.763619736927076, accuracy: 0.5166666666666667\n",
      "iteration no 75: Loss: 0.7633646135960278, accuracy: 0.5166666666666667\n",
      "iteration no 76: Loss: 0.7631198739101372, accuracy: 0.5166666666666667\n",
      "iteration no 77: Loss: 0.7628850367380607, accuracy: 0.5166666666666667\n",
      "iteration no 78: Loss: 0.7626596466687239, accuracy: 0.5166666666666667\n",
      "iteration no 79: Loss: 0.7624432724237926, accuracy: 0.5166666666666667\n",
      "iteration no 80: Loss: 0.7622355053822699, accuracy: 0.5166666666666667\n",
      "iteration no 81: Loss: 0.762035958208279, accuracy: 0.5166666666666667\n",
      "iteration no 82: Loss: 0.761844263573881, accuracy: 0.5166666666666667\n",
      "iteration no 83: Loss: 0.7616600729694962, accuracy: 0.5166666666666667\n",
      "iteration no 84: Loss: 0.7614830555951445, accuracy: 0.5133333333333333\n",
      "iteration no 85: Loss: 0.7613128973263054, accuracy: 0.5133333333333333\n",
      "iteration no 86: Loss: 0.7611492997487282, accuracy: 0.5133333333333333\n",
      "iteration no 87: Loss: 0.7609919792570069, accuracy: 0.5133333333333333\n",
      "iteration no 88: Loss: 0.7608406662121608, accuracy: 0.5133333333333333\n",
      "iteration no 89: Loss: 0.7606951041538653, accuracy: 0.5133333333333333\n",
      "iteration no 90: Loss: 0.7605550490633337, accuracy: 0.5133333333333333\n",
      "iteration no 91: Loss: 0.7604202686731741, accuracy: 0.5133333333333333\n",
      "iteration no 92: Loss: 0.760290541820846, accuracy: 0.5133333333333333\n",
      "iteration no 93: Loss: 0.7601656578426115, accuracy: 0.5133333333333333\n",
      "iteration no 94: Loss: 0.7600454160051195, accuracy: 0.5133333333333333\n",
      "iteration no 95: Loss: 0.7599296249719906, accuracy: 0.5133333333333333\n",
      "iteration no 96: Loss: 0.7598181023029731, accuracy: 0.5133333333333333\n",
      "iteration no 97: Loss: 0.759710673983428, accuracy: 0.5133333333333333\n",
      "iteration no 98: Loss: 0.7596071739820768, accuracy: 0.5133333333333333\n",
      "iteration no 99: Loss: 0.7595074438350958, accuracy: 0.5133333333333333\n",
      "iteration no 100: Loss: 0.7594113322547951, accuracy: 0.5133333333333333\n",
      "iteration no 101: Loss: 0.7593186947612391, accuracy: 0.5133333333333333\n",
      "iteration no 102: Loss: 0.7592293933353023, accuracy: 0.5166666666666667\n",
      "iteration no 103: Loss: 0.7591432960917515, accuracy: 0.5166666666666667\n",
      "iteration no 104: Loss: 0.7590602769710559, accuracy: 0.5166666666666667\n",
      "iteration no 105: Loss: 0.7589802154487186, accuracy: 0.5166666666666667\n",
      "iteration no 106: Loss: 0.7589029962610099, accuracy: 0.5166666666666667\n",
      "iteration no 107: Loss: 0.7588285091460599, accuracy: 0.5166666666666667\n",
      "iteration no 108: Loss: 0.7587566485993475, accuracy: 0.5166666666666667\n",
      "iteration no 109: Loss: 0.7586873136426813, accuracy: 0.5166666666666667\n",
      "iteration no 110: Loss: 0.7586204076058424, accuracy: 0.5166666666666667\n",
      "iteration no 111: Loss: 0.7585558379201047, accuracy: 0.5166666666666667\n",
      "iteration no 112: Loss: 0.7584935159229105, accuracy: 0.5166666666666667\n",
      "iteration no 113: Loss: 0.7584333566730228, accuracy: 0.5166666666666667\n",
      "iteration no 114: Loss: 0.7583752787755278, accuracy: 0.5166666666666667\n",
      "iteration no 115: Loss: 0.7583192042160933, accuracy: 0.5166666666666667\n",
      "iteration no 116: Loss: 0.7582650582039399, accuracy: 0.5166666666666667\n",
      "iteration no 117: Loss: 0.7582127690230068, accuracy: 0.5166666666666667\n",
      "iteration no 118: Loss: 0.7581622678908384, accuracy: 0.5166666666666667\n",
      "iteration no 119: Loss: 0.758113488824736, accuracy: 0.5166666666666667\n",
      "iteration no 120: Loss: 0.758066368514763, accuracy: 0.5166666666666667\n",
      "iteration no 121: Loss: 0.7580208462032071, accuracy: 0.5166666666666667\n",
      "iteration no 122: Loss: 0.7579768635701312, accuracy: 0.5166666666666667\n",
      "iteration no 123: Loss: 0.7579343646246725, accuracy: 0.5166666666666667\n",
      "iteration no 124: Loss: 0.7578932956017644, accuracy: 0.5166666666666667\n",
      "iteration no 125: Loss: 0.7578536048639785, accuracy: 0.5166666666666667\n",
      "iteration no 126: Loss: 0.7578152428082056, accuracy: 0.5166666666666667\n",
      "iteration no 127: Loss: 0.7577781617769069, accuracy: 0.5166666666666667\n",
      "iteration no 128: Loss: 0.7577423159736869, accuracy: 0.5166666666666667\n",
      "iteration no 129: Loss: 0.7577076613829515, accuracy: 0.5133333333333333\n",
      "iteration no 130: Loss: 0.7576741556934335, accuracy: 0.5133333333333333\n",
      "iteration no 131: Loss: 0.7576417582253748, accuracy: 0.5133333333333333\n",
      "iteration no 132: Loss: 0.7576104298611729, accuracy: 0.5133333333333333\n",
      "iteration no 133: Loss: 0.7575801329793055, accuracy: 0.5133333333333333\n",
      "iteration no 134: Loss: 0.7575508313913649, accuracy: 0.5133333333333333\n",
      "iteration no 135: Loss: 0.7575224902820332, accuracy: 0.5133333333333333\n",
      "iteration no 136: Loss: 0.7574950761518507, accuracy: 0.5133333333333333\n",
      "iteration no 137: Loss: 0.7574685567626303, accuracy: 0.5133333333333333\n",
      "iteration no 138: Loss: 0.7574429010853823, accuracy: 0.5133333333333333\n",
      "iteration no 139: Loss: 0.757418079250622, accuracy: 0.5133333333333333\n",
      "iteration no 140: Loss: 0.7573940625009368, accuracy: 0.5133333333333333\n",
      "iteration no 141: Loss: 0.7573708231457025, accuracy: 0.5133333333333333\n",
      "iteration no 142: Loss: 0.7573483345178369, accuracy: 0.5133333333333333\n",
      "iteration no 143: Loss: 0.75732657093249, accuracy: 0.5133333333333333\n",
      "iteration no 144: Loss: 0.7573055076475771, accuracy: 0.5133333333333333\n",
      "iteration no 145: Loss: 0.7572851208260603, accuracy: 0.5133333333333333\n",
      "iteration no 146: Loss: 0.7572653874998929, accuracy: 0.5133333333333333\n",
      "iteration no 147: Loss: 0.75724628553555, accuracy: 0.5133333333333333\n",
      "iteration no 148: Loss: 0.7572277936010604, accuracy: 0.5133333333333333\n",
      "iteration no 149: Loss: 0.7572098911344743, accuracy: 0.5133333333333333\n",
      "iteration no 150: Loss: 0.7571925583136954, accuracy: 0.5133333333333333\n",
      "iteration no 151: Loss: 0.7571757760276074, accuracy: 0.5133333333333333\n",
      "iteration no 152: Loss: 0.7571595258484441, accuracy: 0.5133333333333333\n",
      "iteration no 153: Loss: 0.7571437900053329, accuracy: 0.5133333333333333\n",
      "iteration no 154: Loss: 0.7571285513589622, accuracy: 0.5133333333333333\n",
      "iteration no 155: Loss: 0.7571137933773212, accuracy: 0.5133333333333333\n",
      "iteration no 156: Loss: 0.7570995001124581, accuracy: 0.5133333333333333\n",
      "iteration no 157: Loss: 0.7570856561782121, accuracy: 0.5133333333333333\n",
      "iteration no 158: Loss: 0.7570722467288737, accuracy: 0.5133333333333333\n",
      "iteration no 159: Loss: 0.7570592574387313, accuracy: 0.5133333333333333\n",
      "iteration no 160: Loss: 0.7570466744824609, accuracy: 0.5133333333333333\n",
      "iteration no 161: Loss: 0.7570344845163253, accuracy: 0.5133333333333333\n",
      "iteration no 162: Loss: 0.7570226746601418, accuracy: 0.5133333333333333\n",
      "iteration no 163: Loss: 0.7570112324799857, accuracy: 0.5133333333333333\n",
      "iteration no 164: Loss: 0.7570001459715971, accuracy: 0.5133333333333333\n",
      "iteration no 165: Loss: 0.7569894035444579, accuracy: 0.5133333333333333\n",
      "iteration no 166: Loss: 0.7569789940065113, accuracy: 0.5133333333333333\n",
      "iteration no 167: Loss: 0.7569689065494937, accuracy: 0.5133333333333333\n",
      "iteration no 168: Loss: 0.7569591307348535, accuracy: 0.5133333333333333\n",
      "iteration no 169: Loss: 0.7569496564802299, accuracy: 0.5133333333333333\n",
      "iteration no 170: Loss: 0.7569404740464684, accuracy: 0.5133333333333333\n",
      "iteration no 171: Loss: 0.7569315740251465, accuracy: 0.5133333333333333\n",
      "iteration no 172: Loss: 0.7569229473265948, accuracy: 0.5133333333333333\n",
      "iteration no 173: Loss: 0.756914585168384, accuracy: 0.5133333333333333\n",
      "iteration no 174: Loss: 0.7569064790642611, accuracy: 0.5133333333333333\n",
      "iteration no 175: Loss: 0.7568986208135201, accuracy: 0.5133333333333333\n",
      "iteration no 176: Loss: 0.7568910024907785, accuracy: 0.5133333333333333\n",
      "iteration no 177: Loss: 0.7568836164361539, accuracy: 0.5133333333333333\n",
      "iteration no 178: Loss: 0.7568764552458161, accuracy: 0.5133333333333333\n",
      "iteration no 179: Loss: 0.756869511762901, accuracy: 0.5133333333333333\n",
      "iteration no 180: Loss: 0.756862779068773, accuracy: 0.5133333333333333\n",
      "iteration no 181: Loss: 0.7568562504746182, accuracy: 0.5133333333333333\n",
      "iteration no 182: Loss: 0.7568499195133593, accuracy: 0.5133333333333333\n",
      "iteration no 183: Loss: 0.7568437799318726, accuracy: 0.5133333333333333\n",
      "iteration no 184: Loss: 0.7568378256835003, accuracy: 0.5133333333333333\n",
      "iteration no 185: Loss: 0.7568320509208449, accuracy: 0.5133333333333333\n",
      "iteration no 186: Loss: 0.7568264499888303, accuracy: 0.5133333333333333\n",
      "iteration no 187: Loss: 0.7568210174180247, accuracy: 0.5133333333333333\n",
      "iteration no 188: Loss: 0.7568157479182094, accuracy: 0.5133333333333333\n",
      "iteration no 189: Loss: 0.7568106363721876, accuracy: 0.5133333333333333\n",
      "iteration no 190: Loss: 0.7568056778298211, accuracy: 0.5133333333333333\n",
      "iteration no 191: Loss: 0.7568008675022869, accuracy: 0.5133333333333333\n",
      "iteration no 192: Loss: 0.7567962007565443, accuracy: 0.5133333333333333\n",
      "iteration no 193: Loss: 0.7567916731100051, accuracy: 0.5133333333333333\n",
      "iteration no 194: Loss: 0.7567872802253971, accuracy: 0.5133333333333333\n",
      "iteration no 195: Loss: 0.7567830179058166, accuracy: 0.5133333333333333\n",
      "iteration no 196: Loss: 0.756778882089957, accuracy: 0.5133333333333333\n",
      "iteration no 197: Loss: 0.7567748688475144, accuracy: 0.5133333333333333\n",
      "iteration no 198: Loss: 0.7567709743747539, accuracy: 0.5133333333333333\n",
      "iteration no 199: Loss: 0.7567671949902396, accuracy: 0.5133333333333333\n",
      "iteration no 200: Loss: 0.7567635271307148, accuracy: 0.5133333333333333\n",
      "iteration no 1: Loss: 1.129936064916826, accuracy: 0.29333333333333333\n",
      "iteration no 2: Loss: 1.0955825430745958, accuracy: 0.44\n",
      "iteration no 3: Loss: 1.0650944707068684, accuracy: 0.5\n",
      "iteration no 4: Loss: 1.0379946896582677, accuracy: 0.4866666666666667\n",
      "iteration no 5: Loss: 1.0138670143085264, accuracy: 0.49333333333333335\n",
      "iteration no 6: Loss: 0.9923457584032607, accuracy: 0.49666666666666665\n",
      "iteration no 7: Loss: 0.973109768655679, accuracy: 0.49333333333333335\n",
      "iteration no 8: Loss: 0.9558779154014944, accuracy: 0.5\n",
      "iteration no 9: Loss: 0.9404049756265546, accuracy: 0.5066666666666667\n",
      "iteration no 10: Loss: 0.9264776537776903, accuracy: 0.51\n",
      "iteration no 11: Loss: 0.9139107650704739, accuracy: 0.51\n",
      "iteration no 12: Loss: 0.9025436659164392, accuracy: 0.5166666666666667\n",
      "iteration no 13: Loss: 0.8922369990393282, accuracy: 0.52\n",
      "iteration no 14: Loss: 0.8828697866007418, accuracy: 0.52\n",
      "iteration no 15: Loss: 0.8743368747411362, accuracy: 0.52\n",
      "iteration no 16: Loss: 0.8665467126291759, accuracy: 0.5166666666666667\n",
      "iteration no 17: Loss: 0.8594194376523399, accuracy: 0.5166666666666667\n",
      "iteration no 18: Loss: 0.8528852334167658, accuracy: 0.5166666666666667\n",
      "iteration no 19: Loss: 0.8468829264553379, accuracy: 0.5166666666666667\n",
      "iteration no 20: Loss: 0.8413587892087541, accuracy: 0.5166666666666667\n",
      "iteration no 21: Loss: 0.8362655197448414, accuracy: 0.5166666666666667\n",
      "iteration no 22: Loss: 0.8315613720611148, accuracy: 0.5166666666666667\n",
      "iteration no 23: Loss: 0.8272094142313208, accuracy: 0.5133333333333333\n",
      "iteration no 24: Loss: 0.8231768948696087, accuracy: 0.5166666666666667\n",
      "iteration no 25: Loss: 0.8194347012838741, accuracy: 0.5166666666666667\n",
      "iteration no 26: Loss: 0.8159568952354156, accuracy: 0.5166666666666667\n",
      "iteration no 27: Loss: 0.8127203144196454, accuracy: 0.5166666666666667\n",
      "iteration no 28: Loss: 0.8097042296577959, accuracy: 0.5166666666666667\n",
      "iteration no 29: Loss: 0.8068900493771947, accuracy: 0.52\n",
      "iteration no 30: Loss: 0.8042610642948046, accuracy: 0.52\n",
      "iteration no 31: Loss: 0.8018022263411669, accuracy: 0.5166666666666667\n",
      "iteration no 32: Loss: 0.7994999568021899, accuracy: 0.5166666666666667\n",
      "iteration no 33: Loss: 0.7973419794432085, accuracy: 0.5166666666666667\n",
      "iteration no 34: Loss: 0.7953171750382173, accuracy: 0.52\n",
      "iteration no 35: Loss: 0.7934154542783358, accuracy: 0.52\n",
      "iteration no 36: Loss: 0.7916276464952233, accuracy: 0.52\n",
      "iteration no 37: Loss: 0.7899454020222785, accuracy: 0.52\n",
      "iteration no 38: Loss: 0.7883611063415171, accuracy: 0.52\n",
      "iteration no 39: Loss: 0.7868678044373537, accuracy: 0.52\n",
      "iteration no 40: Loss: 0.7854591340087729, accuracy: 0.5166666666666667\n",
      "iteration no 41: Loss: 0.7841292663856629, accuracy: 0.5166666666666667\n",
      "iteration no 42: Loss: 0.7828728541593355, accuracy: 0.5166666666666667\n",
      "iteration no 43: Loss: 0.7816849846763917, accuracy: 0.5166666666666667\n",
      "iteration no 44: Loss: 0.78056113866315, accuracy: 0.5166666666666667\n",
      "iteration no 45: Loss: 0.779497153348268, accuracy: 0.5166666666666667\n",
      "iteration no 46: Loss: 0.7784891895367229, accuracy: 0.5166666666666667\n",
      "iteration no 47: Loss: 0.777533702161351, accuracy: 0.5166666666666667\n",
      "iteration no 48: Loss: 0.7766274139006227, accuracy: 0.5166666666666667\n",
      "iteration no 49: Loss: 0.775767291504862, accuracy: 0.5166666666666667\n",
      "iteration no 50: Loss: 0.7749505245191202, accuracy: 0.5166666666666667\n",
      "iteration no 51: Loss: 0.7741745061304631, accuracy: 0.5166666666666667\n",
      "iteration no 52: Loss: 0.773436815901555, accuracy: 0.5166666666666667\n",
      "iteration no 53: Loss: 0.7727352041818716, accuracy: 0.5133333333333333\n",
      "iteration no 54: Loss: 0.7720675780133798, accuracy: 0.5133333333333333\n",
      "iteration no 55: Loss: 0.7714319883696142, accuracy: 0.5133333333333333\n",
      "iteration no 56: Loss: 0.7708266185862812, accuracy: 0.5133333333333333\n",
      "iteration no 57: Loss: 0.770249773858216, accuracy: 0.5133333333333333\n",
      "iteration no 58: Loss: 0.7696998716920803, accuracy: 0.5133333333333333\n",
      "iteration no 59: Loss: 0.769175433216884, accuracy: 0.5133333333333333\n",
      "iteration no 60: Loss: 0.7686750752655388, accuracy: 0.5133333333333333\n",
      "iteration no 61: Loss: 0.7681975031503757, accuracy: 0.5133333333333333\n",
      "iteration no 62: Loss: 0.7677415040641042, accuracy: 0.5133333333333333\n",
      "iteration no 63: Loss: 0.7673059410451921, accuracy: 0.5133333333333333\n",
      "iteration no 64: Loss: 0.7668897474532527, accuracy: 0.5133333333333333\n",
      "iteration no 65: Loss: 0.7664919219058468, accuracy: 0.5166666666666667\n",
      "iteration no 66: Loss: 0.7661115236332483, accuracy: 0.5166666666666667\n",
      "iteration no 67: Loss: 0.7657476682122631, accuracy: 0.5166666666666667\n",
      "iteration no 68: Loss: 0.7653995236442199, accuracy: 0.5166666666666667\n",
      "iteration no 69: Loss: 0.7650663067458107, accuracy: 0.5166666666666667\n",
      "iteration no 70: Loss: 0.764747279824636, accuracy: 0.5166666666666667\n",
      "iteration no 71: Loss: 0.7644417476141141, accuracy: 0.5166666666666667\n",
      "iteration no 72: Loss: 0.7641490544449299, accuracy: 0.5166666666666667\n",
      "iteration no 73: Loss: 0.7638685816324273, accuracy: 0.5166666666666667\n",
      "iteration no 74: Loss: 0.7635997450613442, accuracy: 0.5166666666666667\n",
      "iteration no 75: Loss: 0.7633419929510756, accuracy: 0.5166666666666667\n",
      "iteration no 76: Loss: 0.7630948037862371, accuracy: 0.5166666666666667\n",
      "iteration no 77: Loss: 0.7628576843987374, accuracy: 0.5166666666666667\n",
      "iteration no 78: Loss: 0.7626301681888428, accuracy: 0.5166666666666667\n",
      "iteration no 79: Loss: 0.7624118134738732, accuracy: 0.5166666666666667\n",
      "iteration no 80: Loss: 0.7622022019541971, accuracy: 0.5166666666666667\n",
      "iteration no 81: Loss: 0.7620009372871286, accuracy: 0.5166666666666667\n",
      "iteration no 82: Loss: 0.7618076437601652, accuracy: 0.5133333333333333\n",
      "iteration no 83: Loss: 0.761621965055761, accuracy: 0.5133333333333333\n",
      "iteration no 84: Loss: 0.7614435631005149, accuracy: 0.5133333333333333\n",
      "iteration no 85: Loss: 0.761272116992268, accuracy: 0.5133333333333333\n",
      "iteration no 86: Loss: 0.7611073219991641, accuracy: 0.5133333333333333\n",
      "iteration no 87: Loss: 0.7609488886252298, accuracy: 0.5133333333333333\n",
      "iteration no 88: Loss: 0.7607965417374938, accuracy: 0.5133333333333333\n",
      "iteration no 89: Loss: 0.7606500197500771, accuracy: 0.5133333333333333\n",
      "iteration no 90: Loss: 0.7605090738610615, accuracy: 0.5133333333333333\n",
      "iteration no 91: Loss: 0.7603734673382931, accuracy: 0.5133333333333333\n",
      "iteration no 92: Loss: 0.7602429748505845, accuracy: 0.5133333333333333\n",
      "iteration no 93: Loss: 0.760117381841063, accuracy: 0.5133333333333333\n",
      "iteration no 94: Loss: 0.7599964839396788, accuracy: 0.5133333333333333\n",
      "iteration no 95: Loss: 0.7598800864121126, accuracy: 0.5133333333333333\n",
      "iteration no 96: Loss: 0.759768003642547, accuracy: 0.5133333333333333\n",
      "iteration no 97: Loss: 0.7596600586479604, accuracy: 0.5133333333333333\n",
      "iteration no 98: Loss: 0.7595560826217765, accuracy: 0.5133333333333333\n",
      "iteration no 99: Loss: 0.7594559145048795, accuracy: 0.5133333333333333\n",
      "iteration no 100: Loss: 0.7593594005821456, accuracy: 0.5133333333333333\n",
      "iteration no 101: Loss: 0.759266394102784, accuracy: 0.5133333333333333\n",
      "iteration no 102: Loss: 0.7591767549229093, accuracy: 0.5133333333333333\n",
      "iteration no 103: Loss: 0.7590903491688806, accuracy: 0.5133333333333333\n",
      "iteration no 104: Loss: 0.7590070489200518, accuracy: 0.5133333333333333\n",
      "iteration no 105: Loss: 0.7589267319096746, accuracy: 0.5133333333333333\n",
      "iteration no 106: Loss: 0.758849281242791, accuracy: 0.5166666666666667\n",
      "iteration no 107: Loss: 0.7587745851300258, accuracy: 0.5166666666666667\n",
      "iteration no 108: Loss: 0.7587025366362786, accuracy: 0.5166666666666667\n",
      "iteration no 109: Loss: 0.7586330334433752, accuracy: 0.5166666666666667\n",
      "iteration no 110: Loss: 0.7585659776258105, accuracy: 0.5166666666666667\n",
      "iteration no 111: Loss: 0.7585012754387727, accuracy: 0.5166666666666667\n",
      "iteration no 112: Loss: 0.758438837117693, accuracy: 0.5166666666666667\n",
      "iteration no 113: Loss: 0.7583785766886202, accuracy: 0.5166666666666667\n",
      "iteration no 114: Loss: 0.7583204117887625, accuracy: 0.5166666666666667\n",
      "iteration no 115: Loss: 0.7582642634965886, accuracy: 0.5166666666666667\n",
      "iteration no 116: Loss: 0.7582100561709146, accuracy: 0.5166666666666667\n",
      "iteration no 117: Loss: 0.7581577172984465, accuracy: 0.5166666666666667\n",
      "iteration no 118: Loss: 0.7581071773492811, accuracy: 0.5166666666666667\n",
      "iteration no 119: Loss: 0.7580583696398978, accuracy: 0.5166666666666667\n",
      "iteration no 120: Loss: 0.7580112302032092, accuracy: 0.5166666666666667\n",
      "iteration no 121: Loss: 0.7579656976652629, accuracy: 0.5166666666666667\n",
      "iteration no 122: Loss: 0.7579217131282142, accuracy: 0.5166666666666667\n",
      "iteration no 123: Loss: 0.7578792200592123, accuracy: 0.5166666666666667\n",
      "iteration no 124: Loss: 0.7578381641848656, accuracy: 0.5166666666666667\n",
      "iteration no 125: Loss: 0.7577984933909775, accuracy: 0.5166666666666667\n",
      "iteration no 126: Loss: 0.7577601576272497, accuracy: 0.5166666666666667\n",
      "iteration no 127: Loss: 0.7577231088166906, accuracy: 0.5166666666666667\n",
      "iteration no 128: Loss: 0.7576873007694571, accuracy: 0.5166666666666667\n",
      "iteration no 129: Loss: 0.7576526891008972, accuracy: 0.5133333333333333\n",
      "iteration no 130: Loss: 0.7576192311535593, accuracy: 0.5133333333333333\n",
      "iteration no 131: Loss: 0.7575868859229568, accuracy: 0.5133333333333333\n",
      "iteration no 132: Loss: 0.7575556139868863, accuracy: 0.5133333333333333\n",
      "iteration no 133: Loss: 0.7575253774381083, accuracy: 0.5133333333333333\n",
      "iteration no 134: Loss: 0.7574961398202148, accuracy: 0.5133333333333333\n",
      "iteration no 135: Loss: 0.7574678660665126, accuracy: 0.5133333333333333\n",
      "iteration no 136: Loss: 0.7574405224417686, accuracy: 0.5133333333333333\n",
      "iteration no 137: Loss: 0.7574140764866636, accuracy: 0.5133333333333333\n",
      "iteration no 138: Loss: 0.7573884969648179, accuracy: 0.5133333333333333\n",
      "iteration no 139: Loss: 0.757363753812255, accuracy: 0.5133333333333333\n",
      "iteration no 140: Loss: 0.7573398180891788, accuracy: 0.5133333333333333\n",
      "iteration no 141: Loss: 0.7573166619339475, accuracy: 0.5133333333333333\n",
      "iteration no 142: Loss: 0.7572942585191298, accuracy: 0.5133333333333333\n",
      "iteration no 143: Loss: 0.7572725820095454, accuracy: 0.5133333333333333\n",
      "iteration no 144: Loss: 0.7572516075221829, accuracy: 0.5133333333333333\n",
      "iteration no 145: Loss: 0.7572313110879082, accuracy: 0.5133333333333333\n",
      "iteration no 146: Loss: 0.7572116696148694, accuracy: 0.5133333333333333\n",
      "iteration no 147: Loss: 0.7571926608535208, accuracy: 0.5133333333333333\n",
      "iteration no 148: Loss: 0.7571742633631796, accuracy: 0.5133333333333333\n",
      "iteration no 149: Loss: 0.7571564564800488, accuracy: 0.5133333333333333\n",
      "iteration no 150: Loss: 0.7571392202866284, accuracy: 0.5133333333333333\n",
      "iteration no 151: Loss: 0.757122535582452, accuracy: 0.5133333333333333\n",
      "iteration no 152: Loss: 0.7571063838560845, accuracy: 0.5133333333333333\n",
      "iteration no 153: Loss: 0.7570907472583213, accuracy: 0.5133333333333333\n",
      "iteration no 154: Loss: 0.7570756085765287, accuracy: 0.5133333333333333\n",
      "iteration no 155: Loss: 0.7570609512100791, accuracy: 0.5133333333333333\n",
      "iteration no 156: Loss: 0.757046759146821, accuracy: 0.5133333333333333\n",
      "iteration no 157: Loss: 0.7570330169405424, accuracy: 0.5133333333333333\n",
      "iteration no 158: Loss: 0.7570197096893763, accuracy: 0.5133333333333333\n",
      "iteration no 159: Loss: 0.75700682301511, accuracy: 0.5133333333333333\n",
      "iteration no 160: Loss: 0.7569943430433522, accuracy: 0.5133333333333333\n",
      "iteration no 161: Loss: 0.7569822563845198, accuracy: 0.5133333333333333\n",
      "iteration no 162: Loss: 0.7569705501156099, accuracy: 0.5133333333333333\n",
      "iteration no 163: Loss: 0.7569592117627169, accuracy: 0.5133333333333333\n",
      "iteration no 164: Loss: 0.7569482292842655, accuracy: 0.5133333333333333\n",
      "iteration no 165: Loss: 0.7569375910549231, accuracy: 0.5133333333333333\n",
      "iteration no 166: Loss: 0.7569272858501667, accuracy: 0.5133333333333333\n",
      "iteration no 167: Loss: 0.7569173028314691, accuracy: 0.5133333333333333\n",
      "iteration no 168: Loss: 0.7569076315320817, accuracy: 0.5133333333333333\n",
      "iteration no 169: Loss: 0.7568982618433864, accuracy: 0.5133333333333333\n",
      "iteration no 170: Loss: 0.7568891840017886, accuracy: 0.5133333333333333\n",
      "iteration no 171: Loss: 0.7568803885761344, accuracy: 0.5133333333333333\n",
      "iteration no 172: Loss: 0.7568718664556217, accuracy: 0.5133333333333333\n",
      "iteration no 173: Loss: 0.7568636088381884, accuracy: 0.5133333333333333\n",
      "iteration no 174: Loss: 0.756855607219357, accuracy: 0.5133333333333333\n",
      "iteration no 175: Loss: 0.756847853381514, accuracy: 0.5133333333333333\n",
      "iteration no 176: Loss: 0.7568403393836067, accuracy: 0.5133333333333333\n",
      "iteration no 177: Loss: 0.7568330575512403, accuracy: 0.5133333333333333\n",
      "iteration no 178: Loss: 0.7568260004671566, accuracy: 0.5133333333333333\n",
      "iteration no 179: Loss: 0.7568191609620812, accuracy: 0.5133333333333333\n",
      "iteration no 180: Loss: 0.7568125321059189, accuracy: 0.5133333333333333\n",
      "iteration no 181: Loss: 0.756806107199289, accuracy: 0.5133333333333333\n",
      "iteration no 182: Loss: 0.7567998797653802, accuracy: 0.5133333333333333\n",
      "iteration no 183: Loss: 0.7567938435421167, accuracy: 0.5133333333333333\n",
      "iteration no 184: Loss: 0.7567879924746189, accuracy: 0.5133333333333333\n",
      "iteration no 185: Loss: 0.7567823207079505, accuracy: 0.5133333333333333\n",
      "iteration no 186: Loss: 0.7567768225801363, accuracy: 0.5133333333333333\n",
      "iteration no 187: Loss: 0.7567714926154413, accuracy: 0.5133333333333333\n",
      "iteration no 188: Loss: 0.7567663255179033, accuracy: 0.5133333333333333\n",
      "iteration no 189: Loss: 0.7567613161651024, accuracy: 0.5133333333333333\n",
      "iteration no 190: Loss: 0.7567564596021643, accuracy: 0.5133333333333333\n",
      "iteration no 191: Loss: 0.7567517510359847, accuracy: 0.5133333333333333\n",
      "iteration no 192: Loss: 0.7567471858296644, accuracy: 0.5133333333333333\n",
      "iteration no 193: Loss: 0.7567427594971534, accuracy: 0.5133333333333333\n",
      "iteration no 194: Loss: 0.7567384676980861, accuracy: 0.5133333333333333\n",
      "iteration no 195: Loss: 0.7567343062328075, accuracy: 0.5133333333333333\n",
      "iteration no 196: Loss: 0.7567302710375815, accuracy: 0.5133333333333333\n",
      "iteration no 197: Loss: 0.75672635817997, accuracy: 0.5133333333333333\n",
      "iteration no 198: Loss: 0.7567225638543831, accuracy: 0.5133333333333333\n",
      "iteration no 199: Loss: 0.7567188843777853, accuracy: 0.5133333333333333\n",
      "iteration no 200: Loss: 0.7567153161855603, accuracy: 0.5133333333333333\n",
      "iteration no 1: Loss: 1.0925806336209993, accuracy: 0.4666666666666667\n",
      "iteration no 2: Loss: 1.0627881716387575, accuracy: 0.5066666666666667\n",
      "iteration no 3: Loss: 1.0362483967529994, accuracy: 0.49\n",
      "iteration no 4: Loss: 1.0125700498884604, accuracy: 0.5066666666666667\n",
      "iteration no 5: Loss: 0.991406535130642, accuracy: 0.5066666666666667\n",
      "iteration no 6: Loss: 0.9724527038647288, accuracy: 0.5066666666666667\n",
      "iteration no 7: Loss: 0.9554412140434948, accuracy: 0.5066666666666667\n",
      "iteration no 8: Loss: 0.9401386670287573, accuracy: 0.51\n",
      "iteration no 9: Loss: 0.9263417518002149, accuracy: 0.51\n",
      "iteration no 10: Loss: 0.913873570541485, accuracy: 0.5133333333333333\n",
      "iteration no 11: Loss: 0.9025802516764848, accuracy: 0.51\n",
      "iteration no 12: Loss: 0.8923279007844477, accuracy: 0.51\n",
      "iteration no 13: Loss: 0.8829999011902689, accuracy: 0.51\n",
      "iteration no 14: Loss: 0.8744945520938875, accuracy: 0.5066666666666667\n",
      "iteration no 15: Loss: 0.8667230189286682, accuracy: 0.51\n",
      "iteration no 16: Loss: 0.8596075646405688, accuracy: 0.51\n",
      "iteration no 17: Loss: 0.8530800290434807, accuracy: 0.51\n",
      "iteration no 18: Loss: 0.8470805244556786, accuracy: 0.51\n",
      "iteration no 19: Loss: 0.8415563182341368, accuracy: 0.51\n",
      "iteration no 20: Loss: 0.8364608758351701, accuracy: 0.51\n",
      "iteration no 21: Loss: 0.8317530411874111, accuracy: 0.51\n",
      "iteration no 22: Loss: 0.8273963342113322, accuracy: 0.51\n",
      "iteration no 23: Loss: 0.823358348127244, accuracy: 0.51\n",
      "iteration no 24: Loss: 0.8196102317050653, accuracy: 0.51\n",
      "iteration no 25: Loss: 0.8161262438122134, accuracy: 0.51\n",
      "iteration no 26: Loss: 0.8128833695229962, accuracy: 0.51\n",
      "iteration no 27: Loss: 0.8098609886884628, accuracy: 0.51\n",
      "iteration no 28: Loss: 0.8070405892593526, accuracy: 0.51\n",
      "iteration no 29: Loss: 0.8044055188370179, accuracy: 0.51\n",
      "iteration no 30: Loss: 0.8019407689270198, accuracy: 0.5133333333333333\n",
      "iteration no 31: Loss: 0.7996327872139881, accuracy: 0.5133333333333333\n",
      "iteration no 32: Loss: 0.7974693138878395, accuracy: 0.5133333333333333\n",
      "iteration no 33: Loss: 0.7954392386510016, accuracy: 0.5133333333333333\n",
      "iteration no 34: Loss: 0.7935324755415677, accuracy: 0.5133333333333333\n",
      "iteration no 35: Loss: 0.7917398531332497, accuracy: 0.5133333333333333\n",
      "iteration no 36: Loss: 0.7900530180323758, accuracy: 0.5166666666666667\n",
      "iteration no 37: Loss: 0.7884643498956352, accuracy: 0.5133333333333333\n",
      "iteration no 38: Loss: 0.7869668864488587, accuracy: 0.5133333333333333\n",
      "iteration no 39: Loss: 0.7855542572043147, accuracy: 0.5133333333333333\n",
      "iteration no 40: Loss: 0.784220624758148, accuracy: 0.5133333333333333\n",
      "iteration no 41: Loss: 0.7829606327059383, accuracy: 0.5133333333333333\n",
      "iteration no 42: Loss: 0.7817693593473207, accuracy: 0.5133333333333333\n",
      "iteration no 43: Loss: 0.780642276463883, accuracy: 0.5133333333333333\n",
      "iteration no 44: Loss: 0.7795752125512079, accuracy: 0.5133333333333333\n",
      "iteration no 45: Loss: 0.7785643199685486, accuracy: 0.5133333333333333\n",
      "iteration no 46: Loss: 0.7776060455403728, accuracy: 0.5133333333333333\n",
      "iteration no 47: Loss: 0.7766971042046958, accuracy: 0.5133333333333333\n",
      "iteration no 48: Loss: 0.7758344553552741, accuracy: 0.5133333333333333\n",
      "iteration no 49: Loss: 0.775015281569624, accuracy: 0.5133333333333333\n",
      "iteration no 50: Loss: 0.774236969453538, accuracy: 0.5133333333333333\n",
      "iteration no 51: Loss: 0.7734970923662222, accuracy: 0.5133333333333333\n",
      "iteration no 52: Loss: 0.77279339481911, accuracy: 0.5133333333333333\n",
      "iteration no 53: Loss: 0.7721237783665029, accuracy: 0.5133333333333333\n",
      "iteration no 54: Loss: 0.7714862888279654, accuracy: 0.5133333333333333\n",
      "iteration no 55: Loss: 0.7708791047013557, accuracy: 0.5133333333333333\n",
      "iteration no 56: Loss: 0.7703005266418741, accuracy: 0.5166666666666667\n",
      "iteration no 57: Loss: 0.7697489678969276, accuracy: 0.5166666666666667\n",
      "iteration no 58: Loss: 0.7692229455991876, accuracy: 0.5166666666666667\n",
      "iteration no 59: Loss: 0.7687210728312551, accuracy: 0.5166666666666667\n",
      "iteration no 60: Loss: 0.7682420513850023, accuracy: 0.5166666666666667\n",
      "iteration no 61: Loss: 0.7677846651471566, accuracy: 0.5166666666666667\n",
      "iteration no 62: Loss: 0.7673477740501548, accuracy: 0.5166666666666667\n",
      "iteration no 63: Loss: 0.7669303085338707, accuracy: 0.5166666666666667\n",
      "iteration no 64: Loss: 0.7665312644696243, accuracy: 0.5166666666666667\n",
      "iteration no 65: Loss: 0.7661496985030023, accuracy: 0.5166666666666667\n",
      "iteration no 66: Loss: 0.7657847237765524, accuracy: 0.5166666666666667\n",
      "iteration no 67: Loss: 0.7654355059974342, accuracy: 0.5166666666666667\n",
      "iteration no 68: Loss: 0.7651012598186646, accuracy: 0.5166666666666667\n",
      "iteration no 69: Loss: 0.7647812455057716, accuracy: 0.5166666666666667\n",
      "iteration no 70: Loss: 0.7644747658634685, accuracy: 0.5166666666666667\n",
      "iteration no 71: Loss: 0.7641811633994863, accuracy: 0.5166666666666667\n",
      "iteration no 72: Loss: 0.7638998177049195, accuracy: 0.5166666666666667\n",
      "iteration no 73: Loss: 0.7636301430324496, accuracy: 0.5166666666666667\n",
      "iteration no 74: Loss: 0.7633715860555846, accuracy: 0.5166666666666667\n",
      "iteration no 75: Loss: 0.763123623793659, accuracy: 0.5166666666666667\n",
      "iteration no 76: Loss: 0.7628857616887571, accuracy: 0.5166666666666667\n",
      "iteration no 77: Loss: 0.7626575318220196, accuracy: 0.5166666666666667\n",
      "iteration no 78: Loss: 0.7624384912579354, accuracy: 0.5166666666666667\n",
      "iteration no 79: Loss: 0.76222822050626, accuracy: 0.5133333333333333\n",
      "iteration no 80: Loss: 0.7620263220921376, accuracy: 0.5133333333333333\n",
      "iteration no 81: Loss: 0.7618324192258401, accuracy: 0.5133333333333333\n",
      "iteration no 82: Loss: 0.7616461545642965, accuracy: 0.5133333333333333\n",
      "iteration no 83: Loss: 0.7614671890572704, accuracy: 0.5133333333333333\n",
      "iteration no 84: Loss: 0.7612952008716604, accuracy: 0.5133333333333333\n",
      "iteration no 85: Loss: 0.7611298843879614, accuracy: 0.5133333333333333\n",
      "iteration no 86: Loss: 0.7609709492634272, accuracy: 0.5133333333333333\n",
      "iteration no 87: Loss: 0.7608181195569388, accuracy: 0.5133333333333333\n",
      "iteration no 88: Loss: 0.7606711329109935, accuracy: 0.5133333333333333\n",
      "iteration no 89: Loss: 0.7605297397866192, accuracy: 0.5133333333333333\n",
      "iteration no 90: Loss: 0.760393702747347, accuracy: 0.5133333333333333\n",
      "iteration no 91: Loss: 0.7602627957887067, accuracy: 0.5133333333333333\n",
      "iteration no 92: Loss: 0.7601368037099763, accuracy: 0.5133333333333333\n",
      "iteration no 93: Loss: 0.7600155215251925, accuracy: 0.5133333333333333\n",
      "iteration no 94: Loss: 0.7598987539106572, accuracy: 0.5133333333333333\n",
      "iteration no 95: Loss: 0.7597863146863915, accuracy: 0.5133333333333333\n",
      "iteration no 96: Loss: 0.759678026329192, accuracy: 0.5133333333333333\n",
      "iteration no 97: Loss: 0.7595737195151213, accuracy: 0.5133333333333333\n",
      "iteration no 98: Loss: 0.7594732326894276, accuracy: 0.5133333333333333\n",
      "iteration no 99: Loss: 0.7593764116620464, accuracy: 0.5133333333333333\n",
      "iteration no 100: Loss: 0.7592831092269735, accuracy: 0.5133333333333333\n",
      "iteration no 101: Loss: 0.7591931848039184, accuracy: 0.5166666666666667\n",
      "iteration no 102: Loss: 0.7591065041007813, accuracy: 0.5166666666666667\n",
      "iteration no 103: Loss: 0.7590229387955848, accuracy: 0.5166666666666667\n",
      "iteration no 104: Loss: 0.7589423662366059, accuracy: 0.5166666666666667\n",
      "iteration no 105: Loss: 0.7588646691595357, accuracy: 0.5166666666666667\n",
      "iteration no 106: Loss: 0.7587897354205795, accuracy: 0.5166666666666667\n",
      "iteration no 107: Loss: 0.7587174577444921, accuracy: 0.5166666666666667\n",
      "iteration no 108: Loss: 0.7586477334866064, accuracy: 0.5166666666666667\n",
      "iteration no 109: Loss: 0.7585804644079852, accuracy: 0.5166666666666667\n",
      "iteration no 110: Loss: 0.7585155564628827, accuracy: 0.5166666666666667\n",
      "iteration no 111: Loss: 0.7584529195977611, accuracy: 0.5166666666666667\n",
      "iteration no 112: Loss: 0.7583924675611564, accuracy: 0.5166666666666667\n",
      "iteration no 113: Loss: 0.7583341177237386, accuracy: 0.5166666666666667\n",
      "iteration no 114: Loss: 0.7582777909079501, accuracy: 0.5166666666666667\n",
      "iteration no 115: Loss: 0.7582234112266569, accuracy: 0.5166666666666667\n",
      "iteration no 116: Loss: 0.7581709059302715, accuracy: 0.5166666666666667\n",
      "iteration no 117: Loss: 0.7581202052618554, accuracy: 0.5166666666666667\n",
      "iteration no 118: Loss: 0.7580712423197307, accuracy: 0.5166666666666667\n",
      "iteration no 119: Loss: 0.7580239529271685, accuracy: 0.5166666666666667\n",
      "iteration no 120: Loss: 0.7579782755087434, accuracy: 0.5166666666666667\n",
      "iteration no 121: Loss: 0.7579341509729748, accuracy: 0.5166666666666667\n",
      "iteration no 122: Loss: 0.7578915226008966, accuracy: 0.5166666666666667\n",
      "iteration no 123: Loss: 0.7578503359402213, accuracy: 0.5166666666666667\n",
      "iteration no 124: Loss: 0.7578105387047837, accuracy: 0.5166666666666667\n",
      "iteration no 125: Loss: 0.7577720806789708, accuracy: 0.5166666666666667\n",
      "iteration no 126: Loss: 0.7577349136268623, accuracy: 0.5166666666666667\n",
      "iteration no 127: Loss: 0.7576989912058202, accuracy: 0.5166666666666667\n",
      "iteration no 128: Loss: 0.7576642688842872, accuracy: 0.5166666666666667\n",
      "iteration no 129: Loss: 0.7576307038635655, accuracy: 0.5166666666666667\n",
      "iteration no 130: Loss: 0.7575982550033565, accuracy: 0.5166666666666667\n",
      "iteration no 131: Loss: 0.7575668827508664, accuracy: 0.5166666666666667\n",
      "iteration no 132: Loss: 0.7575365490732816, accuracy: 0.5166666666666667\n",
      "iteration no 133: Loss: 0.7575072173934396, accuracy: 0.5166666666666667\n",
      "iteration no 134: Loss: 0.7574788525285259, accuracy: 0.5166666666666667\n",
      "iteration no 135: Loss: 0.7574514206316364, accuracy: 0.5133333333333333\n",
      "iteration no 136: Loss: 0.7574248891360592, accuracy: 0.5133333333333333\n",
      "iteration no 137: Loss: 0.7573992267021339, accuracy: 0.5133333333333333\n",
      "iteration no 138: Loss: 0.7573744031665557, accuracy: 0.5133333333333333\n",
      "iteration no 139: Loss: 0.7573503894939997, accuracy: 0.5133333333333333\n",
      "iteration no 140: Loss: 0.7573271577309464, accuracy: 0.5133333333333333\n",
      "iteration no 141: Loss: 0.7573046809616, accuracy: 0.5133333333333333\n",
      "iteration no 142: Loss: 0.7572829332657898, accuracy: 0.5133333333333333\n",
      "iteration no 143: Loss: 0.757261889678761, accuracy: 0.5133333333333333\n",
      "iteration no 144: Loss: 0.7572415261527554, accuracy: 0.5133333333333333\n",
      "iteration no 145: Loss: 0.7572218195202992, accuracy: 0.5133333333333333\n",
      "iteration no 146: Loss: 0.7572027474591089, accuracy: 0.5133333333333333\n",
      "iteration no 147: Loss: 0.7571842884585396, accuracy: 0.5133333333333333\n",
      "iteration no 148: Loss: 0.7571664217875002, accuracy: 0.5133333333333333\n",
      "iteration no 149: Loss: 0.7571491274637622, accuracy: 0.5133333333333333\n",
      "iteration no 150: Loss: 0.7571323862245987, accuracy: 0.5133333333333333\n",
      "iteration no 151: Loss: 0.7571161794986879, accuracy: 0.5133333333333333\n",
      "iteration no 152: Loss: 0.7571004893792199, accuracy: 0.5133333333333333\n",
      "iteration no 153: Loss: 0.7570852985981527, accuracy: 0.5133333333333333\n",
      "iteration no 154: Loss: 0.7570705905015601, accuracy: 0.5133333333333333\n",
      "iteration no 155: Loss: 0.7570563490260238, accuracy: 0.5133333333333333\n",
      "iteration no 156: Loss: 0.7570425586760181, accuracy: 0.5133333333333333\n",
      "iteration no 157: Loss: 0.7570292045022414, accuracy: 0.5133333333333333\n",
      "iteration no 158: Loss: 0.7570162720808532, accuracy: 0.5133333333333333\n",
      "iteration no 159: Loss: 0.7570037474935725, accuracy: 0.5133333333333333\n",
      "iteration no 160: Loss: 0.7569916173085991, accuracy: 0.5133333333333333\n",
      "iteration no 161: Loss: 0.7569798685623211, accuracy: 0.5133333333333333\n",
      "iteration no 162: Loss: 0.7569684887417713, accuracy: 0.5133333333333333\n",
      "iteration no 163: Loss: 0.7569574657678003, accuracy: 0.5133333333333333\n",
      "iteration no 164: Loss: 0.7569467879789342, accuracy: 0.5133333333333333\n",
      "iteration no 165: Loss: 0.7569364441158848, accuracy: 0.5133333333333333\n",
      "iteration no 166: Loss: 0.7569264233066844, accuracy: 0.5133333333333333\n",
      "iteration no 167: Loss: 0.7569167150524191, accuracy: 0.5133333333333333\n",
      "iteration no 168: Loss: 0.7569073092135302, accuracy: 0.5133333333333333\n",
      "iteration no 169: Loss: 0.7568981959966629, accuracy: 0.5133333333333333\n",
      "iteration no 170: Loss: 0.7568893659420365, accuracy: 0.5133333333333333\n",
      "iteration no 171: Loss: 0.756880809911314, accuracy: 0.5133333333333333\n",
      "iteration no 172: Loss: 0.7568725190759481, accuracy: 0.5133333333333333\n",
      "iteration no 173: Loss: 0.7568644849059857, accuracy: 0.5133333333333333\n",
      "iteration no 174: Loss: 0.7568566991593093, accuracy: 0.5133333333333333\n",
      "iteration no 175: Loss: 0.7568491538712968, accuracy: 0.5133333333333333\n",
      "iteration no 176: Loss: 0.756841841344883, accuracy: 0.5133333333333333\n",
      "iteration no 177: Loss: 0.7568347541410042, accuracy: 0.5133333333333333\n",
      "iteration no 178: Loss: 0.7568278850694121, accuracy: 0.5133333333333333\n",
      "iteration no 179: Loss: 0.7568212271798388, accuracy: 0.5133333333333333\n",
      "iteration no 180: Loss: 0.7568147737534997, accuracy: 0.5133333333333333\n",
      "iteration no 181: Loss: 0.7568085182949211, accuracy: 0.5133333333333333\n",
      "iteration no 182: Loss: 0.7568024545240766, accuracy: 0.5133333333333333\n",
      "iteration no 183: Loss: 0.756796576368822, accuracy: 0.5133333333333333\n",
      "iteration no 184: Loss: 0.7567908779576155, accuracy: 0.5133333333333333\n",
      "iteration no 185: Loss: 0.7567853536125101, accuracy: 0.5133333333333333\n",
      "iteration no 186: Loss: 0.7567799978424101, accuracy: 0.5133333333333333\n",
      "iteration no 187: Loss: 0.7567748053365788, accuracy: 0.5133333333333333\n",
      "iteration no 188: Loss: 0.7567697709583865, accuracy: 0.5133333333333333\n",
      "iteration no 189: Loss: 0.7567648897392922, accuracy: 0.5133333333333333\n",
      "iteration no 190: Loss: 0.7567601568730474, accuracy: 0.5133333333333333\n",
      "iteration no 191: Loss: 0.7567555677101118, accuracy: 0.5133333333333333\n",
      "iteration no 192: Loss: 0.756751117752276, accuracy: 0.5133333333333333\n",
      "iteration no 193: Loss: 0.7567468026474815, accuracy: 0.5133333333333333\n",
      "iteration no 194: Loss: 0.7567426181848276, accuracy: 0.5133333333333333\n",
      "iteration no 195: Loss: 0.7567385602897618, accuracy: 0.5133333333333333\n",
      "iteration no 196: Loss: 0.7567346250194446, accuracy: 0.5133333333333333\n",
      "iteration no 197: Loss: 0.7567308085582816, accuracy: 0.5133333333333333\n",
      "iteration no 198: Loss: 0.7567271072136169, accuracy: 0.5133333333333333\n",
      "iteration no 199: Loss: 0.7567235174115814, accuracy: 0.5133333333333333\n",
      "iteration no 200: Loss: 0.7567200356930903, accuracy: 0.5166666666666667\n",
      "iteration no 1: Loss: 1.138586322710045, accuracy: 0.32\n",
      "iteration no 2: Loss: 1.1033413244257178, accuracy: 0.47333333333333333\n",
      "iteration no 3: Loss: 1.0720336764961447, accuracy: 0.51\n",
      "iteration no 4: Loss: 1.0441938763962364, accuracy: 0.5033333333333333\n",
      "iteration no 5: Loss: 1.0194036710066146, accuracy: 0.5066666666666667\n",
      "iteration no 6: Loss: 0.9972916241388785, accuracy: 0.5066666666666667\n",
      "iteration no 7: Loss: 0.9775297900231399, accuracy: 0.52\n",
      "iteration no 8: Loss: 0.959830290932177, accuracy: 0.5233333333333333\n",
      "iteration no 9: Loss: 0.9439416039926143, accuracy: 0.5233333333333333\n",
      "iteration no 10: Loss: 0.9296446998892705, accuracy: 0.5233333333333333\n",
      "iteration no 11: Loss: 0.9167492324177421, accuracy: 0.5166666666666667\n",
      "iteration no 12: Loss: 0.9050899360232206, accuracy: 0.5133333333333333\n",
      "iteration no 13: Loss: 0.8945233270815339, accuracy: 0.5133333333333333\n",
      "iteration no 14: Loss: 0.8849247522061191, accuracy: 0.5166666666666667\n",
      "iteration no 15: Loss: 0.8761857900229318, accuracy: 0.5166666666666667\n",
      "iteration no 16: Loss: 0.8682119901980166, accuracy: 0.5166666666666667\n",
      "iteration no 17: Loss: 0.860920921395672, accuracy: 0.5166666666666667\n",
      "iteration no 18: Loss: 0.8542404947742253, accuracy: 0.5166666666666667\n",
      "iteration no 19: Loss: 0.8481075288731581, accuracy: 0.5166666666666667\n",
      "iteration no 20: Loss: 0.8424665234231524, accuracy: 0.5166666666666667\n",
      "iteration no 21: Loss: 0.8372686124983827, accuracy: 0.5166666666666667\n",
      "iteration no 22: Loss: 0.8324706707810878, accuracy: 0.5166666666666667\n",
      "iteration no 23: Loss: 0.828034550090856, accuracy: 0.5166666666666667\n",
      "iteration no 24: Loss: 0.823926426514572, accuracy: 0.5166666666666667\n",
      "iteration no 25: Loss: 0.8201162413494096, accuracy: 0.5133333333333333\n",
      "iteration no 26: Loss: 0.8165772216042301, accuracy: 0.5133333333333333\n",
      "iteration no 27: Loss: 0.8132854679977701, accuracy: 0.5166666666666667\n",
      "iteration no 28: Loss: 0.8102196002691607, accuracy: 0.5166666666666667\n",
      "iteration no 29: Loss: 0.8073604512106535, accuracy: 0.5166666666666667\n",
      "iteration no 30: Loss: 0.8046908021795288, accuracy: 0.5166666666666667\n",
      "iteration no 31: Loss: 0.8021951539804492, accuracy: 0.5133333333333333\n",
      "iteration no 32: Loss: 0.799859527962595, accuracy: 0.5133333333333333\n",
      "iteration no 33: Loss: 0.7976712929758393, accuracy: 0.5133333333333333\n",
      "iteration no 34: Loss: 0.7956190145012825, accuracy: 0.5166666666666667\n",
      "iteration no 35: Loss: 0.7936923228345458, accuracy: 0.5166666666666667\n",
      "iteration no 36: Loss: 0.7918817976729373, accuracy: 0.5166666666666667\n",
      "iteration no 37: Loss: 0.7901788668548239, accuracy: 0.5166666666666667\n",
      "iteration no 38: Loss: 0.7885757173337103, accuracy: 0.5166666666666667\n",
      "iteration no 39: Loss: 0.7870652167510208, accuracy: 0.5166666666666667\n",
      "iteration no 40: Loss: 0.7856408442090707, accuracy: 0.5166666666666667\n",
      "iteration no 41: Loss: 0.7842966290463959, accuracy: 0.5166666666666667\n",
      "iteration no 42: Loss: 0.7830270965874857, accuracy: 0.5166666666666667\n",
      "iteration no 43: Loss: 0.7818272199830009, accuracy: 0.5166666666666667\n",
      "iteration no 44: Loss: 0.7806923773789348, accuracy: 0.5166666666666667\n",
      "iteration no 45: Loss: 0.7796183137573196, accuracy: 0.5166666666666667\n",
      "iteration no 46: Loss: 0.7786011068798773, accuracy: 0.5166666666666667\n",
      "iteration no 47: Loss: 0.7776371368418835, accuracy: 0.5166666666666667\n",
      "iteration no 48: Loss: 0.7767230588084402, accuracy: 0.5133333333333333\n",
      "iteration no 49: Loss: 0.77585577856103, accuracy: 0.5133333333333333\n",
      "iteration no 50: Loss: 0.7750324305300629, accuracy: 0.5133333333333333\n",
      "iteration no 51: Loss: 0.7742503580302923, accuracy: 0.5133333333333333\n",
      "iteration no 52: Loss: 0.77350709545149, accuracy: 0.5133333333333333\n",
      "iteration no 53: Loss: 0.7728003521874224, accuracy: 0.5133333333333333\n",
      "iteration no 54: Loss: 0.7721279981127338, accuracy: 0.5133333333333333\n",
      "iteration no 55: Loss: 0.7714880504403367, accuracy: 0.5133333333333333\n",
      "iteration no 56: Loss: 0.770878661811907, accuracy: 0.5133333333333333\n",
      "iteration no 57: Loss: 0.7702981094914619, accuracy: 0.5133333333333333\n",
      "iteration no 58: Loss: 0.7697447855471578, accuracy: 0.5133333333333333\n",
      "iteration no 59: Loss: 0.7692171879196679, accuracy: 0.5133333333333333\n",
      "iteration no 60: Loss: 0.7687139122870699, accuracy: 0.5133333333333333\n",
      "iteration no 61: Loss: 0.7682336446462996, accuracy: 0.5133333333333333\n",
      "iteration no 62: Loss: 0.7677751545401161, accuracy: 0.5133333333333333\n",
      "iteration no 63: Loss: 0.7673372888663281, accuracy: 0.5166666666666667\n",
      "iteration no 64: Loss: 0.7669189662128981, accuracy: 0.5166666666666667\n",
      "iteration no 65: Loss: 0.7665191716686022, accuracy: 0.5166666666666667\n",
      "iteration no 66: Loss: 0.7661369520642537, accuracy: 0.5166666666666667\n",
      "iteration no 67: Loss: 0.7657714116042327, accuracy: 0.5166666666666667\n",
      "iteration no 68: Loss: 0.7654217078522312, accuracy: 0.5166666666666667\n",
      "iteration no 69: Loss: 0.7650870480388373, accuracy: 0.5166666666666667\n",
      "iteration no 70: Loss: 0.7647666856618623, accuracy: 0.5166666666666667\n",
      "iteration no 71: Loss: 0.7644599173532378, accuracy: 0.5166666666666667\n",
      "iteration no 72: Loss: 0.7641660799889112, accuracy: 0.5166666666666667\n",
      "iteration no 73: Loss: 0.7638845480204853, accuracy: 0.5166666666666667\n",
      "iteration no 74: Loss: 0.763614731009408, accuracy: 0.5166666666666667\n",
      "iteration no 75: Loss: 0.7633560713463737, accuracy: 0.5166666666666667\n",
      "iteration no 76: Loss: 0.7631080421402425, accuracy: 0.5166666666666667\n",
      "iteration no 77: Loss: 0.7628701452622623, accuracy: 0.5166666666666667\n",
      "iteration no 78: Loss: 0.7626419095327109, accuracy: 0.5166666666666667\n",
      "iteration no 79: Loss: 0.7624228890382552, accuracy: 0.5133333333333333\n",
      "iteration no 80: Loss: 0.7622126615694, accuracy: 0.5133333333333333\n",
      "iteration no 81: Loss: 0.7620108271683615, accuracy: 0.5133333333333333\n",
      "iteration no 82: Loss: 0.761817006778562, accuracy: 0.5133333333333333\n",
      "iteration no 83: Loss: 0.7616308409877254, accuracy: 0.5133333333333333\n",
      "iteration no 84: Loss: 0.7614519888572592, accuracy: 0.5133333333333333\n",
      "iteration no 85: Loss: 0.7612801268312419, accuracy: 0.5133333333333333\n",
      "iteration no 86: Loss: 0.7611149477189145, accuracy: 0.5133333333333333\n",
      "iteration no 87: Loss: 0.7609561597450893, accuracy: 0.5133333333333333\n",
      "iteration no 88: Loss: 0.7608034856633709, accuracy: 0.5133333333333333\n",
      "iteration no 89: Loss: 0.7606566619275005, accuracy: 0.5133333333333333\n",
      "iteration no 90: Loss: 0.7605154379165362, accuracy: 0.5133333333333333\n",
      "iteration no 91: Loss: 0.7603795752099228, accuracy: 0.5133333333333333\n",
      "iteration no 92: Loss: 0.760248846908836, accuracy: 0.5133333333333333\n",
      "iteration no 93: Loss: 0.7601230370004705, accuracy: 0.5133333333333333\n",
      "iteration no 94: Loss: 0.7600019397622146, accuracy: 0.5133333333333333\n",
      "iteration no 95: Loss: 0.75988535920289, accuracy: 0.5133333333333333\n",
      "iteration no 96: Loss: 0.7597731085384624, accuracy: 0.5133333333333333\n",
      "iteration no 97: Loss: 0.7596650096998299, accuracy: 0.5133333333333333\n",
      "iteration no 98: Loss: 0.7595608928704786, accuracy: 0.5133333333333333\n",
      "iteration no 99: Loss: 0.7594605960519661, accuracy: 0.5133333333333333\n",
      "iteration no 100: Loss: 0.7593639646553523, accuracy: 0.5133333333333333\n",
      "iteration no 101: Loss: 0.7592708511168262, accuracy: 0.5133333333333333\n",
      "iteration no 102: Loss: 0.75918111453593, accuracy: 0.5133333333333333\n",
      "iteration no 103: Loss: 0.7590946203348738, accuracy: 0.5133333333333333\n",
      "iteration no 104: Loss: 0.7590112399375682, accuracy: 0.5133333333333333\n",
      "iteration no 105: Loss: 0.7589308504670886, accuracy: 0.5166666666666667\n",
      "iteration no 106: Loss: 0.7588533344603813, accuracy: 0.5166666666666667\n",
      "iteration no 107: Loss: 0.7587785795991108, accuracy: 0.5166666666666667\n",
      "iteration no 108: Loss: 0.758706478455619, accuracy: 0.5166666666666667\n",
      "iteration no 109: Loss: 0.7586369282530484, accuracy: 0.5166666666666667\n",
      "iteration no 110: Loss: 0.7585698306387372, accuracy: 0.5166666666666667\n",
      "iteration no 111: Loss: 0.7585050914700681, accuracy: 0.5166666666666667\n",
      "iteration no 112: Loss: 0.7584426206119986, accuracy: 0.5166666666666667\n",
      "iteration no 113: Loss: 0.758382331745562, accuracy: 0.5166666666666667\n",
      "iteration no 114: Loss: 0.7583241421866688, accuracy: 0.5166666666666667\n",
      "iteration no 115: Loss: 0.7582679727145909, accuracy: 0.5166666666666667\n",
      "iteration no 116: Loss: 0.7582137474095468, accuracy: 0.5166666666666667\n",
      "iteration no 117: Loss: 0.7581613934988478, accuracy: 0.5166666666666667\n",
      "iteration no 118: Loss: 0.7581108412110995, accuracy: 0.5166666666666667\n",
      "iteration no 119: Loss: 0.758062023637986, accuracy: 0.5166666666666667\n",
      "iteration no 120: Loss: 0.7580148766031972, accuracy: 0.5166666666666667\n",
      "iteration no 121: Loss: 0.7579693385380841, accuracy: 0.5166666666666667\n",
      "iteration no 122: Loss: 0.7579253503636568, accuracy: 0.5166666666666667\n",
      "iteration no 123: Loss: 0.7578828553785638, accuracy: 0.5166666666666667\n",
      "iteration no 124: Loss: 0.7578417991527132, accuracy: 0.5166666666666667\n",
      "iteration no 125: Loss: 0.7578021294262189, accuracy: 0.5166666666666667\n",
      "iteration no 126: Loss: 0.7577637960133724, accuracy: 0.5166666666666667\n",
      "iteration no 127: Loss: 0.7577267507113638, accuracy: 0.5166666666666667\n",
      "iteration no 128: Loss: 0.7576909472134878, accuracy: 0.5166666666666667\n",
      "iteration no 129: Loss: 0.7576563410265899, accuracy: 0.5166666666666667\n",
      "iteration no 130: Loss: 0.7576228893925195, accuracy: 0.5166666666666667\n",
      "iteration no 131: Loss: 0.7575905512133778, accuracy: 0.5166666666666667\n",
      "iteration no 132: Loss: 0.7575592869803507, accuracy: 0.5133333333333333\n",
      "iteration no 133: Loss: 0.7575290587059383, accuracy: 0.5133333333333333\n",
      "iteration no 134: Loss: 0.7574998298594003, accuracy: 0.5133333333333333\n",
      "iteration no 135: Loss: 0.7574715653052464, accuracy: 0.5133333333333333\n",
      "iteration no 136: Loss: 0.7574442312446119, accuracy: 0.5133333333333333\n",
      "iteration no 137: Loss: 0.7574177951593695, accuracy: 0.5133333333333333\n",
      "iteration no 138: Loss: 0.7573922257588326, accuracy: 0.5133333333333333\n",
      "iteration no 139: Loss: 0.7573674929289189, accuracy: 0.5133333333333333\n",
      "iteration no 140: Loss: 0.7573435676836474, accuracy: 0.5133333333333333\n",
      "iteration no 141: Loss: 0.7573204221188491, accuracy: 0.5133333333333333\n",
      "iteration no 142: Loss: 0.7572980293679796, accuracy: 0.5133333333333333\n",
      "iteration no 143: Loss: 0.7572763635599289, accuracy: 0.5133333333333333\n",
      "iteration no 144: Loss: 0.7572553997787256, accuracy: 0.5133333333333333\n",
      "iteration no 145: Loss: 0.7572351140250442, accuracy: 0.5133333333333333\n",
      "iteration no 146: Loss: 0.7572154831794237, accuracy: 0.5133333333333333\n",
      "iteration no 147: Loss: 0.7571964849671143, accuracy: 0.5133333333333333\n",
      "iteration no 148: Loss: 0.7571780979244729, accuracy: 0.5133333333333333\n",
      "iteration no 149: Loss: 0.7571603013668295, accuracy: 0.5133333333333333\n",
      "iteration no 150: Loss: 0.7571430753577573, accuracy: 0.5133333333333333\n",
      "iteration no 151: Loss: 0.7571264006796733, accuracy: 0.5133333333333333\n",
      "iteration no 152: Loss: 0.7571102588057116, accuracy: 0.5133333333333333\n",
      "iteration no 153: Loss: 0.7570946318728039, accuracy: 0.5133333333333333\n",
      "iteration no 154: Loss: 0.7570795026559124, accuracy: 0.5133333333333333\n",
      "iteration no 155: Loss: 0.7570648545433585, accuracy: 0.5133333333333333\n",
      "iteration no 156: Loss: 0.7570506715132, accuracy: 0.5133333333333333\n",
      "iteration no 157: Loss: 0.7570369381106022, accuracy: 0.5133333333333333\n",
      "iteration no 158: Loss: 0.7570236394261611, accuracy: 0.5133333333333333\n",
      "iteration no 159: Loss: 0.757010761075132, accuracy: 0.5133333333333333\n",
      "iteration no 160: Loss: 0.7569982891775245, accuracy: 0.5133333333333333\n",
      "iteration no 161: Loss: 0.7569862103390202, accuracy: 0.5133333333333333\n",
      "iteration no 162: Loss: 0.7569745116326791, accuracy: 0.5133333333333333\n",
      "iteration no 163: Loss: 0.7569631805813984, accuracy: 0.5133333333333333\n",
      "iteration no 164: Loss: 0.7569522051410875, accuracy: 0.5133333333333333\n",
      "iteration no 165: Loss: 0.7569415736845294, accuracy: 0.5133333333333333\n",
      "iteration no 166: Loss: 0.7569312749858966, accuracy: 0.5133333333333333\n",
      "iteration no 167: Loss: 0.7569212982058927, accuracy: 0.5133333333333333\n",
      "iteration no 168: Loss: 0.7569116328774922, accuracy: 0.5133333333333333\n",
      "iteration no 169: Loss: 0.7569022688922508, accuracy: 0.5133333333333333\n",
      "iteration no 170: Loss: 0.7568931964871637, accuracy: 0.5133333333333333\n",
      "iteration no 171: Loss: 0.7568844062320451, accuracy: 0.5133333333333333\n",
      "iteration no 172: Loss: 0.756875889017408, accuracy: 0.5133333333333333\n",
      "iteration no 173: Loss: 0.7568676360428221, accuracy: 0.5133333333333333\n",
      "iteration no 174: Loss: 0.7568596388057296, accuracy: 0.5133333333333333\n",
      "iteration no 175: Loss: 0.7568518890906974, accuracy: 0.5133333333333333\n",
      "iteration no 176: Loss: 0.756844378959091, accuracy: 0.5133333333333333\n",
      "iteration no 177: Loss: 0.7568371007391462, accuracy: 0.5133333333333333\n",
      "iteration no 178: Loss: 0.7568300470164293, accuracy: 0.5133333333333333\n",
      "iteration no 179: Loss: 0.7568232106246612, accuracy: 0.5133333333333333\n",
      "iteration no 180: Loss: 0.7568165846368968, accuracy: 0.5133333333333333\n",
      "iteration no 181: Loss: 0.7568101623570408, accuracy: 0.5133333333333333\n",
      "iteration no 182: Loss: 0.756803937311689, accuracy: 0.5133333333333333\n",
      "iteration no 183: Loss: 0.7567979032422768, accuracy: 0.5133333333333333\n",
      "iteration no 184: Loss: 0.7567920540975284, accuracy: 0.5133333333333333\n",
      "iteration no 185: Loss: 0.7567863840261896, accuracy: 0.5133333333333333\n",
      "iteration no 186: Loss: 0.7567808873700335, accuracy: 0.5133333333333333\n",
      "iteration no 187: Loss: 0.7567755586571302, accuracy: 0.5133333333333333\n",
      "iteration no 188: Loss: 0.756770392595367, accuracy: 0.5133333333333333\n",
      "iteration no 189: Loss: 0.7567653840662104, accuracy: 0.5133333333333333\n",
      "iteration no 190: Loss: 0.756760528118699, accuracy: 0.5133333333333333\n",
      "iteration no 191: Loss: 0.7567558199636606, accuracy: 0.5133333333333333\n",
      "iteration no 192: Loss: 0.7567512549681411, accuracy: 0.5133333333333333\n",
      "iteration no 193: Loss: 0.7567468286500384, accuracy: 0.5133333333333333\n",
      "iteration no 194: Loss: 0.7567425366729345, accuracy: 0.5133333333333333\n",
      "iteration no 195: Loss: 0.7567383748411148, accuracy: 0.5133333333333333\n",
      "iteration no 196: Loss: 0.756734339094771, accuracy: 0.5133333333333333\n",
      "iteration no 197: Loss: 0.756730425505375, accuracy: 0.5133333333333333\n",
      "iteration no 198: Loss: 0.7567266302712248, accuracy: 0.5133333333333333\n",
      "iteration no 199: Loss: 0.7567229497131481, accuracy: 0.5133333333333333\n",
      "iteration no 200: Loss: 0.7567193802703611, accuracy: 0.5133333333333333\n",
      "iteration no 1: Loss: 1.102496458703455, accuracy: 0.3466666666666667\n",
      "iteration no 2: Loss: 1.0715665555658098, accuracy: 0.44666666666666666\n",
      "iteration no 3: Loss: 1.0440582149172484, accuracy: 0.4766666666666667\n",
      "iteration no 4: Loss: 1.019545051712057, accuracy: 0.49333333333333335\n",
      "iteration no 5: Loss: 0.9976585084566079, accuracy: 0.49666666666666665\n",
      "iteration no 6: Loss: 0.978077190087048, accuracy: 0.49333333333333335\n",
      "iteration no 7: Loss: 0.9605203461330851, accuracy: 0.49\n",
      "iteration no 8: Loss: 0.9447429828667421, accuracy: 0.49333333333333335\n",
      "iteration no 9: Loss: 0.9305316606912917, accuracy: 0.49333333333333335\n",
      "iteration no 10: Loss: 0.9177006788066033, accuracy: 0.49666666666666665\n",
      "iteration no 11: Loss: 0.9060885849882624, accuracy: 0.49666666666666665\n",
      "iteration no 12: Loss: 0.8955550145416626, accuracy: 0.5\n",
      "iteration no 13: Loss: 0.885977866957512, accuracy: 0.5\n",
      "iteration no 14: Loss: 0.8772508162952768, accuracy: 0.5\n",
      "iteration no 15: Loss: 0.8692811383535785, accuracy: 0.5\n",
      "iteration no 16: Loss: 0.8619878289090058, accuracy: 0.5066666666666667\n",
      "iteration no 17: Loss: 0.8552999829490289, accuracy: 0.51\n",
      "iteration no 18: Loss: 0.849155403817132, accuracy: 0.5066666666666667\n",
      "iteration no 19: Loss: 0.8434994123339845, accuracy: 0.5066666666666667\n",
      "iteration no 20: Loss: 0.8382838283056357, accuracy: 0.51\n",
      "iteration no 21: Loss: 0.8334660997202303, accuracy: 0.51\n",
      "iteration no 22: Loss: 0.8290085579532576, accuracy: 0.51\n",
      "iteration no 23: Loss: 0.8248777802070832, accuracy: 0.51\n",
      "iteration no 24: Loss: 0.8210440430783581, accuracy: 0.51\n",
      "iteration no 25: Loss: 0.8174808535240391, accuracy: 0.51\n",
      "iteration no 26: Loss: 0.8141645455732978, accuracy: 0.51\n",
      "iteration no 27: Loss: 0.8110739329221145, accuracy: 0.51\n",
      "iteration no 28: Loss: 0.8081900090752917, accuracy: 0.5066666666666667\n",
      "iteration no 29: Loss: 0.8054956879968779, accuracy: 0.51\n",
      "iteration no 30: Loss: 0.802975579324939, accuracy: 0.51\n",
      "iteration no 31: Loss: 0.8006157931289665, accuracy: 0.5133333333333333\n",
      "iteration no 32: Loss: 0.7984037699638304, accuracy: 0.5133333333333333\n",
      "iteration no 33: Loss: 0.7963281326259111, accuracy: 0.5133333333333333\n",
      "iteration no 34: Loss: 0.7943785565645483, accuracy: 0.5133333333333333\n",
      "iteration no 35: Loss: 0.7925456563620557, accuracy: 0.5133333333333333\n",
      "iteration no 36: Loss: 0.7908208860824771, accuracy: 0.5133333333333333\n",
      "iteration no 37: Loss: 0.7891964516149871, accuracy: 0.5166666666666667\n",
      "iteration no 38: Loss: 0.787665233412365, accuracy: 0.5166666666666667\n",
      "iteration no 39: Loss: 0.7862207182566985, accuracy: 0.5166666666666667\n",
      "iteration no 40: Loss: 0.7848569388803457, accuracy: 0.5166666666666667\n",
      "iteration no 41: Loss: 0.7835684204360585, accuracy: 0.5166666666666667\n",
      "iteration no 42: Loss: 0.7823501329508549, accuracy: 0.5166666666666667\n",
      "iteration no 43: Loss: 0.7811974490177921, accuracy: 0.5166666666666667\n",
      "iteration no 44: Loss: 0.7801061060815554, accuracy: 0.5166666666666667\n",
      "iteration no 45: Loss: 0.7790721727606004, accuracy: 0.5166666666666667\n",
      "iteration no 46: Loss: 0.7780920187227528, accuracy: 0.5166666666666667\n",
      "iteration no 47: Loss: 0.777162287694682, accuracy: 0.5133333333333333\n",
      "iteration no 48: Loss: 0.7762798732401317, accuracy: 0.5133333333333333\n",
      "iteration no 49: Loss: 0.7754418969886029, accuracy: 0.5133333333333333\n",
      "iteration no 50: Loss: 0.7746456890364875, accuracy: 0.5133333333333333\n",
      "iteration no 51: Loss: 0.7738887702774074, accuracy: 0.5133333333333333\n",
      "iteration no 52: Loss: 0.7731688364485638, accuracy: 0.5133333333333333\n",
      "iteration no 53: Loss: 0.772483743705897, accuracy: 0.5133333333333333\n",
      "iteration no 54: Loss: 0.7718314955634163, accuracy: 0.5133333333333333\n",
      "iteration no 55: Loss: 0.7712102310516543, accuracy: 0.5133333333333333\n",
      "iteration no 56: Loss: 0.770618213967249, accuracy: 0.5133333333333333\n",
      "iteration no 57: Loss: 0.7700538231005287, accuracy: 0.5133333333333333\n",
      "iteration no 58: Loss: 0.7695155433409532, accuracy: 0.5133333333333333\n",
      "iteration no 59: Loss: 0.7690019575716246, accuracy: 0.5166666666666667\n",
      "iteration no 60: Loss: 0.7685117392740254, accuracy: 0.5166666666666667\n",
      "iteration no 61: Loss: 0.7680436457728774, accuracy: 0.5166666666666667\n",
      "iteration no 62: Loss: 0.7675965120586854, accuracy: 0.5166666666666667\n",
      "iteration no 63: Loss: 0.7671692451322855, accuracy: 0.5166666666666667\n",
      "iteration no 64: Loss: 0.766760818821671, accuracy: 0.5166666666666667\n",
      "iteration no 65: Loss: 0.7663702690266309, accuracy: 0.5166666666666667\n",
      "iteration no 66: Loss: 0.7659966893513774, accuracy: 0.5166666666666667\n",
      "iteration no 67: Loss: 0.7656392270894649, accuracy: 0.5166666666666667\n",
      "iteration no 68: Loss: 0.7652970795289409, accuracy: 0.5166666666666667\n",
      "iteration no 69: Loss: 0.7649694905489214, accuracy: 0.5166666666666667\n",
      "iteration no 70: Loss: 0.764655747481656, accuracy: 0.5166666666666667\n",
      "iteration no 71: Loss: 0.7643551782167184, accuracy: 0.5166666666666667\n",
      "iteration no 72: Loss: 0.7640671485262406, accuracy: 0.5166666666666667\n",
      "iteration no 73: Loss: 0.7637910595921552, accuracy: 0.5166666666666667\n",
      "iteration no 74: Loss: 0.7635263457182271, accuracy: 0.5166666666666667\n",
      "iteration no 75: Loss: 0.7632724722112952, accuracy: 0.5166666666666667\n",
      "iteration no 76: Loss: 0.7630289334176027, accuracy: 0.5166666666666667\n",
      "iteration no 77: Loss: 0.7627952509014044, accuracy: 0.5166666666666667\n",
      "iteration no 78: Loss: 0.7625709717542232, accuracy: 0.5166666666666667\n",
      "iteration no 79: Loss: 0.7623556670241779, accuracy: 0.5166666666666667\n",
      "iteration no 80: Loss: 0.762148930255765, accuracy: 0.5166666666666667\n",
      "iteration no 81: Loss: 0.7619503761313297, accuracy: 0.5166666666666667\n",
      "iteration no 82: Loss: 0.7617596392062388, accuracy: 0.5166666666666667\n",
      "iteration no 83: Loss: 0.7615763727304635, accuracy: 0.5166666666666667\n",
      "iteration no 84: Loss: 0.7614002475499161, accuracy: 0.5133333333333333\n",
      "iteration no 85: Loss: 0.7612309510814522, accuracy: 0.5133333333333333\n",
      "iteration no 86: Loss: 0.761068186355969, accuracy: 0.5133333333333333\n",
      "iteration no 87: Loss: 0.7609116711244971, accuracy: 0.5133333333333333\n",
      "iteration no 88: Loss: 0.7607611370226153, accuracy: 0.5133333333333333\n",
      "iteration no 89: Loss: 0.7606163287888935, accuracy: 0.5133333333333333\n",
      "iteration no 90: Loss: 0.7604770035334303, accuracy: 0.5133333333333333\n",
      "iteration no 91: Loss: 0.7603429300528669, accuracy: 0.5133333333333333\n",
      "iteration no 92: Loss: 0.7602138881885467, accuracy: 0.5133333333333333\n",
      "iteration no 93: Loss: 0.7600896682247649, accuracy: 0.5133333333333333\n",
      "iteration no 94: Loss: 0.7599700703242815, accuracy: 0.5133333333333333\n",
      "iteration no 95: Loss: 0.7598549039985059, accuracy: 0.5133333333333333\n",
      "iteration no 96: Loss: 0.7597439876099469, accuracy: 0.5133333333333333\n",
      "iteration no 97: Loss: 0.7596371479047276, accuracy: 0.5133333333333333\n",
      "iteration no 98: Loss: 0.7595342195731084, accuracy: 0.5133333333333333\n",
      "iteration no 99: Loss: 0.7594350448361398, accuracy: 0.5133333333333333\n",
      "iteration no 100: Loss: 0.7593394730566914, accuracy: 0.5133333333333333\n",
      "iteration no 101: Loss: 0.7592473603732427, accuracy: 0.5133333333333333\n",
      "iteration no 102: Loss: 0.7591585693549388, accuracy: 0.5133333333333333\n",
      "iteration no 103: Loss: 0.7590729686765191, accuracy: 0.5166666666666667\n",
      "iteration no 104: Loss: 0.7589904328118374, accuracy: 0.5166666666666667\n",
      "iteration no 105: Loss: 0.7589108417447736, accuracy: 0.5166666666666667\n",
      "iteration no 106: Loss: 0.7588340806964329, accuracy: 0.5166666666666667\n",
      "iteration no 107: Loss: 0.7587600398675968, accuracy: 0.5166666666666667\n",
      "iteration no 108: Loss: 0.758688614195475, accuracy: 0.5166666666666667\n",
      "iteration no 109: Loss: 0.7586197031238598, accuracy: 0.5166666666666667\n",
      "iteration no 110: Loss: 0.7585532103858601, accuracy: 0.5166666666666667\n",
      "iteration no 111: Loss: 0.7584890437984384, accuracy: 0.5166666666666667\n",
      "iteration no 112: Loss: 0.7584271150680353, accuracy: 0.5166666666666667\n",
      "iteration no 113: Loss: 0.7583673396066071, accuracy: 0.5166666666666667\n",
      "iteration no 114: Loss: 0.758309636357454, accuracy: 0.5166666666666667\n",
      "iteration no 115: Loss: 0.7582539276302532, accuracy: 0.5166666666666667\n",
      "iteration no 116: Loss: 0.7582001389447525, accuracy: 0.5166666666666667\n",
      "iteration no 117: Loss: 0.7581481988826168, accuracy: 0.5166666666666667\n",
      "iteration no 118: Loss: 0.7580980389469467, accuracy: 0.5166666666666667\n",
      "iteration no 119: Loss: 0.7580495934290322, accuracy: 0.5166666666666667\n",
      "iteration no 120: Loss: 0.758002799281915, accuracy: 0.5166666666666667\n",
      "iteration no 121: Loss: 0.7579575960003798, accuracy: 0.5166666666666667\n",
      "iteration no 122: Loss: 0.7579139255070021, accuracy: 0.5166666666666667\n",
      "iteration no 123: Loss: 0.7578717320439134, accuracy: 0.5166666666666667\n",
      "iteration no 124: Loss: 0.7578309620699656, accuracy: 0.5166666666666667\n",
      "iteration no 125: Loss: 0.7577915641629871, accuracy: 0.5166666666666667\n",
      "iteration no 126: Loss: 0.7577534889268585, accuracy: 0.5166666666666667\n",
      "iteration no 127: Loss: 0.7577166889031329, accuracy: 0.5166666666666667\n",
      "iteration no 128: Loss: 0.7576811184869611, accuracy: 0.5166666666666667\n",
      "iteration no 129: Loss: 0.7576467338470803, accuracy: 0.5166666666666667\n",
      "iteration no 130: Loss: 0.7576134928496555, accuracy: 0.5166666666666667\n",
      "iteration no 131: Loss: 0.7575813549857583, accuracy: 0.5166666666666667\n",
      "iteration no 132: Loss: 0.7575502813022963, accuracy: 0.5133333333333333\n",
      "iteration no 133: Loss: 0.7575202343362064, accuracy: 0.5133333333333333\n",
      "iteration no 134: Loss: 0.7574911780517422, accuracy: 0.5133333333333333\n",
      "iteration no 135: Loss: 0.7574630777806921, accuracy: 0.5133333333333333\n",
      "iteration no 136: Loss: 0.7574359001653769, accuracy: 0.5133333333333333\n",
      "iteration no 137: Loss: 0.7574096131042829, accuracy: 0.5133333333333333\n",
      "iteration no 138: Loss: 0.757384185700195, accuracy: 0.5133333333333333\n",
      "iteration no 139: Loss: 0.7573595882107009, accuracy: 0.5133333333333333\n",
      "iteration no 140: Loss: 0.7573357920009491, accuracy: 0.5133333333333333\n",
      "iteration no 141: Loss: 0.757312769498543, accuracy: 0.5133333333333333\n",
      "iteration no 142: Loss: 0.7572904941504662, accuracy: 0.5133333333333333\n",
      "iteration no 143: Loss: 0.7572689403819378, accuracy: 0.5133333333333333\n",
      "iteration no 144: Loss: 0.7572480835570994, accuracy: 0.5133333333333333\n",
      "iteration no 145: Loss: 0.7572278999414468, accuracy: 0.5133333333333333\n",
      "iteration no 146: Loss: 0.7572083666659178, accuracy: 0.5133333333333333\n",
      "iteration no 147: Loss: 0.7571894616925562, accuracy: 0.5133333333333333\n",
      "iteration no 148: Loss: 0.7571711637816765, accuracy: 0.5133333333333333\n",
      "iteration no 149: Loss: 0.7571534524604546, accuracy: 0.5133333333333333\n",
      "iteration no 150: Loss: 0.7571363079928785, accuracy: 0.5133333333333333\n",
      "iteration no 151: Loss: 0.7571197113509911, accuracy: 0.5133333333333333\n",
      "iteration no 152: Loss: 0.7571036441873668, accuracy: 0.5133333333333333\n",
      "iteration no 153: Loss: 0.75708808880876, accuracy: 0.5133333333333333\n",
      "iteration no 154: Loss: 0.7570730281508748, accuracy: 0.5133333333333333\n",
      "iteration no 155: Loss: 0.757058445754198, accuracy: 0.5133333333333333\n",
      "iteration no 156: Loss: 0.7570443257408509, accuracy: 0.5133333333333333\n",
      "iteration no 157: Loss: 0.757030652792409, accuracy: 0.5133333333333333\n",
      "iteration no 158: Loss: 0.7570174121286475, accuracy: 0.5133333333333333\n",
      "iteration no 159: Loss: 0.757004589487168, accuracy: 0.5133333333333333\n",
      "iteration no 160: Loss: 0.7569921711038679, accuracy: 0.5133333333333333\n",
      "iteration no 161: Loss: 0.7569801436942132, accuracy: 0.5133333333333333\n",
      "iteration no 162: Loss: 0.7569684944352778, accuracy: 0.5133333333333333\n",
      "iteration no 163: Loss: 0.7569572109485173, accuracy: 0.5133333333333333\n",
      "iteration no 164: Loss: 0.7569462812832414, accuracy: 0.5133333333333333\n",
      "iteration no 165: Loss: 0.7569356939007543, accuracy: 0.5133333333333333\n",
      "iteration no 166: Loss: 0.7569254376591371, accuracy: 0.5133333333333333\n",
      "iteration no 167: Loss: 0.7569155017986364, accuracy: 0.5133333333333333\n",
      "iteration no 168: Loss: 0.7569058759276417, accuracy: 0.5133333333333333\n",
      "iteration no 169: Loss: 0.7568965500092177, accuracy: 0.5133333333333333\n",
      "iteration no 170: Loss: 0.7568875143481715, accuracy: 0.5133333333333333\n",
      "iteration no 171: Loss: 0.7568787595786315, accuracy: 0.5133333333333333\n",
      "iteration no 172: Loss: 0.756870276652114, accuracy: 0.5133333333333333\n",
      "iteration no 173: Loss: 0.7568620568260588, accuracy: 0.5133333333333333\n",
      "iteration no 174: Loss: 0.7568540916528101, accuracy: 0.5133333333333333\n",
      "iteration no 175: Loss: 0.7568463729690291, accuracy: 0.5133333333333333\n",
      "iteration no 176: Loss: 0.7568388928855154, accuracy: 0.5133333333333333\n",
      "iteration no 177: Loss: 0.7568316437774205, accuracy: 0.5133333333333333\n",
      "iteration no 178: Loss: 0.7568246182748399, accuracy: 0.5133333333333333\n",
      "iteration no 179: Loss: 0.7568178092537657, accuracy: 0.5133333333333333\n",
      "iteration no 180: Loss: 0.7568112098273829, accuracy: 0.5133333333333333\n",
      "iteration no 181: Loss: 0.7568048133376982, accuracy: 0.5133333333333333\n",
      "iteration no 182: Loss: 0.7567986133474881, accuracy: 0.5133333333333333\n",
      "iteration no 183: Loss: 0.7567926036325483, accuracy: 0.5133333333333333\n",
      "iteration no 184: Loss: 0.7567867781742378, accuracy: 0.5133333333333333\n",
      "iteration no 185: Loss: 0.7567811311523016, accuracy: 0.5133333333333333\n",
      "iteration no 186: Loss: 0.756775656937962, accuracy: 0.5133333333333333\n",
      "iteration no 187: Loss: 0.7567703500872689, accuracy: 0.5133333333333333\n",
      "iteration no 188: Loss: 0.7567652053346956, accuracy: 0.5133333333333333\n",
      "iteration no 189: Loss: 0.756760217586974, accuracy: 0.5133333333333333\n",
      "iteration no 190: Loss: 0.7567553819171554, accuracy: 0.5133333333333333\n",
      "iteration no 191: Loss: 0.7567506935588921, accuracy: 0.5133333333333333\n",
      "iteration no 192: Loss: 0.7567461479009272, accuracy: 0.5133333333333333\n",
      "iteration no 193: Loss: 0.756741740481788, accuracy: 0.5133333333333333\n",
      "iteration no 194: Loss: 0.7567374669846706, accuracy: 0.5133333333333333\n",
      "iteration no 195: Loss: 0.7567333232325135, accuracy: 0.5133333333333333\n",
      "iteration no 196: Loss: 0.7567293051832477, accuracy: 0.5133333333333333\n",
      "iteration no 197: Loss: 0.756725408925219, accuracy: 0.5133333333333333\n",
      "iteration no 198: Loss: 0.7567216306727769, accuracy: 0.5133333333333333\n",
      "iteration no 199: Loss: 0.75671796676202, accuracy: 0.5133333333333333\n",
      "iteration no 200: Loss: 0.756714413646695, accuracy: 0.5133333333333333\n",
      "iteration no 1: Loss: 1.1088171748176059, accuracy: 0.43333333333333335\n",
      "iteration no 2: Loss: 1.0770166853657448, accuracy: 0.48333333333333334\n",
      "iteration no 3: Loss: 1.0487600923820304, accuracy: 0.49666666666666665\n",
      "iteration no 4: Loss: 1.023602959403946, accuracy: 0.5\n",
      "iteration no 5: Loss: 1.0011628476081846, accuracy: 0.5\n",
      "iteration no 6: Loss: 0.9811064832527318, accuracy: 0.49666666666666665\n",
      "iteration no 7: Loss: 0.9631425266428812, accuracy: 0.5\n",
      "iteration no 8: Loss: 0.9470165044623324, accuracy: 0.49666666666666665\n",
      "iteration no 9: Loss: 0.9325065740352434, accuracy: 0.5\n",
      "iteration no 10: Loss: 0.9194196812424084, accuracy: 0.5\n",
      "iteration no 11: Loss: 0.907588014011201, accuracy: 0.5033333333333333\n",
      "iteration no 12: Loss: 0.8968657576612238, accuracy: 0.5066666666666667\n",
      "iteration no 13: Loss: 0.8871261726562455, accuracy: 0.5066666666666667\n",
      "iteration no 14: Loss: 0.8782590021321895, accuracy: 0.5066666666666667\n",
      "iteration no 15: Loss: 0.870168200125826, accuracy: 0.5166666666666667\n",
      "iteration no 16: Loss: 0.8627699592584782, accuracy: 0.5166666666666667\n",
      "iteration no 17: Loss: 0.8559910097684953, accuracy: 0.5166666666666667\n",
      "iteration no 18: Loss: 0.849767159178074, accuracy: 0.5166666666666667\n",
      "iteration no 19: Loss: 0.8440420421166769, accuracy: 0.5133333333333333\n",
      "iteration no 20: Loss: 0.8387660517252997, accuracy: 0.5133333333333333\n",
      "iteration no 21: Loss: 0.8338954267960103, accuracy: 0.5133333333333333\n",
      "iteration no 22: Loss: 0.8293914718208073, accuracy: 0.5133333333333333\n",
      "iteration no 23: Loss: 0.8252198901135328, accuracy: 0.5133333333333333\n",
      "iteration no 24: Loss: 0.8213502129559315, accuracy: 0.5133333333333333\n",
      "iteration no 25: Loss: 0.8177553102244827, accuracy: 0.5133333333333333\n",
      "iteration no 26: Loss: 0.8144109701542566, accuracy: 0.5133333333333333\n",
      "iteration no 27: Loss: 0.811295537796836, accuracy: 0.5133333333333333\n",
      "iteration no 28: Loss: 0.808389603354313, accuracy: 0.5166666666666667\n",
      "iteration no 29: Loss: 0.805675732950415, accuracy: 0.5166666666666667\n",
      "iteration no 30: Loss: 0.803138235564364, accuracy: 0.5166666666666667\n",
      "iteration no 31: Loss: 0.8007629608333482, accuracy: 0.5166666666666667\n",
      "iteration no 32: Loss: 0.798537123253044, accuracy: 0.5166666666666667\n",
      "iteration no 33: Loss: 0.7964491489968285, accuracy: 0.5166666666666667\n",
      "iteration no 34: Loss: 0.7944885421542659, accuracy: 0.5166666666666667\n",
      "iteration no 35: Loss: 0.7926457676761853, accuracy: 0.5166666666666667\n",
      "iteration no 36: Loss: 0.7909121487224285, accuracy: 0.5166666666666667\n",
      "iteration no 37: Loss: 0.7892797764519898, accuracy: 0.5166666666666667\n",
      "iteration no 38: Loss: 0.787741430584496, accuracy: 0.5166666666666667\n",
      "iteration no 39: Loss: 0.7862905093057946, accuracy: 0.5166666666666667\n",
      "iteration no 40: Loss: 0.78492096729623, accuracy: 0.5166666666666667\n",
      "iteration no 41: Loss: 0.7836272608342618, accuracy: 0.5166666666666667\n",
      "iteration no 42: Loss: 0.7824042990755325, accuracy: 0.5166666666666667\n",
      "iteration no 43: Loss: 0.7812474007326446, accuracy: 0.5166666666666667\n",
      "iteration no 44: Loss: 0.780152255487322, accuracy: 0.5166666666666667\n",
      "iteration no 45: Loss: 0.7791148895572908, accuracy: 0.5166666666666667\n",
      "iteration no 46: Loss: 0.7781316349175953, accuracy: 0.5166666666666667\n",
      "iteration no 47: Loss: 0.7771991017422373, accuracy: 0.5133333333333333\n",
      "iteration no 48: Loss: 0.7763141536887374, accuracy: 0.5133333333333333\n",
      "iteration no 49: Loss: 0.7754738856968848, accuracy: 0.5133333333333333\n",
      "iteration no 50: Loss: 0.774675604014819, accuracy: 0.5133333333333333\n",
      "iteration no 51: Loss: 0.7739168082016558, accuracy: 0.5133333333333333\n",
      "iteration no 52: Loss: 0.7731951748870296, accuracy: 0.5133333333333333\n",
      "iteration no 53: Loss: 0.772508543094856, accuracy: 0.5133333333333333\n",
      "iteration no 54: Loss: 0.771854900961965, accuracy: 0.5133333333333333\n",
      "iteration no 55: Loss: 0.7712323737025243, accuracy: 0.5133333333333333\n",
      "iteration no 56: Loss: 0.7706392126867856, accuracy: 0.5133333333333333\n",
      "iteration no 57: Loss: 0.770073785518044, accuracy: 0.5133333333333333\n",
      "iteration no 58: Loss: 0.76953456700509, accuracy: 0.5133333333333333\n",
      "iteration no 59: Loss: 0.7690201309391418, accuracy: 0.5133333333333333\n",
      "iteration no 60: Loss: 0.7685291425944969, accuracy: 0.5166666666666667\n",
      "iteration no 61: Loss: 0.7680603518811266, accuracy: 0.5166666666666667\n",
      "iteration no 62: Loss: 0.7676125870853334, accuracy: 0.5166666666666667\n",
      "iteration no 63: Loss: 0.767184749141535, accuracy: 0.5166666666666667\n",
      "iteration no 64: Loss: 0.7667758063843538, accuracy: 0.5166666666666667\n",
      "iteration no 65: Loss: 0.7663847897355937, accuracy: 0.5166666666666667\n",
      "iteration no 66: Loss: 0.7660107882854499, accuracy: 0.5166666666666667\n",
      "iteration no 67: Loss: 0.7656529452315254, accuracy: 0.5166666666666667\n",
      "iteration no 68: Loss: 0.7653104541429603, accuracy: 0.5166666666666667\n",
      "iteration no 69: Loss: 0.7649825555203068, accuracy: 0.5166666666666667\n",
      "iteration no 70: Loss: 0.7646685336247263, accuracy: 0.5166666666666667\n",
      "iteration no 71: Loss: 0.7643677135527146, accuracy: 0.5166666666666667\n",
      "iteration no 72: Loss: 0.7640794585348962, accuracy: 0.5166666666666667\n",
      "iteration no 73: Loss: 0.7638031674395136, accuracy: 0.5166666666666667\n",
      "iteration no 74: Loss: 0.7635382724631078, accuracy: 0.5166666666666667\n",
      "iteration no 75: Loss: 0.7632842369925422, accuracy: 0.5166666666666667\n",
      "iteration no 76: Loss: 0.7630405536240231, accuracy: 0.5166666666666667\n",
      "iteration no 77: Loss: 0.7628067423261047, accuracy: 0.5166666666666667\n",
      "iteration no 78: Loss: 0.7625823487348631, accuracy: 0.5166666666666667\n",
      "iteration no 79: Loss: 0.7623669425705117, accuracy: 0.5166666666666667\n",
      "iteration no 80: Loss: 0.7621601161656896, accuracy: 0.5166666666666667\n",
      "iteration no 81: Loss: 0.7619614830965395, accuracy: 0.5166666666666667\n",
      "iteration no 82: Loss: 0.7617706769084706, accuracy: 0.5166666666666667\n",
      "iteration no 83: Loss: 0.7615873499292178, accuracy: 0.5166666666666667\n",
      "iteration no 84: Loss: 0.7614111721624518, accuracy: 0.5133333333333333\n",
      "iteration no 85: Loss: 0.7612418302557697, accuracy: 0.5133333333333333\n",
      "iteration no 86: Loss: 0.7610790265374281, accuracy: 0.5133333333333333\n",
      "iteration no 87: Loss: 0.7609224781166547, accuracy: 0.5133333333333333\n",
      "iteration no 88: Loss: 0.7607719160428047, accuracy: 0.5133333333333333\n",
      "iteration no 89: Loss: 0.7606270845190224, accuracy: 0.5133333333333333\n",
      "iteration no 90: Loss: 0.7604877401664268, accuracy: 0.5133333333333333\n",
      "iteration no 91: Loss: 0.7603536513351599, accuracy: 0.5133333333333333\n",
      "iteration no 92: Loss: 0.7602245974589347, accuracy: 0.5133333333333333\n",
      "iteration no 93: Loss: 0.7601003684499889, accuracy: 0.5133333333333333\n",
      "iteration no 94: Loss: 0.7599807641315938, accuracy: 0.5133333333333333\n",
      "iteration no 95: Loss: 0.7598655937054921, accuracy: 0.5133333333333333\n",
      "iteration no 96: Loss: 0.7597546752518459, accuracy: 0.5133333333333333\n",
      "iteration no 97: Loss: 0.759647835259457, accuracy: 0.5133333333333333\n",
      "iteration no 98: Loss: 0.7595449081841995, accuracy: 0.5133333333333333\n",
      "iteration no 99: Loss: 0.7594457360337554, accuracy: 0.5133333333333333\n",
      "iteration no 100: Loss: 0.7593501679768925, accuracy: 0.5133333333333333\n",
      "iteration no 101: Loss: 0.7592580599756504, accuracy: 0.5133333333333333\n",
      "iteration no 102: Loss: 0.7591692744389255, accuracy: 0.5133333333333333\n",
      "iteration no 103: Loss: 0.7590836798960559, accuracy: 0.5166666666666667\n",
      "iteration no 104: Loss: 0.7590011506891053, accuracy: 0.5166666666666667\n",
      "iteration no 105: Loss: 0.7589215666826448, accuracy: 0.5166666666666667\n",
      "iteration no 106: Loss: 0.7588448129899125, accuracy: 0.5166666666666667\n",
      "iteration no 107: Loss: 0.7587707797143151, accuracy: 0.5166666666666667\n",
      "iteration no 108: Loss: 0.7586993617053027, accuracy: 0.5166666666666667\n",
      "iteration no 109: Loss: 0.7586304583277234, accuracy: 0.5166666666666667\n",
      "iteration no 110: Loss: 0.7585639732438186, accuracy: 0.5166666666666667\n",
      "iteration no 111: Loss: 0.7584998142070855, accuracy: 0.5166666666666667\n",
      "iteration no 112: Loss: 0.7584378928672771, accuracy: 0.5166666666666667\n",
      "iteration no 113: Loss: 0.7583781245858708, accuracy: 0.5166666666666667\n",
      "iteration no 114: Loss: 0.75832042826137, accuracy: 0.5166666666666667\n",
      "iteration no 115: Loss: 0.7582647261638549, accuracy: 0.5166666666666667\n",
      "iteration no 116: Loss: 0.7582109437782323, accuracy: 0.5166666666666667\n",
      "iteration no 117: Loss: 0.7581590096556736, accuracy: 0.5166666666666667\n",
      "iteration no 118: Loss: 0.7581088552727587, accuracy: 0.5166666666666667\n",
      "iteration no 119: Loss: 0.7580604148978833, accuracy: 0.5166666666666667\n",
      "iteration no 120: Loss: 0.7580136254645052, accuracy: 0.5166666666666667\n",
      "iteration no 121: Loss: 0.7579684264508422, accuracy: 0.5166666666666667\n",
      "iteration no 122: Loss: 0.7579247597656513, accuracy: 0.5166666666666667\n",
      "iteration no 123: Loss: 0.7578825696397488, accuracy: 0.5166666666666667\n",
      "iteration no 124: Loss: 0.7578418025229435, accuracy: 0.5166666666666667\n",
      "iteration no 125: Loss: 0.7578024069860883, accuracy: 0.5166666666666667\n",
      "iteration no 126: Loss: 0.75776433362796, accuracy: 0.5166666666666667\n",
      "iteration no 127: Loss: 0.7577275349867048, accuracy: 0.5166666666666667\n",
      "iteration no 128: Loss: 0.7576919654555998, accuracy: 0.5166666666666667\n",
      "iteration no 129: Loss: 0.7576575812028938, accuracy: 0.5133333333333333\n",
      "iteration no 130: Loss: 0.7576243400955092, accuracy: 0.5133333333333333\n",
      "iteration no 131: Loss: 0.7575922016263966, accuracy: 0.5133333333333333\n",
      "iteration no 132: Loss: 0.7575611268453475, accuracy: 0.5133333333333333\n",
      "iteration no 133: Loss: 0.7575310782930809, accuracy: 0.5133333333333333\n",
      "iteration no 134: Loss: 0.7575020199384334, accuracy: 0.5133333333333333\n",
      "iteration no 135: Loss: 0.7574739171184875, accuracy: 0.5133333333333333\n",
      "iteration no 136: Loss: 0.7574467364814874, accuracy: 0.5133333333333333\n",
      "iteration no 137: Loss: 0.7574204459323957, accuracy: 0.5133333333333333\n",
      "iteration no 138: Loss: 0.757395014580958, accuracy: 0.5133333333333333\n",
      "iteration no 139: Loss: 0.7573704126921453, accuracy: 0.5133333333333333\n",
      "iteration no 140: Loss: 0.7573466116388514, accuracy: 0.5133333333333333\n",
      "iteration no 141: Loss: 0.7573235838567365, accuracy: 0.5133333333333333\n",
      "iteration no 142: Loss: 0.7573013028011022, accuracy: 0.5133333333333333\n",
      "iteration no 143: Loss: 0.7572797429057049, accuracy: 0.5133333333333333\n",
      "iteration no 144: Loss: 0.7572588795434004, accuracy: 0.5133333333333333\n",
      "iteration no 145: Loss: 0.7572386889885412, accuracy: 0.5133333333333333\n",
      "iteration no 146: Loss: 0.757219148381028, accuracy: 0.5133333333333333\n",
      "iteration no 147: Loss: 0.7572002356919474, accuracy: 0.5133333333333333\n",
      "iteration no 148: Loss: 0.7571819296907051, accuracy: 0.5133333333333333\n",
      "iteration no 149: Loss: 0.7571642099135945, accuracy: 0.5133333333333333\n",
      "iteration no 150: Loss: 0.7571470566337237, accuracy: 0.5133333333333333\n",
      "iteration no 151: Loss: 0.7571304508322385, accuracy: 0.5133333333333333\n",
      "iteration no 152: Loss: 0.7571143741707803, accuracy: 0.5133333333333333\n",
      "iteration no 153: Loss: 0.757098808965119, accuracy: 0.5133333333333333\n",
      "iteration no 154: Loss: 0.7570837381599069, accuracy: 0.5133333333333333\n",
      "iteration no 155: Loss: 0.7570691453044994, accuracy: 0.5133333333333333\n",
      "iteration no 156: Loss: 0.757055014529795, accuracy: 0.5133333333333333\n",
      "iteration no 157: Loss: 0.7570413305260443, accuracy: 0.5133333333333333\n",
      "iteration no 158: Loss: 0.7570280785215868, accuracy: 0.5133333333333333\n",
      "iteration no 159: Loss: 0.7570152442624699, accuracy: 0.5133333333333333\n",
      "iteration no 160: Loss: 0.7570028139929103, accuracy: 0.5133333333333333\n",
      "iteration no 161: Loss: 0.7569907744365616, accuracy: 0.5133333333333333\n",
      "iteration no 162: Loss: 0.7569791127785492, accuracy: 0.5133333333333333\n",
      "iteration no 163: Loss: 0.7569678166482376, accuracy: 0.5133333333333333\n",
      "iteration no 164: Loss: 0.7569568741027011, accuracy: 0.5133333333333333\n",
      "iteration no 165: Loss: 0.7569462736108605, accuracy: 0.5133333333333333\n",
      "iteration no 166: Loss: 0.7569360040382633, accuracy: 0.5133333333333333\n",
      "iteration no 167: Loss: 0.7569260546324701, accuracy: 0.5133333333333333\n",
      "iteration no 168: Loss: 0.7569164150090315, accuracy: 0.5133333333333333\n",
      "iteration no 169: Loss: 0.7569070751380181, accuracy: 0.5133333333333333\n",
      "iteration no 170: Loss: 0.7568980253310896, accuracy: 0.5133333333333333\n",
      "iteration no 171: Loss: 0.7568892562290713, accuracy: 0.5133333333333333\n",
      "iteration no 172: Loss: 0.7568807587900221, accuracy: 0.5133333333333333\n",
      "iteration no 173: Loss: 0.7568725242777704, accuracy: 0.5133333333333333\n",
      "iteration no 174: Loss: 0.7568645442508959, accuracy: 0.5133333333333333\n",
      "iteration no 175: Loss: 0.7568568105521437, accuracy: 0.5133333333333333\n",
      "iteration no 176: Loss: 0.7568493152982461, accuracy: 0.5133333333333333\n",
      "iteration no 177: Loss: 0.7568420508701387, accuracy: 0.5133333333333333\n",
      "iteration no 178: Loss: 0.7568350099035536, accuracy: 0.5133333333333333\n",
      "iteration no 179: Loss: 0.7568281852799731, accuracy: 0.5133333333333333\n",
      "iteration no 180: Loss: 0.7568215701179289, accuracy: 0.5133333333333333\n",
      "iteration no 181: Loss: 0.756815157764634, accuracy: 0.5133333333333333\n",
      "iteration no 182: Loss: 0.7568089417879308, accuracy: 0.5133333333333333\n",
      "iteration no 183: Loss: 0.7568029159685441, accuracy: 0.5133333333333333\n",
      "iteration no 184: Loss: 0.7567970742926268, accuracy: 0.5133333333333333\n",
      "iteration no 185: Loss: 0.756791410944586, accuracy: 0.5133333333333333\n",
      "iteration no 186: Loss: 0.7567859203001762, accuracy: 0.5133333333333333\n",
      "iteration no 187: Loss: 0.7567805969198519, accuracy: 0.5133333333333333\n",
      "iteration no 188: Loss: 0.7567754355423666, accuracy: 0.5133333333333333\n",
      "iteration no 189: Loss: 0.7567704310786096, accuracy: 0.5133333333333333\n",
      "iteration no 190: Loss: 0.7567655786056705, accuracy: 0.5133333333333333\n",
      "iteration no 191: Loss: 0.7567608733611224, accuracy: 0.5133333333333333\n",
      "iteration no 192: Loss: 0.7567563107375151, accuracy: 0.5133333333333333\n",
      "iteration no 193: Loss: 0.7567518862770702, accuracy: 0.5133333333333333\n",
      "iteration no 194: Loss: 0.7567475956665696, accuracy: 0.5133333333333333\n",
      "iteration no 195: Loss: 0.7567434347324304, accuracy: 0.5133333333333333\n",
      "iteration no 196: Loss: 0.7567393994359575, accuracy: 0.5133333333333333\n",
      "iteration no 197: Loss: 0.7567354858687704, accuracy: 0.5133333333333333\n",
      "iteration no 198: Loss: 0.7567316902483915, accuracy: 0.5133333333333333\n",
      "iteration no 199: Loss: 0.7567280089139968, accuracy: 0.5133333333333333\n",
      "iteration no 200: Loss: 0.7567244383223161, accuracy: 0.5133333333333333\n",
      "iteration no 1: Loss: 1.1098418532559498, accuracy: 0.36333333333333334\n",
      "iteration no 2: Loss: 1.0777740823216424, accuracy: 0.4766666666666667\n",
      "iteration no 3: Loss: 1.0492856263908668, accuracy: 0.49\n",
      "iteration no 4: Loss: 1.0239392147816557, accuracy: 0.49\n",
      "iteration no 5: Loss: 1.0013489153866824, accuracy: 0.49333333333333335\n",
      "iteration no 6: Loss: 0.981174856766769, accuracy: 0.5\n",
      "iteration no 7: Loss: 0.9631190375130331, accuracy: 0.5\n",
      "iteration no 8: Loss: 0.9469212877197268, accuracy: 0.5033333333333333\n",
      "iteration no 9: Loss: 0.9323552247287877, accuracy: 0.5133333333333333\n",
      "iteration no 10: Loss: 0.9192242945144432, accuracy: 0.5233333333333333\n",
      "iteration no 11: Loss: 0.9073580263464956, accuracy: 0.5233333333333333\n",
      "iteration no 12: Loss: 0.8966085943983363, accuracy: 0.5233333333333333\n",
      "iteration no 13: Loss: 0.8868477343545577, accuracy: 0.5233333333333333\n",
      "iteration no 14: Loss: 0.8779640259034255, accuracy: 0.5233333333333333\n",
      "iteration no 15: Loss: 0.8698605272645874, accuracy: 0.5166666666666667\n",
      "iteration no 16: Loss: 0.8624527338823144, accuracy: 0.5166666666666667\n",
      "iteration no 17: Loss: 0.8556668270667358, accuracy: 0.5166666666666667\n",
      "iteration no 18: Loss: 0.8494381769313936, accuracy: 0.5166666666666667\n",
      "iteration no 19: Loss: 0.8437100653972914, accuracy: 0.5133333333333333\n",
      "iteration no 20: Loss: 0.8384325979256461, accuracy: 0.5133333333333333\n",
      "iteration no 21: Loss: 0.833561776135037, accuracy: 0.5166666666666667\n",
      "iteration no 22: Loss: 0.8290587070433753, accuracy: 0.5166666666666667\n",
      "iteration no 23: Loss: 0.8248889280740165, accuracy: 0.5166666666666667\n",
      "iteration no 24: Loss: 0.8210218300454992, accuracy: 0.5166666666666667\n",
      "iteration no 25: Loss: 0.8174301630781534, accuracy: 0.5166666666666667\n",
      "iteration no 26: Loss: 0.8140896126981696, accuracy: 0.5166666666666667\n",
      "iteration no 27: Loss: 0.8109784354253349, accuracy: 0.5166666666666667\n",
      "iteration no 28: Loss: 0.8080771448301172, accuracy: 0.5166666666666667\n",
      "iteration no 29: Loss: 0.8053682404779405, accuracy: 0.5166666666666667\n",
      "iteration no 30: Loss: 0.8028359733810678, accuracy: 0.52\n",
      "iteration no 31: Loss: 0.8004661425861354, accuracy: 0.52\n",
      "iteration no 32: Loss: 0.7982459183687112, accuracy: 0.5166666666666667\n",
      "iteration no 33: Loss: 0.7961636882118241, accuracy: 0.5166666666666667\n",
      "iteration no 34: Loss: 0.7942089223358891, accuracy: 0.5166666666666667\n",
      "iteration no 35: Loss: 0.7923720560419308, accuracy: 0.5166666666666667\n",
      "iteration no 36: Loss: 0.7906443865445317, accuracy: 0.5166666666666667\n",
      "iteration no 37: Loss: 0.7890179823188667, accuracy: 0.5166666666666667\n",
      "iteration no 38: Loss: 0.7874856032786701, accuracy: 0.5166666666666667\n",
      "iteration no 39: Loss: 0.7860406303482438, accuracy: 0.5166666666666667\n",
      "iteration no 40: Loss: 0.7846770031993373, accuracy: 0.5166666666666667\n",
      "iteration no 41: Loss: 0.7833891650992478, accuracy: 0.5166666666666667\n",
      "iteration no 42: Loss: 0.7821720139650864, accuracy: 0.5166666666666667\n",
      "iteration no 43: Loss: 0.7810208588452102, accuracy: 0.5166666666666667\n",
      "iteration no 44: Loss: 0.7799313811559355, accuracy: 0.5166666666666667\n",
      "iteration no 45: Loss: 0.7788996000928884, accuracy: 0.5166666666666667\n",
      "iteration no 46: Loss: 0.7779218417141813, accuracy: 0.5166666666666667\n",
      "iteration no 47: Loss: 0.776994711259165, accuracy: 0.5166666666666667\n",
      "iteration no 48: Loss: 0.7761150683235054, accuracy: 0.5166666666666667\n",
      "iteration no 49: Loss: 0.7752800045602811, accuracy: 0.5166666666666667\n",
      "iteration no 50: Loss: 0.7744868236188678, accuracy: 0.5166666666666667\n",
      "iteration no 51: Loss: 0.7737330230696392, accuracy: 0.5166666666666667\n",
      "iteration no 52: Loss: 0.7730162780938151, accuracy: 0.5166666666666667\n",
      "iteration no 53: Loss: 0.7723344267448546, accuracy: 0.5166666666666667\n",
      "iteration no 54: Loss: 0.771685456611248, accuracy: 0.5133333333333333\n",
      "iteration no 55: Loss: 0.7710674927309222, accuracy: 0.5133333333333333\n",
      "iteration no 56: Loss: 0.7704787866251755, accuracy: 0.5133333333333333\n",
      "iteration no 57: Loss: 0.7699177063354806, accuracy: 0.5133333333333333\n",
      "iteration no 58: Loss: 0.7693827273599485, accuracy: 0.5133333333333333\n",
      "iteration no 59: Loss: 0.768872424398012, accuracy: 0.5133333333333333\n",
      "iteration no 60: Loss: 0.7683854638221774, accuracy: 0.5133333333333333\n",
      "iteration no 61: Loss: 0.7679205968047301, accuracy: 0.5133333333333333\n",
      "iteration no 62: Loss: 0.7674766530352035, accuracy: 0.5133333333333333\n",
      "iteration no 63: Loss: 0.767052534971404, accuracy: 0.5133333333333333\n",
      "iteration no 64: Loss: 0.7666472125729248, accuracy: 0.5133333333333333\n",
      "iteration no 65: Loss: 0.7662597184715122, accuracy: 0.5166666666666667\n",
      "iteration no 66: Loss: 0.7658891435374308, accuracy: 0.5166666666666667\n",
      "iteration no 67: Loss: 0.7655346328052268, accuracy: 0.5166666666666667\n",
      "iteration no 68: Loss: 0.7651953817260376, accuracy: 0.5166666666666667\n",
      "iteration no 69: Loss: 0.7648706327169353, accuracy: 0.5166666666666667\n",
      "iteration no 70: Loss: 0.7645596719807563, accuracy: 0.5166666666666667\n",
      "iteration no 71: Loss: 0.7642618265725032, accuracy: 0.5166666666666667\n",
      "iteration no 72: Loss: 0.7639764616907592, accuracy: 0.5166666666666667\n",
      "iteration no 73: Loss: 0.7637029781746489, accuracy: 0.5166666666666667\n",
      "iteration no 74: Loss: 0.7634408101887513, accuracy: 0.5166666666666667\n",
      "iteration no 75: Loss: 0.7631894230800468, accuracy: 0.5166666666666667\n",
      "iteration no 76: Loss: 0.7629483113924825, accuracy: 0.5166666666666667\n",
      "iteration no 77: Loss: 0.7627169970260773, accuracy: 0.5166666666666667\n",
      "iteration no 78: Loss: 0.762495027528701, accuracy: 0.5166666666666667\n",
      "iteration no 79: Loss: 0.7622819745097446, accuracy: 0.5166666666666667\n",
      "iteration no 80: Loss: 0.7620774321658721, accuracy: 0.5166666666666667\n",
      "iteration no 81: Loss: 0.761881015909925, accuracy: 0.5133333333333333\n",
      "iteration no 82: Loss: 0.7616923610948404, accuracy: 0.5133333333333333\n",
      "iteration no 83: Loss: 0.7615111218251613, accuracy: 0.5133333333333333\n",
      "iteration no 84: Loss: 0.7613369698493588, accuracy: 0.5133333333333333\n",
      "iteration no 85: Loss: 0.7611695935267727, accuracy: 0.5133333333333333\n",
      "iteration no 86: Loss: 0.7610086968635074, accuracy: 0.5133333333333333\n",
      "iteration no 87: Loss: 0.7608539986120932, accuracy: 0.5133333333333333\n",
      "iteration no 88: Loss: 0.7607052314301618, accuracy: 0.5133333333333333\n",
      "iteration no 89: Loss: 0.7605621410937787, accuracy: 0.5133333333333333\n",
      "iteration no 90: Loss: 0.760424485761431, accuracy: 0.5133333333333333\n",
      "iteration no 91: Loss: 0.7602920352849989, accuracy: 0.5133333333333333\n",
      "iteration no 92: Loss: 0.7601645705643311, accuracy: 0.5133333333333333\n",
      "iteration no 93: Loss: 0.7600418829423186, accuracy: 0.5133333333333333\n",
      "iteration no 94: Loss: 0.759923773637606, accuracy: 0.5133333333333333\n",
      "iteration no 95: Loss: 0.7598100532123041, accuracy: 0.5133333333333333\n",
      "iteration no 96: Loss: 0.7597005410722744, accuracy: 0.5133333333333333\n",
      "iteration no 97: Loss: 0.7595950649977409, accuracy: 0.5133333333333333\n",
      "iteration no 98: Loss: 0.7594934607021621, accuracy: 0.5133333333333333\n",
      "iteration no 99: Loss: 0.7593955714174474, accuracy: 0.5133333333333333\n",
      "iteration no 100: Loss: 0.7593012475037483, accuracy: 0.5133333333333333\n",
      "iteration no 101: Loss: 0.7592103460821891, accuracy: 0.5133333333333333\n",
      "iteration no 102: Loss: 0.7591227306890215, accuracy: 0.5133333333333333\n",
      "iteration no 103: Loss: 0.7590382709497971, accuracy: 0.5133333333333333\n",
      "iteration no 104: Loss: 0.7589568422722573, accuracy: 0.5133333333333333\n",
      "iteration no 105: Loss: 0.7588783255567341, accuracy: 0.5166666666666667\n",
      "iteration no 106: Loss: 0.7588026069229368, accuracy: 0.5166666666666667\n",
      "iteration no 107: Loss: 0.7587295774520894, accuracy: 0.5166666666666667\n",
      "iteration no 108: Loss: 0.7586591329434447, accuracy: 0.5166666666666667\n",
      "iteration no 109: Loss: 0.758591173684281, accuracy: 0.5166666666666667\n",
      "iteration no 110: Loss: 0.7585256042325397, accuracy: 0.5166666666666667\n",
      "iteration no 111: Loss: 0.7584623332113277, accuracy: 0.5166666666666667\n",
      "iteration no 112: Loss: 0.758401273114558, accuracy: 0.5166666666666667\n",
      "iteration no 113: Loss: 0.7583423401230479, accuracy: 0.5166666666666667\n",
      "iteration no 114: Loss: 0.7582854539304519, accuracy: 0.5166666666666667\n",
      "iteration no 115: Loss: 0.7582305375784325, accuracy: 0.5166666666666667\n",
      "iteration no 116: Loss: 0.7581775173005246, accuracy: 0.5166666666666667\n",
      "iteration no 117: Loss: 0.7581263223741787, accuracy: 0.5166666666666667\n",
      "iteration no 118: Loss: 0.7580768849805021, accuracy: 0.5166666666666667\n",
      "iteration no 119: Loss: 0.7580291400712533, accuracy: 0.5166666666666667\n",
      "iteration no 120: Loss: 0.757983025242666, accuracy: 0.5166666666666667\n",
      "iteration no 121: Loss: 0.7579384806157133, accuracy: 0.5166666666666667\n",
      "iteration no 122: Loss: 0.7578954487224442, accuracy: 0.5166666666666667\n",
      "iteration no 123: Loss: 0.7578538743980476, accuracy: 0.5166666666666667\n",
      "iteration no 124: Loss: 0.7578137046783209, accuracy: 0.5166666666666667\n",
      "iteration no 125: Loss: 0.7577748887022433, accuracy: 0.5166666666666667\n",
      "iteration no 126: Loss: 0.7577373776193672, accuracy: 0.5166666666666667\n",
      "iteration no 127: Loss: 0.7577011245017639, accuracy: 0.5166666666666667\n",
      "iteration no 128: Loss: 0.7576660842602718, accuracy: 0.5166666666666667\n",
      "iteration no 129: Loss: 0.757632213564816, accuracy: 0.5166666666666667\n",
      "iteration no 130: Loss: 0.7575994707685735, accuracy: 0.5133333333333333\n",
      "iteration no 131: Loss: 0.7575678158357821, accuracy: 0.5133333333333333\n",
      "iteration no 132: Loss: 0.7575372102729957, accuracy: 0.5133333333333333\n",
      "iteration no 133: Loss: 0.7575076170636014, accuracy: 0.5133333333333333\n",
      "iteration no 134: Loss: 0.7574790006054297, accuracy: 0.5133333333333333\n",
      "iteration no 135: Loss: 0.7574513266512919, accuracy: 0.5133333333333333\n",
      "iteration no 136: Loss: 0.7574245622522938, accuracy: 0.5133333333333333\n",
      "iteration no 137: Loss: 0.75739867570378, accuracy: 0.5133333333333333\n",
      "iteration no 138: Loss: 0.7573736364937752, accuracy: 0.5133333333333333\n",
      "iteration no 139: Loss: 0.7573494152537923, accuracy: 0.5133333333333333\n",
      "iteration no 140: Loss: 0.757325983711888, accuracy: 0.5133333333333333\n",
      "iteration no 141: Loss: 0.7573033146478513, accuracy: 0.5133333333333333\n",
      "iteration no 142: Loss: 0.7572813818504167, accuracy: 0.5133333333333333\n",
      "iteration no 143: Loss: 0.7572601600764027, accuracy: 0.5133333333333333\n",
      "iteration no 144: Loss: 0.7572396250116762, accuracy: 0.5133333333333333\n",
      "iteration no 145: Loss: 0.7572197532338569, accuracy: 0.5133333333333333\n",
      "iteration no 146: Loss: 0.7572005221766707, accuracy: 0.5133333333333333\n",
      "iteration no 147: Loss: 0.7571819100958753, accuracy: 0.5133333333333333\n",
      "iteration no 148: Loss: 0.7571638960366793, accuracy: 0.5133333333333333\n",
      "iteration no 149: Loss: 0.7571464598025829, accuracy: 0.5133333333333333\n",
      "iteration no 150: Loss: 0.7571295819255709, accuracy: 0.5133333333333333\n",
      "iteration no 151: Loss: 0.7571132436375956, accuracy: 0.5133333333333333\n",
      "iteration no 152: Loss: 0.7570974268432832, accuracy: 0.5133333333333333\n",
      "iteration no 153: Loss: 0.7570821140938117, accuracy: 0.5133333333333333\n",
      "iteration no 154: Loss: 0.7570672885618983, accuracy: 0.5133333333333333\n",
      "iteration no 155: Loss: 0.7570529340178513, accuracy: 0.5133333333333333\n",
      "iteration no 156: Loss: 0.7570390348066292, accuracy: 0.5133333333333333\n",
      "iteration no 157: Loss: 0.757025575825867, accuracy: 0.5133333333333333\n",
      "iteration no 158: Loss: 0.7570125425048198, accuracy: 0.5133333333333333\n",
      "iteration no 159: Loss: 0.7569999207841841, accuracy: 0.5133333333333333\n",
      "iteration no 160: Loss: 0.7569876970967557, accuracy: 0.5133333333333333\n",
      "iteration no 161: Loss: 0.7569758583488876, accuracy: 0.5133333333333333\n",
      "iteration no 162: Loss: 0.7569643919027066, accuracy: 0.5133333333333333\n",
      "iteration no 163: Loss: 0.7569532855590623, accuracy: 0.5133333333333333\n",
      "iteration no 164: Loss: 0.7569425275411675, accuracy: 0.5133333333333333\n",
      "iteration no 165: Loss: 0.7569321064789057, accuracy: 0.5133333333333333\n",
      "iteration no 166: Loss: 0.7569220113937695, accuracy: 0.5133333333333333\n",
      "iteration no 167: Loss: 0.7569122316844087, accuracy: 0.5133333333333333\n",
      "iteration no 168: Loss: 0.7569027571127565, accuracy: 0.5133333333333333\n",
      "iteration no 169: Loss: 0.7568935777907082, accuracy: 0.5133333333333333\n",
      "iteration no 170: Loss: 0.7568846841673335, accuracy: 0.5133333333333333\n",
      "iteration no 171: Loss: 0.7568760670165913, accuracy: 0.5133333333333333\n",
      "iteration no 172: Loss: 0.7568677174255325, accuracy: 0.5133333333333333\n",
      "iteration no 173: Loss: 0.7568596267829649, accuracy: 0.5133333333333333\n",
      "iteration no 174: Loss: 0.7568517867685624, accuracy: 0.5133333333333333\n",
      "iteration no 175: Loss: 0.7568441893423996, accuracy: 0.5133333333333333\n",
      "iteration no 176: Loss: 0.7568368267348929, accuracy: 0.5133333333333333\n",
      "iteration no 177: Loss: 0.7568296914371311, accuracy: 0.5133333333333333\n",
      "iteration no 178: Loss: 0.7568227761915786, accuracy: 0.5133333333333333\n",
      "iteration no 179: Loss: 0.7568160739831373, accuracy: 0.5133333333333333\n",
      "iteration no 180: Loss: 0.7568095780305499, accuracy: 0.5133333333333333\n",
      "iteration no 181: Loss: 0.7568032817781318, accuracy: 0.5133333333333333\n",
      "iteration no 182: Loss: 0.7567971788878173, accuracy: 0.5133333333333333\n",
      "iteration no 183: Loss: 0.7567912632315074, accuracy: 0.5133333333333333\n",
      "iteration no 184: Loss: 0.7567855288837064, accuracy: 0.5133333333333333\n",
      "iteration no 185: Loss: 0.7567799701144369, accuracy: 0.5133333333333333\n",
      "iteration no 186: Loss: 0.7567745813824189, accuracy: 0.5133333333333333\n",
      "iteration no 187: Loss: 0.7567693573285051, accuracy: 0.5133333333333333\n",
      "iteration no 188: Loss: 0.7567642927693615, accuracy: 0.5133333333333333\n",
      "iteration no 189: Loss: 0.7567593826913807, accuracy: 0.5133333333333333\n",
      "iteration no 190: Loss: 0.7567546222448229, accuracy: 0.5133333333333333\n",
      "iteration no 191: Loss: 0.7567500067381718, accuracy: 0.5133333333333333\n",
      "iteration no 192: Loss: 0.7567455316326989, accuracy: 0.5133333333333333\n",
      "iteration no 193: Loss: 0.756741192537227, accuracy: 0.5133333333333333\n",
      "iteration no 194: Loss: 0.7567369852030846, accuracy: 0.5133333333333333\n",
      "iteration no 195: Loss: 0.7567329055192459, accuracy: 0.5133333333333333\n",
      "iteration no 196: Loss: 0.7567289495076457, accuracy: 0.5133333333333333\n",
      "iteration no 197: Loss: 0.7567251133186654, accuracy: 0.5133333333333333\n",
      "iteration no 198: Loss: 0.7567213932267807, accuracy: 0.5133333333333333\n",
      "iteration no 199: Loss: 0.7567177856263684, accuracy: 0.5133333333333333\n",
      "iteration no 200: Loss: 0.7567142870276616, accuracy: 0.5133333333333333\n",
      "iteration no 1: Loss: 1.0955000449690075, accuracy: 0.43666666666666665\n",
      "iteration no 2: Loss: 1.0652887573944796, accuracy: 0.48333333333333334\n",
      "iteration no 3: Loss: 1.0384140328883629, accuracy: 0.49333333333333335\n",
      "iteration no 4: Loss: 1.0144635639461894, accuracy: 0.5\n",
      "iteration no 5: Loss: 0.9930773043849732, accuracy: 0.5\n",
      "iteration no 6: Loss: 0.9739403519064859, accuracy: 0.5\n",
      "iteration no 7: Loss: 0.9567777146338445, accuracy: 0.5\n",
      "iteration no 8: Loss: 0.9413497878367159, accuracy: 0.5\n",
      "iteration no 9: Loss: 0.9274481883947555, accuracy: 0.5\n",
      "iteration no 10: Loss: 0.9148918919102955, accuracy: 0.5033333333333333\n",
      "iteration no 11: Loss: 0.903523697823644, accuracy: 0.5033333333333333\n",
      "iteration no 12: Loss: 0.8932070496414652, accuracy: 0.5033333333333333\n",
      "iteration no 13: Loss: 0.8838232181038349, accuracy: 0.51\n",
      "iteration no 14: Loss: 0.8752688359571218, accuracy: 0.5133333333333333\n",
      "iteration no 15: Loss: 0.8674537598206654, accuracy: 0.5133333333333333\n",
      "iteration no 16: Loss: 0.8602992275799488, accuracy: 0.5133333333333333\n",
      "iteration no 17: Loss: 0.8537362773000544, accuracy: 0.5133333333333333\n",
      "iteration no 18: Loss: 0.8477043942032373, accuracy: 0.5133333333333333\n",
      "iteration no 19: Loss: 0.8421503545098972, accuracy: 0.51\n",
      "iteration no 20: Loss: 0.8370272380185675, accuracy: 0.5133333333333333\n",
      "iteration no 21: Loss: 0.8322935846391072, accuracy: 0.5133333333333333\n",
      "iteration no 22: Loss: 0.8279126733675404, accuracy: 0.5133333333333333\n",
      "iteration no 23: Loss: 0.8238519052276521, accuracy: 0.5133333333333333\n",
      "iteration no 24: Loss: 0.8200822744257952, accuracy: 0.51\n",
      "iteration no 25: Loss: 0.8165779143502986, accuracy: 0.5133333333333333\n",
      "iteration no 26: Loss: 0.8133157071058537, accuracy: 0.5133333333333333\n",
      "iteration no 27: Loss: 0.8102749470326882, accuracy: 0.5133333333333333\n",
      "iteration no 28: Loss: 0.8074370501532689, accuracy: 0.5133333333333333\n",
      "iteration no 29: Loss: 0.804785302750085, accuracy: 0.5133333333333333\n",
      "iteration no 30: Loss: 0.8023046433395168, accuracy: 0.5133333333333333\n",
      "iteration no 31: Loss: 0.7999814731987529, accuracy: 0.5166666666666667\n",
      "iteration no 32: Loss: 0.7978034913514784, accuracy: 0.5166666666666667\n",
      "iteration no 33: Loss: 0.7957595505464506, accuracy: 0.5166666666666667\n",
      "iteration no 34: Loss: 0.7938395312905586, accuracy: 0.5166666666666667\n",
      "iteration no 35: Loss: 0.7920342314410204, accuracy: 0.5133333333333333\n",
      "iteration no 36: Loss: 0.7903352692338926, accuracy: 0.5133333333333333\n",
      "iteration no 37: Loss: 0.7887349979396224, accuracy: 0.5166666666666667\n",
      "iteration no 38: Loss: 0.7872264306006931, accuracy: 0.5166666666666667\n",
      "iteration no 39: Loss: 0.7858031735295616, accuracy: 0.5166666666666667\n",
      "iteration no 40: Loss: 0.7844593674337748, accuracy: 0.5166666666666667\n",
      "iteration no 41: Loss: 0.7831896351949941, accuracy: 0.5166666666666667\n",
      "iteration no 42: Loss: 0.7819890354643045, accuracy: 0.5166666666666667\n",
      "iteration no 43: Loss: 0.7808530213514844, accuracy: 0.5133333333333333\n",
      "iteration no 44: Loss: 0.7797774035841527, accuracy: 0.5133333333333333\n",
      "iteration no 45: Loss: 0.7787583175965125, accuracy: 0.5133333333333333\n",
      "iteration no 46: Loss: 0.777792194079073, accuracy: 0.5133333333333333\n",
      "iteration no 47: Loss: 0.7768757325821253, accuracy: 0.5133333333333333\n",
      "iteration no 48: Loss: 0.7760058778184146, accuracy: 0.5133333333333333\n",
      "iteration no 49: Loss: 0.7751797983557586, accuracy: 0.5133333333333333\n",
      "iteration no 50: Loss: 0.7743948674293782, accuracy: 0.5133333333333333\n",
      "iteration no 51: Loss: 0.7736486456373791, accuracy: 0.5133333333333333\n",
      "iteration no 52: Loss: 0.7729388653119406, accuracy: 0.5133333333333333\n",
      "iteration no 53: Loss: 0.7722634163839914, accuracy: 0.5133333333333333\n",
      "iteration no 54: Loss: 0.7716203335810237, accuracy: 0.5133333333333333\n",
      "iteration no 55: Loss: 0.7710077848167287, accuracy: 0.5133333333333333\n",
      "iteration no 56: Loss: 0.7704240606476929, accuracy: 0.5133333333333333\n",
      "iteration no 57: Loss: 0.7698675646868425, accuracy: 0.5133333333333333\n",
      "iteration no 58: Loss: 0.769336804875941, accuracy: 0.5133333333333333\n",
      "iteration no 59: Loss: 0.7688303855304952, accuracy: 0.5133333333333333\n",
      "iteration no 60: Loss: 0.7683470000800976, accuracy: 0.5166666666666667\n",
      "iteration no 61: Loss: 0.7678854244357409, accuracy: 0.5166666666666667\n",
      "iteration no 62: Loss: 0.767444510923105, accuracy: 0.5166666666666667\n",
      "iteration no 63: Loss: 0.7670231827274023, accuracy: 0.5166666666666667\n",
      "iteration no 64: Loss: 0.7666204288011701, accuracy: 0.5166666666666667\n",
      "iteration no 65: Loss: 0.7662352991915233, accuracy: 0.5166666666666667\n",
      "iteration no 66: Loss: 0.7658669007479174, accuracy: 0.5166666666666667\n",
      "iteration no 67: Loss: 0.7655143931754844, accuracy: 0.5166666666666667\n",
      "iteration no 68: Loss: 0.7651769854025711, accuracy: 0.5166666666666667\n",
      "iteration no 69: Loss: 0.7648539322342676, accuracy: 0.5166666666666667\n",
      "iteration no 70: Loss: 0.7645445312665352, accuracy: 0.5166666666666667\n",
      "iteration no 71: Loss: 0.7642481200380423, accuracy: 0.5166666666666667\n",
      "iteration no 72: Loss: 0.7639640733990526, accuracy: 0.5166666666666667\n",
      "iteration no 73: Loss: 0.7636918010787136, accuracy: 0.5166666666666667\n",
      "iteration no 74: Loss: 0.7634307454338628, accuracy: 0.5166666666666667\n",
      "iteration no 75: Loss: 0.7631803793640785, accuracy: 0.5166666666666667\n",
      "iteration no 76: Loss: 0.7629402043791221, accuracy: 0.5166666666666667\n",
      "iteration no 77: Loss: 0.7627097488062149, accuracy: 0.5166666666666667\n",
      "iteration no 78: Loss: 0.7624885661257255, accuracy: 0.5166666666666667\n",
      "iteration no 79: Loss: 0.7622762334249064, accuracy: 0.5166666666666667\n",
      "iteration no 80: Loss: 0.7620723499602227, accuracy: 0.5166666666666667\n",
      "iteration no 81: Loss: 0.7618765358196841, accuracy: 0.5166666666666667\n",
      "iteration no 82: Loss: 0.7616884306773246, accuracy: 0.5133333333333333\n",
      "iteration no 83: Loss: 0.7615076926326807, accuracy: 0.5133333333333333\n",
      "iteration no 84: Loss: 0.7613339971287227, accuracy: 0.5133333333333333\n",
      "iteration no 85: Loss: 0.7611670359422621, accuracy: 0.5133333333333333\n",
      "iteration no 86: Loss: 0.7610065162413605, accuracy: 0.5133333333333333\n",
      "iteration no 87: Loss: 0.7608521597047327, accuracy: 0.5133333333333333\n",
      "iteration no 88: Loss: 0.7607037016985425, accuracy: 0.5133333333333333\n",
      "iteration no 89: Loss: 0.7605608905063815, accuracy: 0.5133333333333333\n",
      "iteration no 90: Loss: 0.7604234866085564, accuracy: 0.5133333333333333\n",
      "iteration no 91: Loss: 0.7602912620071307, accuracy: 0.5133333333333333\n",
      "iteration no 92: Loss: 0.7601639995934453, accuracy: 0.5133333333333333\n",
      "iteration no 93: Loss: 0.7600414925551116, accuracy: 0.5133333333333333\n",
      "iteration no 94: Loss: 0.7599235438196996, accuracy: 0.5133333333333333\n",
      "iteration no 95: Loss: 0.7598099655325682, accuracy: 0.5133333333333333\n",
      "iteration no 96: Loss: 0.7597005785664751, accuracy: 0.5133333333333333\n",
      "iteration no 97: Loss: 0.7595952120607956, accuracy: 0.5133333333333333\n",
      "iteration no 98: Loss: 0.7594937029883319, accuracy: 0.5133333333333333\n",
      "iteration no 99: Loss: 0.7593958957478603, accuracy: 0.5133333333333333\n",
      "iteration no 100: Loss: 0.759301641780695, accuracy: 0.5133333333333333\n",
      "iteration no 101: Loss: 0.7592107992096739, accuracy: 0.5133333333333333\n",
      "iteration no 102: Loss: 0.7591232324990969, accuracy: 0.5133333333333333\n",
      "iteration no 103: Loss: 0.7590388121342473, accuracy: 0.5166666666666667\n",
      "iteration no 104: Loss: 0.7589574143192286, accuracy: 0.5166666666666667\n",
      "iteration no 105: Loss: 0.7588789206919445, accuracy: 0.5166666666666667\n",
      "iteration no 106: Loss: 0.7588032180551251, accuracy: 0.5166666666666667\n",
      "iteration no 107: Loss: 0.7587301981223885, accuracy: 0.5166666666666667\n",
      "iteration no 108: Loss: 0.7586597572783925, accuracy: 0.5166666666666667\n",
      "iteration no 109: Loss: 0.7585917963522004, accuracy: 0.5166666666666667\n",
      "iteration no 110: Loss: 0.7585262204030426, accuracy: 0.5166666666666667\n",
      "iteration no 111: Loss: 0.7584629385177156, accuracy: 0.5166666666666667\n",
      "iteration no 112: Loss: 0.7584018636189065, accuracy: 0.5166666666666667\n",
      "iteration no 113: Loss: 0.7583429122837869, accuracy: 0.5166666666666667\n",
      "iteration no 114: Loss: 0.7582860045722545, accuracy: 0.5166666666666667\n",
      "iteration no 115: Loss: 0.7582310638642527, accuracy: 0.5166666666666667\n",
      "iteration no 116: Loss: 0.7581780167056267, accuracy: 0.5166666666666667\n",
      "iteration no 117: Loss: 0.758126792662017, accuracy: 0.5166666666666667\n",
      "iteration no 118: Loss: 0.7580773241803197, accuracy: 0.5166666666666667\n",
      "iteration no 119: Loss: 0.7580295464572768, accuracy: 0.5166666666666667\n",
      "iteration no 120: Loss: 0.7579833973147838, accuracy: 0.5166666666666667\n",
      "iteration no 121: Loss: 0.7579388170815332, accuracy: 0.5166666666666667\n",
      "iteration no 122: Loss: 0.7578957484806312, accuracy: 0.5166666666666667\n",
      "iteration no 123: Loss: 0.7578541365228537, accuracy: 0.5166666666666667\n",
      "iteration no 124: Loss: 0.7578139284052224, accuracy: 0.5166666666666667\n",
      "iteration no 125: Loss: 0.7577750734146075, accuracy: 0.5166666666666667\n",
      "iteration no 126: Loss: 0.7577375228360772, accuracy: 0.5166666666666667\n",
      "iteration no 127: Loss: 0.7577012298657338, accuracy: 0.5166666666666667\n",
      "iteration no 128: Loss: 0.7576661495277918, accuracy: 0.5166666666666667\n",
      "iteration no 129: Loss: 0.7576322385956661, accuracy: 0.5166666666666667\n",
      "iteration no 130: Loss: 0.757599455516857, accuracy: 0.5166666666666667\n",
      "iteration no 131: Loss: 0.7575677603414245, accuracy: 0.5166666666666667\n",
      "iteration no 132: Loss: 0.7575371146538648, accuracy: 0.5133333333333333\n",
      "iteration no 133: Loss: 0.757507481508206, accuracy: 0.5133333333333333\n",
      "iteration no 134: Loss: 0.757478825366155, accuracy: 0.5133333333333333\n",
      "iteration no 135: Loss: 0.7574511120381362, accuracy: 0.5133333333333333\n",
      "iteration no 136: Loss: 0.757424308627069, accuracy: 0.5133333333333333\n",
      "iteration no 137: Loss: 0.7573983834747476, accuracy: 0.5133333333333333\n",
      "iteration no 138: Loss: 0.7573733061106834, accuracy: 0.5133333333333333\n",
      "iteration no 139: Loss: 0.7573490472032899, accuracy: 0.5133333333333333\n",
      "iteration no 140: Loss: 0.7573255785132864, accuracy: 0.5133333333333333\n",
      "iteration no 141: Loss: 0.7573028728492113, accuracy: 0.5133333333333333\n",
      "iteration no 142: Loss: 0.757280904024938, accuracy: 0.5133333333333333\n",
      "iteration no 143: Loss: 0.757259646819094, accuracy: 0.5133333333333333\n",
      "iteration no 144: Loss: 0.7572390769362873, accuracy: 0.5133333333333333\n",
      "iteration no 145: Loss: 0.7572191709700541, accuracy: 0.5133333333333333\n",
      "iteration no 146: Loss: 0.757199906367437, accuracy: 0.5133333333333333\n",
      "iteration no 147: Loss: 0.7571812613951232, accuracy: 0.5133333333333333\n",
      "iteration no 148: Loss: 0.7571632151070571, accuracy: 0.5133333333333333\n",
      "iteration no 149: Loss: 0.7571457473134636, accuracy: 0.5133333333333333\n",
      "iteration no 150: Loss: 0.7571288385512112, accuracy: 0.5133333333333333\n",
      "iteration no 151: Loss: 0.7571124700554487, accuracy: 0.5133333333333333\n",
      "iteration no 152: Loss: 0.757096623732461, accuracy: 0.5133333333333333\n",
      "iteration no 153: Loss: 0.7570812821336799, accuracy: 0.5133333333333333\n",
      "iteration no 154: Loss: 0.7570664284307985, accuracy: 0.5133333333333333\n",
      "iteration no 155: Loss: 0.7570520463919383, accuracy: 0.5133333333333333\n",
      "iteration no 156: Loss: 0.7570381203588189, accuracy: 0.5133333333333333\n",
      "iteration no 157: Loss: 0.7570246352248822, accuracy: 0.5133333333333333\n",
      "iteration no 158: Loss: 0.7570115764143303, accuracy: 0.5133333333333333\n",
      "iteration no 159: Loss: 0.7569989298620333, accuracy: 0.5133333333333333\n",
      "iteration no 160: Loss: 0.7569866819942669, accuracy: 0.5133333333333333\n",
      "iteration no 161: Loss: 0.756974819710243, accuracy: 0.5133333333333333\n",
      "iteration no 162: Loss: 0.7569633303643967, accuracy: 0.5133333333333333\n",
      "iteration no 163: Loss: 0.7569522017493973, accuracy: 0.5133333333333333\n",
      "iteration no 164: Loss: 0.7569414220798479, accuracy: 0.5133333333333333\n",
      "iteration no 165: Loss: 0.7569309799766457, accuracy: 0.5133333333333333\n",
      "iteration no 166: Loss: 0.7569208644519727, accuracy: 0.5133333333333333\n",
      "iteration no 167: Loss: 0.7569110648948869, accuracy: 0.5133333333333333\n",
      "iteration no 168: Loss: 0.7569015710574928, accuracy: 0.5133333333333333\n",
      "iteration no 169: Loss: 0.7568923730416589, accuracy: 0.5133333333333333\n",
      "iteration no 170: Loss: 0.7568834612862652, accuracy: 0.5133333333333333\n",
      "iteration no 171: Loss: 0.7568748265549512, accuracy: 0.5133333333333333\n",
      "iteration no 172: Loss: 0.7568664599243499, accuracy: 0.5133333333333333\n",
      "iteration no 173: Loss: 0.7568583527727789, accuracy: 0.5133333333333333\n",
      "iteration no 174: Loss: 0.7568504967693767, accuracy: 0.5133333333333333\n",
      "iteration no 175: Loss: 0.7568428838636588, accuracy: 0.5133333333333333\n",
      "iteration no 176: Loss: 0.7568355062754812, accuracy: 0.5133333333333333\n",
      "iteration no 177: Loss: 0.7568283564853903, accuracy: 0.5133333333333333\n",
      "iteration no 178: Loss: 0.7568214272253426, accuracy: 0.5133333333333333\n",
      "iteration no 179: Loss: 0.7568147114697842, accuracy: 0.5133333333333333\n",
      "iteration no 180: Loss: 0.7568082024270673, accuracy: 0.5133333333333333\n",
      "iteration no 181: Loss: 0.756801893531195, accuracy: 0.5133333333333333\n",
      "iteration no 182: Loss: 0.75679577843388, accuracy: 0.5133333333333333\n",
      "iteration no 183: Loss: 0.7567898509969028, accuracy: 0.5133333333333333\n",
      "iteration no 184: Loss: 0.7567841052847575, accuracy: 0.5133333333333333\n",
      "iteration no 185: Loss: 0.7567785355575746, accuracy: 0.5133333333333333\n",
      "iteration no 186: Loss: 0.7567731362643086, accuracy: 0.5133333333333333\n",
      "iteration no 187: Loss: 0.7567679020361802, accuracy: 0.5133333333333333\n",
      "iteration no 188: Loss: 0.7567628276803597, accuracy: 0.5133333333333333\n",
      "iteration no 189: Loss: 0.7567579081738893, accuracy: 0.5133333333333333\n",
      "iteration no 190: Loss: 0.7567531386578255, accuracy: 0.5133333333333333\n",
      "iteration no 191: Loss: 0.7567485144315994, accuracy: 0.5133333333333333\n",
      "iteration no 192: Loss: 0.7567440309475846, accuracy: 0.5133333333333333\n",
      "iteration no 193: Loss: 0.7567396838058624, accuracy: 0.5133333333333333\n",
      "iteration no 194: Loss: 0.7567354687491787, accuracy: 0.5133333333333333\n",
      "iteration no 195: Loss: 0.7567313816580851, accuracy: 0.5133333333333333\n",
      "iteration no 196: Loss: 0.7567274185462554, accuracy: 0.5133333333333333\n",
      "iteration no 197: Loss: 0.7567235755559716, accuracy: 0.5133333333333333\n",
      "iteration no 198: Loss: 0.7567198489537731, accuracy: 0.5133333333333333\n",
      "iteration no 199: Loss: 0.7567162351262618, accuracy: 0.5133333333333333\n",
      "iteration no 200: Loss: 0.756712730576059, accuracy: 0.5133333333333333\n",
      "iteration no 1: Loss: 1.1045740405347304, accuracy: 0.4166666666666667\n",
      "iteration no 2: Loss: 1.073106284969742, accuracy: 0.4766666666666667\n",
      "iteration no 3: Loss: 1.0451549392573918, accuracy: 0.49333333333333335\n",
      "iteration no 4: Loss: 1.020279748549495, accuracy: 0.5\n",
      "iteration no 5: Loss: 0.9981001650478883, accuracy: 0.5066666666666667\n",
      "iteration no 6: Loss: 0.9782840083893884, accuracy: 0.51\n",
      "iteration no 7: Loss: 0.9605408653965737, accuracy: 0.51\n",
      "iteration no 8: Loss: 0.9446172491083265, accuracy: 0.51\n",
      "iteration no 9: Loss: 0.9302924112579904, accuracy: 0.51\n",
      "iteration no 10: Loss: 0.9173744781171407, accuracy: 0.5066666666666667\n",
      "iteration no 11: Loss: 0.9056968613584716, accuracy: 0.51\n",
      "iteration no 12: Loss: 0.8951149714026073, accuracy: 0.5066666666666667\n",
      "iteration no 13: Loss: 0.885503261440351, accuracy: 0.5066666666666667\n",
      "iteration no 14: Loss: 0.8767526110303387, accuracy: 0.5133333333333333\n",
      "iteration no 15: Loss: 0.8687680392258781, accuracy: 0.5166666666666667\n",
      "iteration no 16: Loss: 0.8614667242119006, accuracy: 0.5166666666666667\n",
      "iteration no 17: Loss: 0.8547762995230179, accuracy: 0.5166666666666667\n",
      "iteration no 18: Loss: 0.848633394550439, accuracy: 0.5166666666666667\n",
      "iteration no 19: Loss: 0.8429823876067615, accuracy: 0.5166666666666667\n",
      "iteration no 20: Loss: 0.8377743420304024, accuracy: 0.5166666666666667\n",
      "iteration no 21: Loss: 0.8329660988040833, accuracy: 0.5166666666666667\n",
      "iteration no 22: Loss: 0.8285195023886297, accuracy: 0.5166666666666667\n",
      "iteration no 23: Loss: 0.8244007396195617, accuracy: 0.5166666666666667\n",
      "iteration no 24: Loss: 0.8205797744158585, accuracy: 0.5166666666666667\n",
      "iteration no 25: Loss: 0.8170298636373632, accuracy: 0.5133333333333333\n",
      "iteration no 26: Loss: 0.8137271416835637, accuracy: 0.5133333333333333\n",
      "iteration no 27: Loss: 0.8106502633655533, accuracy: 0.5166666666666667\n",
      "iteration no 28: Loss: 0.8077800962330163, accuracy: 0.5166666666666667\n",
      "iteration no 29: Loss: 0.8050994549328274, accuracy: 0.5166666666666667\n",
      "iteration no 30: Loss: 0.8025928713496238, accuracy: 0.5166666666666667\n",
      "iteration no 31: Loss: 0.8002463952637684, accuracy: 0.5166666666666667\n",
      "iteration no 32: Loss: 0.7980474210875392, accuracy: 0.52\n",
      "iteration no 33: Loss: 0.7959845369315046, accuracy: 0.52\n",
      "iteration no 34: Loss: 0.7940473928317779, accuracy: 0.52\n",
      "iteration no 35: Loss: 0.7922265854536472, accuracy: 0.52\n",
      "iteration no 36: Loss: 0.7905135569935928, accuracy: 0.52\n",
      "iteration no 37: Loss: 0.7889005063429513, accuracy: 0.52\n",
      "iteration no 38: Loss: 0.7873803108633619, accuracy: 0.52\n",
      "iteration no 39: Loss: 0.7859464573656663, accuracy: 0.52\n",
      "iteration no 40: Loss: 0.7845929810876472, accuracy: 0.52\n",
      "iteration no 41: Loss: 0.7833144116381173, accuracy: 0.52\n",
      "iteration no 42: Loss: 0.7821057250205765, accuracy: 0.52\n",
      "iteration no 43: Loss: 0.7809623009732217, accuracy: 0.52\n",
      "iteration no 44: Loss: 0.7798798849671248, accuracy: 0.5166666666666667\n",
      "iteration no 45: Loss: 0.778854554293795, accuracy: 0.5166666666666667\n",
      "iteration no 46: Loss: 0.7778826877496395, accuracy: 0.5166666666666667\n",
      "iteration no 47: Loss: 0.7769609384900413, accuracy: 0.5166666666666667\n",
      "iteration no 48: Loss: 0.7760862096816347, accuracy: 0.5133333333333333\n",
      "iteration no 49: Loss: 0.775255632629298, accuracy: 0.5133333333333333\n",
      "iteration no 50: Loss: 0.7744665470955961, accuracy: 0.5133333333333333\n",
      "iteration no 51: Loss: 0.7737164835659287, accuracy: 0.5133333333333333\n",
      "iteration no 52: Loss: 0.7730031472432827, accuracy: 0.5133333333333333\n",
      "iteration no 53: Loss: 0.7723244035830098, accuracy: 0.5133333333333333\n",
      "iteration no 54: Loss: 0.7716782652010039, accuracy: 0.5133333333333333\n",
      "iteration no 55: Loss: 0.7710628800085975, accuracy: 0.5133333333333333\n",
      "iteration no 56: Loss: 0.7704765204448276, accuracy: 0.5133333333333333\n",
      "iteration no 57: Loss: 0.7699175736918165, accuracy: 0.5133333333333333\n",
      "iteration no 58: Loss: 0.7693845327721917, accuracy: 0.5133333333333333\n",
      "iteration no 59: Loss: 0.7688759884389818, accuracy: 0.5133333333333333\n",
      "iteration no 60: Loss: 0.7683906217785036, accuracy: 0.5133333333333333\n",
      "iteration no 61: Loss: 0.7679271974556007, accuracy: 0.5133333333333333\n",
      "iteration no 62: Loss: 0.7674845575383524, accuracy: 0.5133333333333333\n",
      "iteration no 63: Loss: 0.7670616158462082, accuracy: 0.5166666666666667\n",
      "iteration no 64: Loss: 0.7666573527715131, accuracy: 0.5166666666666667\n",
      "iteration no 65: Loss: 0.7662708105297071, accuracy: 0.5166666666666667\n",
      "iteration no 66: Loss: 0.7659010887981691, accuracy: 0.5166666666666667\n",
      "iteration no 67: Loss: 0.7655473407078288, accuracy: 0.5166666666666667\n",
      "iteration no 68: Loss: 0.765208769155355, accuracy: 0.5166666666666667\n",
      "iteration no 69: Loss: 0.7648846234069856, accuracy: 0.5166666666666667\n",
      "iteration no 70: Loss: 0.7645741959679777, accuracy: 0.5166666666666667\n",
      "iteration no 71: Loss: 0.7642768196942306, accuracy: 0.5166666666666667\n",
      "iteration no 72: Loss: 0.7639918651249411, accuracy: 0.5166666666666667\n",
      "iteration no 73: Loss: 0.7637187380172004, accuracy: 0.5166666666666667\n",
      "iteration no 74: Loss: 0.7634568770652789, accuracy: 0.5166666666666667\n",
      "iteration no 75: Loss: 0.7632057517889851, accuracy: 0.5166666666666667\n",
      "iteration no 76: Loss: 0.7629648605769506, accuracy: 0.5166666666666667\n",
      "iteration no 77: Loss: 0.7627337288720162, accuracy: 0.5166666666666667\n",
      "iteration no 78: Loss: 0.7625119074870729, accuracy: 0.5166666666666667\n",
      "iteration no 79: Loss: 0.7622989710407738, accuracy: 0.5166666666666667\n",
      "iteration no 80: Loss: 0.76209451650349, accuracy: 0.5166666666666667\n",
      "iteration no 81: Loss: 0.7618981618447445, accuracy: 0.5166666666666667\n",
      "iteration no 82: Loss: 0.7617095447741348, accuracy: 0.5166666666666667\n",
      "iteration no 83: Loss: 0.7615283215684527, accuracy: 0.5133333333333333\n",
      "iteration no 84: Loss: 0.7613541659783468, accuracy: 0.5133333333333333\n",
      "iteration no 85: Loss: 0.7611867682084429, accuracy: 0.5133333333333333\n",
      "iteration no 86: Loss: 0.7610258339653557, accuracy: 0.5133333333333333\n",
      "iteration no 87: Loss: 0.7608710835684984, accuracy: 0.5133333333333333\n",
      "iteration no 88: Loss: 0.7607222511190165, accuracy: 0.5133333333333333\n",
      "iteration no 89: Loss: 0.7605790837225651, accuracy: 0.5133333333333333\n",
      "iteration no 90: Loss: 0.760441340761999, accuracy: 0.5133333333333333\n",
      "iteration no 91: Loss: 0.7603087932163589, accuracy: 0.5133333333333333\n",
      "iteration no 92: Loss: 0.7601812230228402, accuracy: 0.5133333333333333\n",
      "iteration no 93: Loss: 0.7600584224786803, accuracy: 0.5133333333333333\n",
      "iteration no 94: Loss: 0.7599401936801599, accuracy: 0.5133333333333333\n",
      "iteration no 95: Loss: 0.7598263479961168, accuracy: 0.5133333333333333\n",
      "iteration no 96: Loss: 0.7597167055735886, accuracy: 0.5133333333333333\n",
      "iteration no 97: Loss: 0.7596110948733723, accuracy: 0.5133333333333333\n",
      "iteration no 98: Loss: 0.7595093522334659, accuracy: 0.5133333333333333\n",
      "iteration no 99: Loss: 0.7594113214585076, accuracy: 0.5133333333333333\n",
      "iteration no 100: Loss: 0.7593168534334713, accuracy: 0.5133333333333333\n",
      "iteration no 101: Loss: 0.759225805760007, accuracy: 0.5133333333333333\n",
      "iteration no 102: Loss: 0.759138042413932, accuracy: 0.5133333333333333\n",
      "iteration no 103: Loss: 0.7590534334224928, accuracy: 0.5133333333333333\n",
      "iteration no 104: Loss: 0.7589718545601107, accuracy: 0.5133333333333333\n",
      "iteration no 105: Loss: 0.7588931870614266, accuracy: 0.5166666666666667\n",
      "iteration no 106: Loss: 0.758817317350537, accuracy: 0.5166666666666667\n",
      "iteration no 107: Loss: 0.7587441367853953, accuracy: 0.5166666666666667\n",
      "iteration no 108: Loss: 0.7586735414164266, accuracy: 0.5166666666666667\n",
      "iteration no 109: Loss: 0.7586054317584657, accuracy: 0.5166666666666667\n",
      "iteration no 110: Loss: 0.758539712575197, accuracy: 0.5166666666666667\n",
      "iteration no 111: Loss: 0.7584762926753243, accuracy: 0.5166666666666667\n",
      "iteration no 112: Loss: 0.7584150847197557, accuracy: 0.5166666666666667\n",
      "iteration no 113: Loss: 0.7583560050391364, accuracy: 0.5166666666666667\n",
      "iteration no 114: Loss: 0.7582989734611071, accuracy: 0.5166666666666667\n",
      "iteration no 115: Loss: 0.7582439131467056, accuracy: 0.5166666666666667\n",
      "iteration no 116: Loss: 0.758190750435372, accuracy: 0.5166666666666667\n",
      "iteration no 117: Loss: 0.7581394146980482, accuracy: 0.5166666666666667\n",
      "iteration no 118: Loss: 0.7580898381979004, accuracy: 0.5166666666666667\n",
      "iteration no 119: Loss: 0.75804195595822, accuracy: 0.5166666666666667\n",
      "iteration no 120: Loss: 0.7579957056370918, accuracy: 0.5166666666666667\n",
      "iteration no 121: Loss: 0.7579510274084382, accuracy: 0.5166666666666667\n",
      "iteration no 122: Loss: 0.7579078638490785, accuracy: 0.5166666666666667\n",
      "iteration no 123: Loss: 0.7578661598314657, accuracy: 0.5166666666666667\n",
      "iteration no 124: Loss: 0.7578258624217753, accuracy: 0.5166666666666667\n",
      "iteration no 125: Loss: 0.7577869207830564, accuracy: 0.5166666666666667\n",
      "iteration no 126: Loss: 0.7577492860831566, accuracy: 0.5166666666666667\n",
      "iteration no 127: Loss: 0.7577129114071625, accuracy: 0.5133333333333333\n",
      "iteration no 128: Loss: 0.7576777516741079, accuracy: 0.5133333333333333\n",
      "iteration no 129: Loss: 0.7576437635577162, accuracy: 0.5133333333333333\n",
      "iteration no 130: Loss: 0.7576109054109611, accuracy: 0.5133333333333333\n",
      "iteration no 131: Loss: 0.7575791371942394, accuracy: 0.5133333333333333\n",
      "iteration no 132: Loss: 0.7575484204069627, accuracy: 0.5133333333333333\n",
      "iteration no 133: Loss: 0.7575187180223887, accuracy: 0.5133333333333333\n",
      "iteration no 134: Loss: 0.7574899944255188, accuracy: 0.5133333333333333\n",
      "iteration no 135: Loss: 0.7574622153539033, accuracy: 0.5133333333333333\n",
      "iteration no 136: Loss: 0.7574353478412023, accuracy: 0.5133333333333333\n",
      "iteration no 137: Loss: 0.7574093601633587, accuracy: 0.5133333333333333\n",
      "iteration no 138: Loss: 0.7573842217872508, accuracy: 0.5133333333333333\n",
      "iteration no 139: Loss: 0.7573599033216943, accuracy: 0.5133333333333333\n",
      "iteration no 140: Loss: 0.757336376470679, accuracy: 0.5133333333333333\n",
      "iteration no 141: Loss: 0.7573136139887229, accuracy: 0.5133333333333333\n",
      "iteration no 142: Loss: 0.7572915896382383, accuracy: 0.5133333333333333\n",
      "iteration no 143: Loss: 0.7572702781488115, accuracy: 0.5133333333333333\n",
      "iteration no 144: Loss: 0.7572496551782981, accuracy: 0.5133333333333333\n",
      "iteration no 145: Loss: 0.7572296972756447, accuracy: 0.5133333333333333\n",
      "iteration no 146: Loss: 0.7572103818453545, accuracy: 0.5133333333333333\n",
      "iteration no 147: Loss: 0.7571916871135123, accuracy: 0.5133333333333333\n",
      "iteration no 148: Loss: 0.7571735920952964, accuracy: 0.5133333333333333\n",
      "iteration no 149: Loss: 0.7571560765639058, accuracy: 0.5133333333333333\n",
      "iteration no 150: Loss: 0.7571391210208288, accuracy: 0.5133333333333333\n",
      "iteration no 151: Loss: 0.7571227066673977, accuracy: 0.5133333333333333\n",
      "iteration no 152: Loss: 0.7571068153775611, accuracy: 0.5133333333333333\n",
      "iteration no 153: Loss: 0.7570914296718193, accuracy: 0.5133333333333333\n",
      "iteration no 154: Loss: 0.7570765326922674, accuracy: 0.5133333333333333\n",
      "iteration no 155: Loss: 0.7570621081786945, accuracy: 0.5133333333333333\n",
      "iteration no 156: Loss: 0.7570481404456888, accuracy: 0.5133333333333333\n",
      "iteration no 157: Loss: 0.7570346143607021, accuracy: 0.5133333333333333\n",
      "iteration no 158: Loss: 0.7570215153230315, accuracy: 0.5133333333333333\n",
      "iteration no 159: Loss: 0.7570088292436725, accuracy: 0.5133333333333333\n",
      "iteration no 160: Loss: 0.7569965425260068, accuracy: 0.5133333333333333\n",
      "iteration no 161: Loss: 0.7569846420472839, accuracy: 0.5133333333333333\n",
      "iteration no 162: Loss: 0.7569731151408653, accuracy: 0.5133333333333333\n",
      "iteration no 163: Loss: 0.7569619495791896, accuracy: 0.5133333333333333\n",
      "iteration no 164: Loss: 0.7569511335574347, accuracy: 0.5133333333333333\n",
      "iteration no 165: Loss: 0.756940655677838, accuracy: 0.5133333333333333\n",
      "iteration no 166: Loss: 0.7569305049346516, accuracy: 0.5133333333333333\n",
      "iteration no 167: Loss: 0.7569206706997003, accuracy: 0.5133333333333333\n",
      "iteration no 168: Loss: 0.7569111427085176, accuracy: 0.5133333333333333\n",
      "iteration no 169: Loss: 0.7569019110470354, accuracy: 0.5133333333333333\n",
      "iteration no 170: Loss: 0.7568929661387999, accuracy: 0.5133333333333333\n",
      "iteration no 171: Loss: 0.7568842987326946, accuracy: 0.5133333333333333\n",
      "iteration no 172: Loss: 0.7568758998911453, accuracy: 0.5133333333333333\n",
      "iteration no 173: Loss: 0.7568677609787903, accuracy: 0.5133333333333333\n",
      "iteration no 174: Loss: 0.7568598736515898, accuracy: 0.5133333333333333\n",
      "iteration no 175: Loss: 0.7568522298463624, accuracy: 0.5133333333333333\n",
      "iteration no 176: Loss: 0.7568448217707261, accuracy: 0.5133333333333333\n",
      "iteration no 177: Loss: 0.7568376418934287, accuracy: 0.5133333333333333\n",
      "iteration no 178: Loss: 0.7568306829350495, accuracy: 0.5133333333333333\n",
      "iteration no 179: Loss: 0.7568239378590588, accuracy: 0.5133333333333333\n",
      "iteration no 180: Loss: 0.7568173998632186, accuracy: 0.5133333333333333\n",
      "iteration no 181: Loss: 0.7568110623713119, accuracy: 0.5133333333333333\n",
      "iteration no 182: Loss: 0.7568049190251842, accuracy: 0.5133333333333333\n",
      "iteration no 183: Loss: 0.7567989636770883, accuracy: 0.5133333333333333\n",
      "iteration no 184: Loss: 0.7567931903823156, accuracy: 0.5133333333333333\n",
      "iteration no 185: Loss: 0.7567875933921064, accuracy: 0.5133333333333333\n",
      "iteration no 186: Loss: 0.7567821671468238, accuracy: 0.5133333333333333\n",
      "iteration no 187: Loss: 0.7567769062693838, accuracy: 0.5133333333333333\n",
      "iteration no 188: Loss: 0.756771805558929, accuracy: 0.5133333333333333\n",
      "iteration no 189: Loss: 0.7567668599847375, accuracy: 0.5133333333333333\n",
      "iteration no 190: Loss: 0.7567620646803572, accuracy: 0.5133333333333333\n",
      "iteration no 191: Loss: 0.7567574149379551, accuracy: 0.5133333333333333\n",
      "iteration no 192: Loss: 0.756752906202875, accuracy: 0.5133333333333333\n",
      "iteration no 193: Loss: 0.7567485340683953, accuracy: 0.5133333333333333\n",
      "iteration no 194: Loss: 0.7567442942706758, accuracy: 0.5133333333333333\n",
      "iteration no 195: Loss: 0.7567401826838925, accuracy: 0.5133333333333333\n",
      "iteration no 196: Loss: 0.7567361953155433, accuracy: 0.5133333333333333\n",
      "iteration no 197: Loss: 0.7567323283019294, accuracy: 0.5133333333333333\n",
      "iteration no 198: Loss: 0.7567285779037958, accuracy: 0.5133333333333333\n",
      "iteration no 199: Loss: 0.7567249405021305, accuracy: 0.5133333333333333\n",
      "iteration no 200: Loss: 0.7567214125941131, accuracy: 0.5133333333333333\n",
      "iteration no 1: Loss: 1.077305500564642, accuracy: 0.45666666666666667\n",
      "iteration no 2: Loss: 1.0491653796871325, accuracy: 0.48\n",
      "iteration no 3: Loss: 1.0240924458253635, accuracy: 0.49\n",
      "iteration no 4: Loss: 1.0017129319668088, accuracy: 0.4866666666666667\n",
      "iteration no 5: Loss: 0.9816973484624563, accuracy: 0.51\n",
      "iteration no 6: Loss: 0.9637569464322192, accuracy: 0.5133333333333333\n",
      "iteration no 7: Loss: 0.9476397471046308, accuracy: 0.5133333333333333\n",
      "iteration no 8: Loss: 0.9331264579996328, accuracy: 0.5133333333333333\n",
      "iteration no 9: Loss: 0.9200265158222544, accuracy: 0.5133333333333333\n",
      "iteration no 10: Loss: 0.9081744047176586, accuracy: 0.5133333333333333\n",
      "iteration no 11: Loss: 0.8974263251035445, accuracy: 0.5133333333333333\n",
      "iteration no 12: Loss: 0.887657237498307, accuracy: 0.5166666666666667\n",
      "iteration no 13: Loss: 0.8787582741670497, accuracy: 0.5166666666666667\n",
      "iteration no 14: Loss: 0.8706344937603842, accuracy: 0.5133333333333333\n",
      "iteration no 15: Loss: 0.8632029457396007, accuracy: 0.51\n",
      "iteration no 16: Loss: 0.8563910087519911, accuracy: 0.51\n",
      "iteration no 17: Loss: 0.8501349678285584, accuracy: 0.5133333333333333\n",
      "iteration no 18: Loss: 0.8443787977576506, accuracy: 0.5133333333333333\n",
      "iteration no 19: Loss: 0.8390731232719892, accuracy: 0.5133333333333333\n",
      "iteration no 20: Loss: 0.8341743301979011, accuracy: 0.5133333333333333\n",
      "iteration no 21: Loss: 0.8296438051318598, accuracy: 0.5133333333333333\n",
      "iteration no 22: Loss: 0.8254472843649145, accuracy: 0.5133333333333333\n",
      "iteration no 23: Loss: 0.8215542955986365, accuracy: 0.5133333333333333\n",
      "iteration no 24: Loss: 0.817937678469854, accuracy: 0.5133333333333333\n",
      "iteration no 25: Loss: 0.814573172038661, accuracy: 0.5133333333333333\n",
      "iteration no 26: Loss: 0.8114390592228498, accuracy: 0.5133333333333333\n",
      "iteration no 27: Loss: 0.8085158597161684, accuracy: 0.5133333333333333\n",
      "iteration no 28: Loss: 0.805786064242707, accuracy: 0.5133333333333333\n",
      "iteration no 29: Loss: 0.803233904108742, accuracy: 0.5133333333333333\n",
      "iteration no 30: Loss: 0.8008451509469386, accuracy: 0.5133333333333333\n",
      "iteration no 31: Loss: 0.7986069423328512, accuracy: 0.5133333333333333\n",
      "iteration no 32: Loss: 0.7965076296135234, accuracy: 0.5166666666666667\n",
      "iteration no 33: Loss: 0.7945366448427127, accuracy: 0.5133333333333333\n",
      "iteration no 34: Loss: 0.7926843841838188, accuracy: 0.5133333333333333\n",
      "iteration no 35: Loss: 0.790942105534316, accuracy: 0.51\n",
      "iteration no 36: Loss: 0.7893018384564464, accuracy: 0.51\n",
      "iteration no 37: Loss: 0.7877563047781325, accuracy: 0.51\n",
      "iteration no 38: Loss: 0.7862988484640011, accuracy: 0.5133333333333333\n",
      "iteration no 39: Loss: 0.7849233735560454, accuracy: 0.5133333333333333\n",
      "iteration no 40: Loss: 0.7836242891526638, accuracy: 0.5133333333333333\n",
      "iteration no 41: Loss: 0.7823964605384841, accuracy: 0.5133333333333333\n",
      "iteration no 42: Loss: 0.7812351656995696, accuracy: 0.5133333333333333\n",
      "iteration no 43: Loss: 0.7801360565627152, accuracy: 0.5133333333333333\n",
      "iteration no 44: Loss: 0.7790951243864113, accuracy: 0.5133333333333333\n",
      "iteration no 45: Loss: 0.7781086688070407, accuracy: 0.5133333333333333\n",
      "iteration no 46: Loss: 0.7771732701089809, accuracy: 0.5133333333333333\n",
      "iteration no 47: Loss: 0.7762857643431623, accuracy: 0.5133333333333333\n",
      "iteration no 48: Loss: 0.7754432209666714, accuracy: 0.5133333333333333\n",
      "iteration no 49: Loss: 0.7746429227173851, accuracy: 0.5133333333333333\n",
      "iteration no 50: Loss: 0.7738823474733273, accuracy: 0.5133333333333333\n",
      "iteration no 51: Loss: 0.7731591518773219, accuracy: 0.5133333333333333\n",
      "iteration no 52: Loss: 0.7724711565342542, accuracy: 0.5133333333333333\n",
      "iteration no 53: Loss: 0.7718163326114514, accuracy: 0.5133333333333333\n",
      "iteration no 54: Loss: 0.7711927896928505, accuracy: 0.5133333333333333\n",
      "iteration no 55: Loss: 0.7705987647551829, accuracy: 0.5133333333333333\n",
      "iteration no 56: Loss: 0.77003261214969, accuracy: 0.5133333333333333\n",
      "iteration no 57: Loss: 0.7694927944862744, accuracy: 0.5133333333333333\n",
      "iteration no 58: Loss: 0.7689778743286636, accuracy: 0.5133333333333333\n",
      "iteration no 59: Loss: 0.7684865066194252, accuracy: 0.5133333333333333\n",
      "iteration no 60: Loss: 0.7680174317626591, accuracy: 0.5166666666666667\n",
      "iteration no 61: Loss: 0.7675694693000962, accuracy: 0.5166666666666667\n",
      "iteration no 62: Loss: 0.7671415121232941, accuracy: 0.5166666666666667\n",
      "iteration no 63: Loss: 0.7667325211707552, accuracy: 0.5166666666666667\n",
      "iteration no 64: Loss: 0.7663415205642068, accuracy: 0.5166666666666667\n",
      "iteration no 65: Loss: 0.765967593143074, accuracy: 0.5166666666666667\n",
      "iteration no 66: Loss: 0.7656098763604161, accuracy: 0.5166666666666667\n",
      "iteration no 67: Loss: 0.7652675585073565, accuracy: 0.5166666666666667\n",
      "iteration no 68: Loss: 0.7649398752363764, accuracy: 0.5166666666666667\n",
      "iteration no 69: Loss: 0.7646261063568071, accuracy: 0.5166666666666667\n",
      "iteration no 70: Loss: 0.7643255728784997, accuracy: 0.5166666666666667\n",
      "iteration no 71: Loss: 0.7640376342820083, accuracy: 0.5166666666666667\n",
      "iteration no 72: Loss: 0.7637616859957176, accuracy: 0.5166666666666667\n",
      "iteration no 73: Loss: 0.7634971570622318, accuracy: 0.5166666666666667\n",
      "iteration no 74: Loss: 0.7632435079780167, accuracy: 0.5166666666666667\n",
      "iteration no 75: Loss: 0.7630002286917928, accuracy: 0.5166666666666667\n",
      "iteration no 76: Loss: 0.7627668367485283, accuracy: 0.5166666666666667\n",
      "iteration no 77: Loss: 0.7625428755670909, accuracy: 0.5166666666666667\n",
      "iteration no 78: Loss: 0.7623279128407088, accuracy: 0.5166666666666667\n",
      "iteration no 79: Loss: 0.7621215390503657, accuracy: 0.5133333333333333\n",
      "iteration no 80: Loss: 0.7619233660821452, accuracy: 0.5133333333333333\n",
      "iteration no 81: Loss: 0.7617330259403284, accuracy: 0.5133333333333333\n",
      "iteration no 82: Loss: 0.7615501695487731, accuracy: 0.5133333333333333\n",
      "iteration no 83: Loss: 0.7613744656337464, accuracy: 0.5133333333333333\n",
      "iteration no 84: Loss: 0.7612055996819748, accuracy: 0.5133333333333333\n",
      "iteration no 85: Loss: 0.7610432729682042, accuracy: 0.5133333333333333\n",
      "iteration no 86: Loss: 0.7608872016470459, accuracy: 0.5133333333333333\n",
      "iteration no 87: Loss: 0.7607371159043202, accuracy: 0.5133333333333333\n",
      "iteration no 88: Loss: 0.7605927591635069, accuracy: 0.5133333333333333\n",
      "iteration no 89: Loss: 0.7604538873432718, accuracy: 0.5133333333333333\n",
      "iteration no 90: Loss: 0.76032026816237, accuracy: 0.5133333333333333\n",
      "iteration no 91: Loss: 0.7601916804885172, accuracy: 0.5133333333333333\n",
      "iteration no 92: Loss: 0.7600679137281038, accuracy: 0.5133333333333333\n",
      "iteration no 93: Loss: 0.7599487672538623, accuracy: 0.5133333333333333\n",
      "iteration no 94: Loss: 0.7598340498678365, accuracy: 0.5133333333333333\n",
      "iteration no 95: Loss: 0.7597235792971999, accuracy: 0.5133333333333333\n",
      "iteration no 96: Loss: 0.7596171817206625, accuracy: 0.5133333333333333\n",
      "iteration no 97: Loss: 0.7595146913233816, accuracy: 0.5133333333333333\n",
      "iteration no 98: Loss: 0.7594159498784425, accuracy: 0.5133333333333333\n",
      "iteration no 99: Loss: 0.7593208063531316, accuracy: 0.5133333333333333\n",
      "iteration no 100: Loss: 0.7592291165383452, accuracy: 0.5133333333333333\n",
      "iteration no 101: Loss: 0.7591407426996108, accuracy: 0.5133333333333333\n",
      "iteration no 102: Loss: 0.7590555532483022, accuracy: 0.5166666666666667\n",
      "iteration no 103: Loss: 0.7589734224317353, accuracy: 0.5166666666666667\n",
      "iteration no 104: Loss: 0.7588942300409298, accuracy: 0.5166666666666667\n",
      "iteration no 105: Loss: 0.7588178611349026, accuracy: 0.5166666666666667\n",
      "iteration no 106: Loss: 0.7587442057804454, accuracy: 0.5166666666666667\n",
      "iteration no 107: Loss: 0.7586731588064101, accuracy: 0.5166666666666667\n",
      "iteration no 108: Loss: 0.7586046195715924, accuracy: 0.5166666666666667\n",
      "iteration no 109: Loss: 0.7585384917453748, accuracy: 0.5166666666666667\n",
      "iteration no 110: Loss: 0.7584746831003346, accuracy: 0.5166666666666667\n",
      "iteration no 111: Loss: 0.7584131053160941, accuracy: 0.5166666666666667\n",
      "iteration no 112: Loss: 0.7583536737937202, accuracy: 0.5166666666666667\n",
      "iteration no 113: Loss: 0.7582963074800487, accuracy: 0.5166666666666667\n",
      "iteration no 114: Loss: 0.7582409287013282, accuracy: 0.5166666666666667\n",
      "iteration no 115: Loss: 0.7581874630056371, accuracy: 0.5166666666666667\n",
      "iteration no 116: Loss: 0.7581358390135536, accuracy: 0.5166666666666667\n",
      "iteration no 117: Loss: 0.7580859882765927, accuracy: 0.5166666666666667\n",
      "iteration no 118: Loss: 0.7580378451429625, accuracy: 0.5166666666666667\n",
      "iteration no 119: Loss: 0.7579913466302128, accuracy: 0.5166666666666667\n",
      "iteration no 120: Loss: 0.7579464323043827, accuracy: 0.5166666666666667\n",
      "iteration no 121: Loss: 0.7579030441652744, accuracy: 0.5166666666666667\n",
      "iteration no 122: Loss: 0.7578611265375084, accuracy: 0.5166666666666667\n",
      "iteration no 123: Loss: 0.7578206259670326, accuracy: 0.5166666666666667\n",
      "iteration no 124: Loss: 0.7577814911227818, accuracy: 0.5166666666666667\n",
      "iteration no 125: Loss: 0.7577436727032019, accuracy: 0.5166666666666667\n",
      "iteration no 126: Loss: 0.7577071233473666, accuracy: 0.5166666666666667\n",
      "iteration no 127: Loss: 0.7576717975504416, accuracy: 0.5166666666666667\n",
      "iteration no 128: Loss: 0.7576376515832504, accuracy: 0.5166666666666667\n",
      "iteration no 129: Loss: 0.7576046434157296, accuracy: 0.5166666666666667\n",
      "iteration no 130: Loss: 0.7575727326440552, accuracy: 0.5166666666666667\n",
      "iteration no 131: Loss: 0.7575418804212494, accuracy: 0.5166666666666667\n",
      "iteration no 132: Loss: 0.7575120493910809, accuracy: 0.5166666666666667\n",
      "iteration no 133: Loss: 0.7574832036250846, accuracy: 0.5166666666666667\n",
      "iteration no 134: Loss: 0.7574553085625376, accuracy: 0.5133333333333333\n",
      "iteration no 135: Loss: 0.7574283309532355, accuracy: 0.5133333333333333\n",
      "iteration no 136: Loss: 0.7574022388029265, accuracy: 0.5133333333333333\n",
      "iteration no 137: Loss: 0.7573770013212628, accuracy: 0.5133333333333333\n",
      "iteration no 138: Loss: 0.7573525888721444, accuracy: 0.5133333333333333\n",
      "iteration no 139: Loss: 0.7573289729263268, accuracy: 0.5133333333333333\n",
      "iteration no 140: Loss: 0.7573061260161865, accuracy: 0.5133333333333333\n",
      "iteration no 141: Loss: 0.7572840216925274, accuracy: 0.5133333333333333\n",
      "iteration no 142: Loss: 0.7572626344833298, accuracy: 0.5133333333333333\n",
      "iteration no 143: Loss: 0.7572419398543463, accuracy: 0.5133333333333333\n",
      "iteration no 144: Loss: 0.7572219141714494, accuracy: 0.5133333333333333\n",
      "iteration no 145: Loss: 0.7572025346646473, accuracy: 0.5133333333333333\n",
      "iteration no 146: Loss: 0.7571837793936846, accuracy: 0.5133333333333333\n",
      "iteration no 147: Loss: 0.7571656272151518, accuracy: 0.5133333333333333\n",
      "iteration no 148: Loss: 0.7571480577510288, accuracy: 0.5133333333333333\n",
      "iteration no 149: Loss: 0.7571310513585948, accuracy: 0.5133333333333333\n",
      "iteration no 150: Loss: 0.7571145891016391, accuracy: 0.5133333333333333\n",
      "iteration no 151: Loss: 0.7570986527229078, accuracy: 0.5133333333333333\n",
      "iteration no 152: Loss: 0.7570832246177323, accuracy: 0.5133333333333333\n",
      "iteration no 153: Loss: 0.7570682878087798, accuracy: 0.5133333333333333\n",
      "iteration no 154: Loss: 0.7570538259218749, accuracy: 0.5133333333333333\n",
      "iteration no 155: Loss: 0.7570398231628419, accuracy: 0.5133333333333333\n",
      "iteration no 156: Loss: 0.7570262642953209, accuracy: 0.5133333333333333\n",
      "iteration no 157: Loss: 0.7570131346195113, accuracy: 0.5133333333333333\n",
      "iteration no 158: Loss: 0.7570004199517999, accuracy: 0.5133333333333333\n",
      "iteration no 159: Loss: 0.7569881066052365, accuracy: 0.5133333333333333\n",
      "iteration no 160: Loss: 0.7569761813708117, accuracy: 0.5133333333333333\n",
      "iteration no 161: Loss: 0.7569646314995083, accuracy: 0.5133333333333333\n",
      "iteration no 162: Loss: 0.7569534446850853, accuracy: 0.5133333333333333\n",
      "iteration no 163: Loss: 0.7569426090475633, accuracy: 0.5133333333333333\n",
      "iteration no 164: Loss: 0.7569321131173818, accuracy: 0.5133333333333333\n",
      "iteration no 165: Loss: 0.7569219458201968, accuracy: 0.5133333333333333\n",
      "iteration no 166: Loss: 0.7569120964622886, accuracy: 0.5133333333333333\n",
      "iteration no 167: Loss: 0.7569025547165578, accuracy: 0.5133333333333333\n",
      "iteration no 168: Loss: 0.7568933106090775, accuracy: 0.5133333333333333\n",
      "iteration no 169: Loss: 0.7568843545061829, accuracy: 0.5133333333333333\n",
      "iteration no 170: Loss: 0.7568756771020703, accuracy: 0.5133333333333333\n",
      "iteration no 171: Loss: 0.7568672694068885, accuracy: 0.5133333333333333\n",
      "iteration no 172: Loss: 0.7568591227352954, accuracy: 0.5133333333333333\n",
      "iteration no 173: Loss: 0.7568512286954643, accuracy: 0.5133333333333333\n",
      "iteration no 174: Loss: 0.7568435791785195, accuracy: 0.5133333333333333\n",
      "iteration no 175: Loss: 0.7568361663483822, accuracy: 0.5133333333333333\n",
      "iteration no 176: Loss: 0.7568289826320088, accuracy: 0.5133333333333333\n",
      "iteration no 177: Loss: 0.7568220207100083, accuracy: 0.5133333333333333\n",
      "iteration no 178: Loss: 0.7568152735076177, accuracy: 0.5133333333333333\n",
      "iteration no 179: Loss: 0.7568087341860253, accuracy: 0.5133333333333333\n",
      "iteration no 180: Loss: 0.7568023961340243, accuracy: 0.5133333333333333\n",
      "iteration no 181: Loss: 0.7567962529599843, accuracy: 0.5133333333333333\n",
      "iteration no 182: Loss: 0.756790298484127, accuracy: 0.5133333333333333\n",
      "iteration no 183: Loss: 0.7567845267310938, accuracy: 0.5133333333333333\n",
      "iteration no 184: Loss: 0.7567789319227936, accuracy: 0.5133333333333333\n",
      "iteration no 185: Loss: 0.7567735084715187, accuracy: 0.5133333333333333\n",
      "iteration no 186: Loss: 0.7567682509733186, accuracy: 0.5133333333333333\n",
      "iteration no 187: Loss: 0.75676315420162, accuracy: 0.5133333333333333\n",
      "iteration no 188: Loss: 0.7567582131010857, accuracy: 0.5133333333333333\n",
      "iteration no 189: Loss: 0.7567534227816985, accuracy: 0.5133333333333333\n",
      "iteration no 190: Loss: 0.7567487785130658, accuracy: 0.5133333333333333\n",
      "iteration no 191: Loss: 0.7567442757189325, accuracy: 0.5133333333333333\n",
      "iteration no 192: Loss: 0.7567399099718966, accuracy: 0.5133333333333333\n",
      "iteration no 193: Loss: 0.7567356769883161, accuracy: 0.5133333333333333\n",
      "iteration no 194: Loss: 0.7567315726234036, accuracy: 0.5133333333333333\n",
      "iteration no 195: Loss: 0.7567275928664992, accuracy: 0.5133333333333333\n",
      "iteration no 196: Loss: 0.7567237338365121, accuracy: 0.5133333333333333\n",
      "iteration no 197: Loss: 0.7567199917775312, accuracy: 0.5133333333333333\n",
      "iteration no 198: Loss: 0.7567163630545911, accuracy: 0.5133333333333333\n",
      "iteration no 199: Loss: 0.7567128441495902, accuracy: 0.5133333333333333\n",
      "iteration no 200: Loss: 0.7567094316573567, accuracy: 0.5166666666666667\n",
      "iteration no 1: Loss: 1.1101485789942147, accuracy: 0.53\n",
      "iteration no 2: Loss: 1.0780790824400277, accuracy: 0.5366666666666666\n",
      "iteration no 3: Loss: 1.0495558252945578, accuracy: 0.54\n",
      "iteration no 4: Loss: 1.0241521224964567, accuracy: 0.5233333333333333\n",
      "iteration no 5: Loss: 1.0014912388708892, accuracy: 0.5266666666666666\n",
      "iteration no 6: Loss: 0.9812405790379315, accuracy: 0.52\n",
      "iteration no 7: Loss: 0.9631075220801691, accuracy: 0.5266666666666666\n",
      "iteration no 8: Loss: 0.9468356378645637, accuracy: 0.5233333333333333\n",
      "iteration no 9: Loss: 0.9322009634455896, accuracy: 0.52\n",
      "iteration no 10: Loss: 0.9190083545163479, accuracy: 0.5133333333333333\n",
      "iteration no 11: Loss: 0.9070880083866972, accuracy: 0.5166666666666667\n",
      "iteration no 12: Loss: 0.8962922436176648, accuracy: 0.52\n",
      "iteration no 13: Loss: 0.8864925864442678, accuracy: 0.52\n",
      "iteration no 14: Loss: 0.877577181294916, accuracy: 0.5166666666666667\n",
      "iteration no 15: Loss: 0.8694485191571784, accuracy: 0.5166666666666667\n",
      "iteration no 16: Loss: 0.8620214631917664, accuracy: 0.5166666666666667\n",
      "iteration no 17: Loss: 0.8552215436978122, accuracy: 0.5166666666666667\n",
      "iteration no 18: Loss: 0.8489834919760614, accuracy: 0.5166666666666667\n",
      "iteration no 19: Loss: 0.8432499829822134, accuracy: 0.5166666666666667\n",
      "iteration no 20: Loss: 0.8379705586158817, accuracy: 0.5166666666666667\n",
      "iteration no 21: Loss: 0.8331007062087267, accuracy: 0.5166666666666667\n",
      "iteration no 22: Loss: 0.8286010697420979, accuracy: 0.5166666666666667\n",
      "iteration no 23: Loss: 0.8244367742433061, accuracy: 0.5166666666666667\n",
      "iteration no 24: Loss: 0.8205768465239625, accuracy: 0.5166666666666667\n",
      "iteration no 25: Loss: 0.816993717863124, accuracy: 0.5166666666666667\n",
      "iteration no 26: Loss: 0.8136627963821861, accuracy: 0.5166666666666667\n",
      "iteration no 27: Loss: 0.8105620987154959, accuracy: 0.5166666666666667\n",
      "iteration no 28: Loss: 0.8076719321727795, accuracy: 0.5166666666666667\n",
      "iteration no 29: Loss: 0.804974619944962, accuracy: 0.5166666666666667\n",
      "iteration no 30: Loss: 0.8024542630535423, accuracy: 0.5166666666666667\n",
      "iteration no 31: Loss: 0.8000965337138684, accuracy: 0.5166666666666667\n",
      "iteration no 32: Loss: 0.797888495600541, accuracy: 0.5166666666666667\n",
      "iteration no 33: Loss: 0.7958184471919191, accuracy: 0.5166666666666667\n",
      "iteration no 34: Loss: 0.7938757849504411, accuracy: 0.5166666666666667\n",
      "iteration no 35: Loss: 0.7920508835835226, accuracy: 0.5133333333333333\n",
      "iteration no 36: Loss: 0.7903349910408389, accuracy: 0.5133333333333333\n",
      "iteration no 37: Loss: 0.7887201362502757, accuracy: 0.5133333333333333\n",
      "iteration no 38: Loss: 0.7871990478871927, accuracy: 0.5133333333333333\n",
      "iteration no 39: Loss: 0.7857650827186352, accuracy: 0.5133333333333333\n",
      "iteration no 40: Loss: 0.7844121622730854, accuracy: 0.5133333333333333\n",
      "iteration no 41: Loss: 0.7831347167633952, accuracy: 0.5133333333333333\n",
      "iteration no 42: Loss: 0.7819276353407791, accuracy: 0.5133333333333333\n",
      "iteration no 43: Loss: 0.7807862218854663, accuracy: 0.5133333333333333\n",
      "iteration no 44: Loss: 0.7797061556483448, accuracy: 0.5133333333333333\n",
      "iteration no 45: Loss: 0.7786834561506946, accuracy: 0.5133333333333333\n",
      "iteration no 46: Loss: 0.7777144518283565, accuracy: 0.5133333333333333\n",
      "iteration no 47: Loss: 0.7767957519745289, accuracy: 0.5133333333333333\n",
      "iteration no 48: Loss: 0.775924221593564, accuracy: 0.5133333333333333\n",
      "iteration no 49: Loss: 0.7750969588281075, accuracy: 0.5133333333333333\n",
      "iteration no 50: Loss: 0.7743112746649468, accuracy: 0.5133333333333333\n",
      "iteration no 51: Loss: 0.773564674662003, accuracy: 0.5133333333333333\n",
      "iteration no 52: Loss: 0.7728548424709392, accuracy: 0.5133333333333333\n",
      "iteration no 53: Loss: 0.7721796249575555, accuracy: 0.5133333333333333\n",
      "iteration no 54: Loss: 0.7715370187461633, accuracy: 0.5133333333333333\n",
      "iteration no 55: Loss: 0.7709251580349701, accuracy: 0.5133333333333333\n",
      "iteration no 56: Loss: 0.7703423035476441, accuracy: 0.5133333333333333\n",
      "iteration no 57: Loss: 0.7697868325020124, accuracy: 0.5166666666666667\n",
      "iteration no 58: Loss: 0.7692572294906302, accuracy: 0.5166666666666667\n",
      "iteration no 59: Loss: 0.7687520781799979, accuracy: 0.5166666666666667\n",
      "iteration no 60: Loss: 0.7682700537457416, accuracy: 0.5166666666666667\n",
      "iteration no 61: Loss: 0.7678099159703171, accuracy: 0.5166666666666667\n",
      "iteration no 62: Loss: 0.7673705029379052, accuracy: 0.5166666666666667\n",
      "iteration no 63: Loss: 0.7669507252683087, accuracy: 0.5166666666666667\n",
      "iteration no 64: Loss: 0.7665495608379329, accuracy: 0.5166666666666667\n",
      "iteration no 65: Loss: 0.7661660499414908, accuracy: 0.5166666666666667\n",
      "iteration no 66: Loss: 0.7657992908529512, accuracy: 0.5166666666666667\n",
      "iteration no 67: Loss: 0.7654484357485896, accuracy: 0.5166666666666667\n",
      "iteration no 68: Loss: 0.7651126869588334, accuracy: 0.5166666666666667\n",
      "iteration no 69: Loss: 0.7647912935189939, accuracy: 0.5166666666666667\n",
      "iteration no 70: Loss: 0.7644835479919964, accuracy: 0.5166666666666667\n",
      "iteration no 71: Loss: 0.7641887835389136, accuracy: 0.5166666666666667\n",
      "iteration no 72: Loss: 0.7639063712154909, accuracy: 0.5166666666666667\n",
      "iteration no 73: Loss: 0.7636357174749901, accuracy: 0.5166666666666667\n",
      "iteration no 74: Loss: 0.763376261859585, accuracy: 0.5166666666666667\n",
      "iteration no 75: Loss: 0.7631274748642337, accuracy: 0.5166666666666667\n",
      "iteration no 76: Loss: 0.7628888559584908, accuracy: 0.5166666666666667\n",
      "iteration no 77: Loss: 0.7626599317530721, accuracy: 0.5166666666666667\n",
      "iteration no 78: Loss: 0.7624402542992191, accuracy: 0.5133333333333333\n",
      "iteration no 79: Loss: 0.7622293995100035, accuracy: 0.5133333333333333\n",
      "iteration no 80: Loss: 0.7620269656937031, accuracy: 0.5133333333333333\n",
      "iteration no 81: Loss: 0.7618325721902706, accuracy: 0.5133333333333333\n",
      "iteration no 82: Loss: 0.7616458581027122, accuracy: 0.5133333333333333\n",
      "iteration no 83: Loss: 0.7614664811159217, accuracy: 0.5133333333333333\n",
      "iteration no 84: Loss: 0.7612941163961684, accuracy: 0.5133333333333333\n",
      "iteration no 85: Loss: 0.761128455565018, accuracy: 0.5133333333333333\n",
      "iteration no 86: Loss: 0.7609692057420123, accuracy: 0.5133333333333333\n",
      "iteration no 87: Loss: 0.7608160886509042, accuracy: 0.5133333333333333\n",
      "iteration no 88: Loss: 0.7606688397846896, accuracy: 0.5133333333333333\n",
      "iteration no 89: Loss: 0.7605272076250741, accuracy: 0.5133333333333333\n",
      "iteration no 90: Loss: 0.7603909529123696, accuracy: 0.5133333333333333\n",
      "iteration no 91: Loss: 0.7602598479621483, accuracy: 0.5133333333333333\n",
      "iteration no 92: Loss: 0.7601336760252762, accuracy: 0.5133333333333333\n",
      "iteration no 93: Loss: 0.7600122306882227, accuracy: 0.5133333333333333\n",
      "iteration no 94: Loss: 0.7598953153107894, accuracy: 0.5133333333333333\n",
      "iteration no 95: Loss: 0.7597827424986238, accuracy: 0.5133333333333333\n",
      "iteration no 96: Loss: 0.7596743336080988, accuracy: 0.5133333333333333\n",
      "iteration no 97: Loss: 0.7595699182813139, accuracy: 0.5133333333333333\n",
      "iteration no 98: Loss: 0.75946933400916, accuracy: 0.5133333333333333\n",
      "iteration no 99: Loss: 0.7593724257205366, accuracy: 0.5133333333333333\n",
      "iteration no 100: Loss: 0.7592790453959612, accuracy: 0.5133333333333333\n",
      "iteration no 101: Loss: 0.7591890517039391, accuracy: 0.5133333333333333\n",
      "iteration no 102: Loss: 0.7591023096585867, accuracy: 0.5133333333333333\n",
      "iteration no 103: Loss: 0.7590186902971091, accuracy: 0.5166666666666667\n",
      "iteration no 104: Loss: 0.7589380703758367, accuracy: 0.5166666666666667\n",
      "iteration no 105: Loss: 0.7588603320836225, accuracy: 0.5166666666666667\n",
      "iteration no 106: Loss: 0.7587853627714806, accuracy: 0.5166666666666667\n",
      "iteration no 107: Loss: 0.7587130546974384, accuracy: 0.5166666666666667\n",
      "iteration no 108: Loss: 0.7586433047856325, accuracy: 0.5166666666666667\n",
      "iteration no 109: Loss: 0.7585760143987614, accuracy: 0.5166666666666667\n",
      "iteration no 110: Loss: 0.7585110891230611, accuracy: 0.5166666666666667\n",
      "iteration no 111: Loss: 0.7584484385650286, accuracy: 0.5166666666666667\n",
      "iteration no 112: Loss: 0.7583879761591749, accuracy: 0.5166666666666667\n",
      "iteration no 113: Loss: 0.758329618986135, accuracy: 0.5166666666666667\n",
      "iteration no 114: Loss: 0.7582732876005094, accuracy: 0.5166666666666667\n",
      "iteration no 115: Loss: 0.7582189058678529, accuracy: 0.5166666666666667\n",
      "iteration no 116: Loss: 0.7581664008102658, accuracy: 0.5166666666666667\n",
      "iteration no 117: Loss: 0.7581157024600798, accuracy: 0.5166666666666667\n",
      "iteration no 118: Loss: 0.7580667437211612, accuracy: 0.5166666666666667\n",
      "iteration no 119: Loss: 0.7580194602373901, accuracy: 0.5166666666666667\n",
      "iteration no 120: Loss: 0.757973790267896, accuracy: 0.5166666666666667\n",
      "iteration no 121: Loss: 0.7579296745686661, accuracy: 0.5166666666666667\n",
      "iteration no 122: Loss: 0.7578870562801573, accuracy: 0.5166666666666667\n",
      "iteration no 123: Loss: 0.7578458808205758, accuracy: 0.5166666666666667\n",
      "iteration no 124: Loss: 0.7578060957845016, accuracy: 0.5166666666666667\n",
      "iteration no 125: Loss: 0.7577676508465588, accuracy: 0.5166666666666667\n",
      "iteration no 126: Loss: 0.7577304976698543, accuracy: 0.5166666666666667\n",
      "iteration no 127: Loss: 0.7576945898189176, accuracy: 0.5166666666666667\n",
      "iteration no 128: Loss: 0.7576598826768957, accuracy: 0.5166666666666667\n",
      "iteration no 129: Loss: 0.7576263333667727, accuracy: 0.5166666666666667\n",
      "iteration no 130: Loss: 0.7575939006763938, accuracy: 0.5133333333333333\n",
      "iteration no 131: Loss: 0.7575625449870896, accuracy: 0.5133333333333333\n",
      "iteration no 132: Loss: 0.7575322282057076, accuracy: 0.5133333333333333\n",
      "iteration no 133: Loss: 0.7575029136998712, accuracy: 0.5133333333333333\n",
      "iteration no 134: Loss: 0.7574745662362924, accuracy: 0.5133333333333333\n",
      "iteration no 135: Loss: 0.7574471519219812, accuracy: 0.5133333333333333\n",
      "iteration no 136: Loss: 0.7574206381481985, accuracy: 0.5133333333333333\n",
      "iteration no 137: Loss: 0.7573949935370095, accuracy: 0.5133333333333333\n",
      "iteration no 138: Loss: 0.7573701878903054, accuracy: 0.5133333333333333\n",
      "iteration no 139: Loss: 0.7573461921411644, accuracy: 0.5133333333333333\n",
      "iteration no 140: Loss: 0.7573229783074343, accuracy: 0.5133333333333333\n",
      "iteration no 141: Loss: 0.7573005194474242, accuracy: 0.5133333333333333\n",
      "iteration no 142: Loss: 0.7572787896175969, accuracy: 0.5133333333333333\n",
      "iteration no 143: Loss: 0.7572577638321637, accuracy: 0.5133333333333333\n",
      "iteration no 144: Loss: 0.7572374180244866, accuracy: 0.5133333333333333\n",
      "iteration no 145: Loss: 0.7572177290101967, accuracy: 0.5133333333333333\n",
      "iteration no 146: Loss: 0.7571986744519451, accuracy: 0.5133333333333333\n",
      "iteration no 147: Loss: 0.7571802328257076, accuracy: 0.5133333333333333\n",
      "iteration no 148: Loss: 0.7571623833885653, accuracy: 0.5133333333333333\n",
      "iteration no 149: Loss: 0.7571451061478901, accuracy: 0.5133333333333333\n",
      "iteration no 150: Loss: 0.7571283818318683, accuracy: 0.5133333333333333\n",
      "iteration no 151: Loss: 0.7571121918612957, accuracy: 0.5133333333333333\n",
      "iteration no 152: Loss: 0.7570965183225888, accuracy: 0.5133333333333333\n",
      "iteration no 153: Loss: 0.757081343941945, accuracy: 0.5133333333333333\n",
      "iteration no 154: Loss: 0.7570666520606094, accuracy: 0.5133333333333333\n",
      "iteration no 155: Loss: 0.7570524266111848, accuracy: 0.5133333333333333\n",
      "iteration no 156: Loss: 0.7570386520949439, accuracy: 0.5133333333333333\n",
      "iteration no 157: Loss: 0.7570253135600952, accuracy: 0.5133333333333333\n",
      "iteration no 158: Loss: 0.757012396580955, accuracy: 0.5133333333333333\n",
      "iteration no 159: Loss: 0.7569998872379881, accuracy: 0.5133333333333333\n",
      "iteration no 160: Loss: 0.7569877720986764, accuracy: 0.5133333333333333\n",
      "iteration no 161: Loss: 0.7569760381991757, accuracy: 0.5133333333333333\n",
      "iteration no 162: Loss: 0.756964673026726, accuracy: 0.5133333333333333\n",
      "iteration no 163: Loss: 0.756953664502783, accuracy: 0.5133333333333333\n",
      "iteration no 164: Loss: 0.7569430009668353, accuracy: 0.5133333333333333\n",
      "iteration no 165: Loss: 0.7569326711608794, accuracy: 0.5133333333333333\n",
      "iteration no 166: Loss: 0.7569226642145203, accuracy: 0.5133333333333333\n",
      "iteration no 167: Loss: 0.7569129696306732, accuracy: 0.5133333333333333\n",
      "iteration no 168: Loss: 0.7569035772718381, accuracy: 0.5133333333333333\n",
      "iteration no 169: Loss: 0.7568944773469206, accuracy: 0.5133333333333333\n",
      "iteration no 170: Loss: 0.7568856603985791, accuracy: 0.5133333333333333\n",
      "iteration no 171: Loss: 0.7568771172910704, accuracy: 0.5133333333333333\n",
      "iteration no 172: Loss: 0.756868839198577, accuracy: 0.5133333333333333\n",
      "iteration no 173: Loss: 0.7568608175939915, accuracy: 0.5133333333333333\n",
      "iteration no 174: Loss: 0.7568530442381416, accuracy: 0.5133333333333333\n",
      "iteration no 175: Loss: 0.7568455111694333, accuracy: 0.5133333333333333\n",
      "iteration no 176: Loss: 0.7568382106938981, accuracy: 0.5133333333333333\n",
      "iteration no 177: Loss: 0.7568311353756247, accuracy: 0.5133333333333333\n",
      "iteration no 178: Loss: 0.7568242780275595, accuracy: 0.5133333333333333\n",
      "iteration no 179: Loss: 0.756817631702662, accuracy: 0.5133333333333333\n",
      "iteration no 180: Loss: 0.756811189685397, accuracy: 0.5133333333333333\n",
      "iteration no 181: Loss: 0.7568049454835521, accuracy: 0.5133333333333333\n",
      "iteration no 182: Loss: 0.7567988928203675, accuracy: 0.5133333333333333\n",
      "iteration no 183: Loss: 0.7567930256269622, accuracy: 0.5133333333333333\n",
      "iteration no 184: Loss: 0.7567873380350467, accuracy: 0.5133333333333333\n",
      "iteration no 185: Loss: 0.7567818243699109, accuracy: 0.5133333333333333\n",
      "iteration no 186: Loss: 0.7567764791436741, accuracy: 0.5133333333333333\n",
      "iteration no 187: Loss: 0.7567712970487871, accuracy: 0.5133333333333333\n",
      "iteration no 188: Loss: 0.756766272951777, accuracy: 0.5133333333333333\n",
      "iteration no 189: Loss: 0.7567614018872244, accuracy: 0.5133333333333333\n",
      "iteration no 190: Loss: 0.7567566790519631, accuracy: 0.5133333333333333\n",
      "iteration no 191: Loss: 0.7567520997994934, accuracy: 0.5133333333333333\n",
      "iteration no 192: Loss: 0.7567476596346032, accuracy: 0.5133333333333333\n",
      "iteration no 193: Loss: 0.7567433542081818, accuracy: 0.5133333333333333\n",
      "iteration no 194: Loss: 0.7567391793122298, accuracy: 0.5133333333333333\n",
      "iteration no 195: Loss: 0.7567351308750436, accuracy: 0.5133333333333333\n",
      "iteration no 196: Loss: 0.7567312049565813, accuracy: 0.5133333333333333\n",
      "iteration no 197: Loss: 0.7567273977439921, accuracy: 0.5133333333333333\n",
      "iteration no 198: Loss: 0.7567237055473098, accuracy: 0.5133333333333333\n",
      "iteration no 199: Loss: 0.7567201247952997, accuracy: 0.5133333333333333\n",
      "iteration no 200: Loss: 0.7567166520314556, accuracy: 0.5166666666666667\n",
      "iteration no 1: Loss: 1.0954331907529957, accuracy: 0.4266666666666667\n",
      "iteration no 2: Loss: 1.0654346896478417, accuracy: 0.4533333333333333\n",
      "iteration no 3: Loss: 1.0387171598572364, accuracy: 0.47\n",
      "iteration no 4: Loss: 1.0148813165706314, accuracy: 0.4766666666666667\n",
      "iteration no 5: Loss: 0.9935762364173643, accuracy: 0.47333333333333333\n",
      "iteration no 6: Loss: 0.9744940826756243, accuracy: 0.4866666666666667\n",
      "iteration no 7: Loss: 0.9573655923642244, accuracy: 0.49666666666666665\n",
      "iteration no 8: Loss: 0.9419558535043928, accuracy: 0.49333333333333335\n",
      "iteration no 9: Loss: 0.9280603019575793, accuracy: 0.49666666666666665\n",
      "iteration no 10: Loss: 0.9155009827774384, accuracy: 0.49333333333333335\n",
      "iteration no 11: Loss: 0.9041231293537583, accuracy: 0.49333333333333335\n",
      "iteration no 12: Loss: 0.8937920907186577, accuracy: 0.5\n",
      "iteration no 13: Loss: 0.8843906120932733, accuracy: 0.51\n",
      "iteration no 14: Loss: 0.8758164548204614, accuracy: 0.51\n",
      "iteration no 15: Loss: 0.8679803302511595, accuracy: 0.51\n",
      "iteration no 16: Loss: 0.8608041164742706, accuracy: 0.51\n",
      "iteration no 17: Loss: 0.8542193251625158, accuracy: 0.51\n",
      "iteration no 18: Loss: 0.8481657867038429, accuracy: 0.51\n",
      "iteration no 19: Loss: 0.8425905240811137, accuracy: 0.51\n",
      "iteration no 20: Loss: 0.8374467889083991, accuracy: 0.51\n",
      "iteration no 21: Loss: 0.8326932361670555, accuracy: 0.51\n",
      "iteration no 22: Loss: 0.8282932172389041, accuracy: 0.51\n",
      "iteration no 23: Loss: 0.8242141736633996, accuracy: 0.51\n",
      "iteration no 24: Loss: 0.8204271165858509, accuracy: 0.5066666666666667\n",
      "iteration no 25: Loss: 0.8169061790969394, accuracy: 0.51\n",
      "iteration no 26: Loss: 0.8136282305991305, accuracy: 0.51\n",
      "iteration no 27: Loss: 0.8105725439961948, accuracy: 0.51\n",
      "iteration no 28: Loss: 0.8077205079169796, accuracy: 0.51\n",
      "iteration no 29: Loss: 0.8050553773843568, accuracy: 0.51\n",
      "iteration no 30: Loss: 0.8025620573542966, accuracy: 0.51\n",
      "iteration no 31: Loss: 0.8002269144052169, accuracy: 0.51\n",
      "iteration no 32: Loss: 0.7980376125781623, accuracy: 0.51\n",
      "iteration no 33: Loss: 0.7959829699748815, accuracy: 0.5133333333333333\n",
      "iteration no 34: Loss: 0.7940528332315083, accuracy: 0.5133333333333333\n",
      "iteration no 35: Loss: 0.7922379674156494, accuracy: 0.5133333333333333\n",
      "iteration no 36: Loss: 0.7905299592572075, accuracy: 0.5133333333333333\n",
      "iteration no 37: Loss: 0.7889211319291501, accuracy: 0.5133333333333333\n",
      "iteration no 38: Loss: 0.7874044698528373, accuracy: 0.5133333333333333\n",
      "iteration no 39: Loss: 0.7859735522210924, accuracy: 0.5133333333333333\n",
      "iteration no 40: Loss: 0.7846224941173985, accuracy: 0.5166666666666667\n",
      "iteration no 41: Loss: 0.7833458942667133, accuracy: 0.5166666666666667\n",
      "iteration no 42: Loss: 0.7821387885869632, accuracy: 0.5166666666666667\n",
      "iteration no 43: Loss: 0.780996608823967, accuracy: 0.5133333333333333\n",
      "iteration no 44: Loss: 0.7799151456495309, accuracy: 0.5133333333333333\n",
      "iteration no 45: Loss: 0.7788905156853055, accuracy: 0.5133333333333333\n",
      "iteration no 46: Loss: 0.777919131985927, accuracy: 0.5133333333333333\n",
      "iteration no 47: Loss: 0.7769976775757773, accuracy: 0.5133333333333333\n",
      "iteration no 48: Loss: 0.7761230816859482, accuracy: 0.5133333333333333\n",
      "iteration no 49: Loss: 0.7752924983829584, accuracy: 0.5133333333333333\n",
      "iteration no 50: Loss: 0.7745032873195365, accuracy: 0.5133333333333333\n",
      "iteration no 51: Loss: 0.7737529963712687, accuracy: 0.5133333333333333\n",
      "iteration no 52: Loss: 0.7730393459518797, accuracy: 0.5133333333333333\n",
      "iteration no 53: Loss: 0.772360214825027, accuracy: 0.5133333333333333\n",
      "iteration no 54: Loss: 0.7717136272522914, accuracy: 0.5133333333333333\n",
      "iteration no 55: Loss: 0.7710977413360054, accuracy: 0.5133333333333333\n",
      "iteration no 56: Loss: 0.7705108384320903, accuracy: 0.5133333333333333\n",
      "iteration no 57: Loss: 0.7699513135224811, accuracy: 0.5133333333333333\n",
      "iteration no 58: Loss: 0.7694176664493242, accuracy: 0.5166666666666667\n",
      "iteration no 59: Loss: 0.7689084939241664, accuracy: 0.5166666666666667\n",
      "iteration no 60: Loss: 0.7684224822350247, accuracy: 0.5166666666666667\n",
      "iteration no 61: Loss: 0.7679584005827287, accuracy: 0.5166666666666667\n",
      "iteration no 62: Loss: 0.7675150949853945, accuracy: 0.5166666666666667\n",
      "iteration no 63: Loss: 0.7670914826964773, accuracy: 0.5166666666666667\n",
      "iteration no 64: Loss: 0.7666865470876577, accuracy: 0.5166666666666667\n",
      "iteration no 65: Loss: 0.7662993329529469, accuracy: 0.5166666666666667\n",
      "iteration no 66: Loss: 0.7659289421949381, accuracy: 0.5166666666666667\n",
      "iteration no 67: Loss: 0.7655745298581513, accuracy: 0.5166666666666667\n",
      "iteration no 68: Loss: 0.7652353004779937, accuracy: 0.5166666666666667\n",
      "iteration no 69: Loss: 0.764910504717019, accuracy: 0.5166666666666667\n",
      "iteration no 70: Loss: 0.7645994362630039, accuracy: 0.5166666666666667\n",
      "iteration no 71: Loss: 0.7643014289658533, accuracy: 0.5166666666666667\n",
      "iteration no 72: Loss: 0.7640158541926098, accuracy: 0.5166666666666667\n",
      "iteration no 73: Loss: 0.7637421183818227, accuracy: 0.5166666666666667\n",
      "iteration no 74: Loss: 0.7634796607803331, accuracy: 0.5166666666666667\n",
      "iteration no 75: Loss: 0.7632279513471334, accuracy: 0.5166666666666667\n",
      "iteration no 76: Loss: 0.762986488810387, accuracy: 0.5166666666666667\n",
      "iteration no 77: Loss: 0.7627547988649908, accuracy: 0.5166666666666667\n",
      "iteration no 78: Loss: 0.7625324324992137, accuracy: 0.5166666666666667\n",
      "iteration no 79: Loss: 0.7623189644399888, accuracy: 0.5166666666666667\n",
      "iteration no 80: Loss: 0.7621139917073692, accuracy: 0.5166666666666667\n",
      "iteration no 81: Loss: 0.7619171322695084, accuracy: 0.5166666666666667\n",
      "iteration no 82: Loss: 0.7617280237902789, accuracy: 0.5133333333333333\n",
      "iteration no 83: Loss: 0.761546322462338, accuracy: 0.5133333333333333\n",
      "iteration no 84: Loss: 0.7613717019190673, accuracy: 0.5133333333333333\n",
      "iteration no 85: Loss: 0.761203852219378, accuracy: 0.5133333333333333\n",
      "iteration no 86: Loss: 0.7610424788998785, accuracy: 0.5133333333333333\n",
      "iteration no 87: Loss: 0.7608873020893718, accuracy: 0.5133333333333333\n",
      "iteration no 88: Loss: 0.7607380556810578, accuracy: 0.5133333333333333\n",
      "iteration no 89: Loss: 0.7605944865582095, accuracy: 0.5133333333333333\n",
      "iteration no 90: Loss: 0.7604563538694279, accuracy: 0.5133333333333333\n",
      "iteration no 91: Loss: 0.7603234283499005, accuracy: 0.5133333333333333\n",
      "iteration no 92: Loss: 0.7601954916853759, accuracy: 0.5133333333333333\n",
      "iteration no 93: Loss: 0.7600723359158259, accuracy: 0.5133333333333333\n",
      "iteration no 94: Loss: 0.7599537628760074, accuracy: 0.5133333333333333\n",
      "iteration no 95: Loss: 0.7598395836703553, accuracy: 0.5133333333333333\n",
      "iteration no 96: Loss: 0.7597296181798318, accuracy: 0.5133333333333333\n",
      "iteration no 97: Loss: 0.7596236945985486, accuracy: 0.5133333333333333\n",
      "iteration no 98: Loss: 0.7595216489981362, accuracy: 0.5133333333333333\n",
      "iteration no 99: Loss: 0.7594233249179909, accuracy: 0.5133333333333333\n",
      "iteration no 100: Loss: 0.7593285729796758, accuracy: 0.5133333333333333\n",
      "iteration no 101: Loss: 0.759237250523868, accuracy: 0.5133333333333333\n",
      "iteration no 102: Loss: 0.7591492212683767, accuracy: 0.5166666666666667\n",
      "iteration no 103: Loss: 0.7590643549858523, accuracy: 0.5166666666666667\n",
      "iteration no 104: Loss: 0.7589825271999182, accuracy: 0.5166666666666667\n",
      "iteration no 105: Loss: 0.7589036188985361, accuracy: 0.5166666666666667\n",
      "iteration no 106: Loss: 0.7588275162635134, accuracy: 0.5166666666666667\n",
      "iteration no 107: Loss: 0.7587541104151272, accuracy: 0.5166666666666667\n",
      "iteration no 108: Loss: 0.7586832971709196, accuracy: 0.5166666666666667\n",
      "iteration no 109: Loss: 0.7586149768177812, accuracy: 0.5166666666666667\n",
      "iteration no 110: Loss: 0.7585490538964998, accuracy: 0.5166666666666667\n",
      "iteration no 111: Loss: 0.7584854369980126, accuracy: 0.5166666666666667\n",
      "iteration no 112: Loss: 0.7584240385706471, accuracy: 0.5166666666666667\n",
      "iteration no 113: Loss: 0.7583647747376858, accuracy: 0.5166666666666667\n",
      "iteration no 114: Loss: 0.7583075651246379, accuracy: 0.5166666666666667\n",
      "iteration no 115: Loss: 0.7582523326956364, accuracy: 0.5166666666666667\n",
      "iteration no 116: Loss: 0.7581990035984223, accuracy: 0.5166666666666667\n",
      "iteration no 117: Loss: 0.7581475070174102, accuracy: 0.5166666666666667\n",
      "iteration no 118: Loss: 0.7580977750343653, accuracy: 0.5166666666666667\n",
      "iteration no 119: Loss: 0.7580497424962493, accuracy: 0.5166666666666667\n",
      "iteration no 120: Loss: 0.7580033468898226, accuracy: 0.5166666666666667\n",
      "iteration no 121: Loss: 0.7579585282226176, accuracy: 0.5166666666666667\n",
      "iteration no 122: Loss: 0.7579152289099192, accuracy: 0.5166666666666667\n",
      "iteration no 123: Loss: 0.7578733936674176, accuracy: 0.5166666666666667\n",
      "iteration no 124: Loss: 0.7578329694092096, accuracy: 0.5166666666666667\n",
      "iteration no 125: Loss: 0.7577939051508571, accuracy: 0.5166666666666667\n",
      "iteration no 126: Loss: 0.7577561519172169, accuracy: 0.5166666666666667\n",
      "iteration no 127: Loss: 0.7577196626547851, accuracy: 0.5166666666666667\n",
      "iteration no 128: Loss: 0.757684392148306, accuracy: 0.5166666666666667\n",
      "iteration no 129: Loss: 0.7576502969414138, accuracy: 0.5166666666666667\n",
      "iteration no 130: Loss: 0.7576173352610935, accuracy: 0.5166666666666667\n",
      "iteration no 131: Loss: 0.7575854669457516, accuracy: 0.5166666666666667\n",
      "iteration no 132: Loss: 0.7575546533767065, accuracy: 0.5166666666666667\n",
      "iteration no 133: Loss: 0.757524857412919, accuracy: 0.5166666666666667\n",
      "iteration no 134: Loss: 0.7574960433287895, accuracy: 0.5166666666666667\n",
      "iteration no 135: Loss: 0.7574681767548637, accuracy: 0.5133333333333333\n",
      "iteration no 136: Loss: 0.7574412246212935, accuracy: 0.5133333333333333\n",
      "iteration no 137: Loss: 0.7574151551039138, accuracy: 0.5133333333333333\n",
      "iteration no 138: Loss: 0.7573899375727976, accuracy: 0.5133333333333333\n",
      "iteration no 139: Loss: 0.7573655425431642, accuracy: 0.5133333333333333\n",
      "iteration no 140: Loss: 0.757341941628522, accuracy: 0.5133333333333333\n",
      "iteration no 141: Loss: 0.7573191074959305, accuracy: 0.5133333333333333\n",
      "iteration no 142: Loss: 0.7572970138232767, accuracy: 0.5133333333333333\n",
      "iteration no 143: Loss: 0.7572756352584676, accuracy: 0.5133333333333333\n",
      "iteration no 144: Loss: 0.7572549473804373, accuracy: 0.5133333333333333\n",
      "iteration no 145: Loss: 0.7572349266618856, accuracy: 0.5133333333333333\n",
      "iteration no 146: Loss: 0.7572155504336601, accuracy: 0.5133333333333333\n",
      "iteration no 147: Loss: 0.7571967968507013, accuracy: 0.5133333333333333\n",
      "iteration no 148: Loss: 0.7571786448594756, accuracy: 0.5133333333333333\n",
      "iteration no 149: Loss: 0.7571610741668242, accuracy: 0.5133333333333333\n",
      "iteration no 150: Loss: 0.7571440652101598, accuracy: 0.5133333333333333\n",
      "iteration no 151: Loss: 0.7571275991289453, accuracy: 0.5133333333333333\n",
      "iteration no 152: Loss: 0.7571116577373972, accuracy: 0.5133333333333333\n",
      "iteration no 153: Loss: 0.7570962234983507, accuracy: 0.5133333333333333\n",
      "iteration no 154: Loss: 0.7570812794982363, accuracy: 0.5133333333333333\n",
      "iteration no 155: Loss: 0.7570668094231129, accuracy: 0.5133333333333333\n",
      "iteration no 156: Loss: 0.7570527975357098, accuracy: 0.5133333333333333\n",
      "iteration no 157: Loss: 0.7570392286534302, accuracy: 0.5133333333333333\n",
      "iteration no 158: Loss: 0.757026088127272, accuracy: 0.5133333333333333\n",
      "iteration no 159: Loss: 0.7570133618216224, accuracy: 0.5133333333333333\n",
      "iteration no 160: Loss: 0.7570010360948901, accuracy: 0.5133333333333333\n",
      "iteration no 161: Loss: 0.7569890977809327, accuracy: 0.5133333333333333\n",
      "iteration no 162: Loss: 0.7569775341712455, accuracy: 0.5133333333333333\n",
      "iteration no 163: Loss: 0.7569663329978782, accuracy: 0.5133333333333333\n",
      "iteration no 164: Loss: 0.7569554824170442, accuracy: 0.5133333333333333\n",
      "iteration no 165: Loss: 0.7569449709933948, accuracy: 0.5133333333333333\n",
      "iteration no 166: Loss: 0.7569347876849258, accuracy: 0.5133333333333333\n",
      "iteration no 167: Loss: 0.7569249218284908, accuracy: 0.5133333333333333\n",
      "iteration no 168: Loss: 0.7569153631258942, accuracy: 0.5133333333333333\n",
      "iteration no 169: Loss: 0.7569061016305362, accuracy: 0.5133333333333333\n",
      "iteration no 170: Loss: 0.7568971277345893, accuracy: 0.5133333333333333\n",
      "iteration no 171: Loss: 0.7568884321566802, accuracy: 0.5133333333333333\n",
      "iteration no 172: Loss: 0.7568800059300579, accuracy: 0.5133333333333333\n",
      "iteration no 173: Loss: 0.7568718403912242, accuracy: 0.5133333333333333\n",
      "iteration no 174: Loss: 0.7568639271690091, accuracy: 0.5133333333333333\n",
      "iteration no 175: Loss: 0.7568562581740712, accuracy: 0.5133333333333333\n",
      "iteration no 176: Loss: 0.756848825588805, accuracy: 0.5133333333333333\n",
      "iteration no 177: Loss: 0.7568416218576388, accuracy: 0.5133333333333333\n",
      "iteration no 178: Loss: 0.756834639677705, accuracy: 0.5133333333333333\n",
      "iteration no 179: Loss: 0.7568278719898692, accuracy: 0.5133333333333333\n",
      "iteration no 180: Loss: 0.7568213119701002, accuracy: 0.5133333333333333\n",
      "iteration no 181: Loss: 0.7568149530211723, accuracy: 0.5133333333333333\n",
      "iteration no 182: Loss: 0.7568087887646786, accuracy: 0.5133333333333333\n",
      "iteration no 183: Loss: 0.7568028130333483, accuracy: 0.5133333333333333\n",
      "iteration no 184: Loss: 0.7567970198636542, accuracy: 0.5133333333333333\n",
      "iteration no 185: Loss: 0.7567914034886962, accuracy: 0.5133333333333333\n",
      "iteration no 186: Loss: 0.7567859583313521, accuracy: 0.5133333333333333\n",
      "iteration no 187: Loss: 0.7567806789976848, accuracy: 0.5133333333333333\n",
      "iteration no 188: Loss: 0.7567755602705926, accuracy: 0.5133333333333333\n",
      "iteration no 189: Loss: 0.7567705971036975, accuracy: 0.5133333333333333\n",
      "iteration no 190: Loss: 0.7567657846154581, accuracy: 0.5133333333333333\n",
      "iteration no 191: Loss: 0.7567611180834983, accuracy: 0.5133333333333333\n",
      "iteration no 192: Loss: 0.7567565929391467, accuracy: 0.5133333333333333\n",
      "iteration no 193: Loss: 0.7567522047621741, accuracy: 0.5133333333333333\n",
      "iteration no 194: Loss: 0.7567479492757238, accuracy: 0.5133333333333333\n",
      "iteration no 195: Loss: 0.7567438223414269, accuracy: 0.5133333333333333\n",
      "iteration no 196: Loss: 0.7567398199546933, accuracy: 0.5133333333333333\n",
      "iteration no 197: Loss: 0.7567359382401753, accuracy: 0.5133333333333333\n",
      "iteration no 198: Loss: 0.7567321734473921, accuracy: 0.5133333333333333\n",
      "iteration no 199: Loss: 0.756728521946515, accuracy: 0.5133333333333333\n",
      "iteration no 200: Loss: 0.7567249802243007, accuracy: 0.5133333333333333\n",
      "iteration no 1: Loss: 1.079645125941331, accuracy: 0.5166666666666667\n",
      "iteration no 2: Loss: 1.0511417937639713, accuracy: 0.5033333333333333\n",
      "iteration no 3: Loss: 1.0257553754958073, accuracy: 0.5033333333333333\n",
      "iteration no 4: Loss: 1.0031043779199948, accuracy: 0.49666666666666665\n",
      "iteration no 5: Loss: 0.9828551573343144, accuracy: 0.49666666666666665\n",
      "iteration no 6: Loss: 0.9647155193497869, accuracy: 0.5\n",
      "iteration no 7: Loss: 0.9484299683450471, accuracy: 0.49666666666666665\n",
      "iteration no 8: Loss: 0.9337755845377251, accuracy: 0.5133333333333333\n",
      "iteration no 9: Loss: 0.920558222502538, accuracy: 0.5166666666666667\n",
      "iteration no 10: Loss: 0.908608983959491, accuracy: 0.5166666666666667\n",
      "iteration no 11: Loss: 0.89778098764298, accuracy: 0.5133333333333333\n",
      "iteration no 12: Loss: 0.8879464604004638, accuracy: 0.5133333333333333\n",
      "iteration no 13: Loss: 0.8789941565817333, accuracy: 0.5133333333333333\n",
      "iteration no 14: Loss: 0.8708270957696004, accuracy: 0.5133333333333333\n",
      "iteration no 15: Loss: 0.863360597128122, accuracy: 0.5133333333333333\n",
      "iteration no 16: Loss: 0.8565205822663224, accuracy: 0.51\n",
      "iteration no 17: Loss: 0.8502421162314687, accuracy: 0.51\n",
      "iteration no 18: Loss: 0.8444681566342769, accuracy: 0.51\n",
      "iteration no 19: Loss: 0.8391484828387538, accuracy: 0.5133333333333333\n",
      "iteration no 20: Loss: 0.8342387798371462, accuracy: 0.51\n",
      "iteration no 21: Loss: 0.829699854375819, accuracy: 0.51\n",
      "iteration no 22: Loss: 0.8254969638055003, accuracy: 0.51\n",
      "iteration no 23: Loss: 0.8215992408398455, accuracy: 0.51\n",
      "iteration no 24: Loss: 0.8179791998459601, accuracy: 0.51\n",
      "iteration no 25: Loss: 0.8146123124368536, accuracy: 0.5133333333333333\n",
      "iteration no 26: Loss: 0.8114766419952626, accuracy: 0.5133333333333333\n",
      "iteration no 27: Loss: 0.8085525283522828, accuracy: 0.5133333333333333\n",
      "iteration no 28: Loss: 0.8058223152008116, accuracy: 0.5133333333333333\n",
      "iteration no 29: Loss: 0.8032701139725755, accuracy: 0.5133333333333333\n",
      "iteration no 30: Loss: 0.8008815988771408, accuracy: 0.5133333333333333\n",
      "iteration no 31: Loss: 0.7986438286180256, accuracy: 0.5166666666666667\n",
      "iteration no 32: Loss: 0.7965450909881945, accuracy: 0.5166666666666667\n",
      "iteration no 33: Loss: 0.7945747671251038, accuracy: 0.5166666666666667\n",
      "iteration no 34: Loss: 0.792723212691529, accuracy: 0.5166666666666667\n",
      "iteration no 35: Loss: 0.7909816536574216, accuracy: 0.5166666666666667\n",
      "iteration no 36: Loss: 0.7893420947025405, accuracy: 0.5166666666666667\n",
      "iteration no 37: Loss: 0.7877972385500573, accuracy: 0.5166666666666667\n",
      "iteration no 38: Loss: 0.7863404147865535, accuracy: 0.5166666666666667\n",
      "iteration no 39: Loss: 0.7849655169311407, accuracy: 0.5166666666666667\n",
      "iteration no 40: Loss: 0.7836669466920022, accuracy: 0.5166666666666667\n",
      "iteration no 41: Loss: 0.7824395644975349, accuracy: 0.5166666666666667\n",
      "iteration no 42: Loss: 0.7812786455157996, accuracy: 0.5166666666666667\n",
      "iteration no 43: Loss: 0.7801798404836519, accuracy: 0.5166666666666667\n",
      "iteration no 44: Loss: 0.7791391407587478, accuracy: 0.5133333333333333\n",
      "iteration no 45: Loss: 0.7781528470860398, accuracy: 0.5133333333333333\n",
      "iteration no 46: Loss: 0.777217541637504, accuracy: 0.5133333333333333\n",
      "iteration no 47: Loss: 0.7763300629413784, accuracy: 0.5133333333333333\n",
      "iteration no 48: Loss: 0.7754874833666277, accuracy: 0.5133333333333333\n",
      "iteration no 49: Loss: 0.7746870888708799, accuracy: 0.5133333333333333\n",
      "iteration no 50: Loss: 0.7739263607567489, accuracy: 0.5133333333333333\n",
      "iteration no 51: Loss: 0.773202959213127, accuracy: 0.5133333333333333\n",
      "iteration no 52: Loss: 0.7725147084454299, accuracy: 0.5133333333333333\n",
      "iteration no 53: Loss: 0.7718595832225268, accuracy: 0.5133333333333333\n",
      "iteration no 54: Loss: 0.7712356966886991, accuracy: 0.5133333333333333\n",
      "iteration no 55: Loss: 0.7706412893069154, accuracy: 0.5166666666666667\n",
      "iteration no 56: Loss: 0.770074718815324, accuracy: 0.5166666666666667\n",
      "iteration no 57: Loss: 0.7695344510924989, accuracy: 0.5166666666666667\n",
      "iteration no 58: Loss: 0.7690190518388971, accuracy: 0.5166666666666667\n",
      "iteration no 59: Loss: 0.7685271789924162, accuracy: 0.5166666666666667\n",
      "iteration no 60: Loss: 0.7680575758050833, accuracy: 0.5166666666666667\n",
      "iteration no 61: Loss: 0.7676090645159579, accuracy: 0.5166666666666667\n",
      "iteration no 62: Loss: 0.7671805405623805, accuracy: 0.5166666666666667\n",
      "iteration no 63: Loss: 0.7667709672779415, accuracy: 0.5166666666666667\n",
      "iteration no 64: Loss: 0.7663793710310285, accuracy: 0.5166666666666667\n",
      "iteration no 65: Loss: 0.7660048367626694, accuracy: 0.5166666666666667\n",
      "iteration no 66: Loss: 0.7656465038866791, accuracy: 0.5166666666666667\n",
      "iteration no 67: Loss: 0.765303562518929, accuracy: 0.5166666666666667\n",
      "iteration no 68: Loss: 0.7649752500059271, accuracy: 0.5166666666666667\n",
      "iteration no 69: Loss: 0.7646608477259041, accuracy: 0.5166666666666667\n",
      "iteration no 70: Loss: 0.7643596781382633, accuracy: 0.5166666666666667\n",
      "iteration no 71: Loss: 0.7640711020596321, accuracy: 0.5166666666666667\n",
      "iteration no 72: Loss: 0.7637945161468755, accuracy: 0.5166666666666667\n",
      "iteration no 73: Loss: 0.7635293505693204, accuracy: 0.5166666666666667\n",
      "iteration no 74: Loss: 0.763275066854136, accuracy: 0.5166666666666667\n",
      "iteration no 75: Loss: 0.7630311558903334, accuracy: 0.5166666666666667\n",
      "iteration no 76: Loss: 0.7627971360782001, accuracy: 0.5166666666666667\n",
      "iteration no 77: Loss: 0.7625725516122094, accuracy: 0.5166666666666667\n",
      "iteration no 78: Loss: 0.7623569708865381, accuracy: 0.5166666666666667\n",
      "iteration no 79: Loss: 0.7621499850133102, accuracy: 0.5166666666666667\n",
      "iteration no 80: Loss: 0.7619512064445717, accuracy: 0.5166666666666667\n",
      "iteration no 81: Loss: 0.7617602676898011, accuracy: 0.5133333333333333\n",
      "iteration no 82: Loss: 0.7615768201214808, accuracy: 0.5133333333333333\n",
      "iteration no 83: Loss: 0.7614005328619062, accuracy: 0.5133333333333333\n",
      "iteration no 84: Loss: 0.7612310917449974, accuracy: 0.5133333333333333\n",
      "iteration no 85: Loss: 0.7610681983474135, accuracy: 0.5133333333333333\n",
      "iteration no 86: Loss: 0.7609115690837495, accuracy: 0.5133333333333333\n",
      "iteration no 87: Loss: 0.7607609343610353, accuracy: 0.5133333333333333\n",
      "iteration no 88: Loss: 0.7606160377881536, accuracy: 0.5133333333333333\n",
      "iteration no 89: Loss: 0.7604766354361533, accuracy: 0.5133333333333333\n",
      "iteration no 90: Loss: 0.7603424951457654, accuracy: 0.5133333333333333\n",
      "iteration no 91: Loss: 0.7602133958787236, accuracy: 0.5133333333333333\n",
      "iteration no 92: Loss: 0.7600891271097703, accuracy: 0.5133333333333333\n",
      "iteration no 93: Loss: 0.7599694882564666, accuracy: 0.5133333333333333\n",
      "iteration no 94: Loss: 0.759854288144163, accuracy: 0.5133333333333333\n",
      "iteration no 95: Loss: 0.7597433445036862, accuracy: 0.5133333333333333\n",
      "iteration no 96: Loss: 0.7596364834994879, accuracy: 0.5133333333333333\n",
      "iteration no 97: Loss: 0.7595335392861767, accuracy: 0.5133333333333333\n",
      "iteration no 98: Loss: 0.7594343535915122, accuracy: 0.5133333333333333\n",
      "iteration no 99: Loss: 0.7593387753240796, accuracy: 0.5133333333333333\n",
      "iteration no 100: Loss: 0.7592466602040064, accuracy: 0.5133333333333333\n",
      "iteration no 101: Loss: 0.7591578704151949, accuracy: 0.5166666666666667\n",
      "iteration no 102: Loss: 0.7590722742776618, accuracy: 0.5166666666666667\n",
      "iteration no 103: Loss: 0.7589897459386772, accuracy: 0.5166666666666667\n",
      "iteration no 104: Loss: 0.758910165081492, accuracy: 0.5166666666666667\n",
      "iteration no 105: Loss: 0.7588334166505237, accuracy: 0.5166666666666667\n",
      "iteration no 106: Loss: 0.7587593905919621, accuracy: 0.5166666666666667\n",
      "iteration no 107: Loss: 0.7586879816088157, accuracy: 0.5166666666666667\n",
      "iteration no 108: Loss: 0.7586190889295029, accuracy: 0.5166666666666667\n",
      "iteration no 109: Loss: 0.7585526160891409, accuracy: 0.5166666666666667\n",
      "iteration no 110: Loss: 0.7584884707227549, accuracy: 0.5166666666666667\n",
      "iteration no 111: Loss: 0.7584265643696757, accuracy: 0.5166666666666667\n",
      "iteration no 112: Loss: 0.7583668122884487, accuracy: 0.5166666666666667\n",
      "iteration no 113: Loss: 0.7583091332816188, accuracy: 0.5166666666666667\n",
      "iteration no 114: Loss: 0.7582534495298021, accuracy: 0.5166666666666667\n",
      "iteration no 115: Loss: 0.7581996864344915, accuracy: 0.5166666666666667\n",
      "iteration no 116: Loss: 0.7581477724690827, accuracy: 0.5166666666666667\n",
      "iteration no 117: Loss: 0.7580976390376365, accuracy: 0.5166666666666667\n",
      "iteration no 118: Loss: 0.7580492203409306, accuracy: 0.5166666666666667\n",
      "iteration no 119: Loss: 0.7580024532493775, accuracy: 0.5166666666666667\n",
      "iteration no 120: Loss: 0.7579572771824161, accuracy: 0.5166666666666667\n",
      "iteration no 121: Loss: 0.7579136339940059, accuracy: 0.5166666666666667\n",
      "iteration no 122: Loss: 0.7578714678638827, accuracy: 0.5166666666666667\n",
      "iteration no 123: Loss: 0.7578307251942471, accuracy: 0.5166666666666667\n",
      "iteration no 124: Loss: 0.7577913545115847, accuracy: 0.5166666666666667\n",
      "iteration no 125: Loss: 0.7577533063733343, accuracy: 0.5166666666666667\n",
      "iteration no 126: Loss: 0.757716533279134, accuracy: 0.5166666666666667\n",
      "iteration no 127: Loss: 0.757680989586398, accuracy: 0.5166666666666667\n",
      "iteration no 128: Loss: 0.757646631429984, accuracy: 0.5166666666666667\n",
      "iteration no 129: Loss: 0.7576134166457351, accuracy: 0.5166666666666667\n",
      "iteration no 130: Loss: 0.7575813046976839, accuracy: 0.5133333333333333\n",
      "iteration no 131: Loss: 0.7575502566087244, accuracy: 0.5133333333333333\n",
      "iteration no 132: Loss: 0.7575202348945685, accuracy: 0.5133333333333333\n",
      "iteration no 133: Loss: 0.7574912035008142, accuracy: 0.5133333333333333\n",
      "iteration no 134: Loss: 0.7574631277429583, accuracy: 0.5133333333333333\n",
      "iteration no 135: Loss: 0.7574359742492071, accuracy: 0.5133333333333333\n",
      "iteration no 136: Loss: 0.7574097109059327, accuracy: 0.5133333333333333\n",
      "iteration no 137: Loss: 0.757384306805645, accuracy: 0.5133333333333333\n",
      "iteration no 138: Loss: 0.757359732197345, accuracy: 0.5133333333333333\n",
      "iteration no 139: Loss: 0.7573359584391438, accuracy: 0.5133333333333333\n",
      "iteration no 140: Loss: 0.7573129579530269, accuracy: 0.5133333333333333\n",
      "iteration no 141: Loss: 0.7572907041816599, accuracy: 0.5133333333333333\n",
      "iteration no 142: Loss: 0.7572691715471329, accuracy: 0.5133333333333333\n",
      "iteration no 143: Loss: 0.7572483354115457, accuracy: 0.5133333333333333\n",
      "iteration no 144: Loss: 0.7572281720393422, accuracy: 0.5133333333333333\n",
      "iteration no 145: Loss: 0.7572086585613116, accuracy: 0.5133333333333333\n",
      "iteration no 146: Loss: 0.7571897729401703, accuracy: 0.5133333333333333\n",
      "iteration no 147: Loss: 0.75717149393765, accuracy: 0.5133333333333333\n",
      "iteration no 148: Loss: 0.7571538010830204, accuracy: 0.5133333333333333\n",
      "iteration no 149: Loss: 0.7571366746429726, accuracy: 0.5133333333333333\n",
      "iteration no 150: Loss: 0.7571200955928036, accuracy: 0.5133333333333333\n",
      "iteration no 151: Loss: 0.7571040455888364, accuracy: 0.5133333333333333\n",
      "iteration no 152: Loss: 0.7570885069420182, accuracy: 0.5133333333333333\n",
      "iteration no 153: Loss: 0.7570734625926417, accuracy: 0.5133333333333333\n",
      "iteration no 154: Loss: 0.757058896086135, accuracy: 0.5133333333333333\n",
      "iteration no 155: Loss: 0.7570447915498723, accuracy: 0.5133333333333333\n",
      "iteration no 156: Loss: 0.7570311336709565, accuracy: 0.5133333333333333\n",
      "iteration no 157: Loss: 0.7570179076749308, accuracy: 0.5133333333333333\n",
      "iteration no 158: Loss: 0.7570050993053725, accuracy: 0.5133333333333333\n",
      "iteration no 159: Loss: 0.7569926948043343, accuracy: 0.5133333333333333\n",
      "iteration no 160: Loss: 0.7569806808935888, accuracy: 0.5133333333333333\n",
      "iteration no 161: Loss: 0.7569690447566454, accuracy: 0.5133333333333333\n",
      "iteration no 162: Loss: 0.7569577740214981, accuracy: 0.5133333333333333\n",
      "iteration no 163: Loss: 0.7569468567440792, accuracy: 0.5133333333333333\n",
      "iteration no 164: Loss: 0.7569362813923809, accuracy: 0.5133333333333333\n",
      "iteration no 165: Loss: 0.7569260368312175, accuracy: 0.5133333333333333\n",
      "iteration no 166: Loss: 0.756916112307601, accuracy: 0.5133333333333333\n",
      "iteration no 167: Loss: 0.7569064974367018, accuracy: 0.5133333333333333\n",
      "iteration no 168: Loss: 0.7568971821883679, accuracy: 0.5133333333333333\n",
      "iteration no 169: Loss: 0.7568881568741811, accuracy: 0.5133333333333333\n",
      "iteration no 170: Loss: 0.7568794121350229, accuracy: 0.5133333333333333\n",
      "iteration no 171: Loss: 0.7568709389291326, accuracy: 0.5133333333333333\n",
      "iteration no 172: Loss: 0.7568627285206323, accuracy: 0.5133333333333333\n",
      "iteration no 173: Loss: 0.7568547724685017, accuracy: 0.5133333333333333\n",
      "iteration no 174: Loss: 0.7568470626159808, accuracy: 0.5133333333333333\n",
      "iteration no 175: Loss: 0.756839591080386, accuracy: 0.5133333333333333\n",
      "iteration no 176: Loss: 0.7568323502433176, accuracy: 0.5133333333333333\n",
      "iteration no 177: Loss: 0.7568253327412469, accuracy: 0.5133333333333333\n",
      "iteration no 178: Loss: 0.7568185314564619, accuracy: 0.5133333333333333\n",
      "iteration no 179: Loss: 0.7568119395083626, accuracy: 0.5133333333333333\n",
      "iteration no 180: Loss: 0.756805550245084, accuracy: 0.5133333333333333\n",
      "iteration no 181: Loss: 0.7567993572354419, accuracy: 0.5133333333333333\n",
      "iteration no 182: Loss: 0.756793354261179, accuracy: 0.5133333333333333\n",
      "iteration no 183: Loss: 0.7567875353095069, accuracy: 0.5133333333333333\n",
      "iteration no 184: Loss: 0.7567818945659271, accuracy: 0.5133333333333333\n",
      "iteration no 185: Loss: 0.7567764264073205, accuracy: 0.5133333333333333\n",
      "iteration no 186: Loss: 0.7567711253952958, accuracy: 0.5133333333333333\n",
      "iteration no 187: Loss: 0.7567659862697854, accuracy: 0.5133333333333333\n",
      "iteration no 188: Loss: 0.7567610039428792, accuracy: 0.5133333333333333\n",
      "iteration no 189: Loss: 0.7567561734928843, accuracy: 0.5133333333333333\n",
      "iteration no 190: Loss: 0.7567514901586074, accuracy: 0.5133333333333333\n",
      "iteration no 191: Loss: 0.7567469493338441, accuracy: 0.5133333333333333\n",
      "iteration no 192: Loss: 0.7567425465620721, accuracy: 0.5133333333333333\n",
      "iteration no 193: Loss: 0.7567382775313365, accuracy: 0.5133333333333333\n",
      "iteration no 194: Loss: 0.7567341380693233, accuracy: 0.5133333333333333\n",
      "iteration no 195: Loss: 0.7567301241386106, accuracy: 0.5133333333333333\n",
      "iteration no 196: Loss: 0.7567262318320918, accuracy: 0.5133333333333333\n",
      "iteration no 197: Loss: 0.7567224573685636, accuracy: 0.5133333333333333\n",
      "iteration no 198: Loss: 0.7567187970884742, accuracy: 0.5133333333333333\n",
      "iteration no 199: Loss: 0.7567152474498235, accuracy: 0.5133333333333333\n",
      "iteration no 200: Loss: 0.7567118050242083, accuracy: 0.5133333333333333\n",
      "iteration no 1: Loss: 1.1022296247989223, accuracy: 0.5366666666666666\n",
      "iteration no 2: Loss: 1.0711412611077702, accuracy: 0.53\n",
      "iteration no 3: Loss: 1.0434826332324674, accuracy: 0.5266666666666666\n",
      "iteration no 4: Loss: 1.0188371184124945, accuracy: 0.5166666666666667\n",
      "iteration no 5: Loss: 0.9968387999461829, accuracy: 0.5133333333333333\n",
      "iteration no 6: Loss: 0.9771658681418587, accuracy: 0.5166666666666667\n",
      "iteration no 7: Loss: 0.9595359513967024, accuracy: 0.5166666666666667\n",
      "iteration no 8: Loss: 0.9437020499709936, accuracy: 0.5133333333333333\n",
      "iteration no 9: Loss: 0.9294486866585919, accuracy: 0.52\n",
      "iteration no 10: Loss: 0.9165882351767132, accuracy: 0.5166666666666667\n",
      "iteration no 11: Loss: 0.9049574818774638, accuracy: 0.5166666666666667\n",
      "iteration no 12: Loss: 0.8944144773896645, accuracy: 0.5166666666666667\n",
      "iteration no 13: Loss: 0.8848357096645367, accuracy: 0.5133333333333333\n",
      "iteration no 14: Loss: 0.8761136042560245, accuracy: 0.51\n",
      "iteration no 15: Loss: 0.8681543390105685, accuracy: 0.51\n",
      "iteration no 16: Loss: 0.8608759491719666, accuracy: 0.5166666666666667\n",
      "iteration no 17: Loss: 0.8542066935745476, accuracy: 0.5133333333333333\n",
      "iteration no 18: Loss: 0.8480836511983326, accuracy: 0.5133333333333333\n",
      "iteration no 19: Loss: 0.8424515183434359, accuracy: 0.5133333333333333\n",
      "iteration no 20: Loss: 0.8372615789513937, accuracy: 0.5133333333333333\n",
      "iteration no 21: Loss: 0.8324708234451531, accuracy: 0.5133333333333333\n",
      "iteration no 22: Loss: 0.8280411944430789, accuracy: 0.5133333333333333\n",
      "iteration no 23: Loss: 0.8239389405794694, accuracy: 0.5133333333333333\n",
      "iteration no 24: Loss: 0.820134062308691, accuracy: 0.5133333333333333\n",
      "iteration no 25: Loss: 0.8165998359292373, accuracy: 0.5133333333333333\n",
      "iteration no 26: Loss: 0.8133124041277118, accuracy: 0.5133333333333333\n",
      "iteration no 27: Loss: 0.8102504231239871, accuracy: 0.5166666666666667\n",
      "iteration no 28: Loss: 0.8073947580223024, accuracy: 0.5166666666666667\n",
      "iteration no 29: Loss: 0.804728219267929, accuracy: 0.5166666666666667\n",
      "iteration no 30: Loss: 0.8022353342048912, accuracy: 0.5166666666666667\n",
      "iteration no 31: Loss: 0.7999021486550787, accuracy: 0.5166666666666667\n",
      "iteration no 32: Loss: 0.7977160542182689, accuracy: 0.5166666666666667\n",
      "iteration no 33: Loss: 0.7956656376484612, accuracy: 0.5166666666666667\n",
      "iteration no 34: Loss: 0.7937405492138802, accuracy: 0.5166666666666667\n",
      "iteration no 35: Loss: 0.7919313874125993, accuracy: 0.5166666666666667\n",
      "iteration no 36: Loss: 0.7902295978070298, accuracy: 0.5166666666666667\n",
      "iteration no 37: Loss: 0.7886273840703723, accuracy: 0.5166666666666667\n",
      "iteration no 38: Loss: 0.7871176296164978, accuracy: 0.5166666666666667\n",
      "iteration no 39: Loss: 0.785693828419959, accuracy: 0.5166666666666667\n",
      "iteration no 40: Loss: 0.7843500238318841, accuracy: 0.5166666666666667\n",
      "iteration no 41: Loss: 0.7830807543662136, accuracy: 0.5133333333333333\n",
      "iteration no 42: Loss: 0.781881005573968, accuracy: 0.5133333333333333\n",
      "iteration no 43: Loss: 0.780746167245017, accuracy: 0.5133333333333333\n",
      "iteration no 44: Loss: 0.7796719952805637, accuracy: 0.5133333333333333\n",
      "iteration no 45: Loss: 0.7786545776680863, accuracy: 0.5133333333333333\n",
      "iteration no 46: Loss: 0.7776903040661619, accuracy: 0.5133333333333333\n",
      "iteration no 47: Loss: 0.7767758385714011, accuracy: 0.5133333333333333\n",
      "iteration no 48: Loss: 0.7759080952953361, accuracy: 0.5133333333333333\n",
      "iteration no 49: Loss: 0.7750842164268946, accuracy: 0.5133333333333333\n",
      "iteration no 50: Loss: 0.7743015524972406, accuracy: 0.5133333333333333\n",
      "iteration no 51: Loss: 0.7735576445992689, accuracy: 0.5133333333333333\n",
      "iteration no 52: Loss: 0.7728502083447023, accuracy: 0.5133333333333333\n",
      "iteration no 53: Loss: 0.7721771193683061, accuracy: 0.5133333333333333\n",
      "iteration no 54: Loss: 0.7715364002117479, accuracy: 0.5133333333333333\n",
      "iteration no 55: Loss: 0.7709262084396332, accuracy: 0.5166666666666667\n",
      "iteration no 56: Loss: 0.7703448258576575, accuracy: 0.5166666666666667\n",
      "iteration no 57: Loss: 0.7697906487179673, accuracy: 0.5166666666666667\n",
      "iteration no 58: Loss: 0.7692621788100762, accuracy: 0.5166666666666667\n",
      "iteration no 59: Loss: 0.7687580153472531, accuracy: 0.5166666666666667\n",
      "iteration no 60: Loss: 0.7682768475684378, accuracy: 0.5166666666666667\n",
      "iteration no 61: Loss: 0.7678174479846388, accuracy: 0.5166666666666667\n",
      "iteration no 62: Loss: 0.7673786662065762, accuracy: 0.5166666666666667\n",
      "iteration no 63: Loss: 0.7669594232972112, accuracy: 0.5166666666666667\n",
      "iteration no 64: Loss: 0.7665587065988599, accuracy: 0.5166666666666667\n",
      "iteration no 65: Loss: 0.7661755649899359, accuracy: 0.5166666666666667\n",
      "iteration no 66: Loss: 0.7658091045310899, accuracy: 0.5166666666666667\n",
      "iteration no 67: Loss: 0.7654584844646967, accuracy: 0.5166666666666667\n",
      "iteration no 68: Loss: 0.765122913535343, accuracy: 0.5166666666666667\n",
      "iteration no 69: Loss: 0.7648016466022615, accuracy: 0.5166666666666667\n",
      "iteration no 70: Loss: 0.7644939815175731, accuracy: 0.5166666666666667\n",
      "iteration no 71: Loss: 0.764199256246804, accuracy: 0.5166666666666667\n",
      "iteration no 72: Loss: 0.7639168462104582, accuracy: 0.5166666666666667\n",
      "iteration no 73: Loss: 0.7636461618274918, accuracy: 0.5166666666666667\n",
      "iteration no 74: Loss: 0.7633866462433805, accuracy: 0.5166666666666667\n",
      "iteration no 75: Loss: 0.7631377732271257, accuracy: 0.5166666666666667\n",
      "iteration no 76: Loss: 0.7628990452230162, accuracy: 0.5166666666666667\n",
      "iteration no 77: Loss: 0.762669991544291, accuracy: 0.5166666666666667\n",
      "iteration no 78: Loss: 0.7624501666970369, accuracy: 0.5166666666666667\n",
      "iteration no 79: Loss: 0.7622391488237199, accuracy: 0.5166666666666667\n",
      "iteration no 80: Loss: 0.7620365382567119, accuracy: 0.5133333333333333\n",
      "iteration no 81: Loss: 0.7618419561730398, accuracy: 0.5133333333333333\n",
      "iteration no 82: Loss: 0.761655043342359, accuracy: 0.5133333333333333\n",
      "iteration no 83: Loss: 0.7614754589608607, accuracy: 0.5133333333333333\n",
      "iteration no 84: Loss: 0.7613028795644594, accuracy: 0.5133333333333333\n",
      "iteration no 85: Loss: 0.7611369980151739, accuracy: 0.5133333333333333\n",
      "iteration no 86: Loss: 0.7609775225551406, accuracy: 0.5133333333333333\n",
      "iteration no 87: Loss: 0.7608241759231705, accuracy: 0.5133333333333333\n",
      "iteration no 88: Loss: 0.7606766945291812, accuracy: 0.5133333333333333\n",
      "iteration no 89: Loss: 0.7605348276822291, accuracy: 0.5133333333333333\n",
      "iteration no 90: Loss: 0.7603983368682182, accuracy: 0.5133333333333333\n",
      "iteration no 91: Loss: 0.760266995073677, accuracy: 0.5133333333333333\n",
      "iteration no 92: Loss: 0.7601405861522956, accuracy: 0.5133333333333333\n",
      "iteration no 93: Loss: 0.7600189042311694, accuracy: 0.5133333333333333\n",
      "iteration no 94: Loss: 0.7599017531539495, accuracy: 0.5133333333333333\n",
      "iteration no 95: Loss: 0.7597889459583109, accuracy: 0.5133333333333333\n",
      "iteration no 96: Loss: 0.759680304385357, accuracy: 0.5133333333333333\n",
      "iteration no 97: Loss: 0.7595756584187615, accuracy: 0.5133333333333333\n",
      "iteration no 98: Loss: 0.7594748458516184, accuracy: 0.5133333333333333\n",
      "iteration no 99: Loss: 0.7593777118791217, accuracy: 0.5133333333333333\n",
      "iteration no 100: Loss: 0.7592841087153457, accuracy: 0.5133333333333333\n",
      "iteration no 101: Loss: 0.7591938952325143, accuracy: 0.5133333333333333\n",
      "iteration no 102: Loss: 0.7591069366212813, accuracy: 0.5166666666666667\n",
      "iteration no 103: Loss: 0.7590231040706394, accuracy: 0.5166666666666667\n",
      "iteration no 104: Loss: 0.7589422744661852, accuracy: 0.5166666666666667\n",
      "iteration no 105: Loss: 0.7588643301055563, accuracy: 0.5166666666666667\n",
      "iteration no 106: Loss: 0.7587891584299409, accuracy: 0.5166666666666667\n",
      "iteration no 107: Loss: 0.758716651770641, accuracy: 0.5166666666666667\n",
      "iteration no 108: Loss: 0.7586467071097397, accuracy: 0.5166666666666667\n",
      "iteration no 109: Loss: 0.7585792258539926, accuracy: 0.5166666666666667\n",
      "iteration no 110: Loss: 0.75851411362112, accuracy: 0.5166666666666667\n",
      "iteration no 111: Loss: 0.7584512800377401, accuracy: 0.5166666666666667\n",
      "iteration no 112: Loss: 0.7583906385482291, accuracy: 0.5166666666666667\n",
      "iteration no 113: Loss: 0.7583321062338451, accuracy: 0.5166666666666667\n",
      "iteration no 114: Loss: 0.7582756036414995, accuracy: 0.5166666666666667\n",
      "iteration no 115: Loss: 0.7582210546215972, accuracy: 0.5166666666666667\n",
      "iteration no 116: Loss: 0.7581683861744084, accuracy: 0.5166666666666667\n",
      "iteration no 117: Loss: 0.7581175283044683, accuracy: 0.5166666666666667\n",
      "iteration no 118: Loss: 0.7580684138825348, accuracy: 0.5166666666666667\n",
      "iteration no 119: Loss: 0.758020978514666, accuracy: 0.5166666666666667\n",
      "iteration no 120: Loss: 0.757975160418004, accuracy: 0.5166666666666667\n",
      "iteration no 121: Loss: 0.7579309003028832, accuracy: 0.5166666666666667\n",
      "iteration no 122: Loss: 0.7578881412609024, accuracy: 0.5166666666666667\n",
      "iteration no 123: Loss: 0.7578468286586206, accuracy: 0.5166666666666667\n",
      "iteration no 124: Loss: 0.7578069100365649, accuracy: 0.5166666666666667\n",
      "iteration no 125: Loss: 0.7577683350132501, accuracy: 0.5166666666666667\n",
      "iteration no 126: Loss: 0.7577310551939348, accuracy: 0.5166666666666667\n",
      "iteration no 127: Loss: 0.7576950240838516, accuracy: 0.5166666666666667\n",
      "iteration no 128: Loss: 0.7576601970056658, accuracy: 0.5166666666666667\n",
      "iteration no 129: Loss: 0.7576265310209361, accuracy: 0.5166666666666667\n",
      "iteration no 130: Loss: 0.7575939848553553, accuracy: 0.5166666666666667\n",
      "iteration no 131: Loss: 0.7575625188275741, accuracy: 0.5133333333333333\n",
      "iteration no 132: Loss: 0.757532094781412, accuracy: 0.5133333333333333\n",
      "iteration no 133: Loss: 0.7575026760212783, accuracy: 0.5133333333333333\n",
      "iteration no 134: Loss: 0.7574742272506327, accuracy: 0.5133333333333333\n",
      "iteration no 135: Loss: 0.7574467145133262, accuracy: 0.5133333333333333\n",
      "iteration no 136: Loss: 0.7574201051376743, accuracy: 0.5133333333333333\n",
      "iteration no 137: Loss: 0.7573943676831167, accuracy: 0.5133333333333333\n",
      "iteration no 138: Loss: 0.7573694718893356, accuracy: 0.5133333333333333\n",
      "iteration no 139: Loss: 0.7573453886277022, accuracy: 0.5133333333333333\n",
      "iteration no 140: Loss: 0.7573220898549368, accuracy: 0.5133333333333333\n",
      "iteration no 141: Loss: 0.7572995485688665, accuracy: 0.5133333333333333\n",
      "iteration no 142: Loss: 0.7572777387661782, accuracy: 0.5133333333333333\n",
      "iteration no 143: Loss: 0.7572566354020668, accuracy: 0.5133333333333333\n",
      "iteration no 144: Loss: 0.7572362143516822, accuracy: 0.5133333333333333\n",
      "iteration no 145: Loss: 0.7572164523732884, accuracy: 0.5133333333333333\n",
      "iteration no 146: Loss: 0.7571973270730483, accuracy: 0.5133333333333333\n",
      "iteration no 147: Loss: 0.7571788168713584, accuracy: 0.5133333333333333\n",
      "iteration no 148: Loss: 0.7571609009706525, accuracy: 0.5133333333333333\n",
      "iteration no 149: Loss: 0.7571435593246091, accuracy: 0.5133333333333333\n",
      "iteration no 150: Loss: 0.7571267726086904, accuracy: 0.5133333333333333\n",
      "iteration no 151: Loss: 0.7571105221919517, accuracy: 0.5133333333333333\n",
      "iteration no 152: Loss: 0.7570947901100594, accuracy: 0.5133333333333333\n",
      "iteration no 153: Loss: 0.7570795590394624, accuracy: 0.5133333333333333\n",
      "iteration no 154: Loss: 0.7570648122726592, accuracy: 0.5133333333333333\n",
      "iteration no 155: Loss: 0.757050533694511, accuracy: 0.5133333333333333\n",
      "iteration no 156: Loss: 0.7570367077595551, accuracy: 0.5133333333333333\n",
      "iteration no 157: Loss: 0.757023319470265, accuracy: 0.5133333333333333\n",
      "iteration no 158: Loss: 0.7570103543562213, accuracy: 0.5133333333333333\n",
      "iteration no 159: Loss: 0.7569977984541457, accuracy: 0.5133333333333333\n",
      "iteration no 160: Loss: 0.7569856382887616, accuracy: 0.5133333333333333\n",
      "iteration no 161: Loss: 0.7569738608544433, accuracy: 0.5133333333333333\n",
      "iteration no 162: Loss: 0.7569624535976167, accuracy: 0.5133333333333333\n",
      "iteration no 163: Loss: 0.7569514043998802, accuracy: 0.5133333333333333\n",
      "iteration no 164: Loss: 0.7569407015618098, accuracy: 0.5133333333333333\n",
      "iteration no 165: Loss: 0.7569303337874226, accuracy: 0.5133333333333333\n",
      "iteration no 166: Loss: 0.7569202901692644, accuracy: 0.5133333333333333\n",
      "iteration no 167: Loss: 0.7569105601740973, accuracy: 0.5133333333333333\n",
      "iteration no 168: Loss: 0.7569011336291604, accuracy: 0.5133333333333333\n",
      "iteration no 169: Loss: 0.7568920007089769, accuracy: 0.5133333333333333\n",
      "iteration no 170: Loss: 0.7568831519226851, accuracy: 0.5133333333333333\n",
      "iteration no 171: Loss: 0.7568745781018701, accuracy: 0.5133333333333333\n",
      "iteration no 172: Loss: 0.7568662703888749, accuracy: 0.5133333333333333\n",
      "iteration no 173: Loss: 0.7568582202255703, accuracy: 0.5133333333333333\n",
      "iteration no 174: Loss: 0.756850419342562, accuracy: 0.5133333333333333\n",
      "iteration no 175: Loss: 0.7568428597488209, accuracy: 0.5133333333333333\n",
      "iteration no 176: Loss: 0.7568355337217115, accuracy: 0.5133333333333333\n",
      "iteration no 177: Loss: 0.7568284337974102, accuracy: 0.5133333333333333\n",
      "iteration no 178: Loss: 0.7568215527616883, accuracy: 0.5133333333333333\n",
      "iteration no 179: Loss: 0.7568148836410522, accuracy: 0.5133333333333333\n",
      "iteration no 180: Loss: 0.7568084196942201, accuracy: 0.5133333333333333\n",
      "iteration no 181: Loss: 0.7568021544039231, accuracy: 0.5133333333333333\n",
      "iteration no 182: Loss: 0.7567960814690198, accuracy: 0.5133333333333333\n",
      "iteration no 183: Loss: 0.7567901947969072, accuracy: 0.5133333333333333\n",
      "iteration no 184: Loss: 0.7567844884962185, accuracy: 0.5133333333333333\n",
      "iteration no 185: Loss: 0.7567789568697948, accuracy: 0.5133333333333333\n",
      "iteration no 186: Loss: 0.7567735944079208, accuracy: 0.5133333333333333\n",
      "iteration no 187: Loss: 0.7567683957818128, accuracy: 0.5133333333333333\n",
      "iteration no 188: Loss: 0.7567633558373481, accuracy: 0.5133333333333333\n",
      "iteration no 189: Loss: 0.7567584695890283, accuracy: 0.5133333333333333\n",
      "iteration no 190: Loss: 0.7567537322141648, accuracy: 0.5133333333333333\n",
      "iteration no 191: Loss: 0.7567491390472786, accuracy: 0.5133333333333333\n",
      "iteration no 192: Loss: 0.7567446855747064, accuracy: 0.5133333333333333\n",
      "iteration no 193: Loss: 0.7567403674294038, accuracy: 0.5133333333333333\n",
      "iteration no 194: Loss: 0.7567361803859384, accuracy: 0.5133333333333333\n",
      "iteration no 195: Loss: 0.7567321203556654, accuracy: 0.5133333333333333\n",
      "iteration no 196: Loss: 0.7567281833820773, accuracy: 0.5133333333333333\n",
      "iteration no 197: Loss: 0.7567243656363237, accuracy: 0.5133333333333333\n",
      "iteration no 198: Loss: 0.7567206634128901, accuracy: 0.5133333333333333\n",
      "iteration no 199: Loss: 0.756717073125435, accuracy: 0.5133333333333333\n",
      "iteration no 200: Loss: 0.7567135913027745, accuracy: 0.5133333333333333\n",
      "iteration no 1: Loss: 1.1073394500515017, accuracy: 0.5433333333333333\n",
      "iteration no 2: Loss: 1.0756014775400213, accuracy: 0.5333333333333333\n",
      "iteration no 3: Loss: 1.047373670442489, accuracy: 0.52\n",
      "iteration no 4: Loss: 1.0222263687602997, accuracy: 0.5333333333333333\n",
      "iteration no 5: Loss: 0.9997867294795887, accuracy: 0.5266666666666666\n",
      "iteration no 6: Loss: 0.9797276661346356, accuracy: 0.53\n",
      "iteration no 7: Loss: 0.9617617156867978, accuracy: 0.5266666666666666\n",
      "iteration no 8: Loss: 0.9456366983369004, accuracy: 0.52\n",
      "iteration no 9: Loss: 0.931131985112159, accuracy: 0.52\n",
      "iteration no 10: Loss: 0.9180550112926322, accuracy: 0.5166666666666667\n",
      "iteration no 11: Loss: 0.9062379821751337, accuracy: 0.5166666666666667\n",
      "iteration no 12: Loss: 0.8955348064534805, accuracy: 0.5233333333333333\n",
      "iteration no 13: Loss: 0.885818298089239, accuracy: 0.5233333333333333\n",
      "iteration no 14: Loss: 0.8769776688463008, accuracy: 0.5233333333333333\n",
      "iteration no 15: Loss: 0.8689163133479513, accuracy: 0.5233333333333333\n",
      "iteration no 16: Loss: 0.8615498734935982, accuracy: 0.5233333333333333\n",
      "iteration no 17: Loss: 0.8548045600732738, accuracy: 0.5233333333333333\n",
      "iteration no 18: Loss: 0.8486157052033009, accuracy: 0.5166666666666667\n",
      "iteration no 19: Loss: 0.8429265182425252, accuracy: 0.5166666666666667\n",
      "iteration no 20: Loss: 0.8376870188599088, accuracy: 0.5166666666666667\n",
      "iteration no 21: Loss: 0.8328531229991485, accuracy: 0.5166666666666667\n",
      "iteration no 22: Loss: 0.8283858600274072, accuracy: 0.52\n",
      "iteration no 23: Loss: 0.8242507019987662, accuracy: 0.52\n",
      "iteration no 24: Loss: 0.8204169885019359, accuracy: 0.52\n",
      "iteration no 25: Loss: 0.8168574328906071, accuracy: 0.5166666666666667\n",
      "iteration no 26: Loss: 0.8135476977700629, accuracy: 0.5166666666666667\n",
      "iteration no 27: Loss: 0.8104660294279514, accuracy: 0.5166666666666667\n",
      "iteration no 28: Loss: 0.8075929424627788, accuracy: 0.5166666666666667\n",
      "iteration no 29: Loss: 0.8049109472028452, accuracy: 0.5166666666666667\n",
      "iteration no 30: Loss: 0.8024043136467468, accuracy: 0.5166666666666667\n",
      "iteration no 31: Loss: 0.8000588666203153, accuracy: 0.5166666666666667\n",
      "iteration no 32: Loss: 0.7978618076585642, accuracy: 0.5166666666666667\n",
      "iteration no 33: Loss: 0.7958015598071011, accuracy: 0.5166666666666667\n",
      "iteration no 34: Loss: 0.7938676321151528, accuracy: 0.5166666666666667\n",
      "iteration no 35: Loss: 0.7920505010787919, accuracy: 0.5166666666666667\n",
      "iteration no 36: Loss: 0.7903415067027089, accuracy: 0.5166666666666667\n",
      "iteration no 37: Loss: 0.7887327611942119, accuracy: 0.5166666666666667\n",
      "iteration no 38: Loss: 0.7872170685944814, accuracy: 0.5166666666666667\n",
      "iteration no 39: Loss: 0.785787853898182, accuracy: 0.5166666666666667\n",
      "iteration no 40: Loss: 0.7844391004206263, accuracy: 0.5166666666666667\n",
      "iteration no 41: Loss: 0.7831652943479509, accuracy: 0.5166666666666667\n",
      "iteration no 42: Loss: 0.7819613755552639, accuracy: 0.5166666666666667\n",
      "iteration no 43: Loss: 0.7808226939047653, accuracy: 0.5166666666666667\n",
      "iteration no 44: Loss: 0.7797449703439601, accuracy: 0.5166666666666667\n",
      "iteration no 45: Loss: 0.7787242622162619, accuracy: 0.5133333333333333\n",
      "iteration no 46: Loss: 0.7777569322750254, accuracy: 0.5133333333333333\n",
      "iteration no 47: Loss: 0.7768396209594108, accuracy: 0.5133333333333333\n",
      "iteration no 48: Loss: 0.7759692215482268, accuracy: 0.5133333333333333\n",
      "iteration no 49: Loss: 0.7751428578574925, accuracy: 0.5133333333333333\n",
      "iteration no 50: Loss: 0.7743578641901168, accuracy: 0.5133333333333333\n",
      "iteration no 51: Loss: 0.773611767282855, accuracy: 0.5133333333333333\n",
      "iteration no 52: Loss: 0.7729022700274486, accuracy: 0.5166666666666667\n",
      "iteration no 53: Loss: 0.7722272367703051, accuracy: 0.5166666666666667\n",
      "iteration no 54: Loss: 0.7715846800188575, accuracy: 0.5166666666666667\n",
      "iteration no 55: Loss: 0.7709727484033891, accuracy: 0.5166666666666667\n",
      "iteration no 56: Loss: 0.7703897157610534, accuracy: 0.5166666666666667\n",
      "iteration no 57: Loss: 0.7698339712244453, accuracy: 0.5166666666666667\n",
      "iteration no 58: Loss: 0.7693040102107173, accuracy: 0.5166666666666667\n",
      "iteration no 59: Loss: 0.7687984262191403, accuracy: 0.5166666666666667\n",
      "iteration no 60: Loss: 0.7683159033554331, accuracy: 0.5166666666666667\n",
      "iteration no 61: Loss: 0.7678552095103239, accuracy: 0.5166666666666667\n",
      "iteration no 62: Loss: 0.7674151901278223, accuracy: 0.5166666666666667\n",
      "iteration no 63: Loss: 0.766994762505737, accuracy: 0.5166666666666667\n",
      "iteration no 64: Loss: 0.7665929105771796, accuracy: 0.5166666666666667\n",
      "iteration no 65: Loss: 0.766208680127275, accuracy: 0.5166666666666667\n",
      "iteration no 66: Loss: 0.7658411744041292, accuracy: 0.5166666666666667\n",
      "iteration no 67: Loss: 0.7654895500873858, accuracy: 0.5166666666666667\n",
      "iteration no 68: Loss: 0.7651530135814926, accuracy: 0.5166666666666667\n",
      "iteration no 69: Loss: 0.7648308176041474, accuracy: 0.5166666666666667\n",
      "iteration no 70: Loss: 0.7645222580433934, accuracy: 0.5166666666666667\n",
      "iteration no 71: Loss: 0.7642266710594656, accuracy: 0.5166666666666667\n",
      "iteration no 72: Loss: 0.7639434304098717, accuracy: 0.5166666666666667\n",
      "iteration no 73: Loss: 0.7636719449782816, accuracy: 0.5166666666666667\n",
      "iteration no 74: Loss: 0.7634116564896868, accuracy: 0.5166666666666667\n",
      "iteration no 75: Loss: 0.7631620373959687, accuracy: 0.5166666666666667\n",
      "iteration no 76: Loss: 0.7629225889175187, accuracy: 0.5166666666666667\n",
      "iteration no 77: Loss: 0.7626928392278983, accuracy: 0.5166666666666667\n",
      "iteration no 78: Loss: 0.7624723417697353, accuracy: 0.5166666666666667\n",
      "iteration no 79: Loss: 0.7622606736911405, accuracy: 0.5166666666666667\n",
      "iteration no 80: Loss: 0.7620574343928984, accuracy: 0.5166666666666667\n",
      "iteration no 81: Loss: 0.7618622441775678, accuracy: 0.5133333333333333\n",
      "iteration no 82: Loss: 0.7616747429924152, accuracy: 0.5133333333333333\n",
      "iteration no 83: Loss: 0.7614945892588229, accuracy: 0.5133333333333333\n",
      "iteration no 84: Loss: 0.7613214587814461, accuracy: 0.5133333333333333\n",
      "iteration no 85: Loss: 0.7611550437309889, accuracy: 0.5133333333333333\n",
      "iteration no 86: Loss: 0.7609950516949863, accuracy: 0.5133333333333333\n",
      "iteration no 87: Loss: 0.7608412047914566, accuracy: 0.5133333333333333\n",
      "iteration no 88: Loss: 0.7606932388407255, accuracy: 0.5133333333333333\n",
      "iteration no 89: Loss: 0.7605509025911096, accuracy: 0.5133333333333333\n",
      "iteration no 90: Loss: 0.760413956994507, accuracy: 0.5133333333333333\n",
      "iteration no 91: Loss: 0.7602821745282647, accuracy: 0.5133333333333333\n",
      "iteration no 92: Loss: 0.7601553385599851, accuracy: 0.5133333333333333\n",
      "iteration no 93: Loss: 0.7600332427522071, accuracy: 0.5133333333333333\n",
      "iteration no 94: Loss: 0.7599156905041352, accuracy: 0.5133333333333333\n",
      "iteration no 95: Loss: 0.7598024944278192, accuracy: 0.5133333333333333\n",
      "iteration no 96: Loss: 0.7596934758563845, accuracy: 0.5133333333333333\n",
      "iteration no 97: Loss: 0.759588464382104, accuracy: 0.5133333333333333\n",
      "iteration no 98: Loss: 0.7594872974222706, accuracy: 0.5133333333333333\n",
      "iteration no 99: Loss: 0.7593898198109842, accuracy: 0.5133333333333333\n",
      "iteration no 100: Loss: 0.7592958834151095, accuracy: 0.5133333333333333\n",
      "iteration no 101: Loss: 0.7592053467727954, accuracy: 0.5166666666666667\n",
      "iteration no 102: Loss: 0.759118074753061, accuracy: 0.5166666666666667\n",
      "iteration no 103: Loss: 0.7590339382350657, accuracy: 0.5166666666666667\n",
      "iteration no 104: Loss: 0.7589528138057855, accuracy: 0.5166666666666667\n",
      "iteration no 105: Loss: 0.7588745834749028, accuracy: 0.5166666666666667\n",
      "iteration no 106: Loss: 0.7587991344058103, accuracy: 0.5166666666666667\n",
      "iteration no 107: Loss: 0.7587263586617033, accuracy: 0.5166666666666667\n",
      "iteration no 108: Loss: 0.7586561529658096, accuracy: 0.5166666666666667\n",
      "iteration no 109: Loss: 0.7585884184748705, accuracy: 0.5166666666666667\n",
      "iteration no 110: Loss: 0.7585230605650536, accuracy: 0.5166666666666667\n",
      "iteration no 111: Loss: 0.7584599886295267, accuracy: 0.5166666666666667\n",
      "iteration no 112: Loss: 0.7583991158869825, accuracy: 0.5166666666666667\n",
      "iteration no 113: Loss: 0.7583403592004482, accuracy: 0.5166666666666667\n",
      "iteration no 114: Loss: 0.7582836389057594, accuracy: 0.5166666666666667\n",
      "iteration no 115: Loss: 0.7582288786491218, accuracy: 0.5166666666666667\n",
      "iteration no 116: Loss: 0.7581760052332167, accuracy: 0.5166666666666667\n",
      "iteration no 117: Loss: 0.7581249484713525, accuracy: 0.5166666666666667\n",
      "iteration no 118: Loss: 0.7580756410491849, accuracy: 0.5166666666666667\n",
      "iteration no 119: Loss: 0.7580280183935694, accuracy: 0.5166666666666667\n",
      "iteration no 120: Loss: 0.7579820185481323, accuracy: 0.5166666666666667\n",
      "iteration no 121: Loss: 0.7579375820551758, accuracy: 0.5166666666666667\n",
      "iteration no 122: Loss: 0.7578946518435554, accuracy: 0.5166666666666667\n",
      "iteration no 123: Loss: 0.7578531731221932, accuracy: 0.5166666666666667\n",
      "iteration no 124: Loss: 0.7578130932789072, accuracy: 0.5166666666666667\n",
      "iteration no 125: Loss: 0.7577743617842647, accuracy: 0.5166666666666667\n",
      "iteration no 126: Loss: 0.7577369301001756, accuracy: 0.5166666666666667\n",
      "iteration no 127: Loss: 0.7577007515929686, accuracy: 0.5166666666666667\n",
      "iteration no 128: Loss: 0.7576657814507026, accuracy: 0.5133333333333333\n",
      "iteration no 129: Loss: 0.757631976604484, accuracy: 0.5133333333333333\n",
      "iteration no 130: Loss: 0.7575992956535722, accuracy: 0.5133333333333333\n",
      "iteration no 131: Loss: 0.7575676987940724, accuracy: 0.5133333333333333\n",
      "iteration no 132: Loss: 0.7575371477510192, accuracy: 0.5133333333333333\n",
      "iteration no 133: Loss: 0.7575076057136774, accuracy: 0.5133333333333333\n",
      "iteration no 134: Loss: 0.7574790372738859, accuracy: 0.5133333333333333\n",
      "iteration no 135: Loss: 0.7574514083672853, accuracy: 0.5133333333333333\n",
      "iteration no 136: Loss: 0.7574246862172836, accuracy: 0.5133333333333333\n",
      "iteration no 137: Loss: 0.7573988392816106, accuracy: 0.5133333333333333\n",
      "iteration no 138: Loss: 0.7573738372013356, accuracy: 0.5133333333333333\n",
      "iteration no 139: Loss: 0.757349650752217, accuracy: 0.5133333333333333\n",
      "iteration no 140: Loss: 0.7573262517982673, accuracy: 0.5133333333333333\n",
      "iteration no 141: Loss: 0.757303613247421, accuracy: 0.5133333333333333\n",
      "iteration no 142: Loss: 0.7572817090091998, accuracy: 0.5133333333333333\n",
      "iteration no 143: Loss: 0.7572605139542755, accuracy: 0.5133333333333333\n",
      "iteration no 144: Loss: 0.7572400038758353, accuracy: 0.5133333333333333\n",
      "iteration no 145: Loss: 0.7572201554526613, accuracy: 0.5133333333333333\n",
      "iteration no 146: Loss: 0.7572009462138398, accuracy: 0.5133333333333333\n",
      "iteration no 147: Loss: 0.7571823545050204, accuracy: 0.5133333333333333\n",
      "iteration no 148: Loss: 0.7571643594561484, accuracy: 0.5133333333333333\n",
      "iteration no 149: Loss: 0.7571469409506035, accuracy: 0.5133333333333333\n",
      "iteration no 150: Loss: 0.7571300795956709, accuracy: 0.5133333333333333\n",
      "iteration no 151: Loss: 0.7571137566942853, accuracy: 0.5133333333333333\n",
      "iteration no 152: Loss: 0.7570979542179871, accuracy: 0.5133333333333333\n",
      "iteration no 153: Loss: 0.7570826547810311, accuracy: 0.5133333333333333\n",
      "iteration no 154: Loss: 0.7570678416155956, accuracy: 0.5133333333333333\n",
      "iteration no 155: Loss: 0.7570534985480393, accuracy: 0.5133333333333333\n",
      "iteration no 156: Loss: 0.7570396099761565, accuracy: 0.5133333333333333\n",
      "iteration no 157: Loss: 0.757026160847386, accuracy: 0.5133333333333333\n",
      "iteration no 158: Loss: 0.7570131366379289, accuracy: 0.5133333333333333\n",
      "iteration no 159: Loss: 0.7570005233327323, accuracy: 0.5133333333333333\n",
      "iteration no 160: Loss: 0.7569883074063009, accuracy: 0.5133333333333333\n",
      "iteration no 161: Loss: 0.7569764758042993, accuracy: 0.5133333333333333\n",
      "iteration no 162: Loss: 0.7569650159259073, accuracy: 0.5133333333333333\n",
      "iteration no 163: Loss: 0.756953915606894, accuracy: 0.5133333333333333\n",
      "iteration no 164: Loss: 0.7569431631033839, accuracy: 0.5133333333333333\n",
      "iteration no 165: Loss: 0.7569327470762739, accuracy: 0.5133333333333333\n",
      "iteration no 166: Loss: 0.756922656576284, accuracy: 0.5133333333333333\n",
      "iteration no 167: Loss: 0.7569128810296049, accuracy: 0.5133333333333333\n",
      "iteration no 168: Loss: 0.7569034102241194, accuracy: 0.5133333333333333\n",
      "iteration no 169: Loss: 0.756894234296174, accuracy: 0.5133333333333333\n",
      "iteration no 170: Loss: 0.7568853437178735, accuracy: 0.5133333333333333\n",
      "iteration no 171: Loss: 0.7568767292848793, accuracy: 0.5133333333333333\n",
      "iteration no 172: Loss: 0.756868382104684, accuracy: 0.5133333333333333\n",
      "iteration no 173: Loss: 0.7568602935853526, accuracy: 0.5133333333333333\n",
      "iteration no 174: Loss: 0.7568524554246955, accuracy: 0.5133333333333333\n",
      "iteration no 175: Loss: 0.7568448595998678, accuracy: 0.5133333333333333\n",
      "iteration no 176: Loss: 0.7568374983573715, accuracy: 0.5133333333333333\n",
      "iteration no 177: Loss: 0.7568303642034405, accuracy: 0.5133333333333333\n",
      "iteration no 178: Loss: 0.7568234498947999, accuracy: 0.5133333333333333\n",
      "iteration no 179: Loss: 0.7568167484297753, accuracy: 0.5133333333333333\n",
      "iteration no 180: Loss: 0.7568102530397464, accuracy: 0.5133333333333333\n",
      "iteration no 181: Loss: 0.7568039571809216, accuracy: 0.5133333333333333\n",
      "iteration no 182: Loss: 0.7567978545264279, accuracy: 0.5133333333333333\n",
      "iteration no 183: Loss: 0.7567919389586978, accuracy: 0.5133333333333333\n",
      "iteration no 184: Loss: 0.7567862045621437, accuracy: 0.5133333333333333\n",
      "iteration no 185: Loss: 0.756780645616109, accuracy: 0.5133333333333333\n",
      "iteration no 186: Loss: 0.7567752565880796, accuracy: 0.5133333333333333\n",
      "iteration no 187: Loss: 0.7567700321271524, accuracy: 0.5133333333333333\n",
      "iteration no 188: Loss: 0.7567649670577432, accuracy: 0.5133333333333333\n",
      "iteration no 189: Loss: 0.7567600563735315, accuracy: 0.5133333333333333\n",
      "iteration no 190: Loss: 0.7567552952316252, accuracy: 0.5133333333333333\n",
      "iteration no 191: Loss: 0.756750678946943, accuracy: 0.5133333333333333\n",
      "iteration no 192: Loss: 0.7567462029868022, accuracy: 0.5133333333333333\n",
      "iteration no 193: Loss: 0.756741862965704, accuracy: 0.5133333333333333\n",
      "iteration no 194: Loss: 0.75673765464031, accuracy: 0.5133333333333333\n",
      "iteration no 195: Loss: 0.7567335739046004, accuracy: 0.5133333333333333\n",
      "iteration no 196: Loss: 0.7567296167852096, accuracy: 0.5133333333333333\n",
      "iteration no 197: Loss: 0.7567257794369282, accuracy: 0.5133333333333333\n",
      "iteration no 198: Loss: 0.7567220581383686, accuracy: 0.5133333333333333\n",
      "iteration no 199: Loss: 0.7567184492877872, accuracy: 0.5133333333333333\n",
      "iteration no 200: Loss: 0.7567149493990538, accuracy: 0.5133333333333333\n",
      "iteration no 1: Loss: 1.0936350495866394, accuracy: 0.5\n",
      "iteration no 2: Loss: 1.0636042874202294, accuracy: 0.51\n",
      "iteration no 3: Loss: 1.036870715412701, accuracy: 0.5066666666666667\n",
      "iteration no 4: Loss: 1.0130348639398294, accuracy: 0.5066666666666667\n",
      "iteration no 5: Loss: 0.9917439231315401, accuracy: 0.5066666666666667\n",
      "iteration no 6: Loss: 0.9726876136947208, accuracy: 0.5133333333333333\n",
      "iteration no 7: Loss: 0.9555942272457645, accuracy: 0.5133333333333333\n",
      "iteration no 8: Loss: 0.9402266460567835, accuracy: 0.51\n",
      "iteration no 9: Loss: 0.9263784226418972, accuracy: 0.51\n",
      "iteration no 10: Loss: 0.9138700457125947, accuracy: 0.5133333333333333\n",
      "iteration no 11: Loss: 0.9025454894034253, accuracy: 0.51\n",
      "iteration no 12: Loss: 0.8922690987377341, accuracy: 0.51\n",
      "iteration no 13: Loss: 0.8829228274439055, accuracy: 0.51\n",
      "iteration no 14: Loss: 0.874403819089185, accuracy: 0.51\n",
      "iteration no 15: Loss: 0.8666223077253115, accuracy: 0.51\n",
      "iteration no 16: Loss: 0.859499807027942, accuracy: 0.51\n",
      "iteration no 17: Loss: 0.852967554642199, accuracy: 0.51\n",
      "iteration no 18: Loss: 0.8469651791370356, accuracy: 0.51\n",
      "iteration no 19: Loss: 0.8414395592656422, accuracy: 0.51\n",
      "iteration no 20: Loss: 0.8363438482607433, accuracy: 0.51\n",
      "iteration no 21: Loss: 0.8316366391384453, accuracy: 0.51\n",
      "iteration no 22: Loss: 0.8272812501449434, accuracy: 0.51\n",
      "iteration no 23: Loss: 0.8232451124025696, accuracy: 0.51\n",
      "iteration no 24: Loss: 0.8194992444282106, accuracy: 0.51\n",
      "iteration no 25: Loss: 0.8160178004914733, accuracy: 0.51\n",
      "iteration no 26: Loss: 0.8127776817636391, accuracy: 0.51\n",
      "iteration no 27: Loss: 0.8097582009069306, accuracy: 0.5133333333333333\n",
      "iteration no 28: Loss: 0.8069407921982067, accuracy: 0.5133333333333333\n",
      "iteration no 29: Loss: 0.8043087605042382, accuracy: 0.5133333333333333\n",
      "iteration no 30: Loss: 0.8018470634580427, accuracy: 0.5133333333333333\n",
      "iteration no 31: Loss: 0.7995421220554428, accuracy: 0.5133333333333333\n",
      "iteration no 32: Loss: 0.797381655622922, accuracy: 0.5133333333333333\n",
      "iteration no 33: Loss: 0.7953545377235673, accuracy: 0.5133333333333333\n",
      "iteration no 34: Loss: 0.7934506700859202, accuracy: 0.5166666666666667\n",
      "iteration no 35: Loss: 0.7916608720766132, accuracy: 0.5166666666666667\n",
      "iteration no 36: Loss: 0.7899767836050496, accuracy: 0.5166666666666667\n",
      "iteration no 37: Loss: 0.7883907796581883, accuracy: 0.5166666666666667\n",
      "iteration no 38: Loss: 0.7868958949251237, accuracy: 0.5133333333333333\n",
      "iteration no 39: Loss: 0.7854857571923628, accuracy: 0.5133333333333333\n",
      "iteration no 40: Loss: 0.7841545283780481, accuracy: 0.5133333333333333\n",
      "iteration no 41: Loss: 0.7828968522322882, accuracy: 0.5133333333333333\n",
      "iteration no 42: Loss: 0.7817078078657772, accuracy: 0.5133333333333333\n",
      "iteration no 43: Loss: 0.780582868383798, accuracy: 0.5133333333333333\n",
      "iteration no 44: Loss: 0.7795178640006936, accuracy: 0.5133333333333333\n",
      "iteration no 45: Loss: 0.7785089490935714, accuracy: 0.5133333333333333\n",
      "iteration no 46: Loss: 0.7775525727256233, accuracy: 0.5133333333333333\n",
      "iteration no 47: Loss: 0.776645452230829, accuracy: 0.5133333333333333\n",
      "iteration no 48: Loss: 0.7757845495045276, accuracy: 0.5133333333333333\n",
      "iteration no 49: Loss: 0.7749670496896999, accuracy: 0.5133333333333333\n",
      "iteration no 50: Loss: 0.7741903419878929, accuracy: 0.5133333333333333\n",
      "iteration no 51: Loss: 0.7734520023574684, accuracy: 0.5133333333333333\n",
      "iteration no 52: Loss: 0.7727497778910521, accuracy: 0.5133333333333333\n",
      "iteration no 53: Loss: 0.7720815726893534, accuracy: 0.5133333333333333\n",
      "iteration no 54: Loss: 0.7714454350704757, accuracy: 0.5133333333333333\n",
      "iteration no 55: Loss: 0.7708395459729342, accuracy: 0.5133333333333333\n",
      "iteration no 56: Loss: 0.7702622084272082, accuracy: 0.5133333333333333\n",
      "iteration no 57: Loss: 0.7697118379851675, accuracy: 0.5166666666666667\n",
      "iteration no 58: Loss: 0.7691869540093774, accuracy: 0.5166666666666667\n",
      "iteration no 59: Loss: 0.7686861717353737, accuracy: 0.5166666666666667\n",
      "iteration no 60: Loss: 0.7682081950297249, accuracy: 0.5166666666666667\n",
      "iteration no 61: Loss: 0.7677518097752264, accuracy: 0.5166666666666667\n",
      "iteration no 62: Loss: 0.7673158778220781, accuracy: 0.5166666666666667\n",
      "iteration no 63: Loss: 0.7668993314505026, accuracy: 0.5166666666666667\n",
      "iteration no 64: Loss: 0.7665011682960869, accuracy: 0.5166666666666667\n",
      "iteration no 65: Loss: 0.7661204466942785, accuracy: 0.5166666666666667\n",
      "iteration no 66: Loss: 0.7657562814050147, accuracy: 0.5166666666666667\n",
      "iteration no 67: Loss: 0.7654078396825008, accuracy: 0.5166666666666667\n",
      "iteration no 68: Loss: 0.7650743376587168, accuracy: 0.5166666666666667\n",
      "iteration no 69: Loss: 0.764755037012422, accuracy: 0.5166666666666667\n",
      "iteration no 70: Loss: 0.7644492418982342, accuracy: 0.5166666666666667\n",
      "iteration no 71: Loss: 0.7641562961128855, accuracy: 0.5166666666666667\n",
      "iteration no 72: Loss: 0.7638755804779935, accuracy: 0.5166666666666667\n",
      "iteration no 73: Loss: 0.7636065104206843, accuracy: 0.5166666666666667\n",
      "iteration no 74: Loss: 0.7633485337351983, accuracy: 0.5166666666666667\n",
      "iteration no 75: Loss: 0.7631011285102044, accuracy: 0.5166666666666667\n",
      "iteration no 76: Loss: 0.7628638012079827, accuracy: 0.5166666666666667\n",
      "iteration no 77: Loss: 0.7626360848829229, accuracy: 0.5166666666666667\n",
      "iteration no 78: Loss: 0.76241753752794, accuracy: 0.5166666666666667\n",
      "iteration no 79: Loss: 0.7622077405384422, accuracy: 0.5133333333333333\n",
      "iteration no 80: Loss: 0.7620062972844291, accuracy: 0.5133333333333333\n",
      "iteration no 81: Loss: 0.7618128317821287, accuracy: 0.5133333333333333\n",
      "iteration no 82: Loss: 0.7616269874573498, accuracy: 0.5133333333333333\n",
      "iteration no 83: Loss: 0.7614484259934055, accuracy: 0.5133333333333333\n",
      "iteration no 84: Loss: 0.7612768262570863, accuracy: 0.5133333333333333\n",
      "iteration no 85: Loss: 0.7611118832967187, accuracy: 0.5133333333333333\n",
      "iteration no 86: Loss: 0.7609533074068555, accuracy: 0.5133333333333333\n",
      "iteration no 87: Loss: 0.7608008232545993, accuracy: 0.5133333333333333\n",
      "iteration no 88: Loss: 0.7606541690629831, accuracy: 0.5133333333333333\n",
      "iteration no 89: Loss: 0.7605130958472058, accuracy: 0.5133333333333333\n",
      "iteration no 90: Loss: 0.7603773666998694, accuracy: 0.5133333333333333\n",
      "iteration no 91: Loss: 0.7602467561216725, accuracy: 0.5133333333333333\n",
      "iteration no 92: Loss: 0.7601210493943047, accuracy: 0.5133333333333333\n",
      "iteration no 93: Loss: 0.7600000419925437, accuracy: 0.5133333333333333\n",
      "iteration no 94: Loss: 0.7598835390327954, accuracy: 0.5133333333333333\n",
      "iteration no 95: Loss: 0.7597713547555325, accuracy: 0.5133333333333333\n",
      "iteration no 96: Loss: 0.7596633120392862, accuracy: 0.5133333333333333\n",
      "iteration no 97: Loss: 0.7595592419440238, accuracy: 0.5133333333333333\n",
      "iteration no 98: Loss: 0.7594589832819154, accuracy: 0.5133333333333333\n",
      "iteration no 99: Loss: 0.7593623822136383, accuracy: 0.5133333333333333\n",
      "iteration no 100: Loss: 0.7592692918685111, accuracy: 0.5133333333333333\n",
      "iteration no 101: Loss: 0.7591795719868761, accuracy: 0.5133333333333333\n",
      "iteration no 102: Loss: 0.759093088583263, accuracy: 0.5166666666666667\n",
      "iteration no 103: Loss: 0.7590097136289764, accuracy: 0.5166666666666667\n",
      "iteration no 104: Loss: 0.7589293247528519, accuracy: 0.5166666666666667\n",
      "iteration no 105: Loss: 0.7588518049590051, accuracy: 0.5166666666666667\n",
      "iteration no 106: Loss: 0.758777042360497, accuracy: 0.5166666666666667\n",
      "iteration no 107: Loss: 0.758704929927903, accuracy: 0.5166666666666667\n",
      "iteration no 108: Loss: 0.758635365251849, accuracy: 0.5166666666666667\n",
      "iteration no 109: Loss: 0.7585682503186485, accuracy: 0.5166666666666667\n",
      "iteration no 110: Loss: 0.7585034912982221, accuracy: 0.5166666666666667\n",
      "iteration no 111: Loss: 0.7584409983435533, accuracy: 0.5166666666666667\n",
      "iteration no 112: Loss: 0.758380685400968, accuracy: 0.5166666666666667\n",
      "iteration no 113: Loss: 0.7583224700305911, accuracy: 0.5166666666666667\n",
      "iteration no 114: Loss: 0.7582662732363625, accuracy: 0.5166666666666667\n",
      "iteration no 115: Loss: 0.7582120193050442, accuracy: 0.5166666666666667\n",
      "iteration no 116: Loss: 0.7581596356536862, accuracy: 0.5166666666666667\n",
      "iteration no 117: Loss: 0.7581090526850512, accuracy: 0.5166666666666667\n",
      "iteration no 118: Loss: 0.7580602036505381, accuracy: 0.5166666666666667\n",
      "iteration no 119: Loss: 0.7580130245201605, accuracy: 0.5166666666666667\n",
      "iteration no 120: Loss: 0.7579674538591848, accuracy: 0.5166666666666667\n",
      "iteration no 121: Loss: 0.7579234327110363, accuracy: 0.5166666666666667\n",
      "iteration no 122: Loss: 0.7578809044861241, accuracy: 0.5166666666666667\n",
      "iteration no 123: Loss: 0.7578398148562462, accuracy: 0.5166666666666667\n",
      "iteration no 124: Loss: 0.7578001116542648, accuracy: 0.5166666666666667\n",
      "iteration no 125: Loss: 0.7577617447787555, accuracy: 0.5166666666666667\n",
      "iteration no 126: Loss: 0.7577246661033579, accuracy: 0.5166666666666667\n",
      "iteration no 127: Loss: 0.7576888293905653, accuracy: 0.5166666666666667\n",
      "iteration no 128: Loss: 0.7576541902097134, accuracy: 0.5166666666666667\n",
      "iteration no 129: Loss: 0.7576207058589397, accuracy: 0.5166666666666667\n",
      "iteration no 130: Loss: 0.7575883352908999, accuracy: 0.5166666666666667\n",
      "iteration no 131: Loss: 0.7575570390420349, accuracy: 0.5166666666666667\n",
      "iteration no 132: Loss: 0.7575267791652076, accuracy: 0.5166666666666667\n",
      "iteration no 133: Loss: 0.7574975191655228, accuracy: 0.5133333333333333\n",
      "iteration no 134: Loss: 0.7574692239391676, accuracy: 0.5133333333333333\n",
      "iteration no 135: Loss: 0.7574418597151126, accuracy: 0.5133333333333333\n",
      "iteration no 136: Loss: 0.7574153939995242, accuracy: 0.5133333333333333\n",
      "iteration no 137: Loss: 0.7573897955227492, accuracy: 0.5133333333333333\n",
      "iteration no 138: Loss: 0.7573650341887385, accuracy: 0.5133333333333333\n",
      "iteration no 139: Loss: 0.7573410810267859, accuracy: 0.5133333333333333\n",
      "iteration no 140: Loss: 0.7573179081454634, accuracy: 0.5133333333333333\n",
      "iteration no 141: Loss: 0.7572954886886429, accuracy: 0.5133333333333333\n",
      "iteration no 142: Loss: 0.7572737967934985, accuracy: 0.5133333333333333\n",
      "iteration no 143: Loss: 0.7572528075503921, accuracy: 0.5133333333333333\n",
      "iteration no 144: Loss: 0.7572324969645458, accuracy: 0.5133333333333333\n",
      "iteration no 145: Loss: 0.7572128419194164, accuracy: 0.5133333333333333\n",
      "iteration no 146: Loss: 0.7571938201416861, accuracy: 0.5133333333333333\n",
      "iteration no 147: Loss: 0.7571754101677892, accuracy: 0.5133333333333333\n",
      "iteration no 148: Loss: 0.7571575913119039, accuracy: 0.5133333333333333\n",
      "iteration no 149: Loss: 0.7571403436353361, accuracy: 0.5133333333333333\n",
      "iteration no 150: Loss: 0.757123647917226, accuracy: 0.5133333333333333\n",
      "iteration no 151: Loss: 0.7571074856265196, accuracy: 0.5133333333333333\n",
      "iteration no 152: Loss: 0.7570918388951399, accuracy: 0.5133333333333333\n",
      "iteration no 153: Loss: 0.7570766904923041, accuracy: 0.5133333333333333\n",
      "iteration no 154: Loss: 0.7570620237999307, accuracy: 0.5133333333333333\n",
      "iteration no 155: Loss: 0.757047822789087, accuracy: 0.5133333333333333\n",
      "iteration no 156: Loss: 0.7570340719974282, accuracy: 0.5133333333333333\n",
      "iteration no 157: Loss: 0.7570207565075815, accuracy: 0.5133333333333333\n",
      "iteration no 158: Loss: 0.7570078619264313, accuracy: 0.5133333333333333\n",
      "iteration no 159: Loss: 0.7569953743652658, accuracy: 0.5133333333333333\n",
      "iteration no 160: Loss: 0.7569832804207434, accuracy: 0.5133333333333333\n",
      "iteration no 161: Loss: 0.756971567156644, accuracy: 0.5133333333333333\n",
      "iteration no 162: Loss: 0.7569602220863677, accuracy: 0.5133333333333333\n",
      "iteration no 163: Loss: 0.7569492331561469, accuracy: 0.5133333333333333\n",
      "iteration no 164: Loss: 0.7569385887289437, accuracy: 0.5133333333333333\n",
      "iteration no 165: Loss: 0.756928277568995, accuracy: 0.5133333333333333\n",
      "iteration no 166: Loss: 0.7569182888269856, accuracy: 0.5133333333333333\n",
      "iteration no 167: Loss: 0.7569086120258122, accuracy: 0.5133333333333333\n",
      "iteration no 168: Loss: 0.7568992370469193, accuracy: 0.5133333333333333\n",
      "iteration no 169: Loss: 0.756890154117179, accuracy: 0.5133333333333333\n",
      "iteration no 170: Loss: 0.7568813537962906, accuracy: 0.5133333333333333\n",
      "iteration no 171: Loss: 0.7568728269646787, accuracy: 0.5133333333333333\n",
      "iteration no 172: Loss: 0.7568645648118669, accuracy: 0.5133333333333333\n",
      "iteration no 173: Loss: 0.7568565588253099, accuracy: 0.5133333333333333\n",
      "iteration no 174: Loss: 0.7568488007796584, accuracy: 0.5133333333333333\n",
      "iteration no 175: Loss: 0.7568412827264447, accuracy: 0.5133333333333333\n",
      "iteration no 176: Loss: 0.7568339969841673, accuracy: 0.5133333333333333\n",
      "iteration no 177: Loss: 0.7568269361287578, accuracy: 0.5133333333333333\n",
      "iteration no 178: Loss: 0.756820092984416, accuracy: 0.5133333333333333\n",
      "iteration no 179: Loss: 0.7568134606147962, accuracy: 0.5133333333333333\n",
      "iteration no 180: Loss: 0.75680703231453, accuracy: 0.5133333333333333\n",
      "iteration no 181: Loss: 0.7568008016010728, accuracy: 0.5133333333333333\n",
      "iteration no 182: Loss: 0.7567947622068587, accuracy: 0.5133333333333333\n",
      "iteration no 183: Loss: 0.7567889080717529, accuracy: 0.5133333333333333\n",
      "iteration no 184: Loss: 0.7567832333357888, accuracy: 0.5133333333333333\n",
      "iteration no 185: Loss: 0.756777732332177, accuracy: 0.5133333333333333\n",
      "iteration no 186: Loss: 0.7567723995805774, accuracy: 0.5133333333333333\n",
      "iteration no 187: Loss: 0.7567672297806212, accuracy: 0.5133333333333333\n",
      "iteration no 188: Loss: 0.7567622178056745, accuracy: 0.5133333333333333\n",
      "iteration no 189: Loss: 0.7567573586968337, accuracy: 0.5133333333333333\n",
      "iteration no 190: Loss: 0.756752647657141, accuracy: 0.5133333333333333\n",
      "iteration no 191: Loss: 0.756748080046016, accuracy: 0.5133333333333333\n",
      "iteration no 192: Loss: 0.7567436513738877, accuracy: 0.5133333333333333\n",
      "iteration no 193: Loss: 0.7567393572970285, accuracy: 0.5133333333333333\n",
      "iteration no 194: Loss: 0.756735193612571, accuracy: 0.5133333333333333\n",
      "iteration no 195: Loss: 0.7567311562537108, accuracy: 0.5133333333333333\n",
      "iteration no 196: Loss: 0.7567272412850806, accuracy: 0.5133333333333333\n",
      "iteration no 197: Loss: 0.7567234448982931, accuracy: 0.5133333333333333\n",
      "iteration no 198: Loss: 0.7567197634076442, accuracy: 0.5133333333333333\n",
      "iteration no 199: Loss: 0.7567161932459705, accuracy: 0.5133333333333333\n",
      "iteration no 200: Loss: 0.7567127309606562, accuracy: 0.5133333333333333\n",
      "iteration no 1: Loss: 1.0935329638393556, accuracy: 0.5233333333333333\n",
      "iteration no 2: Loss: 1.0633662713641299, accuracy: 0.51\n",
      "iteration no 3: Loss: 1.03653318471492, accuracy: 0.51\n",
      "iteration no 4: Loss: 1.0126251147218666, accuracy: 0.52\n",
      "iteration no 5: Loss: 0.9912837164936628, accuracy: 0.5166666666666667\n",
      "iteration no 6: Loss: 0.9721946020761759, accuracy: 0.5133333333333333\n",
      "iteration no 7: Loss: 0.955082634807141, accuracy: 0.5133333333333333\n",
      "iteration no 8: Loss: 0.9397077345027628, accuracy: 0.5133333333333333\n",
      "iteration no 9: Loss: 0.9258609094540672, accuracy: 0.5133333333333333\n",
      "iteration no 10: Loss: 0.9133605067237224, accuracy: 0.5133333333333333\n",
      "iteration no 11: Loss: 0.9020487376026565, accuracy: 0.5133333333333333\n",
      "iteration no 12: Loss: 0.891788526172064, accuracy: 0.5133333333333333\n",
      "iteration no 13: Loss: 0.8824607021836984, accuracy: 0.5133333333333333\n",
      "iteration no 14: Loss: 0.8739615352708532, accuracy: 0.5133333333333333\n",
      "iteration no 15: Loss: 0.8662005910228187, accuracy: 0.5133333333333333\n",
      "iteration no 16: Loss: 0.8590988803071421, accuracy: 0.5133333333333333\n",
      "iteration no 17: Loss: 0.8525872694945704, accuracy: 0.5133333333333333\n",
      "iteration no 18: Loss: 0.846605119016225, accuracy: 0.5133333333333333\n",
      "iteration no 19: Loss: 0.8410991194837718, accuracy: 0.5133333333333333\n",
      "iteration no 20: Loss: 0.8360222974209854, accuracy: 0.5133333333333333\n",
      "iteration no 21: Loss: 0.8313331658515974, accuracy: 0.5133333333333333\n",
      "iteration no 22: Loss: 0.8269949981888258, accuracy: 0.5133333333333333\n",
      "iteration no 23: Loss: 0.8229752068741351, accuracy: 0.5133333333333333\n",
      "iteration no 24: Loss: 0.8192448109215711, accuracy: 0.5166666666666667\n",
      "iteration no 25: Loss: 0.8157779789083285, accuracy: 0.5166666666666667\n",
      "iteration no 26: Loss: 0.8125516360166015, accuracy: 0.5166666666666667\n",
      "iteration no 27: Loss: 0.8095451254993583, accuracy: 0.5166666666666667\n",
      "iteration no 28: Loss: 0.8067399164446524, accuracy: 0.5166666666666667\n",
      "iteration no 29: Loss: 0.8041193509828277, accuracy: 0.5166666666666667\n",
      "iteration no 30: Loss: 0.8016684251507512, accuracy: 0.5166666666666667\n",
      "iteration no 31: Loss: 0.7993735985266699, accuracy: 0.52\n",
      "iteration no 32: Loss: 0.7972226285046488, accuracy: 0.52\n",
      "iteration no 33: Loss: 0.7952044257116967, accuracy: 0.52\n",
      "iteration no 34: Loss: 0.7933089276030899, accuracy: 0.52\n",
      "iteration no 35: Loss: 0.7915269877186897, accuracy: 0.5166666666666667\n",
      "iteration no 36: Loss: 0.7898502784591367, accuracy: 0.5166666666666667\n",
      "iteration no 37: Loss: 0.788271205557397, accuracy: 0.5166666666666667\n",
      "iteration no 38: Loss: 0.7867828326879981, accuracy: 0.5166666666666667\n",
      "iteration no 39: Loss: 0.7853788148815877, accuracy: 0.5133333333333333\n",
      "iteration no 40: Loss: 0.7840533396029327, accuracy: 0.5133333333333333\n",
      "iteration no 41: Loss: 0.7828010745118266, accuracy: 0.5133333333333333\n",
      "iteration no 42: Loss: 0.7816171210632733, accuracy: 0.5133333333333333\n",
      "iteration no 43: Loss: 0.7804969732196781, accuracy: 0.5133333333333333\n",
      "iteration no 44: Loss: 0.7794364806468808, accuracy: 0.5133333333333333\n",
      "iteration no 45: Loss: 0.7784318158504069, accuracy: 0.5133333333333333\n",
      "iteration no 46: Loss: 0.7774794447805881, accuracy: 0.5133333333333333\n",
      "iteration no 47: Loss: 0.7765761004970869, accuracy: 0.5133333333333333\n",
      "iteration no 48: Loss: 0.7757187595364676, accuracy: 0.5133333333333333\n",
      "iteration no 49: Loss: 0.774904620672096, accuracy: 0.5133333333333333\n",
      "iteration no 50: Loss: 0.7741310857949573, accuracy: 0.5133333333333333\n",
      "iteration no 51: Loss: 0.7733957426779036, accuracy: 0.5133333333333333\n",
      "iteration no 52: Loss: 0.7726963494151441, accuracy: 0.5133333333333333\n",
      "iteration no 53: Loss: 0.7720308203541797, accuracy: 0.5133333333333333\n",
      "iteration no 54: Loss: 0.7713972133593963, accuracy: 0.5133333333333333\n",
      "iteration no 55: Loss: 0.7707937182656637, accuracy: 0.5133333333333333\n",
      "iteration no 56: Loss: 0.7702186463969357, accuracy: 0.5133333333333333\n",
      "iteration no 57: Loss: 0.7696704210393671, accuracy: 0.5133333333333333\n",
      "iteration no 58: Loss: 0.7691475687711423, accuracy: 0.5133333333333333\n",
      "iteration no 59: Loss: 0.7686487115623041, accuracy: 0.5166666666666667\n",
      "iteration no 60: Loss: 0.7681725595675852, accuracy: 0.5166666666666667\n",
      "iteration no 61: Loss: 0.76771790454378, accuracy: 0.5166666666666667\n",
      "iteration no 62: Loss: 0.7672836138306859, accuracy: 0.5166666666666667\n",
      "iteration no 63: Loss: 0.7668686248412449, accuracy: 0.5166666666666667\n",
      "iteration no 64: Loss: 0.7664719400123309, accuracy: 0.5166666666666667\n",
      "iteration no 65: Loss: 0.7660926221727716, accuracy: 0.5166666666666667\n",
      "iteration no 66: Loss: 0.7657297902897264, accuracy: 0.5166666666666667\n",
      "iteration no 67: Loss: 0.7653826155585687, accuracy: 0.5166666666666667\n",
      "iteration no 68: Loss: 0.7650503178049857, accuracy: 0.5166666666666667\n",
      "iteration no 69: Loss: 0.7647321621711746, accuracy: 0.5166666666666667\n",
      "iteration no 70: Loss: 0.7644274560608257, accuracy: 0.5166666666666667\n",
      "iteration no 71: Loss: 0.7641355463200934, accuracy: 0.5166666666666667\n",
      "iteration no 72: Loss: 0.7638558166339843, accuracy: 0.5166666666666667\n",
      "iteration no 73: Loss: 0.7635876851195912, accuracy: 0.5166666666666667\n",
      "iteration no 74: Loss: 0.763330602099375, accuracy: 0.5166666666666667\n",
      "iteration no 75: Loss: 0.7630840480392981, accuracy: 0.5166666666666667\n",
      "iteration no 76: Loss: 0.7628475316380348, accuracy: 0.5166666666666667\n",
      "iteration no 77: Loss: 0.7626205880547685, accuracy: 0.5166666666666667\n",
      "iteration no 78: Loss: 0.7624027772642321, accuracy: 0.5166666666666667\n",
      "iteration no 79: Loss: 0.7621936825286815, accuracy: 0.5166666666666667\n",
      "iteration no 80: Loss: 0.7619929089774249, accuracy: 0.5133333333333333\n",
      "iteration no 81: Loss: 0.7618000822853643, accuracy: 0.5133333333333333\n",
      "iteration no 82: Loss: 0.7616148474427622, accuracy: 0.5133333333333333\n",
      "iteration no 83: Loss: 0.7614368676091303, accuracy: 0.5133333333333333\n",
      "iteration no 84: Loss: 0.7612658230447511, accuracy: 0.5133333333333333\n",
      "iteration no 85: Loss: 0.7611014101138994, accuracy: 0.5133333333333333\n",
      "iteration no 86: Loss: 0.760943340354339, accuracy: 0.5133333333333333\n",
      "iteration no 87: Loss: 0.7607913396081252, accuracy: 0.5133333333333333\n",
      "iteration no 88: Loss: 0.7606451472091583, accuracy: 0.5133333333333333\n",
      "iteration no 89: Loss: 0.7605045152233104, accuracy: 0.5133333333333333\n",
      "iteration no 90: Loss: 0.7603692077372917, accuracy: 0.5133333333333333\n",
      "iteration no 91: Loss: 0.7602390001927316, accuracy: 0.5133333333333333\n",
      "iteration no 92: Loss: 0.7601136787622355, accuracy: 0.5133333333333333\n",
      "iteration no 93: Loss: 0.7599930397644382, accuracy: 0.5133333333333333\n",
      "iteration no 94: Loss: 0.7598768891153043, accuracy: 0.5133333333333333\n",
      "iteration no 95: Loss: 0.7597650418131487, accuracy: 0.5133333333333333\n",
      "iteration no 96: Loss: 0.7596573214550435, accuracy: 0.5133333333333333\n",
      "iteration no 97: Loss: 0.7595535597824569, accuracy: 0.5133333333333333\n",
      "iteration no 98: Loss: 0.759453596254136, accuracy: 0.5133333333333333\n",
      "iteration no 99: Loss: 0.7593572776443933, accuracy: 0.5133333333333333\n",
      "iteration no 100: Loss: 0.7592644576650999, accuracy: 0.5133333333333333\n",
      "iteration no 101: Loss: 0.7591749966098085, accuracy: 0.5133333333333333\n",
      "iteration no 102: Loss: 0.7590887610185525, accuracy: 0.5133333333333333\n",
      "iteration no 103: Loss: 0.7590056233619659, accuracy: 0.5166666666666667\n",
      "iteration no 104: Loss: 0.7589254617434783, accuracy: 0.5166666666666667\n",
      "iteration no 105: Loss: 0.7588481596184177, accuracy: 0.5166666666666667\n",
      "iteration no 106: Loss: 0.7587736055289438, accuracy: 0.5166666666666667\n",
      "iteration no 107: Loss: 0.7587016928538117, accuracy: 0.5166666666666667\n",
      "iteration no 108: Loss: 0.7586323195720321, accuracy: 0.5166666666666667\n",
      "iteration no 109: Loss: 0.7585653880395619, accuracy: 0.5166666666666667\n",
      "iteration no 110: Loss: 0.75850080477822, accuracy: 0.5166666666666667\n",
      "iteration no 111: Loss: 0.7584384802760765, accuracy: 0.5166666666666667\n",
      "iteration no 112: Loss: 0.7583783287986168, accuracy: 0.5166666666666667\n",
      "iteration no 113: Loss: 0.7583202682100243, accuracy: 0.5166666666666667\n",
      "iteration no 114: Loss: 0.7582642198039832, accuracy: 0.5166666666666667\n",
      "iteration no 115: Loss: 0.7582101081434204, accuracy: 0.5166666666666667\n",
      "iteration no 116: Loss: 0.7581578609086692, accuracy: 0.5166666666666667\n",
      "iteration no 117: Loss: 0.7581074087535509, accuracy: 0.5166666666666667\n",
      "iteration no 118: Loss: 0.758058685168916, accuracy: 0.5166666666666667\n",
      "iteration no 119: Loss: 0.7580116263532117, accuracy: 0.5166666666666667\n",
      "iteration no 120: Loss: 0.7579661710896699, accuracy: 0.5166666666666667\n",
      "iteration no 121: Loss: 0.7579222606297373, accuracy: 0.5166666666666667\n",
      "iteration no 122: Loss: 0.757879838582393, accuracy: 0.5166666666666667\n",
      "iteration no 123: Loss: 0.7578388508090228, accuracy: 0.5166666666666667\n",
      "iteration no 124: Loss: 0.7577992453235335, accuracy: 0.5166666666666667\n",
      "iteration no 125: Loss: 0.7577609721974219, accuracy: 0.5166666666666667\n",
      "iteration no 126: Loss: 0.7577239834695199, accuracy: 0.5166666666666667\n",
      "iteration no 127: Loss: 0.7576882330601586, accuracy: 0.5166666666666667\n",
      "iteration no 128: Loss: 0.7576536766895128, accuracy: 0.5166666666666667\n",
      "iteration no 129: Loss: 0.7576202717998953, accuracy: 0.5166666666666667\n",
      "iteration no 130: Loss: 0.7575879774817911, accuracy: 0.5133333333333333\n",
      "iteration no 131: Loss: 0.7575567544034273, accuracy: 0.5133333333333333\n",
      "iteration no 132: Loss: 0.7575265647436957, accuracy: 0.5133333333333333\n",
      "iteration no 133: Loss: 0.757497372128244, accuracy: 0.5133333333333333\n",
      "iteration no 134: Loss: 0.7574691415685727, accuracy: 0.5133333333333333\n",
      "iteration no 135: Loss: 0.7574418394039824, accuracy: 0.5133333333333333\n",
      "iteration no 136: Loss: 0.7574154332462155, accuracy: 0.5133333333333333\n",
      "iteration no 137: Loss: 0.757389891926664, accuracy: 0.5133333333333333\n",
      "iteration no 138: Loss: 0.7573651854459991, accuracy: 0.5133333333333333\n",
      "iteration no 139: Loss: 0.7573412849261107, accuracy: 0.5133333333333333\n",
      "iteration no 140: Loss: 0.7573181625642315, accuracy: 0.5133333333333333\n",
      "iteration no 141: Loss: 0.7572957915891355, accuracy: 0.5133333333333333\n",
      "iteration no 142: Loss: 0.7572741462193147, accuracy: 0.5133333333333333\n",
      "iteration no 143: Loss: 0.7572532016230232, accuracy: 0.5133333333333333\n",
      "iteration no 144: Loss: 0.7572329338801066, accuracy: 0.5133333333333333\n",
      "iteration no 145: Loss: 0.7572133199455213, accuracy: 0.5133333333333333\n",
      "iteration no 146: Loss: 0.7571943376144651, accuracy: 0.5133333333333333\n",
      "iteration no 147: Loss: 0.757175965489038, accuracy: 0.5133333333333333\n",
      "iteration no 148: Loss: 0.7571581829463598, accuracy: 0.5133333333333333\n",
      "iteration no 149: Loss: 0.7571409701080729, accuracy: 0.5133333333333333\n",
      "iteration no 150: Loss: 0.757124307811166, accuracy: 0.5133333333333333\n",
      "iteration no 151: Loss: 0.7571081775800533, accuracy: 0.5133333333333333\n",
      "iteration no 152: Loss: 0.7570925615998506, accuracy: 0.5133333333333333\n",
      "iteration no 153: Loss: 0.7570774426907909, accuracy: 0.5133333333333333\n",
      "iteration no 154: Loss: 0.7570628042837265, accuracy: 0.5133333333333333\n",
      "iteration no 155: Loss: 0.7570486303966675, accuracy: 0.5133333333333333\n",
      "iteration no 156: Loss: 0.7570349056123054, accuracy: 0.5133333333333333\n",
      "iteration no 157: Loss: 0.7570216150564799, accuracy: 0.5133333333333333\n",
      "iteration no 158: Loss: 0.7570087443775427, accuracy: 0.5133333333333333\n",
      "iteration no 159: Loss: 0.756996279726578, accuracy: 0.5133333333333333\n",
      "iteration no 160: Loss: 0.7569842077384409, accuracy: 0.5133333333333333\n",
      "iteration no 161: Loss: 0.756972515513576, accuracy: 0.5133333333333333\n",
      "iteration no 162: Loss: 0.756961190600581, accuracy: 0.5133333333333333\n",
      "iteration no 163: Loss: 0.7569502209794815, accuracy: 0.5133333333333333\n",
      "iteration no 164: Loss: 0.7569395950456864, accuracy: 0.5133333333333333\n",
      "iteration no 165: Loss: 0.7569293015945914, accuracy: 0.5133333333333333\n",
      "iteration no 166: Loss: 0.7569193298068037, accuracy: 0.5133333333333333\n",
      "iteration no 167: Loss: 0.7569096692339593, accuracy: 0.5133333333333333\n",
      "iteration no 168: Loss: 0.7569003097851079, accuracy: 0.5133333333333333\n",
      "iteration no 169: Loss: 0.7568912417136389, accuracy: 0.5133333333333333\n",
      "iteration no 170: Loss: 0.7568824556047268, accuracy: 0.5133333333333333\n",
      "iteration no 171: Loss: 0.7568739423632727, accuracy: 0.5133333333333333\n",
      "iteration no 172: Loss: 0.7568656932023187, accuracy: 0.5133333333333333\n",
      "iteration no 173: Loss: 0.756857699631919, accuracy: 0.5133333333333333\n",
      "iteration no 174: Loss: 0.7568499534484429, accuracy: 0.5133333333333333\n",
      "iteration no 175: Loss: 0.7568424467242968, accuracy: 0.5133333333333333\n",
      "iteration no 176: Loss: 0.756835171798042, accuracy: 0.5133333333333333\n",
      "iteration no 177: Loss: 0.7568281212648958, accuracy: 0.5133333333333333\n",
      "iteration no 178: Loss: 0.7568212879675981, accuracy: 0.5133333333333333\n",
      "iteration no 179: Loss: 0.756814664987627, accuracy: 0.5133333333333333\n",
      "iteration no 180: Loss: 0.7568082456367513, accuracy: 0.5133333333333333\n",
      "iteration no 181: Loss: 0.7568020234489039, accuracy: 0.5133333333333333\n",
      "iteration no 182: Loss: 0.7567959921723646, accuracy: 0.5133333333333333\n",
      "iteration no 183: Loss: 0.7567901457622365, accuracy: 0.5133333333333333\n",
      "iteration no 184: Loss: 0.7567844783732072, accuracy: 0.5133333333333333\n",
      "iteration no 185: Loss: 0.7567789843525834, accuracy: 0.5133333333333333\n",
      "iteration no 186: Loss: 0.7567736582335823, accuracy: 0.5133333333333333\n",
      "iteration no 187: Loss: 0.756768494728877, accuracy: 0.5133333333333333\n",
      "iteration no 188: Loss: 0.75676348872438, accuracy: 0.5133333333333333\n",
      "iteration no 189: Loss: 0.7567586352732573, accuracy: 0.5133333333333333\n",
      "iteration no 190: Loss: 0.7567539295901646, accuracy: 0.5133333333333333\n",
      "iteration no 191: Loss: 0.7567493670456953, accuracy: 0.5133333333333333\n",
      "iteration no 192: Loss: 0.7567449431610307, accuracy: 0.5133333333333333\n",
      "iteration no 193: Loss: 0.7567406536027895, accuracy: 0.5133333333333333\n",
      "iteration no 194: Loss: 0.7567364941780622, accuracy: 0.5133333333333333\n",
      "iteration no 195: Loss: 0.7567324608296273, accuracy: 0.5133333333333333\n",
      "iteration no 196: Loss: 0.7567285496313415, accuracy: 0.5133333333333333\n",
      "iteration no 197: Loss: 0.7567247567836954, accuracy: 0.5133333333333333\n",
      "iteration no 198: Loss: 0.7567210786095311, accuracy: 0.5133333333333333\n",
      "iteration no 199: Loss: 0.7567175115499121, accuracy: 0.5133333333333333\n",
      "iteration no 200: Loss: 0.756714052160142, accuracy: 0.5133333333333333\n",
      "iteration no 1: Loss: 1.1249218553093967, accuracy: 0.39\n",
      "iteration no 2: Loss: 1.0914061246931797, accuracy: 0.5166666666666667\n",
      "iteration no 3: Loss: 1.061597172477026, accuracy: 0.5066666666666667\n",
      "iteration no 4: Loss: 1.0350546962013958, accuracy: 0.5133333333333333\n",
      "iteration no 5: Loss: 1.011385357380183, accuracy: 0.5166666666666667\n",
      "iteration no 6: Loss: 0.9902403156781489, accuracy: 0.5033333333333333\n",
      "iteration no 7: Loss: 0.9713122737884295, accuracy: 0.5066666666666667\n",
      "iteration no 8: Loss: 0.9543319783506093, accuracy: 0.5133333333333333\n",
      "iteration no 9: Loss: 0.9390644135591542, accuracy: 0.51\n",
      "iteration no 10: Loss: 0.9253049491217892, accuracy: 0.51\n",
      "iteration no 11: Loss: 0.91287564087647, accuracy: 0.51\n",
      "iteration no 12: Loss: 0.9016218064079715, accuracy: 0.51\n",
      "iteration no 13: Loss: 0.891408935848063, accuracy: 0.51\n",
      "iteration no 14: Loss: 0.8821199548400616, accuracy: 0.51\n",
      "iteration no 15: Loss: 0.8736528298221756, accuracy: 0.51\n",
      "iteration no 16: Loss: 0.8659184909416451, accuracy: 0.51\n",
      "iteration no 17: Loss: 0.8588390410480038, accuracy: 0.51\n",
      "iteration no 18: Loss: 0.8523462172963124, accuracy: 0.51\n",
      "iteration no 19: Loss: 0.8463800728293671, accuracy: 0.51\n",
      "iteration no 20: Loss: 0.8408878484447684, accuracy: 0.51\n",
      "iteration no 21: Loss: 0.8358230072487212, accuracy: 0.51\n",
      "iteration no 22: Loss: 0.8311444085578662, accuracy: 0.51\n",
      "iteration no 23: Loss: 0.8268156004574839, accuracy: 0.51\n",
      "iteration no 24: Loss: 0.8228042133191908, accuracy: 0.51\n",
      "iteration no 25: Loss: 0.8190814391653607, accuracy: 0.51\n",
      "iteration no 26: Loss: 0.8156215840294104, accuracy: 0.51\n",
      "iteration no 27: Loss: 0.8124016824148084, accuracy: 0.51\n",
      "iteration no 28: Loss: 0.8094011646277821, accuracy: 0.5133333333333333\n",
      "iteration no 29: Loss: 0.8066015691807724, accuracy: 0.5133333333333333\n",
      "iteration no 30: Loss: 0.8039862936678362, accuracy: 0.5133333333333333\n",
      "iteration no 31: Loss: 0.8015403785299193, accuracy: 0.5133333333333333\n",
      "iteration no 32: Loss: 0.7992503189848106, accuracy: 0.5133333333333333\n",
      "iteration no 33: Loss: 0.7971039011180806, accuracy: 0.51\n",
      "iteration no 34: Loss: 0.7950900587385706, accuracy: 0.51\n",
      "iteration no 35: Loss: 0.7931987481131837, accuracy: 0.5133333333333333\n",
      "iteration no 36: Loss: 0.7914208381262624, accuracy: 0.5133333333333333\n",
      "iteration no 37: Loss: 0.7897480137717382, accuracy: 0.5133333333333333\n",
      "iteration no 38: Loss: 0.7881726911924377, accuracy: 0.5133333333333333\n",
      "iteration no 39: Loss: 0.7866879427396261, accuracy: 0.5133333333333333\n",
      "iteration no 40: Loss: 0.7852874307447162, accuracy: 0.5133333333333333\n",
      "iteration no 41: Loss: 0.7839653488804895, accuracy: 0.5133333333333333\n",
      "iteration no 42: Loss: 0.7827163701465271, accuracy: 0.5133333333333333\n",
      "iteration no 43: Loss: 0.7815356006472831, accuracy: 0.5133333333333333\n",
      "iteration no 44: Loss: 0.7804185384451161, accuracy: 0.5133333333333333\n",
      "iteration no 45: Loss: 0.7793610368677149, accuracy: 0.5133333333333333\n",
      "iteration no 46: Loss: 0.7783592717323419, accuracy: 0.5133333333333333\n",
      "iteration no 47: Loss: 0.7774097120203491, accuracy: 0.5133333333333333\n",
      "iteration no 48: Loss: 0.7765090935963359, accuracy: 0.5133333333333333\n",
      "iteration no 49: Loss: 0.7756543956186255, accuracy: 0.5133333333333333\n",
      "iteration no 50: Loss: 0.7748428193327815, accuracy: 0.5133333333333333\n",
      "iteration no 51: Loss: 0.774071768978681, accuracy: 0.5133333333333333\n",
      "iteration no 52: Loss: 0.7733388345751954, accuracy: 0.5133333333333333\n",
      "iteration no 53: Loss: 0.7726417763755241, accuracy: 0.5133333333333333\n",
      "iteration no 54: Loss: 0.7719785108113566, accuracy: 0.5133333333333333\n",
      "iteration no 55: Loss: 0.7713470977658585, accuracy: 0.5133333333333333\n",
      "iteration no 56: Loss: 0.7707457290344444, accuracy: 0.5133333333333333\n",
      "iteration no 57: Loss: 0.7701727178488252, accuracy: 0.5133333333333333\n",
      "iteration no 58: Loss: 0.7696264893542405, accuracy: 0.5133333333333333\n",
      "iteration no 59: Loss: 0.7691055719423726, accuracy: 0.5166666666666667\n",
      "iteration no 60: Loss: 0.7686085893534786, accuracy: 0.5166666666666667\n",
      "iteration no 61: Loss: 0.7681342534709358, accuracy: 0.5166666666666667\n",
      "iteration no 62: Loss: 0.7676813577398862, accuracy: 0.5166666666666667\n",
      "iteration no 63: Loss: 0.7672487711491291, accuracy: 0.5166666666666667\n",
      "iteration no 64: Loss: 0.7668354327219832, accuracy: 0.5166666666666667\n",
      "iteration no 65: Loss: 0.7664403464676368, accuracy: 0.5166666666666667\n",
      "iteration no 66: Loss: 0.7660625767496225, accuracy: 0.5166666666666667\n",
      "iteration no 67: Loss: 0.7657012440325885, accuracy: 0.5166666666666667\n",
      "iteration no 68: Loss: 0.7653555209725367, accuracy: 0.5166666666666667\n",
      "iteration no 69: Loss: 0.765024628819269, accuracy: 0.5166666666666667\n",
      "iteration no 70: Loss: 0.7647078341029337, accuracy: 0.5166666666666667\n",
      "iteration no 71: Loss: 0.7644044455793754, accuracy: 0.5166666666666667\n",
      "iteration no 72: Loss: 0.7641138114114953, accuracy: 0.5166666666666667\n",
      "iteration no 73: Loss: 0.763835316566058, accuracy: 0.5166666666666667\n",
      "iteration no 74: Loss: 0.7635683804073722, accuracy: 0.5166666666666667\n",
      "iteration no 75: Loss: 0.7633124544710508, accuracy: 0.5166666666666667\n",
      "iteration no 76: Loss: 0.7630670204026525, accuracy: 0.5166666666666667\n",
      "iteration no 77: Loss: 0.7628315880474282, accuracy: 0.5166666666666667\n",
      "iteration no 78: Loss: 0.7626056936786815, accuracy: 0.5166666666666667\n",
      "iteration no 79: Loss: 0.7623888983533922, accuracy: 0.5133333333333333\n",
      "iteration no 80: Loss: 0.7621807863847986, accuracy: 0.5133333333333333\n",
      "iteration no 81: Loss: 0.7619809639225504, accuracy: 0.5133333333333333\n",
      "iteration no 82: Loss: 0.7617890576318888, accuracy: 0.5133333333333333\n",
      "iteration no 83: Loss: 0.7616047134640648, accuracy: 0.5133333333333333\n",
      "iteration no 84: Loss: 0.7614275955108895, accuracy: 0.5133333333333333\n",
      "iteration no 85: Loss: 0.7612573849369209, accuracy: 0.5133333333333333\n",
      "iteration no 86: Loss: 0.7610937789833593, accuracy: 0.5133333333333333\n",
      "iteration no 87: Loss: 0.7609364900382161, accuracy: 0.5133333333333333\n",
      "iteration no 88: Loss: 0.760785244767791, accuracy: 0.5133333333333333\n",
      "iteration no 89: Loss: 0.7606397833048968, accuracy: 0.5133333333333333\n",
      "iteration no 90: Loss: 0.7604998584896543, accuracy: 0.5133333333333333\n",
      "iteration no 91: Loss: 0.7603652351590217, accuracy: 0.5133333333333333\n",
      "iteration no 92: Loss: 0.7602356894815321, accuracy: 0.5133333333333333\n",
      "iteration no 93: Loss: 0.7601110083339999, accuracy: 0.5133333333333333\n",
      "iteration no 94: Loss: 0.7599909887172123, accuracy: 0.5133333333333333\n",
      "iteration no 95: Loss: 0.7598754372078611, accuracy: 0.5133333333333333\n",
      "iteration no 96: Loss: 0.759764169444183, accuracy: 0.5133333333333333\n",
      "iteration no 97: Loss: 0.7596570096429749, accuracy: 0.5133333333333333\n",
      "iteration no 98: Loss: 0.7595537901458291, accuracy: 0.5133333333333333\n",
      "iteration no 99: Loss: 0.7594543509926017, accuracy: 0.5133333333333333\n",
      "iteration no 100: Loss: 0.7593585395202704, accuracy: 0.5133333333333333\n",
      "iteration no 101: Loss: 0.7592662099854878, accuracy: 0.5133333333333333\n",
      "iteration no 102: Loss: 0.7591772232092496, accuracy: 0.5133333333333333\n",
      "iteration no 103: Loss: 0.7590914462422277, accuracy: 0.5166666666666667\n",
      "iteration no 104: Loss: 0.7590087520494099, accuracy: 0.5166666666666667\n",
      "iteration no 105: Loss: 0.758929019212801, accuracy: 0.5166666666666667\n",
      "iteration no 106: Loss: 0.7588521316510187, accuracy: 0.5166666666666667\n",
      "iteration no 107: Loss: 0.7587779783547075, accuracy: 0.5166666666666667\n",
      "iteration no 108: Loss: 0.7587064531367694, accuracy: 0.5166666666666667\n",
      "iteration no 109: Loss: 0.7586374543964767, accuracy: 0.5166666666666667\n",
      "iteration no 110: Loss: 0.7585708848966054, accuracy: 0.5166666666666667\n",
      "iteration no 111: Loss: 0.7585066515527783, accuracy: 0.5166666666666667\n",
      "iteration no 112: Loss: 0.7584446652342702, accuracy: 0.5166666666666667\n",
      "iteration no 113: Loss: 0.7583848405755749, accuracy: 0.5166666666666667\n",
      "iteration no 114: Loss: 0.7583270957980824, accuracy: 0.5166666666666667\n",
      "iteration no 115: Loss: 0.7582713525412569, accuracy: 0.5166666666666667\n",
      "iteration no 116: Loss: 0.7582175357027507, accuracy: 0.5166666666666667\n",
      "iteration no 117: Loss: 0.7581655732869229, accuracy: 0.5166666666666667\n",
      "iteration no 118: Loss: 0.7581153962612672, accuracy: 0.5166666666666667\n",
      "iteration no 119: Loss: 0.7580669384202892, accuracy: 0.5166666666666667\n",
      "iteration no 120: Loss: 0.758020136256396, accuracy: 0.5166666666666667\n",
      "iteration no 121: Loss: 0.7579749288374014, accuracy: 0.5166666666666667\n",
      "iteration no 122: Loss: 0.7579312576902582, accuracy: 0.5166666666666667\n",
      "iteration no 123: Loss: 0.7578890666906722, accuracy: 0.5166666666666667\n",
      "iteration no 124: Loss: 0.7578483019582598, accuracy: 0.5166666666666667\n",
      "iteration no 125: Loss: 0.7578089117569398, accuracy: 0.5166666666666667\n",
      "iteration no 126: Loss: 0.7577708464002694, accuracy: 0.5166666666666667\n",
      "iteration no 127: Loss: 0.7577340581614466, accuracy: 0.5166666666666667\n",
      "iteration no 128: Loss: 0.7576985011877263, accuracy: 0.5166666666666667\n",
      "iteration no 129: Loss: 0.7576641314190051, accuracy: 0.5166666666666667\n",
      "iteration no 130: Loss: 0.7576309065103503, accuracy: 0.5166666666666667\n",
      "iteration no 131: Loss: 0.7575987857582605, accuracy: 0.5166666666666667\n",
      "iteration no 132: Loss: 0.7575677300304551, accuracy: 0.5166666666666667\n",
      "iteration no 133: Loss: 0.7575377016990072, accuracy: 0.5166666666666667\n",
      "iteration no 134: Loss: 0.7575086645766408, accuracy: 0.5133333333333333\n",
      "iteration no 135: Loss: 0.7574805838560259, accuracy: 0.5133333333333333\n",
      "iteration no 136: Loss: 0.7574534260519162, accuracy: 0.5133333333333333\n",
      "iteration no 137: Loss: 0.757427158945978, accuracy: 0.5133333333333333\n",
      "iteration no 138: Loss: 0.7574017515341744, accuracy: 0.5133333333333333\n",
      "iteration no 139: Loss: 0.7573771739765711, accuracy: 0.5133333333333333\n",
      "iteration no 140: Loss: 0.7573533975494412, accuracy: 0.5133333333333333\n",
      "iteration no 141: Loss: 0.7573303945995515, accuracy: 0.5133333333333333\n",
      "iteration no 142: Loss: 0.7573081385005199, accuracy: 0.5133333333333333\n",
      "iteration no 143: Loss: 0.75728660361114, accuracy: 0.5133333333333333\n",
      "iteration no 144: Loss: 0.7572657652355738, accuracy: 0.5133333333333333\n",
      "iteration no 145: Loss: 0.7572455995853226, accuracy: 0.5133333333333333\n",
      "iteration no 146: Loss: 0.7572260837428826, accuracy: 0.5133333333333333\n",
      "iteration no 147: Loss: 0.7572071956270086, accuracy: 0.5133333333333333\n",
      "iteration no 148: Loss: 0.7571889139595008, accuracy: 0.5133333333333333\n",
      "iteration no 149: Loss: 0.7571712182334495, accuracy: 0.5133333333333333\n",
      "iteration no 150: Loss: 0.7571540886828542, accuracy: 0.5133333333333333\n",
      "iteration no 151: Loss: 0.7571375062535655, accuracy: 0.5133333333333333\n",
      "iteration no 152: Loss: 0.7571214525754748, accuracy: 0.5133333333333333\n",
      "iteration no 153: Loss: 0.7571059099358991, accuracy: 0.5133333333333333\n",
      "iteration no 154: Loss: 0.7570908612541013, accuracy: 0.5133333333333333\n",
      "iteration no 155: Loss: 0.7570762900568937, accuracy: 0.5133333333333333\n",
      "iteration no 156: Loss: 0.7570621804552734, accuracy: 0.5133333333333333\n",
      "iteration no 157: Loss: 0.757048517122042, accuracy: 0.5133333333333333\n",
      "iteration no 158: Loss: 0.7570352852703628, accuracy: 0.5133333333333333\n",
      "iteration no 159: Loss: 0.7570224706332136, accuracy: 0.5133333333333333\n",
      "iteration no 160: Loss: 0.7570100594436928, accuracy: 0.5133333333333333\n",
      "iteration no 161: Loss: 0.7569980384161409, accuracy: 0.5133333333333333\n",
      "iteration no 162: Loss: 0.7569863947280383, accuracy: 0.5133333333333333\n",
      "iteration no 163: Loss: 0.7569751160026479, accuracy: 0.5133333333333333\n",
      "iteration no 164: Loss: 0.7569641902923636, accuracy: 0.5133333333333333\n",
      "iteration no 165: Loss: 0.7569536060627388, accuracy: 0.5133333333333333\n",
      "iteration no 166: Loss: 0.7569433521771594, accuracy: 0.5133333333333333\n",
      "iteration no 167: Loss: 0.7569334178821374, accuracy: 0.5133333333333333\n",
      "iteration no 168: Loss: 0.7569237927931938, accuracy: 0.5133333333333333\n",
      "iteration no 169: Loss: 0.7569144668813054, accuracy: 0.5133333333333333\n",
      "iteration no 170: Loss: 0.7569054304598946, accuracy: 0.5133333333333333\n",
      "iteration no 171: Loss: 0.7568966741723328, accuracy: 0.5133333333333333\n",
      "iteration no 172: Loss: 0.7568881889799391, accuracy: 0.5133333333333333\n",
      "iteration no 173: Loss: 0.7568799661504526, accuracy: 0.5133333333333333\n",
      "iteration no 174: Loss: 0.7568719972469543, accuracy: 0.5133333333333333\n",
      "iteration no 175: Loss: 0.7568642741172248, accuracy: 0.5133333333333333\n",
      "iteration no 176: Loss: 0.7568567888835155, accuracy: 0.5133333333333333\n",
      "iteration no 177: Loss: 0.7568495339327151, accuracy: 0.5133333333333333\n",
      "iteration no 178: Loss: 0.7568425019068996, accuracy: 0.5133333333333333\n",
      "iteration no 179: Loss: 0.756835685694242, accuracy: 0.5133333333333333\n",
      "iteration no 180: Loss: 0.7568290784202746, accuracy: 0.5133333333333333\n",
      "iteration no 181: Loss: 0.756822673439483, accuracy: 0.5133333333333333\n",
      "iteration no 182: Loss: 0.7568164643272218, accuracy: 0.5133333333333333\n",
      "iteration no 183: Loss: 0.7568104448719365, accuracy: 0.5133333333333333\n",
      "iteration no 184: Loss: 0.75680460906768, accuracy: 0.5133333333333333\n",
      "iteration no 185: Loss: 0.756798951106911, accuracy: 0.5133333333333333\n",
      "iteration no 186: Loss: 0.7567934653735638, accuracy: 0.5133333333333333\n",
      "iteration no 187: Loss: 0.7567881464363774, accuracy: 0.5133333333333333\n",
      "iteration no 188: Loss: 0.756782989042473, accuracy: 0.5133333333333333\n",
      "iteration no 189: Loss: 0.756777988111171, accuracy: 0.5133333333333333\n",
      "iteration no 190: Loss: 0.7567731387280388, accuracy: 0.5133333333333333\n",
      "iteration no 191: Loss: 0.7567684361391552, accuracy: 0.5133333333333333\n",
      "iteration no 192: Loss: 0.7567638757455901, accuracy: 0.5133333333333333\n",
      "iteration no 193: Loss: 0.7567594530980846, accuracy: 0.5133333333333333\n",
      "iteration no 194: Loss: 0.7567551638919259, accuracy: 0.5133333333333333\n",
      "iteration no 195: Loss: 0.7567510039620121, accuracy: 0.5133333333333333\n",
      "iteration no 196: Loss: 0.7567469692780928, accuracy: 0.5133333333333333\n",
      "iteration no 197: Loss: 0.7567430559401852, accuracy: 0.5133333333333333\n",
      "iteration no 198: Loss: 0.7567392601741552, accuracy: 0.5133333333333333\n",
      "iteration no 199: Loss: 0.7567355783274581, accuracy: 0.5133333333333333\n",
      "iteration no 200: Loss: 0.7567320068650326, accuracy: 0.5133333333333333\n",
      "iteration no 1: Loss: 1.0917645043938027, accuracy: 0.52\n",
      "iteration no 2: Loss: 1.0618869280352476, accuracy: 0.5133333333333333\n",
      "iteration no 3: Loss: 1.0352879290006305, accuracy: 0.51\n",
      "iteration no 4: Loss: 1.0115722328639956, accuracy: 0.5166666666666667\n",
      "iteration no 5: Loss: 0.9903893259653584, accuracy: 0.51\n",
      "iteration no 6: Loss: 0.9714304516563158, accuracy: 0.51\n",
      "iteration no 7: Loss: 0.9544250984472983, accuracy: 0.5133333333333333\n",
      "iteration no 8: Loss: 0.939137184055298, accuracy: 0.5133333333333333\n",
      "iteration no 9: Loss: 0.9253611886552126, accuracy: 0.5133333333333333\n",
      "iteration no 10: Loss: 0.9129184360247422, accuracy: 0.5133333333333333\n",
      "iteration no 11: Loss: 0.9016536474772185, accuracy: 0.5133333333333333\n",
      "iteration no 12: Loss: 0.8914318311077407, accuracy: 0.5133333333333333\n",
      "iteration no 13: Loss: 0.8821355248588402, accuracy: 0.5133333333333333\n",
      "iteration no 14: Loss: 0.8736623843210318, accuracy: 0.5133333333333333\n",
      "iteration no 15: Loss: 0.8659230908013306, accuracy: 0.5133333333333333\n",
      "iteration no 16: Loss: 0.8588395480065386, accuracy: 0.5133333333333333\n",
      "iteration no 17: Loss: 0.8523433336081215, accuracy: 0.5133333333333333\n",
      "iteration no 18: Loss: 0.8463743728335372, accuracy: 0.5133333333333333\n",
      "iteration no 19: Loss: 0.8408798036643245, accuracy: 0.5133333333333333\n",
      "iteration no 20: Loss: 0.8358130063453405, accuracy: 0.5133333333333333\n",
      "iteration no 21: Loss: 0.8311327732090066, accuracy: 0.5133333333333333\n",
      "iteration no 22: Loss: 0.8268025980075473, accuracy: 0.5133333333333333\n",
      "iteration no 23: Loss: 0.8227900668802237, accuracy: 0.5133333333333333\n",
      "iteration no 24: Loss: 0.8190663357009309, accuracy: 0.5133333333333333\n",
      "iteration no 25: Loss: 0.8156056808422005, accuracy: 0.5133333333333333\n",
      "iteration no 26: Loss: 0.8123851123689577, accuracy: 0.5133333333333333\n",
      "iteration no 27: Loss: 0.80938404036648, accuracy: 0.5166666666666667\n",
      "iteration no 28: Loss: 0.8065839865441997, accuracy: 0.5166666666666667\n",
      "iteration no 29: Loss: 0.8039683344730995, accuracy: 0.5166666666666667\n",
      "iteration no 30: Loss: 0.801522112840561, accuracy: 0.5166666666666667\n",
      "iteration no 31: Loss: 0.7992318069707782, accuracy: 0.51\n",
      "iteration no 32: Loss: 0.7970851945860894, accuracy: 0.51\n",
      "iteration no 33: Loss: 0.7950712023963443, accuracy: 0.51\n",
      "iteration no 34: Loss: 0.7931797806181189, accuracy: 0.5133333333333333\n",
      "iteration no 35: Loss: 0.7914017929588637, accuracy: 0.5133333333333333\n",
      "iteration no 36: Loss: 0.7897289199661218, accuracy: 0.5133333333333333\n",
      "iteration no 37: Loss: 0.7881535739498287, accuracy: 0.5133333333333333\n",
      "iteration no 38: Loss: 0.7866688239457251, accuracy: 0.5133333333333333\n",
      "iteration no 39: Loss: 0.785268329407791, accuracy: 0.5133333333333333\n",
      "iteration no 40: Loss: 0.7839462815038492, accuracy: 0.5133333333333333\n",
      "iteration no 41: Loss: 0.7826973510464705, accuracy: 0.5133333333333333\n",
      "iteration no 42: Loss: 0.7815166422255735, accuracy: 0.5133333333333333\n",
      "iteration no 43: Loss: 0.7803996514233761, accuracy: 0.5133333333333333\n",
      "iteration no 44: Loss: 0.7793422304898155, accuracy: 0.5133333333333333\n",
      "iteration no 45: Loss: 0.7783405539397851, accuracy: 0.5133333333333333\n",
      "iteration no 46: Loss: 0.7773910896047738, accuracy: 0.5133333333333333\n",
      "iteration no 47: Loss: 0.7764905723325697, accuracy: 0.5133333333333333\n",
      "iteration no 48: Loss: 0.7756359803811363, accuracy: 0.5133333333333333\n",
      "iteration no 49: Loss: 0.7748245141979, accuracy: 0.5133333333333333\n",
      "iteration no 50: Loss: 0.7740535773145935, accuracy: 0.5133333333333333\n",
      "iteration no 51: Loss: 0.7733207591213774, accuracy: 0.5133333333333333\n",
      "iteration no 52: Loss: 0.772623819313032, accuracy: 0.5133333333333333\n",
      "iteration no 53: Loss: 0.7719606738251715, accuracy: 0.5133333333333333\n",
      "iteration no 54: Loss: 0.7713293821003019, accuracy: 0.5133333333333333\n",
      "iteration no 55: Loss: 0.7707281355425318, accuracy: 0.5133333333333333\n",
      "iteration no 56: Loss: 0.7701552470363029, accuracy: 0.5133333333333333\n",
      "iteration no 57: Loss: 0.7696091414189393, accuracy: 0.5133333333333333\n",
      "iteration no 58: Loss: 0.7690883468094319, accuracy: 0.5166666666666667\n",
      "iteration no 59: Loss: 0.7685914867069142, accuracy: 0.5166666666666667\n",
      "iteration no 60: Loss: 0.7681172727819643, accuracy: 0.5166666666666667\n",
      "iteration no 61: Loss: 0.767664498292365, accuracy: 0.5166666666666667\n",
      "iteration no 62: Loss: 0.7672320320624255, accuracy: 0.5166666666666667\n",
      "iteration no 63: Loss: 0.7668188129715471, accuracy: 0.5166666666666667\n",
      "iteration no 64: Loss: 0.7664238449035167, accuracy: 0.5166666666666667\n",
      "iteration no 65: Loss: 0.7660461921131414, accuracy: 0.5166666666666667\n",
      "iteration no 66: Loss: 0.7656849749713616, accuracy: 0.5166666666666667\n",
      "iteration no 67: Loss: 0.7653393660540027, accuracy: 0.5166666666666667\n",
      "iteration no 68: Loss: 0.7650085865428773, accuracy: 0.5166666666666667\n",
      "iteration no 69: Loss: 0.7646919029111238, accuracy: 0.5166666666666667\n",
      "iteration no 70: Loss: 0.7643886238674624, accuracy: 0.5166666666666667\n",
      "iteration no 71: Loss: 0.7640980975365678, accuracy: 0.5166666666666667\n",
      "iteration no 72: Loss: 0.7638197088549812, accuracy: 0.5166666666666667\n",
      "iteration no 73: Loss: 0.763552877163981, accuracy: 0.5166666666666667\n",
      "iteration no 74: Loss: 0.7632970539826098, accuracy: 0.5166666666666667\n",
      "iteration no 75: Loss: 0.7630517209456488, accuracy: 0.5166666666666667\n",
      "iteration no 76: Loss: 0.7628163878927604, accuracy: 0.5166666666666667\n",
      "iteration no 77: Loss: 0.7625905910962985, accuracy: 0.5166666666666667\n",
      "iteration no 78: Loss: 0.7623738916164358, accuracy: 0.5133333333333333\n",
      "iteration no 79: Loss: 0.7621658737732915, accuracy: 0.5133333333333333\n",
      "iteration no 80: Loss: 0.761966143726672, accuracy: 0.5133333333333333\n",
      "iteration no 81: Loss: 0.7617743281548787, accuracy: 0.5133333333333333\n",
      "iteration no 82: Loss: 0.7615900730247843, accuracy: 0.5133333333333333\n",
      "iteration no 83: Loss: 0.7614130424460749, accuracy: 0.5133333333333333\n",
      "iteration no 84: Loss: 0.7612429176031559, accuracy: 0.5133333333333333\n",
      "iteration no 85: Loss: 0.7610793957587914, accuracy: 0.5133333333333333\n",
      "iteration no 86: Loss: 0.7609221893240427, accuracy: 0.5133333333333333\n",
      "iteration no 87: Loss: 0.7607710249895333, accuracy: 0.5133333333333333\n",
      "iteration no 88: Loss: 0.7606256429134843, accuracy: 0.5133333333333333\n",
      "iteration no 89: Loss: 0.7604857959623348, accuracy: 0.5133333333333333\n",
      "iteration no 90: Loss: 0.7603512490001154, accuracy: 0.5133333333333333\n",
      "iteration no 91: Loss: 0.7602217782230435, accuracy: 0.5133333333333333\n",
      "iteration no 92: Loss: 0.7600971705361006, accuracy: 0.5133333333333333\n",
      "iteration no 93: Loss: 0.7599772229686087, accuracy: 0.5133333333333333\n",
      "iteration no 94: Loss: 0.7598617421260563, accuracy: 0.5133333333333333\n",
      "iteration no 95: Loss: 0.7597505436756452, accuracy: 0.5133333333333333\n",
      "iteration no 96: Loss: 0.7596434518632208, accuracy: 0.5133333333333333\n",
      "iteration no 97: Loss: 0.7595402990594305, accuracy: 0.5133333333333333\n",
      "iteration no 98: Loss: 0.7594409253331244, accuracy: 0.5133333333333333\n",
      "iteration no 99: Loss: 0.7593451780501518, accuracy: 0.5133333333333333\n",
      "iteration no 100: Loss: 0.7592529114958598, accuracy: 0.5133333333333333\n",
      "iteration no 101: Loss: 0.7591639865197143, accuracy: 0.5133333333333333\n",
      "iteration no 102: Loss: 0.7590782702005883, accuracy: 0.5133333333333333\n",
      "iteration no 103: Loss: 0.7589956355313657, accuracy: 0.5166666666666667\n",
      "iteration no 104: Loss: 0.758915961121608, accuracy: 0.5166666666666667\n",
      "iteration no 105: Loss: 0.7588391309171214, accuracy: 0.5166666666666667\n",
      "iteration no 106: Loss: 0.7587650339353463, accuracy: 0.5166666666666667\n",
      "iteration no 107: Loss: 0.758693564015565, accuracy: 0.5166666666666667\n",
      "iteration no 108: Loss: 0.7586246195829971, accuracy: 0.5166666666666667\n",
      "iteration no 109: Loss: 0.758558103425917, accuracy: 0.5166666666666667\n",
      "iteration no 110: Loss: 0.7584939224849835, accuracy: 0.5166666666666667\n",
      "iteration no 111: Loss: 0.7584319876540355, accuracy: 0.5166666666666667\n",
      "iteration no 112: Loss: 0.7583722135916496, accuracy: 0.5166666666666667\n",
      "iteration no 113: Loss: 0.7583145185428111, accuracy: 0.5166666666666667\n",
      "iteration no 114: Loss: 0.7582588241700874, accuracy: 0.5166666666666667\n",
      "iteration no 115: Loss: 0.7582050553937393, accuracy: 0.5166666666666667\n",
      "iteration no 116: Loss: 0.758153140240237, accuracy: 0.5166666666666667\n",
      "iteration no 117: Loss: 0.7581030096986887, accuracy: 0.5166666666666667\n",
      "iteration no 118: Loss: 0.7580545975847174, accuracy: 0.5166666666666667\n",
      "iteration no 119: Loss: 0.7580078404113549, accuracy: 0.5166666666666667\n",
      "iteration no 120: Loss: 0.7579626772665466, accuracy: 0.5166666666666667\n",
      "iteration no 121: Loss: 0.75791904969689, accuracy: 0.5166666666666667\n",
      "iteration no 122: Loss: 0.7578769015972516, accuracy: 0.5166666666666667\n",
      "iteration no 123: Loss: 0.7578361791059305, accuracy: 0.5166666666666667\n",
      "iteration no 124: Loss: 0.7577968305050559, accuracy: 0.5166666666666667\n",
      "iteration no 125: Loss: 0.757758806125928, accuracy: 0.5166666666666667\n",
      "iteration no 126: Loss: 0.7577220582590288, accuracy: 0.5166666666666667\n",
      "iteration no 127: Loss: 0.7576865410684441, accuracy: 0.5166666666666667\n",
      "iteration no 128: Loss: 0.757652210510455, accuracy: 0.5166666666666667\n",
      "iteration no 129: Loss: 0.7576190242560766, accuracy: 0.5166666666666667\n",
      "iteration no 130: Loss: 0.7575869416173245, accuracy: 0.5166666666666667\n",
      "iteration no 131: Loss: 0.7575559234770132, accuracy: 0.5166666666666667\n",
      "iteration no 132: Loss: 0.7575259322218965, accuracy: 0.5166666666666667\n",
      "iteration no 133: Loss: 0.7574969316789739, accuracy: 0.5133333333333333\n",
      "iteration no 134: Loss: 0.7574688870547936, accuracy: 0.5133333333333333\n",
      "iteration no 135: Loss: 0.7574417648775987, accuracy: 0.5133333333333333\n",
      "iteration no 136: Loss: 0.7574155329421642, accuracy: 0.5133333333333333\n",
      "iteration no 137: Loss: 0.7573901602571904, accuracy: 0.5133333333333333\n",
      "iteration no 138: Loss: 0.7573656169951173, accuracy: 0.5133333333333333\n",
      "iteration no 139: Loss: 0.7573418744442365, accuracy: 0.5133333333333333\n",
      "iteration no 140: Loss: 0.7573189049629878, accuracy: 0.5133333333333333\n",
      "iteration no 141: Loss: 0.7572966819363227, accuracy: 0.5133333333333333\n",
      "iteration no 142: Loss: 0.7572751797340401, accuracy: 0.5133333333333333\n",
      "iteration no 143: Loss: 0.7572543736709852, accuracy: 0.5133333333333333\n",
      "iteration no 144: Loss: 0.7572342399690285, accuracy: 0.5133333333333333\n",
      "iteration no 145: Loss: 0.7572147557207305, accuracy: 0.5133333333333333\n",
      "iteration no 146: Loss: 0.7571958988546119, accuracy: 0.5133333333333333\n",
      "iteration no 147: Loss: 0.7571776481019513, accuracy: 0.5133333333333333\n",
      "iteration no 148: Loss: 0.7571599829650323, accuracy: 0.5133333333333333\n",
      "iteration no 149: Loss: 0.7571428836867755, accuracy: 0.5133333333333333\n",
      "iteration no 150: Loss: 0.7571263312216844, accuracy: 0.5133333333333333\n",
      "iteration no 151: Loss: 0.7571103072080435, accuracy: 0.5133333333333333\n",
      "iteration no 152: Loss: 0.7570947939413103, accuracy: 0.5133333333333333\n",
      "iteration no 153: Loss: 0.757079774348643, accuracy: 0.5133333333333333\n",
      "iteration no 154: Loss: 0.7570652319645095, accuracy: 0.5133333333333333\n",
      "iteration no 155: Loss: 0.757051150907331, accuracy: 0.5133333333333333\n",
      "iteration no 156: Loss: 0.757037515857107, accuracy: 0.5133333333333333\n",
      "iteration no 157: Loss: 0.75702431203398, accuracy: 0.5133333333333333\n",
      "iteration no 158: Loss: 0.7570115251776939, accuracy: 0.5133333333333333\n",
      "iteration no 159: Loss: 0.7569991415279067, accuracy: 0.5133333333333333\n",
      "iteration no 160: Loss: 0.7569871478053173, accuracy: 0.5133333333333333\n",
      "iteration no 161: Loss: 0.7569755311935702, accuracy: 0.5133333333333333\n",
      "iteration no 162: Loss: 0.7569642793219024, accuracy: 0.5133333333333333\n",
      "iteration no 163: Loss: 0.7569533802484987, accuracy: 0.5133333333333333\n",
      "iteration no 164: Loss: 0.7569428224445249, accuracy: 0.5133333333333333\n",
      "iteration no 165: Loss: 0.7569325947788069, accuracy: 0.5133333333333333\n",
      "iteration no 166: Loss: 0.7569226865031287, accuracy: 0.5133333333333333\n",
      "iteration no 167: Loss: 0.7569130872381193, accuracy: 0.5133333333333333\n",
      "iteration no 168: Loss: 0.7569037869597077, accuracy: 0.5133333333333333\n",
      "iteration no 169: Loss: 0.7568947759861138, accuracy: 0.5133333333333333\n",
      "iteration no 170: Loss: 0.756886044965358, accuracy: 0.5133333333333333\n",
      "iteration no 171: Loss: 0.7568775848632656, accuracy: 0.5133333333333333\n",
      "iteration no 172: Loss: 0.7568693869519406, accuracy: 0.5133333333333333\n",
      "iteration no 173: Loss: 0.7568614427986946, accuracy: 0.5133333333333333\n",
      "iteration no 174: Loss: 0.7568537442554075, accuracy: 0.5133333333333333\n",
      "iteration no 175: Loss: 0.7568462834483018, accuracy: 0.5133333333333333\n",
      "iteration no 176: Loss: 0.7568390527681158, accuracy: 0.5133333333333333\n",
      "iteration no 177: Loss: 0.7568320448606533, accuracy: 0.5133333333333333\n",
      "iteration no 178: Loss: 0.7568252526177007, accuracy: 0.5133333333333333\n",
      "iteration no 179: Loss: 0.7568186691682913, accuracy: 0.5133333333333333\n",
      "iteration no 180: Loss: 0.7568122878703027, accuracy: 0.5133333333333333\n",
      "iteration no 181: Loss: 0.756806102302376, accuracy: 0.5133333333333333\n",
      "iteration no 182: Loss: 0.7568001062561412, accuracy: 0.5133333333333333\n",
      "iteration no 183: Loss: 0.7567942937287364, accuracy: 0.5133333333333333\n",
      "iteration no 184: Loss: 0.7567886589156105, accuracy: 0.5133333333333333\n",
      "iteration no 185: Loss: 0.7567831962035942, accuracy: 0.5133333333333333\n",
      "iteration no 186: Loss: 0.7567779001642337, accuracy: 0.5133333333333333\n",
      "iteration no 187: Loss: 0.7567727655473694, accuracy: 0.5133333333333333\n",
      "iteration no 188: Loss: 0.7567677872749577, accuracy: 0.5133333333333333\n",
      "iteration no 189: Loss: 0.756762960435119, accuracy: 0.5133333333333333\n",
      "iteration no 190: Loss: 0.7567582802764072, accuracy: 0.5133333333333333\n",
      "iteration no 191: Loss: 0.7567537422022893, accuracy: 0.5133333333333333\n",
      "iteration no 192: Loss: 0.75674934176583, accuracy: 0.5133333333333333\n",
      "iteration no 193: Loss: 0.7567450746645686, accuracy: 0.5133333333333333\n",
      "iteration no 194: Loss: 0.7567409367355848, accuracy: 0.5133333333333333\n",
      "iteration no 195: Loss: 0.7567369239507428, accuracy: 0.5133333333333333\n",
      "iteration no 196: Loss: 0.7567330324121092, accuracy: 0.5133333333333333\n",
      "iteration no 197: Loss: 0.7567292583475359, accuracy: 0.5133333333333333\n",
      "iteration no 198: Loss: 0.7567255981064033, accuracy: 0.5133333333333333\n",
      "iteration no 199: Loss: 0.7567220481555154, accuracy: 0.5133333333333333\n",
      "iteration no 200: Loss: 0.7567186050751437, accuracy: 0.5166666666666667\n",
      "iteration no 1: Loss: 1.086842336129489, accuracy: 0.44666666666666666\n",
      "iteration no 2: Loss: 1.0574324263219033, accuracy: 0.48333333333333334\n",
      "iteration no 3: Loss: 1.0312676969273566, accuracy: 0.49333333333333335\n",
      "iteration no 4: Loss: 1.0079516119496552, accuracy: 0.49666666666666665\n",
      "iteration no 5: Loss: 0.9871337824761454, accuracy: 0.5033333333333333\n",
      "iteration no 6: Loss: 0.9685064870457794, accuracy: 0.5066666666666667\n",
      "iteration no 7: Loss: 0.9518008474714866, accuracy: 0.5133333333333333\n",
      "iteration no 8: Loss: 0.9367827893293663, accuracy: 0.52\n",
      "iteration no 9: Loss: 0.9232490082727488, accuracy: 0.5233333333333333\n",
      "iteration no 10: Loss: 0.9110231260134819, accuracy: 0.5233333333333333\n",
      "iteration no 11: Loss: 0.8999521524409814, accuracy: 0.5233333333333333\n",
      "iteration no 12: Loss: 0.8899033101880285, accuracy: 0.5233333333333333\n",
      "iteration no 13: Loss: 0.8807612347845081, accuracy: 0.52\n",
      "iteration no 14: Loss: 0.8724255365253836, accuracy: 0.5133333333333333\n",
      "iteration no 15: Loss: 0.8648086954089619, accuracy: 0.5133333333333333\n",
      "iteration no 16: Loss: 0.8578342539868481, accuracy: 0.5133333333333333\n",
      "iteration no 17: Loss: 0.851435271563202, accuracy: 0.5166666666666667\n",
      "iteration no 18: Loss: 0.8455530046851267, accuracy: 0.5133333333333333\n",
      "iteration no 19: Loss: 0.8401357818445785, accuracy: 0.5166666666666667\n",
      "iteration no 20: Loss: 0.8351380438842705, accuracy: 0.5166666666666667\n",
      "iteration no 21: Loss: 0.8305195252548422, accuracy: 0.5166666666666667\n",
      "iteration no 22: Loss: 0.8262445547326273, accuracy: 0.5166666666666667\n",
      "iteration no 23: Loss: 0.8222814573453292, accuracy: 0.5166666666666667\n",
      "iteration no 24: Loss: 0.8186020420199247, accuracy: 0.5166666666666667\n",
      "iteration no 25: Loss: 0.8151811618634689, accuracy: 0.5166666666666667\n",
      "iteration no 26: Loss: 0.8119963360379222, accuracy: 0.5166666666666667\n",
      "iteration no 27: Loss: 0.809027423930267, accuracy: 0.5166666666666667\n",
      "iteration no 28: Loss: 0.8062563437879283, accuracy: 0.5166666666666667\n",
      "iteration no 29: Loss: 0.8036668292246871, accuracy: 0.5133333333333333\n",
      "iteration no 30: Loss: 0.8012442180387555, accuracy: 0.5166666666666667\n",
      "iteration no 31: Loss: 0.7989752686533511, accuracy: 0.5133333333333333\n",
      "iteration no 32: Loss: 0.7968480002178355, accuracy: 0.5133333333333333\n",
      "iteration no 33: Loss: 0.7948515530172016, accuracy: 0.5133333333333333\n",
      "iteration no 34: Loss: 0.7929760663488598, accuracy: 0.5133333333333333\n",
      "iteration no 35: Loss: 0.7912125714545862, accuracy: 0.5133333333333333\n",
      "iteration no 36: Loss: 0.7895528974558377, accuracy: 0.5166666666666667\n",
      "iteration no 37: Loss: 0.7879895885437819, accuracy: 0.5166666666666667\n",
      "iteration no 38: Loss: 0.7865158309308142, accuracy: 0.5166666666666667\n",
      "iteration no 39: Loss: 0.785125388285897, accuracy: 0.5166666666666667\n",
      "iteration no 40: Loss: 0.7838125445583158, accuracy: 0.5166666666666667\n",
      "iteration no 41: Loss: 0.7825720532488137, accuracy: 0.5166666666666667\n",
      "iteration no 42: Loss: 0.7813990923180504, accuracy: 0.5166666666666667\n",
      "iteration no 43: Loss: 0.7802892240337118, accuracy: 0.5166666666666667\n",
      "iteration no 44: Loss: 0.7792383591524483, accuracy: 0.5166666666666667\n",
      "iteration no 45: Loss: 0.7782427249137905, accuracy: 0.5166666666666667\n",
      "iteration no 46: Loss: 0.7772988363924163, accuracy: 0.5166666666666667\n",
      "iteration no 47: Loss: 0.776403470814462, accuracy: 0.5166666666666667\n",
      "iteration no 48: Loss: 0.775553644494484, accuracy: 0.5166666666666667\n",
      "iteration no 49: Loss: 0.7747465920934574, accuracy: 0.5133333333333333\n",
      "iteration no 50: Loss: 0.773979747935939, accuracy: 0.5133333333333333\n",
      "iteration no 51: Loss: 0.773250729157082, accuracy: 0.5133333333333333\n",
      "iteration no 52: Loss: 0.7725573204783569, accuracy: 0.5133333333333333\n",
      "iteration no 53: Loss: 0.771897460435243, accuracy: 0.5133333333333333\n",
      "iteration no 54: Loss: 0.7712692289013278, accuracy: 0.5133333333333333\n",
      "iteration no 55: Loss: 0.7706708357716717, accuracy: 0.5133333333333333\n",
      "iteration no 56: Loss: 0.7701006106843338, accuracy: 0.5133333333333333\n",
      "iteration no 57: Loss: 0.7695569936729498, accuracy: 0.5133333333333333\n",
      "iteration no 58: Loss: 0.7690385266554807, accuracy: 0.5133333333333333\n",
      "iteration no 59: Loss: 0.7685438456749554, accuracy: 0.5133333333333333\n",
      "iteration no 60: Loss: 0.7680716738174196, accuracy: 0.5133333333333333\n",
      "iteration no 61: Loss: 0.7676208147405402, accuracy: 0.5133333333333333\n",
      "iteration no 62: Loss: 0.767190146753567, accuracy: 0.5133333333333333\n",
      "iteration no 63: Loss: 0.7667786173957384, accuracy: 0.5166666666666667\n",
      "iteration no 64: Loss: 0.766385238465846, accuracy: 0.5166666666666667\n",
      "iteration no 65: Loss: 0.7660090814606596, accuracy: 0.5166666666666667\n",
      "iteration no 66: Loss: 0.7656492733843017, accuracy: 0.5166666666666667\n",
      "iteration no 67: Loss: 0.7653049928945781, accuracy: 0.5166666666666667\n",
      "iteration no 68: Loss: 0.7649754667557177, accuracy: 0.5166666666666667\n",
      "iteration no 69: Loss: 0.7646599665700589, accuracy: 0.5166666666666667\n",
      "iteration no 70: Loss: 0.7643578057639511, accuracy: 0.5166666666666667\n",
      "iteration no 71: Loss: 0.7640683368055751, accuracy: 0.5166666666666667\n",
      "iteration no 72: Loss: 0.7637909486345632, accuracy: 0.5166666666666667\n",
      "iteration no 73: Loss: 0.7635250642852356, accuracy: 0.5166666666666667\n",
      "iteration no 74: Loss: 0.7632701386870111, accuracy: 0.5166666666666667\n",
      "iteration no 75: Loss: 0.7630256566270951, accuracy: 0.5166666666666667\n",
      "iteration no 76: Loss: 0.762791130861948, accuracy: 0.5166666666666667\n",
      "iteration no 77: Loss: 0.7625661003652815, accuracy: 0.5166666666666667\n",
      "iteration no 78: Loss: 0.7623501287014529, accuracy: 0.5166666666666667\n",
      "iteration no 79: Loss: 0.7621428025141408, accuracy: 0.5133333333333333\n",
      "iteration no 80: Loss: 0.761943730121087, accuracy: 0.5133333333333333\n",
      "iteration no 81: Loss: 0.7617525402065156, accuracy: 0.5133333333333333\n",
      "iteration no 82: Loss: 0.7615688806035745, accuracy: 0.5133333333333333\n",
      "iteration no 83: Loss: 0.761392417159813, accuracy: 0.5133333333333333\n",
      "iteration no 84: Loss: 0.7612228326793147, accuracy: 0.5133333333333333\n",
      "iteration no 85: Loss: 0.7610598259356465, accuracy: 0.5133333333333333\n",
      "iteration no 86: Loss: 0.7609031107502837, accuracy: 0.5133333333333333\n",
      "iteration no 87: Loss: 0.7607524151316173, accuracy: 0.5133333333333333\n",
      "iteration no 88: Loss: 0.7606074804700558, accuracy: 0.5133333333333333\n",
      "iteration no 89: Loss: 0.7604680607851062, accuracy: 0.5133333333333333\n",
      "iteration no 90: Loss: 0.760333922020654, accuracy: 0.5133333333333333\n",
      "iteration no 91: Loss: 0.7602048413849665, accuracy: 0.5133333333333333\n",
      "iteration no 92: Loss: 0.7600806067322248, accuracy: 0.5133333333333333\n",
      "iteration no 93: Loss: 0.759961015982643, accuracy: 0.5133333333333333\n",
      "iteration no 94: Loss: 0.7598458765784648, accuracy: 0.5133333333333333\n",
      "iteration no 95: Loss: 0.7597350049733382, accuracy: 0.5133333333333333\n",
      "iteration no 96: Loss: 0.7596282261527685, accuracy: 0.5133333333333333\n",
      "iteration no 97: Loss: 0.7595253731835166, accuracy: 0.5133333333333333\n",
      "iteration no 98: Loss: 0.7594262867899806, accuracy: 0.5133333333333333\n",
      "iteration no 99: Loss: 0.7593308149557436, accuracy: 0.5133333333333333\n",
      "iteration no 100: Loss: 0.7592388125486083, accuracy: 0.5133333333333333\n",
      "iteration no 101: Loss: 0.7591501409675577, accuracy: 0.5133333333333333\n",
      "iteration no 102: Loss: 0.7590646678102075, accuracy: 0.5133333333333333\n",
      "iteration no 103: Loss: 0.7589822665594089, accuracy: 0.5133333333333333\n",
      "iteration no 104: Loss: 0.7589028162877665, accuracy: 0.5166666666666667\n",
      "iteration no 105: Loss: 0.7588262013789189, accuracy: 0.5166666666666667\n",
      "iteration no 106: Loss: 0.7587523112645168, accuracy: 0.5166666666666667\n",
      "iteration no 107: Loss: 0.7586810401759035, accuracy: 0.5166666666666667\n",
      "iteration no 108: Loss: 0.7586122869095793, accuracy: 0.5166666666666667\n",
      "iteration no 109: Loss: 0.7585459546055874, accuracy: 0.5166666666666667\n",
      "iteration no 110: Loss: 0.7584819505380265, accuracy: 0.5166666666666667\n",
      "iteration no 111: Loss: 0.7584201859169443, accuracy: 0.5166666666666667\n",
      "iteration no 112: Loss: 0.7583605757009181, accuracy: 0.5166666666666667\n",
      "iteration no 113: Loss: 0.7583030384196802, accuracy: 0.5166666666666667\n",
      "iteration no 114: Loss: 0.7582474960061779, accuracy: 0.5166666666666667\n",
      "iteration no 115: Loss: 0.7581938736375164, accuracy: 0.5166666666666667\n",
      "iteration no 116: Loss: 0.7581420995842467, accuracy: 0.5166666666666667\n",
      "iteration no 117: Loss: 0.7580921050675193, accuracy: 0.5166666666666667\n",
      "iteration no 118: Loss: 0.7580438241236365, accuracy: 0.5166666666666667\n",
      "iteration no 119: Loss: 0.7579971934755787, accuracy: 0.5166666666666667\n",
      "iteration no 120: Loss: 0.757952152411102, accuracy: 0.5166666666666667\n",
      "iteration no 121: Loss: 0.7579086426670322, accuracy: 0.5166666666666667\n",
      "iteration no 122: Loss: 0.7578666083193998, accuracy: 0.5166666666666667\n",
      "iteration no 123: Loss: 0.7578259956790935, accuracy: 0.5166666666666667\n",
      "iteration no 124: Loss: 0.757786753192714, accuracy: 0.5166666666666667\n",
      "iteration no 125: Loss: 0.7577488313483464, accuracy: 0.5166666666666667\n",
      "iteration no 126: Loss: 0.7577121825859726, accuracy: 0.5166666666666667\n",
      "iteration no 127: Loss: 0.7576767612122746, accuracy: 0.5166666666666667\n",
      "iteration no 128: Loss: 0.7576425233195843, accuracy: 0.5166666666666667\n",
      "iteration no 129: Loss: 0.7576094267087554, accuracy: 0.5166666666666667\n",
      "iteration no 130: Loss: 0.7575774308157492, accuracy: 0.5166666666666667\n",
      "iteration no 131: Loss: 0.7575464966417295, accuracy: 0.5133333333333333\n",
      "iteration no 132: Loss: 0.7575165866864861, accuracy: 0.5133333333333333\n",
      "iteration no 133: Loss: 0.7574876648850044, accuracy: 0.5133333333333333\n",
      "iteration no 134: Loss: 0.7574596965470204, accuracy: 0.5133333333333333\n",
      "iteration no 135: Loss: 0.7574326482994016, accuracy: 0.5133333333333333\n",
      "iteration no 136: Loss: 0.7574064880312086, accuracy: 0.5133333333333333\n",
      "iteration no 137: Loss: 0.7573811848412989, accuracy: 0.5133333333333333\n",
      "iteration no 138: Loss: 0.7573567089883396, accuracy: 0.5133333333333333\n",
      "iteration no 139: Loss: 0.7573330318431104, accuracy: 0.5133333333333333\n",
      "iteration no 140: Loss: 0.7573101258429755, accuracy: 0.5133333333333333\n",
      "iteration no 141: Loss: 0.7572879644484197, accuracy: 0.5133333333333333\n",
      "iteration no 142: Loss: 0.7572665221015396, accuracy: 0.5133333333333333\n",
      "iteration no 143: Loss: 0.7572457741863982, accuracy: 0.5133333333333333\n",
      "iteration no 144: Loss: 0.7572256969911451, accuracy: 0.5133333333333333\n",
      "iteration no 145: Loss: 0.7572062676718189, accuracy: 0.5133333333333333\n",
      "iteration no 146: Loss: 0.7571874642177451, accuracy: 0.5133333333333333\n",
      "iteration no 147: Loss: 0.757169265418457, accuracy: 0.5133333333333333\n",
      "iteration no 148: Loss: 0.75715165083206, accuracy: 0.5133333333333333\n",
      "iteration no 149: Loss: 0.7571346007549726, accuracy: 0.5133333333333333\n",
      "iteration no 150: Loss: 0.7571180961929789, accuracy: 0.5133333333333333\n",
      "iteration no 151: Loss: 0.7571021188335247, accuracy: 0.5133333333333333\n",
      "iteration no 152: Loss: 0.7570866510192054, accuracy: 0.5133333333333333\n",
      "iteration no 153: Loss: 0.7570716757223833, accuracy: 0.5133333333333333\n",
      "iteration no 154: Loss: 0.757057176520883, accuracy: 0.5133333333333333\n",
      "iteration no 155: Loss: 0.7570431375747153, accuracy: 0.5133333333333333\n",
      "iteration no 156: Loss: 0.7570295436037814, accuracy: 0.5133333333333333\n",
      "iteration no 157: Loss: 0.75701637986651, accuracy: 0.5133333333333333\n",
      "iteration no 158: Loss: 0.7570036321393868, accuracy: 0.5133333333333333\n",
      "iteration no 159: Loss: 0.7569912866973338, accuracy: 0.5133333333333333\n",
      "iteration no 160: Loss: 0.7569793302949009, accuracy: 0.5133333333333333\n",
      "iteration no 161: Loss: 0.7569677501482298, accuracy: 0.5133333333333333\n",
      "iteration no 162: Loss: 0.7569565339177597, accuracy: 0.5133333333333333\n",
      "iteration no 163: Loss: 0.7569456696916389, accuracy: 0.5133333333333333\n",
      "iteration no 164: Loss: 0.7569351459698089, accuracy: 0.5133333333333333\n",
      "iteration no 165: Loss: 0.7569249516487347, accuracy: 0.5133333333333333\n",
      "iteration no 166: Loss: 0.7569150760067509, accuracy: 0.5133333333333333\n",
      "iteration no 167: Loss: 0.7569055086899944, accuracy: 0.5133333333333333\n",
      "iteration no 168: Loss: 0.756896239698902, accuracy: 0.5133333333333333\n",
      "iteration no 169: Loss: 0.7568872593752447, accuracy: 0.5133333333333333\n",
      "iteration no 170: Loss: 0.7568785583896767, accuracy: 0.5133333333333333\n",
      "iteration no 171: Loss: 0.7568701277297764, accuracy: 0.5133333333333333\n",
      "iteration no 172: Loss: 0.7568619586885585, accuracy: 0.5133333333333333\n",
      "iteration no 173: Loss: 0.7568540428534368, accuracy: 0.5133333333333333\n",
      "iteration no 174: Loss: 0.7568463720956179, accuracy: 0.5133333333333333\n",
      "iteration no 175: Loss: 0.7568389385599077, accuracy: 0.5133333333333333\n",
      "iteration no 176: Loss: 0.7568317346549144, accuracy: 0.5133333333333333\n",
      "iteration no 177: Loss: 0.7568247530436286, accuracy: 0.5133333333333333\n",
      "iteration no 178: Loss: 0.756817986634366, accuracy: 0.5133333333333333\n",
      "iteration no 179: Loss: 0.7568114285720599, accuracy: 0.5133333333333333\n",
      "iteration no 180: Loss: 0.7568050722298834, accuracy: 0.5133333333333333\n",
      "iteration no 181: Loss: 0.7567989112011934, accuracy: 0.5133333333333333\n",
      "iteration no 182: Loss: 0.7567929392917786, accuracy: 0.5133333333333333\n",
      "iteration no 183: Loss: 0.7567871505124012, accuracy: 0.5133333333333333\n",
      "iteration no 184: Loss: 0.756781539071621, accuracy: 0.5133333333333333\n",
      "iteration no 185: Loss: 0.7567760993688878, accuracy: 0.5133333333333333\n",
      "iteration no 186: Loss: 0.7567708259878918, accuracy: 0.5133333333333333\n",
      "iteration no 187: Loss: 0.7567657136901647, accuracy: 0.5133333333333333\n",
      "iteration no 188: Loss: 0.7567607574089169, accuracy: 0.5133333333333333\n",
      "iteration no 189: Loss: 0.7567559522431033, accuracy: 0.5133333333333333\n",
      "iteration no 190: Loss: 0.75675129345171, accuracy: 0.5133333333333333\n",
      "iteration no 191: Loss: 0.756746776448249, accuracy: 0.5133333333333333\n",
      "iteration no 192: Loss: 0.7567423967954573, accuracy: 0.5133333333333333\n",
      "iteration no 193: Loss: 0.7567381502001906, accuracy: 0.5133333333333333\n",
      "iteration no 194: Loss: 0.7567340325084999, accuracy: 0.5133333333333333\n",
      "iteration no 195: Loss: 0.7567300397008916, accuracy: 0.5133333333333333\n",
      "iteration no 196: Loss: 0.7567261678877566, accuracy: 0.5133333333333333\n",
      "iteration no 197: Loss: 0.7567224133049659, accuracy: 0.5133333333333333\n",
      "iteration no 198: Loss: 0.7567187723096263, accuracy: 0.5133333333333333\n",
      "iteration no 199: Loss: 0.7567152413759857, accuracy: 0.5133333333333333\n",
      "iteration no 200: Loss: 0.7567118170914885, accuracy: 0.5166666666666667\n",
      "iteration no 1: Loss: 1.1162498339517144, accuracy: 0.38333333333333336\n",
      "iteration no 2: Loss: 1.0838092232247902, accuracy: 0.46\n",
      "iteration no 3: Loss: 1.0549628625318983, accuracy: 0.4766666666666667\n",
      "iteration no 4: Loss: 1.0292710766468507, accuracy: 0.48\n",
      "iteration no 5: Loss: 1.0063482273439934, accuracy: 0.49666666666666665\n",
      "iteration no 6: Loss: 0.9858557020955349, accuracy: 0.49666666666666665\n",
      "iteration no 7: Loss: 0.9674968467268038, accuracy: 0.49333333333333335\n",
      "iteration no 8: Loss: 0.9510125306542827, accuracy: 0.49\n",
      "iteration no 9: Loss: 0.936176969132997, accuracy: 0.49666666666666665\n",
      "iteration no 10: Loss: 0.9227937666002264, accuracy: 0.49666666666666665\n",
      "iteration no 11: Loss: 0.910692235116503, accuracy: 0.5\n",
      "iteration no 12: Loss: 0.8997240404487773, accuracy: 0.51\n",
      "iteration no 13: Loss: 0.8897602023977195, accuracy: 0.51\n",
      "iteration no 14: Loss: 0.8806884504012872, accuracy: 0.51\n",
      "iteration no 15: Loss: 0.8724109172375358, accuracy: 0.51\n",
      "iteration no 16: Loss: 0.8648421430353149, accuracy: 0.51\n",
      "iteration no 17: Loss: 0.8579073570150101, accuracy: 0.51\n",
      "iteration no 18: Loss: 0.8515410034766799, accuracy: 0.51\n",
      "iteration no 19: Loss: 0.8456854799701686, accuracy: 0.51\n",
      "iteration no 20: Loss: 0.8402900582265397, accuracy: 0.5066666666666667\n",
      "iteration no 21: Loss: 0.835309961594644, accuracy: 0.51\n",
      "iteration no 22: Loss: 0.8307055759826829, accuracy: 0.51\n",
      "iteration no 23: Loss: 0.8264417744119741, accuracy: 0.5066666666666667\n",
      "iteration no 24: Loss: 0.822487338128093, accuracy: 0.51\n",
      "iteration no 25: Loss: 0.8188144597352944, accuracy: 0.51\n",
      "iteration no 26: Loss: 0.8153983160180494, accuracy: 0.51\n",
      "iteration no 27: Loss: 0.8122167000059195, accuracy: 0.51\n",
      "iteration no 28: Loss: 0.8092497034531808, accuracy: 0.51\n",
      "iteration no 29: Loss: 0.8064794422749396, accuracy: 0.51\n",
      "iteration no 30: Loss: 0.8038898186393365, accuracy: 0.51\n",
      "iteration no 31: Loss: 0.8014663143911852, accuracy: 0.51\n",
      "iteration no 32: Loss: 0.7991958113033469, accuracy: 0.5133333333333333\n",
      "iteration no 33: Loss: 0.7970664343423228, accuracy: 0.5133333333333333\n",
      "iteration no 34: Loss: 0.7950674147146821, accuracy: 0.5133333333333333\n",
      "iteration no 35: Loss: 0.7931889699487169, accuracy: 0.5133333333333333\n",
      "iteration no 36: Loss: 0.7914221986761107, accuracy: 0.5133333333333333\n",
      "iteration no 37: Loss: 0.7897589881240339, accuracy: 0.5133333333333333\n",
      "iteration no 38: Loss: 0.7881919326194893, accuracy: 0.5166666666666667\n",
      "iteration no 39: Loss: 0.7867142616537836, accuracy: 0.5166666666666667\n",
      "iteration no 40: Loss: 0.7853197762630393, accuracy: 0.5166666666666667\n",
      "iteration no 41: Loss: 0.7840027926568767, accuracy: 0.5166666666666667\n",
      "iteration no 42: Loss: 0.7827580921768649, accuracy: 0.5166666666666667\n",
      "iteration no 43: Loss: 0.7815808767933848, accuracy: 0.5133333333333333\n",
      "iteration no 44: Loss: 0.7804667294576956, accuracy: 0.5133333333333333\n",
      "iteration no 45: Loss: 0.7794115787182346, accuracy: 0.5133333333333333\n",
      "iteration no 46: Loss: 0.7784116670890052, accuracy: 0.5133333333333333\n",
      "iteration no 47: Loss: 0.7774635227253706, accuracy: 0.5133333333333333\n",
      "iteration no 48: Loss: 0.7765639340204452, accuracy: 0.5133333333333333\n",
      "iteration no 49: Loss: 0.7757099267849861, accuracy: 0.5133333333333333\n",
      "iteration no 50: Loss: 0.7748987437164884, accuracy: 0.5133333333333333\n",
      "iteration no 51: Loss: 0.774127825900094, accuracy: 0.5133333333333333\n",
      "iteration no 52: Loss: 0.773394796115805, accuracy: 0.5133333333333333\n",
      "iteration no 53: Loss: 0.7726974437540886, accuracy: 0.5133333333333333\n",
      "iteration no 54: Loss: 0.7720337111658864, accuracy: 0.5133333333333333\n",
      "iteration no 55: Loss: 0.7714016812938141, accuracy: 0.5133333333333333\n",
      "iteration no 56: Loss: 0.7707995664494186, accuracy: 0.5133333333333333\n",
      "iteration no 57: Loss: 0.7702256981171121, accuracy: 0.5133333333333333\n",
      "iteration no 58: Loss: 0.7696785176791527, accuracy: 0.5133333333333333\n",
      "iteration no 59: Loss: 0.7691565679680673, accuracy: 0.5166666666666667\n",
      "iteration no 60: Loss: 0.7686584855634399, accuracy: 0.5166666666666667\n",
      "iteration no 61: Loss: 0.7681829937592285, accuracy: 0.5166666666666667\n",
      "iteration no 62: Loss: 0.7677288961358876, accuracy: 0.5166666666666667\n",
      "iteration no 63: Loss: 0.7672950706787068, accuracy: 0.5166666666666667\n",
      "iteration no 64: Loss: 0.7668804643900831, accuracy: 0.5166666666666667\n",
      "iteration no 65: Loss: 0.7664840883489765, accuracy: 0.5166666666666667\n",
      "iteration no 66: Loss: 0.7661050131757261, accuracy: 0.5166666666666667\n",
      "iteration no 67: Loss: 0.7657423648647297, accuracy: 0.5166666666666667\n",
      "iteration no 68: Loss: 0.7653953209513533, accuracy: 0.5166666666666667\n",
      "iteration no 69: Loss: 0.76506310698284, accuracy: 0.5166666666666667\n",
      "iteration no 70: Loss: 0.7647449932660368, accuracy: 0.5166666666666667\n",
      "iteration no 71: Loss: 0.7644402918674484, accuracy: 0.5166666666666667\n",
      "iteration no 72: Loss: 0.764148353843539, accuracy: 0.5166666666666667\n",
      "iteration no 73: Loss: 0.7638685666813512, accuracy: 0.5166666666666667\n",
      "iteration no 74: Loss: 0.7636003519314294, accuracy: 0.5166666666666667\n",
      "iteration no 75: Loss: 0.7633431630167457, accuracy: 0.5166666666666667\n",
      "iteration no 76: Loss: 0.7630964832028702, accuracy: 0.5166666666666667\n",
      "iteration no 77: Loss: 0.7628598237159991, accuracy: 0.5166666666666667\n",
      "iteration no 78: Loss: 0.762632721996692, accuracy: 0.5166666666666667\n",
      "iteration no 79: Loss: 0.76241474007828, accuracy: 0.5166666666666667\n",
      "iteration no 80: Loss: 0.7622054630799063, accuracy: 0.5166666666666667\n",
      "iteration no 81: Loss: 0.7620044978050584, accuracy: 0.5166666666666667\n",
      "iteration no 82: Loss: 0.7618114714372648, accuracy: 0.5133333333333333\n",
      "iteration no 83: Loss: 0.7616260303253585, accuracy: 0.5133333333333333\n",
      "iteration no 84: Loss: 0.7614478388513714, accuracy: 0.5133333333333333\n",
      "iteration no 85: Loss: 0.7612765783747238, accuracy: 0.5133333333333333\n",
      "iteration no 86: Loss: 0.7611119462469104, accuracy: 0.5133333333333333\n",
      "iteration no 87: Loss: 0.7609536548913757, accuracy: 0.5133333333333333\n",
      "iteration no 88: Loss: 0.7608014309437221, accuracy: 0.5133333333333333\n",
      "iteration no 89: Loss: 0.7606550144477854, accuracy: 0.5133333333333333\n",
      "iteration no 90: Loss: 0.7605141581034937, accuracy: 0.5133333333333333\n",
      "iteration no 91: Loss: 0.7603786265627461, accuracy: 0.5133333333333333\n",
      "iteration no 92: Loss: 0.7602481957698617, accuracy: 0.5133333333333333\n",
      "iteration no 93: Loss: 0.7601226523434189, accuracy: 0.5133333333333333\n",
      "iteration no 94: Loss: 0.7600017929965598, accuracy: 0.5133333333333333\n",
      "iteration no 95: Loss: 0.759885423993066, accuracy: 0.5133333333333333\n",
      "iteration no 96: Loss: 0.7597733606367206, accuracy: 0.5133333333333333\n",
      "iteration no 97: Loss: 0.7596654267916624, accuracy: 0.5133333333333333\n",
      "iteration no 98: Loss: 0.7595614544316172, accuracy: 0.5133333333333333\n",
      "iteration no 99: Loss: 0.7594612832160483, accuracy: 0.5133333333333333\n",
      "iteration no 100: Loss: 0.7593647600914212, accuracy: 0.5133333333333333\n",
      "iteration no 101: Loss: 0.7592717389159045, accuracy: 0.5133333333333333\n",
      "iteration no 102: Loss: 0.7591820801059612, accuracy: 0.5133333333333333\n",
      "iteration no 103: Loss: 0.7590956503033963, accuracy: 0.5166666666666667\n",
      "iteration no 104: Loss: 0.7590123220615246, accuracy: 0.5166666666666667\n",
      "iteration no 105: Loss: 0.7589319735492313, accuracy: 0.5166666666666667\n",
      "iteration no 106: Loss: 0.7588544882717757, accuracy: 0.5166666666666667\n",
      "iteration no 107: Loss: 0.7587797548072748, accuracy: 0.5166666666666667\n",
      "iteration no 108: Loss: 0.7587076665578788, accuracy: 0.5166666666666667\n",
      "iteration no 109: Loss: 0.7586381215147191, accuracy: 0.5166666666666667\n",
      "iteration no 110: Loss: 0.7585710220357715, accuracy: 0.5166666666666667\n",
      "iteration no 111: Loss: 0.7585062746358423, accuracy: 0.5166666666666667\n",
      "iteration no 112: Loss: 0.7584437897879299, accuracy: 0.5166666666666667\n",
      "iteration no 113: Loss: 0.7583834817352773, accuracy: 0.5166666666666667\n",
      "iteration no 114: Loss: 0.758325268313464, accuracy: 0.5166666666666667\n",
      "iteration no 115: Loss: 0.7582690707819433, accuracy: 0.5166666666666667\n",
      "iteration no 116: Loss: 0.7582148136644555, accuracy: 0.5166666666666667\n",
      "iteration no 117: Loss: 0.7581624245978009, accuracy: 0.5166666666666667\n",
      "iteration no 118: Loss: 0.7581118341884762, accuracy: 0.5166666666666667\n",
      "iteration no 119: Loss: 0.7580629758767203, accuracy: 0.5166666666666667\n",
      "iteration no 120: Loss: 0.7580157858075404, accuracy: 0.5166666666666667\n",
      "iteration no 121: Loss: 0.7579702027083155, accuracy: 0.5166666666666667\n",
      "iteration no 122: Loss: 0.7579261677726049, accuracy: 0.5166666666666667\n",
      "iteration no 123: Loss: 0.7578836245498088, accuracy: 0.5166666666666667\n",
      "iteration no 124: Loss: 0.7578425188403516, accuracy: 0.5166666666666667\n",
      "iteration no 125: Loss: 0.7578027985960789, accuracy: 0.5166666666666667\n",
      "iteration no 126: Loss: 0.7577644138255796, accuracy: 0.5166666666666667\n",
      "iteration no 127: Loss: 0.7577273165041601, accuracy: 0.5166666666666667\n",
      "iteration no 128: Loss: 0.7576914604882163, accuracy: 0.5166666666666667\n",
      "iteration no 129: Loss: 0.7576568014337639, accuracy: 0.5166666666666667\n",
      "iteration no 130: Loss: 0.7576232967188998, accuracy: 0.5166666666666667\n",
      "iteration no 131: Loss: 0.7575909053699866, accuracy: 0.5166666666666667\n",
      "iteration no 132: Loss: 0.7575595879913581, accuracy: 0.5166666666666667\n",
      "iteration no 133: Loss: 0.7575293066983607, accuracy: 0.5166666666666667\n",
      "iteration no 134: Loss: 0.7575000250535511, accuracy: 0.5133333333333333\n",
      "iteration no 135: Loss: 0.7574717080058903, accuracy: 0.5133333333333333\n",
      "iteration no 136: Loss: 0.7574443218327723, accuracy: 0.5133333333333333\n",
      "iteration no 137: Loss: 0.7574178340847418, accuracy: 0.5133333333333333\n",
      "iteration no 138: Loss: 0.7573922135327649, accuracy: 0.5133333333333333\n",
      "iteration no 139: Loss: 0.7573674301179169, accuracy: 0.5133333333333333\n",
      "iteration no 140: Loss: 0.7573434549033696, accuracy: 0.5133333333333333\n",
      "iteration no 141: Loss: 0.7573202600285561, accuracy: 0.5133333333333333\n",
      "iteration no 142: Loss: 0.7572978186654092, accuracy: 0.5133333333333333\n",
      "iteration no 143: Loss: 0.7572761049765625, accuracy: 0.5133333333333333\n",
      "iteration no 144: Loss: 0.7572550940754228, accuracy: 0.5133333333333333\n",
      "iteration no 145: Loss: 0.7572347619880181, accuracy: 0.5133333333333333\n",
      "iteration no 146: Loss: 0.757215085616533, accuracy: 0.5133333333333333\n",
      "iteration no 147: Loss: 0.757196042704451, accuracy: 0.5133333333333333\n",
      "iteration no 148: Loss: 0.7571776118032231, accuracy: 0.5133333333333333\n",
      "iteration no 149: Loss: 0.7571597722403908, accuracy: 0.5133333333333333\n",
      "iteration no 150: Loss: 0.7571425040890912, accuracy: 0.5133333333333333\n",
      "iteration no 151: Loss: 0.7571257881388803, accuracy: 0.5133333333333333\n",
      "iteration no 152: Loss: 0.7571096058678094, accuracy: 0.5133333333333333\n",
      "iteration no 153: Loss: 0.7570939394156959, accuracy: 0.5133333333333333\n",
      "iteration no 154: Loss: 0.7570787715585334, accuracy: 0.5133333333333333\n",
      "iteration no 155: Loss: 0.7570640856839852, accuracy: 0.5133333333333333\n",
      "iteration no 156: Loss: 0.7570498657679117, accuracy: 0.5133333333333333\n",
      "iteration no 157: Loss: 0.7570360963518851, accuracy: 0.5133333333333333\n",
      "iteration no 158: Loss: 0.757022762521642, accuracy: 0.5133333333333333\n",
      "iteration no 159: Loss: 0.7570098498864358, accuracy: 0.5133333333333333\n",
      "iteration no 160: Loss: 0.7569973445592414, accuracy: 0.5133333333333333\n",
      "iteration no 161: Loss: 0.7569852331377804, accuracy: 0.5133333333333333\n",
      "iteration no 162: Loss: 0.7569735026863225, accuracy: 0.5133333333333333\n",
      "iteration no 163: Loss: 0.7569621407182348, accuracy: 0.5133333333333333\n",
      "iteration no 164: Loss: 0.7569511351792387, accuracy: 0.5133333333333333\n",
      "iteration no 165: Loss: 0.7569404744313492, accuracy: 0.5133333333333333\n",
      "iteration no 166: Loss: 0.756930147237461, accuracy: 0.5133333333333333\n",
      "iteration no 167: Loss: 0.7569201427465557, accuracy: 0.5133333333333333\n",
      "iteration no 168: Loss: 0.7569104504795017, accuracy: 0.5133333333333333\n",
      "iteration no 169: Loss: 0.7569010603154206, accuracy: 0.5133333333333333\n",
      "iteration no 170: Loss: 0.756891962478597, accuracy: 0.5133333333333333\n",
      "iteration no 171: Loss: 0.7568831475259049, accuracy: 0.5133333333333333\n",
      "iteration no 172: Loss: 0.7568746063347308, accuracy: 0.5133333333333333\n",
      "iteration no 173: Loss: 0.7568663300913723, accuracy: 0.5133333333333333\n",
      "iteration no 174: Loss: 0.7568583102798893, accuracy: 0.5133333333333333\n",
      "iteration no 175: Loss: 0.7568505386713915, accuracy: 0.5133333333333333\n",
      "iteration no 176: Loss: 0.7568430073137411, accuracy: 0.5133333333333333\n",
      "iteration no 177: Loss: 0.7568357085216552, accuracy: 0.5133333333333333\n",
      "iteration no 178: Loss: 0.7568286348671882, accuracy: 0.5133333333333333\n",
      "iteration no 179: Loss: 0.7568217791705824, accuracy: 0.5133333333333333\n",
      "iteration no 180: Loss: 0.7568151344914668, accuracy: 0.5133333333333333\n",
      "iteration no 181: Loss: 0.7568086941203934, accuracy: 0.5133333333333333\n",
      "iteration no 182: Loss: 0.7568024515706943, accuracy: 0.5133333333333333\n",
      "iteration no 183: Loss: 0.7567964005706478, accuracy: 0.5133333333333333\n",
      "iteration no 184: Loss: 0.7567905350559405, accuracy: 0.5133333333333333\n",
      "iteration no 185: Loss: 0.7567848491624131, accuracy: 0.5133333333333333\n",
      "iteration no 186: Loss: 0.7567793372190776, accuracy: 0.5133333333333333\n",
      "iteration no 187: Loss: 0.7567739937413974, accuracy: 0.5133333333333333\n",
      "iteration no 188: Loss: 0.756768813424815, accuracy: 0.5133333333333333\n",
      "iteration no 189: Loss: 0.7567637911385238, accuracy: 0.5133333333333333\n",
      "iteration no 190: Loss: 0.7567589219194668, accuracy: 0.5133333333333333\n",
      "iteration no 191: Loss: 0.7567542009665591, accuracy: 0.5133333333333333\n",
      "iteration no 192: Loss: 0.7567496236351231, accuracy: 0.5133333333333333\n",
      "iteration no 193: Loss: 0.7567451854315265, accuracy: 0.5133333333333333\n",
      "iteration no 194: Loss: 0.7567408820080166, accuracy: 0.5133333333333333\n",
      "iteration no 195: Loss: 0.7567367091577444, accuracy: 0.5133333333333333\n",
      "iteration no 196: Loss: 0.7567326628099684, accuracy: 0.5133333333333333\n",
      "iteration no 197: Loss: 0.7567287390254315, accuracy: 0.5133333333333333\n",
      "iteration no 198: Loss: 0.7567249339919069, accuracy: 0.5133333333333333\n",
      "iteration no 199: Loss: 0.7567212440199027, accuracy: 0.5133333333333333\n",
      "iteration no 200: Loss: 0.7567176655385223, accuracy: 0.5133333333333333\n",
      "iteration no 1: Loss: 1.1255124844069666, accuracy: 0.38666666666666666\n",
      "iteration no 2: Loss: 1.0921742198818611, accuracy: 0.47333333333333333\n",
      "iteration no 3: Loss: 1.0624581403668873, accuracy: 0.5\n",
      "iteration no 4: Loss: 1.035947513103943, accuracy: 0.5433333333333333\n",
      "iteration no 5: Loss: 1.0122667102513947, accuracy: 0.5333333333333333\n",
      "iteration no 6: Loss: 0.9910805634691645, accuracy: 0.5233333333333333\n",
      "iteration no 7: Loss: 0.9720923148657262, accuracy: 0.51\n",
      "iteration no 8: Loss: 0.9550406997728313, accuracy: 0.5166666666666667\n",
      "iteration no 9: Loss: 0.9396966068162141, accuracy: 0.51\n",
      "iteration no 10: Loss: 0.9258596351496967, accuracy: 0.5166666666666667\n",
      "iteration no 11: Loss: 0.9133547551979818, accuracy: 0.5133333333333333\n",
      "iteration no 12: Loss: 0.9020291923746969, accuracy: 0.5066666666666667\n",
      "iteration no 13: Loss: 0.8917495927819563, accuracy: 0.51\n",
      "iteration no 14: Loss: 0.8823994906215894, accuracy: 0.5066666666666667\n",
      "iteration no 15: Loss: 0.8738770731015473, accuracy: 0.5033333333333333\n",
      "iteration no 16: Loss: 0.8660932250980321, accuracy: 0.5\n",
      "iteration no 17: Loss: 0.8589698290874859, accuracy: 0.5133333333333333\n",
      "iteration no 18: Loss: 0.852438293350411, accuracy: 0.5133333333333333\n",
      "iteration no 19: Loss: 0.8464382814685684, accuracy: 0.5133333333333333\n",
      "iteration no 20: Loss: 0.8409166175806407, accuracy: 0.5133333333333333\n",
      "iteration no 21: Loss: 0.8358263440260862, accuracy: 0.5133333333333333\n",
      "iteration no 22: Loss: 0.8311259104584745, accuracy: 0.5133333333333333\n",
      "iteration no 23: Loss: 0.8267784759892504, accuracy: 0.51\n",
      "iteration no 24: Loss: 0.8227513082842238, accuracy: 0.5133333333333333\n",
      "iteration no 25: Loss: 0.8190152657028157, accuracy: 0.5133333333333333\n",
      "iteration no 26: Loss: 0.8155443505126811, accuracy: 0.5133333333333333\n",
      "iteration no 27: Loss: 0.812315322924489, accuracy: 0.5133333333333333\n",
      "iteration no 28: Loss: 0.809307367183071, accuracy: 0.5133333333333333\n",
      "iteration no 29: Loss: 0.8065018022393, accuracy: 0.5166666666666667\n",
      "iteration no 30: Loss: 0.803881830632838, accuracy: 0.5166666666666667\n",
      "iteration no 31: Loss: 0.8014323201608725, accuracy: 0.5133333333333333\n",
      "iteration no 32: Loss: 0.7991396137130289, accuracy: 0.5133333333333333\n",
      "iteration no 33: Loss: 0.7969913633369755, accuracy: 0.5166666666666667\n",
      "iteration no 34: Loss: 0.7949763851801716, accuracy: 0.5166666666666667\n",
      "iteration no 35: Loss: 0.7930845324459468, accuracy: 0.5166666666666667\n",
      "iteration no 36: Loss: 0.7913065839199173, accuracy: 0.5166666666666667\n",
      "iteration no 37: Loss: 0.7896341459770624, accuracy: 0.5166666666666667\n",
      "iteration no 38: Loss: 0.7880595662803719, accuracy: 0.5166666666666667\n",
      "iteration no 39: Loss: 0.7865758576371489, accuracy: 0.5166666666666667\n",
      "iteration no 40: Loss: 0.7851766306958521, accuracy: 0.5166666666666667\n",
      "iteration no 41: Loss: 0.7838560343507674, accuracy: 0.5166666666666667\n",
      "iteration no 42: Loss: 0.7826087028788073, accuracy: 0.5166666666666667\n",
      "iteration no 43: Loss: 0.7814297089666176, accuracy: 0.5166666666666667\n",
      "iteration no 44: Loss: 0.7803145219004582, accuracy: 0.5166666666666667\n",
      "iteration no 45: Loss: 0.7792589702890589, accuracy: 0.5166666666666667\n",
      "iteration no 46: Loss: 0.7782592087733143, accuracy: 0.5166666666666667\n",
      "iteration no 47: Loss: 0.7773116882484534, accuracy: 0.5166666666666667\n",
      "iteration no 48: Loss: 0.776413129185948, accuracy: 0.5166666666666667\n",
      "iteration no 49: Loss: 0.7755604976954544, accuracy: 0.5166666666666667\n",
      "iteration no 50: Loss: 0.774750984012766, accuracy: 0.5166666666666667\n",
      "iteration no 51: Loss: 0.7739819831391956, accuracy: 0.5166666666666667\n",
      "iteration no 52: Loss: 0.7732510773918867, accuracy: 0.5166666666666667\n",
      "iteration no 53: Loss: 0.7725560206540747, accuracy: 0.5166666666666667\n",
      "iteration no 54: Loss: 0.7718947241399099, accuracy: 0.5166666666666667\n",
      "iteration no 55: Loss: 0.771265243510691, accuracy: 0.5166666666666667\n",
      "iteration no 56: Loss: 0.7706657671987031, accuracy: 0.5166666666666667\n",
      "iteration no 57: Loss: 0.7700946058117063, accuracy: 0.5166666666666667\n",
      "iteration no 58: Loss: 0.7695501825058374, accuracy: 0.5166666666666667\n",
      "iteration no 59: Loss: 0.7690310242275374, accuracy: 0.5166666666666667\n",
      "iteration no 60: Loss: 0.7685357537363857, accuracy: 0.5166666666666667\n",
      "iteration no 61: Loss: 0.7680630823305818, accuracy: 0.5166666666666667\n",
      "iteration no 62: Loss: 0.7676118032054852, accuracy: 0.5166666666666667\n",
      "iteration no 63: Loss: 0.7671807853832452, accuracy: 0.5166666666666667\n",
      "iteration no 64: Loss: 0.7667689681582623, accuracy: 0.5166666666666667\n",
      "iteration no 65: Loss: 0.7663753560091389, accuracy: 0.5166666666666667\n",
      "iteration no 66: Loss: 0.7659990139330044, accuracy: 0.5166666666666667\n",
      "iteration no 67: Loss: 0.7656390631627249, accuracy: 0.5166666666666667\n",
      "iteration no 68: Loss: 0.7652946772315936, accuracy: 0.5166666666666667\n",
      "iteration no 69: Loss: 0.7649650783537315, accuracy: 0.5166666666666667\n",
      "iteration no 70: Loss: 0.764649534091649, accuracy: 0.5166666666666667\n",
      "iteration no 71: Loss: 0.7643473542852859, accuracy: 0.5166666666666667\n",
      "iteration no 72: Loss: 0.7640578882193895, accuracy: 0.5166666666666667\n",
      "iteration no 73: Loss: 0.7637805220083755, accuracy: 0.5166666666666667\n",
      "iteration no 74: Loss: 0.7635146761798336, accuracy: 0.5166666666666667\n",
      "iteration no 75: Loss: 0.7632598034396548, accuracy: 0.5166666666666667\n",
      "iteration no 76: Loss: 0.7630153866033845, accuracy: 0.5133333333333333\n",
      "iteration no 77: Loss: 0.7627809366798464, accuracy: 0.5133333333333333\n",
      "iteration no 78: Loss: 0.7625559910943936, accuracy: 0.5133333333333333\n",
      "iteration no 79: Loss: 0.7623401120403057, accuracy: 0.5133333333333333\n",
      "iteration no 80: Loss: 0.7621328849479005, accuracy: 0.5133333333333333\n",
      "iteration no 81: Loss: 0.7619339170618799, accuracy: 0.5133333333333333\n",
      "iteration no 82: Loss: 0.7617428361182678, accuracy: 0.5133333333333333\n",
      "iteration no 83: Loss: 0.76155928911308, accuracy: 0.5133333333333333\n",
      "iteration no 84: Loss: 0.761382941155543, accuracy: 0.5133333333333333\n",
      "iteration no 85: Loss: 0.7612134743993144, accuracy: 0.5133333333333333\n",
      "iteration no 86: Loss: 0.7610505870457168, accuracy: 0.5133333333333333\n",
      "iteration no 87: Loss: 0.7608939924135123, accuracy: 0.5133333333333333\n",
      "iteration no 88: Loss: 0.7607434180702035, accuracy: 0.5133333333333333\n",
      "iteration no 89: Loss: 0.7605986050202752, accuracy: 0.5133333333333333\n",
      "iteration no 90: Loss: 0.7604593069461605, accuracy: 0.5133333333333333\n",
      "iteration no 91: Loss: 0.7603252894980768, accuracy: 0.5133333333333333\n",
      "iteration no 92: Loss: 0.7601963296291752, accuracy: 0.5133333333333333\n",
      "iteration no 93: Loss: 0.7600722149727522, accuracy: 0.5133333333333333\n",
      "iteration no 94: Loss: 0.7599527432585156, accuracy: 0.5133333333333333\n",
      "iteration no 95: Loss: 0.7598377217651496, accuracy: 0.5133333333333333\n",
      "iteration no 96: Loss: 0.7597269668066339, accuracy: 0.5133333333333333\n",
      "iteration no 97: Loss: 0.7596203032499697, accuracy: 0.5133333333333333\n",
      "iteration no 98: Loss: 0.7595175640621521, accuracy: 0.5133333333333333\n",
      "iteration no 99: Loss: 0.759418589884391, accuracy: 0.5133333333333333\n",
      "iteration no 100: Loss: 0.7593232286317317, accuracy: 0.5166666666666667\n",
      "iteration no 101: Loss: 0.759231335116374, accuracy: 0.5166666666666667\n",
      "iteration no 102: Loss: 0.7591427706931069, accuracy: 0.5166666666666667\n",
      "iteration no 103: Loss: 0.7590574029254, accuracy: 0.5166666666666667\n",
      "iteration no 104: Loss: 0.7589751052707971, accuracy: 0.5166666666666667\n",
      "iteration no 105: Loss: 0.7588957567843561, accuracy: 0.5166666666666667\n",
      "iteration no 106: Loss: 0.7588192418389714, accuracy: 0.5166666666666667\n",
      "iteration no 107: Loss: 0.7587454498615009, accuracy: 0.5166666666666667\n",
      "iteration no 108: Loss: 0.7586742750836876, accuracy: 0.5166666666666667\n",
      "iteration no 109: Loss: 0.7586056163069513, accuracy: 0.5166666666666667\n",
      "iteration no 110: Loss: 0.758539376680177, accuracy: 0.5166666666666667\n",
      "iteration no 111: Loss: 0.7584754634896966, accuracy: 0.5166666666666667\n",
      "iteration no 112: Loss: 0.7584137879607098, accuracy: 0.5166666666666667\n",
      "iteration no 113: Loss: 0.7583542650694499, accuracy: 0.5166666666666667\n",
      "iteration no 114: Loss: 0.7582968133654361, accuracy: 0.5166666666666667\n",
      "iteration no 115: Loss: 0.7582413548032088, accuracy: 0.5166666666666667\n",
      "iteration no 116: Loss: 0.7581878145829799, accuracy: 0.5166666666666667\n",
      "iteration no 117: Loss: 0.7581361209996669, accuracy: 0.5166666666666667\n",
      "iteration no 118: Loss: 0.7580862052998187, accuracy: 0.5166666666666667\n",
      "iteration no 119: Loss: 0.7580380015459695, accuracy: 0.5166666666666667\n",
      "iteration no 120: Loss: 0.7579914464879889, accuracy: 0.5166666666666667\n",
      "iteration no 121: Loss: 0.7579464794410259, accuracy: 0.5166666666666667\n",
      "iteration no 122: Loss: 0.7579030421696662, accuracy: 0.5166666666666667\n",
      "iteration no 123: Loss: 0.7578610787779496, accuracy: 0.5166666666666667\n",
      "iteration no 124: Loss: 0.7578205356049185, accuracy: 0.5166666666666667\n",
      "iteration no 125: Loss: 0.7577813611253814, accuracy: 0.5166666666666667\n",
      "iteration no 126: Loss: 0.7577435058556065, accuracy: 0.5166666666666667\n",
      "iteration no 127: Loss: 0.7577069222636662, accuracy: 0.5166666666666667\n",
      "iteration no 128: Loss: 0.7576715646841806, accuracy: 0.5166666666666667\n",
      "iteration no 129: Loss: 0.7576373892372157, accuracy: 0.5166666666666667\n",
      "iteration no 130: Loss: 0.7576043537511126, accuracy: 0.5166666666666667\n",
      "iteration no 131: Loss: 0.7575724176890359, accuracy: 0.5166666666666667\n",
      "iteration no 132: Loss: 0.7575415420790372, accuracy: 0.5166666666666667\n",
      "iteration no 133: Loss: 0.7575116894474505, accuracy: 0.5166666666666667\n",
      "iteration no 134: Loss: 0.7574828237554426, accuracy: 0.5166666666666667\n",
      "iteration no 135: Loss: 0.7574549103385467, accuracy: 0.5166666666666667\n",
      "iteration no 136: Loss: 0.7574279158490308, accuracy: 0.5166666666666667\n",
      "iteration no 137: Loss: 0.7574018082009469, accuracy: 0.5166666666666667\n",
      "iteration no 138: Loss: 0.7573765565177255, accuracy: 0.5133333333333333\n",
      "iteration no 139: Loss: 0.7573521310821819, accuracy: 0.5133333333333333\n",
      "iteration no 140: Loss: 0.7573285032888142, accuracy: 0.5133333333333333\n",
      "iteration no 141: Loss: 0.7573056455982724, accuracy: 0.5133333333333333\n",
      "iteration no 142: Loss: 0.7572835314938928, accuracy: 0.5133333333333333\n",
      "iteration no 143: Loss: 0.7572621354401904, accuracy: 0.5133333333333333\n",
      "iteration no 144: Loss: 0.7572414328432129, accuracy: 0.5133333333333333\n",
      "iteration no 145: Loss: 0.7572214000126645, accuracy: 0.5133333333333333\n",
      "iteration no 146: Loss: 0.7572020141257091, accuracy: 0.5133333333333333\n",
      "iteration no 147: Loss: 0.7571832531923749, accuracy: 0.5133333333333333\n",
      "iteration no 148: Loss: 0.7571650960224773, accuracy: 0.5133333333333333\n",
      "iteration no 149: Loss: 0.7571475221939885, accuracy: 0.5133333333333333\n",
      "iteration no 150: Loss: 0.757130512022785, accuracy: 0.5133333333333333\n",
      "iteration no 151: Loss: 0.7571140465337033, accuracy: 0.5133333333333333\n",
      "iteration no 152: Loss: 0.7570981074328448, accuracy: 0.5133333333333333\n",
      "iteration no 153: Loss: 0.7570826770810677, accuracy: 0.5133333333333333\n",
      "iteration no 154: Loss: 0.7570677384686112, accuracy: 0.5133333333333333\n",
      "iteration no 155: Loss: 0.7570532751907978, accuracy: 0.5133333333333333\n",
      "iteration no 156: Loss: 0.7570392714247639, accuracy: 0.5133333333333333\n",
      "iteration no 157: Loss: 0.7570257119071693, accuracy: 0.5133333333333333\n",
      "iteration no 158: Loss: 0.7570125819128427, accuracy: 0.5133333333333333\n",
      "iteration no 159: Loss: 0.7569998672343171, accuracy: 0.5133333333333333\n",
      "iteration no 160: Loss: 0.7569875541622159, accuracy: 0.5133333333333333\n",
      "iteration no 161: Loss: 0.7569756294664504, accuracy: 0.5133333333333333\n",
      "iteration no 162: Loss: 0.756964080378192, accuracy: 0.5133333333333333\n",
      "iteration no 163: Loss: 0.7569528945725833, accuracy: 0.5133333333333333\n",
      "iteration no 164: Loss: 0.7569420601521563, accuracy: 0.5133333333333333\n",
      "iteration no 165: Loss: 0.7569315656309229, accuracy: 0.5133333333333333\n",
      "iteration no 166: Loss: 0.7569213999191123, accuracy: 0.5133333333333333\n",
      "iteration no 167: Loss: 0.756911552308522, accuracy: 0.5133333333333333\n",
      "iteration no 168: Loss: 0.7569020124584583, accuracy: 0.5133333333333333\n",
      "iteration no 169: Loss: 0.7568927703822387, accuracy: 0.5133333333333333\n",
      "iteration no 170: Loss: 0.756883816434233, accuracy: 0.5133333333333333\n",
      "iteration no 171: Loss: 0.7568751412974165, accuracy: 0.5133333333333333\n",
      "iteration no 172: Loss: 0.7568667359714201, accuracy: 0.5133333333333333\n",
      "iteration no 173: Loss: 0.7568585917610442, accuracy: 0.5133333333333333\n",
      "iteration no 174: Loss: 0.7568507002652294, accuracy: 0.5133333333333333\n",
      "iteration no 175: Loss: 0.7568430533664555, accuracy: 0.5133333333333333\n",
      "iteration no 176: Loss: 0.7568356432205525, accuracy: 0.5133333333333333\n",
      "iteration no 177: Loss: 0.7568284622469091, accuracy: 0.5133333333333333\n",
      "iteration no 178: Loss: 0.7568215031190578, accuracy: 0.5133333333333333\n",
      "iteration no 179: Loss: 0.7568147587556239, accuracy: 0.5133333333333333\n",
      "iteration no 180: Loss: 0.7568082223116205, accuracy: 0.5133333333333333\n",
      "iteration no 181: Loss: 0.756801887170078, accuracy: 0.5133333333333333\n",
      "iteration no 182: Loss: 0.7567957469339909, accuracy: 0.5133333333333333\n",
      "iteration no 183: Loss: 0.7567897954185713, accuracy: 0.5133333333333333\n",
      "iteration no 184: Loss: 0.7567840266437957, accuracy: 0.5133333333333333\n",
      "iteration no 185: Loss: 0.7567784348272324, accuracy: 0.5133333333333333\n",
      "iteration no 186: Loss: 0.7567730143771378, accuracy: 0.5133333333333333\n",
      "iteration no 187: Loss: 0.7567677598858136, accuracy: 0.5133333333333333\n",
      "iteration no 188: Loss: 0.7567626661232092, accuracy: 0.5133333333333333\n",
      "iteration no 189: Loss: 0.7567577280307642, accuracy: 0.5133333333333333\n",
      "iteration no 190: Loss: 0.7567529407154789, accuracy: 0.5133333333333333\n",
      "iteration no 191: Loss: 0.7567482994442029, accuracy: 0.5133333333333333\n",
      "iteration no 192: Loss: 0.7567437996381357, accuracy: 0.5133333333333333\n",
      "iteration no 193: Loss: 0.7567394368675293, accuracy: 0.5133333333333333\n",
      "iteration no 194: Loss: 0.7567352068465828, accuracy: 0.5133333333333333\n",
      "iteration no 195: Loss: 0.7567311054285271, accuracy: 0.5133333333333333\n",
      "iteration no 196: Loss: 0.7567271286008844, accuracy: 0.5133333333333333\n",
      "iteration no 197: Loss: 0.7567232724809037, accuracy: 0.5133333333333333\n",
      "iteration no 198: Loss: 0.7567195333111582, accuracy: 0.5133333333333333\n",
      "iteration no 199: Loss: 0.7567159074553039, accuracy: 0.5133333333333333\n",
      "iteration no 200: Loss: 0.7567123913939902, accuracy: 0.5166666666666667\n",
      "iteration no 1: Loss: 1.0903039852235845, accuracy: 0.48333333333333334\n",
      "iteration no 2: Loss: 1.0605643184139653, accuracy: 0.49666666666666665\n",
      "iteration no 3: Loss: 1.0341085729448836, accuracy: 0.49666666666666665\n",
      "iteration no 4: Loss: 1.0105307506012267, accuracy: 0.5\n",
      "iteration no 5: Loss: 0.9894768943381844, accuracy: 0.5\n",
      "iteration no 6: Loss: 0.9706374558803897, accuracy: 0.5\n",
      "iteration no 7: Loss: 0.9537419895890518, accuracy: 0.5\n",
      "iteration no 8: Loss: 0.9385547137123568, accuracy: 0.5033333333333333\n",
      "iteration no 9: Loss: 0.9248704662358009, accuracy: 0.5\n",
      "iteration no 10: Loss: 0.9125109554705092, accuracy: 0.5\n",
      "iteration no 11: Loss: 0.9013213183463894, accuracy: 0.51\n",
      "iteration no 12: Loss: 0.8911670135012615, accuracy: 0.51\n",
      "iteration no 13: Loss: 0.8819310606773272, accuracy: 0.5133333333333333\n",
      "iteration no 14: Loss: 0.8735116191237587, accuracy: 0.5133333333333333\n",
      "iteration no 15: Loss: 0.8658198838172573, accuracy: 0.5133333333333333\n",
      "iteration no 16: Loss: 0.8587782703983752, accuracy: 0.5133333333333333\n",
      "iteration no 17: Loss: 0.852318856563967, accuracy: 0.5133333333333333\n",
      "iteration no 18: Loss: 0.8463820476789341, accuracy: 0.5133333333333333\n",
      "iteration no 19: Loss: 0.8409154362617726, accuracy: 0.5133333333333333\n",
      "iteration no 20: Loss: 0.8358728278300294, accuracy: 0.5133333333333333\n",
      "iteration no 21: Loss: 0.8312134087669026, accuracy: 0.5133333333333333\n",
      "iteration no 22: Loss: 0.8269010350343351, accuracy: 0.51\n",
      "iteration no 23: Loss: 0.8229036235188907, accuracy: 0.5133333333333333\n",
      "iteration no 24: Loss: 0.819192630464506, accuracy: 0.5133333333333333\n",
      "iteration no 25: Loss: 0.8157426037920112, accuracy: 0.5133333333333333\n",
      "iteration no 26: Loss: 0.8125307981347967, accuracy: 0.5133333333333333\n",
      "iteration no 27: Loss: 0.8095368431564706, accuracy: 0.5166666666666667\n",
      "iteration no 28: Loss: 0.8067424571909946, accuracy: 0.5166666666666667\n",
      "iteration no 29: Loss: 0.8041311994917045, accuracy: 0.5166666666666667\n",
      "iteration no 30: Loss: 0.8016882554247876, accuracy: 0.5166666666666667\n",
      "iteration no 31: Loss: 0.7994002498244877, accuracy: 0.5166666666666667\n",
      "iteration no 32: Loss: 0.7972550844674396, accuracy: 0.5166666666666667\n",
      "iteration no 33: Loss: 0.7952417962446202, accuracy: 0.52\n",
      "iteration no 34: Loss: 0.793350433130681, accuracy: 0.5166666666666667\n",
      "iteration no 35: Loss: 0.7915719454882113, accuracy: 0.5166666666666667\n",
      "iteration no 36: Loss: 0.7898980906124754, accuracy: 0.5166666666666667\n",
      "iteration no 37: Loss: 0.7883213487318772, accuracy: 0.5166666666666667\n",
      "iteration no 38: Loss: 0.7868348489404263, accuracy: 0.5166666666666667\n",
      "iteration no 39: Loss: 0.7854323037587814, accuracy: 0.5166666666666667\n",
      "iteration no 40: Loss: 0.7841079512067185, accuracy: 0.5166666666666667\n",
      "iteration no 41: Loss: 0.7828565034276204, accuracy: 0.5166666666666667\n",
      "iteration no 42: Loss: 0.7816731010394183, accuracy: 0.5166666666666667\n",
      "iteration no 43: Loss: 0.7805532725001952, accuracy: 0.5133333333333333\n",
      "iteration no 44: Loss: 0.7794928978735327, accuracy: 0.5133333333333333\n",
      "iteration no 45: Loss: 0.7784881764613488, accuracy: 0.5133333333333333\n",
      "iteration no 46: Loss: 0.7775355978426379, accuracy: 0.5133333333333333\n",
      "iteration no 47: Loss: 0.7766319159170375, accuracy: 0.5133333333333333\n",
      "iteration no 48: Loss: 0.7757741256040815, accuracy: 0.5133333333333333\n",
      "iteration no 49: Loss: 0.7749594418936387, accuracy: 0.5133333333333333\n",
      "iteration no 50: Loss: 0.7741852809814904, accuracy: 0.5133333333333333\n",
      "iteration no 51: Loss: 0.7734492432571847, accuracy: 0.5133333333333333\n",
      "iteration no 52: Loss: 0.7727490979399776, accuracy: 0.5133333333333333\n",
      "iteration no 53: Loss: 0.7720827691835304, accuracy: 0.5133333333333333\n",
      "iteration no 54: Loss: 0.77144832349157, accuracy: 0.5133333333333333\n",
      "iteration no 55: Loss: 0.7708439583054578, accuracy: 0.5133333333333333\n",
      "iteration no 56: Loss: 0.7702679916409222, accuracy: 0.5133333333333333\n",
      "iteration no 57: Loss: 0.7697188526654309, accuracy: 0.5133333333333333\n",
      "iteration no 58: Loss: 0.7691950731201014, accuracy: 0.5133333333333333\n",
      "iteration no 59: Loss: 0.7686952795009251, accuracy: 0.5133333333333333\n",
      "iteration no 60: Loss: 0.7682181859236058, accuracy: 0.5166666666666667\n",
      "iteration no 61: Loss: 0.7677625876046745, accuracy: 0.5166666666666667\n",
      "iteration no 62: Loss: 0.7673273548989036, accuracy: 0.5166666666666667\n",
      "iteration no 63: Loss: 0.766911427839511, accuracy: 0.5166666666666667\n",
      "iteration no 64: Loss: 0.7665138111333607, accuracy: 0.5166666666666667\n",
      "iteration no 65: Loss: 0.7661335695684024, accuracy: 0.5166666666666667\n",
      "iteration no 66: Loss: 0.7657698237950629, accuracy: 0.5166666666666667\n",
      "iteration no 67: Loss: 0.7654217464472424, accuracy: 0.5166666666666667\n",
      "iteration no 68: Loss: 0.7650885585720791, accuracy: 0.5166666666666667\n",
      "iteration no 69: Loss: 0.7647695263407532, accuracy: 0.5166666666666667\n",
      "iteration no 70: Loss: 0.7644639580153724, accuracy: 0.5166666666666667\n",
      "iteration no 71: Loss: 0.7641712011494408, accuracy: 0.5166666666666667\n",
      "iteration no 72: Loss: 0.7638906400016177, accuracy: 0.5166666666666667\n",
      "iteration no 73: Loss: 0.7636216931444229, accuracy: 0.5166666666666667\n",
      "iteration no 74: Loss: 0.7633638112513117, accuracy: 0.5166666666666667\n",
      "iteration no 75: Loss: 0.7631164750471034, accuracy: 0.5166666666666667\n",
      "iteration no 76: Loss: 0.7628791934081548, accuracy: 0.5166666666666667\n",
      "iteration no 77: Loss: 0.7626515015999334, accuracy: 0.5166666666666667\n",
      "iteration no 78: Loss: 0.7624329596407792, accuracy: 0.5166666666666667\n",
      "iteration no 79: Loss: 0.762223150781657, accuracy: 0.5166666666666667\n",
      "iteration no 80: Loss: 0.7620216800926222, accuracy: 0.5166666666666667\n",
      "iteration no 81: Loss: 0.7618281731475522, accuracy: 0.5166666666666667\n",
      "iteration no 82: Loss: 0.7616422747994323, accuracy: 0.5133333333333333\n",
      "iteration no 83: Loss: 0.7614636480391644, accuracy: 0.5133333333333333\n",
      "iteration no 84: Loss: 0.7612919729314767, accuracy: 0.5133333333333333\n",
      "iteration no 85: Loss: 0.7611269456220535, accuracy: 0.5133333333333333\n",
      "iteration no 86: Loss: 0.7609682774105151, accuracy: 0.5133333333333333\n",
      "iteration no 87: Loss: 0.7608156938843206, accuracy: 0.5133333333333333\n",
      "iteration no 88: Loss: 0.7606689341090785, accuracy: 0.5133333333333333\n",
      "iteration no 89: Loss: 0.7605277498711273, accuracy: 0.5133333333333333\n",
      "iteration no 90: Loss: 0.7603919049685787, accuracy: 0.5133333333333333\n",
      "iteration no 91: Loss: 0.7602611745473334, accuracy: 0.5133333333333333\n",
      "iteration no 92: Loss: 0.760135344478851, accuracy: 0.5133333333333333\n",
      "iteration no 93: Loss: 0.7600142107767187, accuracy: 0.5133333333333333\n",
      "iteration no 94: Loss: 0.7598975790492929, accuracy: 0.5133333333333333\n",
      "iteration no 95: Loss: 0.7597852639859024, accuracy: 0.5133333333333333\n",
      "iteration no 96: Loss: 0.7596770888742964, accuracy: 0.5133333333333333\n",
      "iteration no 97: Loss: 0.7595728851471961, accuracy: 0.5133333333333333\n",
      "iteration no 98: Loss: 0.75947249195598, accuracy: 0.5133333333333333\n",
      "iteration no 99: Loss: 0.7593757557696681, accuracy: 0.5133333333333333\n",
      "iteration no 100: Loss: 0.7592825299975241, accuracy: 0.5133333333333333\n",
      "iteration no 101: Loss: 0.7591926746337044, accuracy: 0.5133333333333333\n",
      "iteration no 102: Loss: 0.759106055922511, accuracy: 0.5133333333333333\n",
      "iteration no 103: Loss: 0.7590225460428998, accuracy: 0.5166666666666667\n",
      "iteration no 104: Loss: 0.7589420228110059, accuracy: 0.5166666666666667\n",
      "iteration no 105: Loss: 0.7588643693995248, accuracy: 0.5166666666666667\n",
      "iteration no 106: Loss: 0.7587894740728767, accuracy: 0.5166666666666667\n",
      "iteration no 107: Loss: 0.7587172299371611, accuracy: 0.5166666666666667\n",
      "iteration no 108: Loss: 0.7586475347039696, accuracy: 0.5166666666666667\n",
      "iteration no 109: Loss: 0.7585802904671975, accuracy: 0.5166666666666667\n",
      "iteration no 110: Loss: 0.7585154034920505, accuracy: 0.5166666666666667\n",
      "iteration no 111: Loss: 0.7584527840154986, accuracy: 0.5166666666666667\n",
      "iteration no 112: Loss: 0.758392346057483, accuracy: 0.5166666666666667\n",
      "iteration no 113: Loss: 0.7583340072422203, accuracy: 0.5166666666666667\n",
      "iteration no 114: Loss: 0.7582776886290095, accuracy: 0.5166666666666667\n",
      "iteration no 115: Loss: 0.7582233145519611, accuracy: 0.5166666666666667\n",
      "iteration no 116: Loss: 0.7581708124681352, accuracy: 0.5166666666666667\n",
      "iteration no 117: Loss: 0.7581201128135852, accuracy: 0.5166666666666667\n",
      "iteration no 118: Loss: 0.7580711488668508, accuracy: 0.5166666666666667\n",
      "iteration no 119: Loss: 0.7580238566194658, accuracy: 0.5166666666666667\n",
      "iteration no 120: Loss: 0.7579781746530827, accuracy: 0.5166666666666667\n",
      "iteration no 121: Loss: 0.7579340440228269, accuracy: 0.5166666666666667\n",
      "iteration no 122: Loss: 0.7578914081465395, accuracy: 0.5166666666666667\n",
      "iteration no 123: Loss: 0.7578502126995642, accuracy: 0.5166666666666667\n",
      "iteration no 124: Loss: 0.7578104055147765, accuracy: 0.5166666666666667\n",
      "iteration no 125: Loss: 0.7577719364875608, accuracy: 0.5166666666666667\n",
      "iteration no 126: Loss: 0.7577347574854616, accuracy: 0.5166666666666667\n",
      "iteration no 127: Loss: 0.7576988222622523, accuracy: 0.5166666666666667\n",
      "iteration no 128: Loss: 0.7576640863761805, accuracy: 0.5166666666666667\n",
      "iteration no 129: Loss: 0.7576305071121652, accuracy: 0.5166666666666667\n",
      "iteration no 130: Loss: 0.7575980434077297, accuracy: 0.5133333333333333\n",
      "iteration no 131: Loss: 0.757566655782474, accuracy: 0.5133333333333333\n",
      "iteration no 132: Loss: 0.757536306270893, accuracy: 0.5133333333333333\n",
      "iteration no 133: Loss: 0.7575069583583718, accuracy: 0.5133333333333333\n",
      "iteration no 134: Loss: 0.7574785769201813, accuracy: 0.5133333333333333\n",
      "iteration no 135: Loss: 0.7574511281633254, accuracy: 0.5133333333333333\n",
      "iteration no 136: Loss: 0.7574245795710872, accuracy: 0.5133333333333333\n",
      "iteration no 137: Loss: 0.7573988998501364, accuracy: 0.5133333333333333\n",
      "iteration no 138: Loss: 0.7573740588800664, accuracy: 0.5133333333333333\n",
      "iteration no 139: Loss: 0.7573500276652364, accuracy: 0.5133333333333333\n",
      "iteration no 140: Loss: 0.7573267782888025, accuracy: 0.5133333333333333\n",
      "iteration no 141: Loss: 0.7573042838688266, accuracy: 0.5133333333333333\n",
      "iteration no 142: Loss: 0.7572825185163596, accuracy: 0.5133333333333333\n",
      "iteration no 143: Loss: 0.7572614572953988, accuracy: 0.5133333333333333\n",
      "iteration no 144: Loss: 0.7572410761846298, accuracy: 0.5133333333333333\n",
      "iteration no 145: Loss: 0.7572213520408596, accuracy: 0.5133333333333333\n",
      "iteration no 146: Loss: 0.7572022625640658, accuracy: 0.5133333333333333\n",
      "iteration no 147: Loss: 0.7571837862639725, accuracy: 0.5133333333333333\n",
      "iteration no 148: Loss: 0.7571659024280898, accuracy: 0.5133333333333333\n",
      "iteration no 149: Loss: 0.7571485910911381, accuracy: 0.5133333333333333\n",
      "iteration no 150: Loss: 0.7571318330057947, accuracy: 0.5133333333333333\n",
      "iteration no 151: Loss: 0.7571156096146993, accuracy: 0.5133333333333333\n",
      "iteration no 152: Loss: 0.7570999030236552, accuracy: 0.5133333333333333\n",
      "iteration no 153: Loss: 0.7570846959759763, accuracy: 0.5133333333333333\n",
      "iteration no 154: Loss: 0.7570699718279177, accuracy: 0.5133333333333333\n",
      "iteration no 155: Loss: 0.7570557145251475, accuracy: 0.5133333333333333\n",
      "iteration no 156: Loss: 0.7570419085802044, accuracy: 0.5133333333333333\n",
      "iteration no 157: Loss: 0.7570285390508998, accuracy: 0.5133333333333333\n",
      "iteration no 158: Loss: 0.7570155915196201, accuracy: 0.5133333333333333\n",
      "iteration no 159: Loss: 0.7570030520734846, accuracy: 0.5133333333333333\n",
      "iteration no 160: Loss: 0.7569909072853265, accuracy: 0.5133333333333333\n",
      "iteration no 161: Loss: 0.7569791441954528, accuracy: 0.5133333333333333\n",
      "iteration no 162: Loss: 0.7569677502941511, accuracy: 0.5133333333333333\n",
      "iteration no 163: Loss: 0.7569567135049119, accuracy: 0.5133333333333333\n",
      "iteration no 164: Loss: 0.7569460221683291, accuracy: 0.5133333333333333\n",
      "iteration no 165: Loss: 0.7569356650266522, accuracy: 0.5133333333333333\n",
      "iteration no 166: Loss: 0.7569256312089621, accuracy: 0.5133333333333333\n",
      "iteration no 167: Loss: 0.7569159102169378, accuracy: 0.5133333333333333\n",
      "iteration no 168: Loss: 0.7569064919111962, accuracy: 0.5133333333333333\n",
      "iteration no 169: Loss: 0.7568973664981702, accuracy: 0.5133333333333333\n",
      "iteration no 170: Loss: 0.7568885245175099, accuracy: 0.5133333333333333\n",
      "iteration no 171: Loss: 0.7568799568299815, accuracy: 0.5133333333333333\n",
      "iteration no 172: Loss: 0.7568716546058394, accuracy: 0.5133333333333333\n",
      "iteration no 173: Loss: 0.7568636093136549, accuracy: 0.5133333333333333\n",
      "iteration no 174: Loss: 0.7568558127095826, accuracy: 0.5133333333333333\n",
      "iteration no 175: Loss: 0.7568482568270402, accuracy: 0.5133333333333333\n",
      "iteration no 176: Loss: 0.7568409339667918, accuracy: 0.5133333333333333\n",
      "iteration no 177: Loss: 0.7568338366874113, accuracy: 0.5133333333333333\n",
      "iteration no 178: Loss: 0.7568269577961141, accuracy: 0.5133333333333333\n",
      "iteration no 179: Loss: 0.756820290339939, accuracy: 0.5133333333333333\n",
      "iteration no 180: Loss: 0.7568138275972668, accuracy: 0.5133333333333333\n",
      "iteration no 181: Loss: 0.7568075630696622, accuracy: 0.5133333333333333\n",
      "iteration no 182: Loss: 0.7568014904740243, accuracy: 0.5133333333333333\n",
      "iteration no 183: Loss: 0.7567956037350347, accuracy: 0.5133333333333333\n",
      "iteration no 184: Loss: 0.7567898969778879, accuracy: 0.5133333333333333\n",
      "iteration no 185: Loss: 0.756784364521297, accuracy: 0.5133333333333333\n",
      "iteration no 186: Loss: 0.7567790008707597, accuracy: 0.5133333333333333\n",
      "iteration no 187: Loss: 0.7567738007120738, accuracy: 0.5133333333333333\n",
      "iteration no 188: Loss: 0.7567687589050978, accuracy: 0.5133333333333333\n",
      "iteration no 189: Loss: 0.7567638704777379, accuracy: 0.5133333333333333\n",
      "iteration no 190: Loss: 0.7567591306201613, accuracy: 0.5133333333333333\n",
      "iteration no 191: Loss: 0.7567545346792182, accuracy: 0.5133333333333333\n",
      "iteration no 192: Loss: 0.7567500781530719, accuracy: 0.5133333333333333\n",
      "iteration no 193: Loss: 0.7567457566860238, accuracy: 0.5133333333333333\n",
      "iteration no 194: Loss: 0.7567415660635276, accuracy: 0.5133333333333333\n",
      "iteration no 195: Loss: 0.7567375022073843, accuracy: 0.5133333333333333\n",
      "iteration no 196: Loss: 0.7567335611711116, accuracy: 0.5133333333333333\n",
      "iteration no 197: Loss: 0.7567297391354801, accuracy: 0.5133333333333333\n",
      "iteration no 198: Loss: 0.7567260324042134, accuracy: 0.5133333333333333\n",
      "iteration no 199: Loss: 0.7567224373998368, accuracy: 0.5133333333333333\n",
      "iteration no 200: Loss: 0.756718950659681, accuracy: 0.5133333333333333\n",
      "iteration no 1: Loss: 1.1123098648566576, accuracy: 0.46\n",
      "iteration no 2: Loss: 1.080202068380084, accuracy: 0.5133333333333333\n",
      "iteration no 3: Loss: 1.0516163480962841, accuracy: 0.5366666666666666\n",
      "iteration no 4: Loss: 1.0261368335955405, accuracy: 0.53\n",
      "iteration no 5: Loss: 1.0033918321434208, accuracy: 0.5166666666666667\n",
      "iteration no 6: Loss: 0.9830517201715172, accuracy: 0.5133333333333333\n",
      "iteration no 7: Loss: 0.9648261005355039, accuracy: 0.5166666666666667\n",
      "iteration no 8: Loss: 0.948460409856758, accuracy: 0.5166666666666667\n",
      "iteration no 9: Loss: 0.9337322697717506, accuracy: 0.52\n",
      "iteration no 10: Loss: 0.920447836037225, accuracy: 0.5133333333333333\n",
      "iteration no 11: Loss: 0.9084383206637476, accuracy: 0.51\n",
      "iteration no 12: Loss: 0.8975567887884371, accuracy: 0.51\n",
      "iteration no 13: Loss: 0.8876752768018716, accuracy: 0.51\n",
      "iteration no 14: Loss: 0.8786822414882007, accuracy: 0.51\n",
      "iteration no 15: Loss: 0.8704803277360451, accuracy: 0.51\n",
      "iteration no 16: Loss: 0.8629844304006957, accuracy: 0.5133333333333333\n",
      "iteration no 17: Loss: 0.8561200205841595, accuracy: 0.5133333333333333\n",
      "iteration no 18: Loss: 0.8498217053660222, accuracy: 0.51\n",
      "iteration no 19: Loss: 0.8440319911210625, accuracy: 0.51\n",
      "iteration no 20: Loss: 0.838700222877909, accuracy: 0.51\n",
      "iteration no 21: Loss: 0.8337816750153806, accuracy: 0.51\n",
      "iteration no 22: Loss: 0.8292367715510648, accuracy: 0.51\n",
      "iteration no 23: Loss: 0.8250304171222398, accuracy: 0.51\n",
      "iteration no 24: Loss: 0.8211314223763657, accuracy: 0.5133333333333333\n",
      "iteration no 25: Loss: 0.817512009828469, accuracy: 0.5133333333333333\n",
      "iteration no 26: Loss: 0.814147388296328, accuracy: 0.5133333333333333\n",
      "iteration no 27: Loss: 0.8110153858035722, accuracy: 0.5133333333333333\n",
      "iteration no 28: Loss: 0.8080961323685699, accuracy: 0.5133333333333333\n",
      "iteration no 29: Loss: 0.805371785400598, accuracy: 0.51\n",
      "iteration no 30: Loss: 0.8028262915323329, accuracy: 0.51\n",
      "iteration no 31: Loss: 0.8004451796558416, accuracy: 0.5133333333333333\n",
      "iteration no 32: Loss: 0.7982153807223785, accuracy: 0.5133333333333333\n",
      "iteration no 33: Loss: 0.7961250705360751, accuracy: 0.5133333333333333\n",
      "iteration no 34: Loss: 0.7941635323369238, accuracy: 0.5133333333333333\n",
      "iteration no 35: Loss: 0.7923210364456035, accuracy: 0.5133333333333333\n",
      "iteration no 36: Loss: 0.7905887346455505, accuracy: 0.5133333333333333\n",
      "iteration no 37: Loss: 0.7889585673180519, accuracy: 0.5133333333333333\n",
      "iteration no 38: Loss: 0.7874231816339685, accuracy: 0.5133333333333333\n",
      "iteration no 39: Loss: 0.7859758593493524, accuracy: 0.5133333333333333\n",
      "iteration no 40: Loss: 0.7846104529587761, accuracy: 0.5133333333333333\n",
      "iteration no 41: Loss: 0.783321329135477, accuracy: 0.5133333333333333\n",
      "iteration no 42: Loss: 0.78210331853645, accuracy: 0.5133333333333333\n",
      "iteration no 43: Loss: 0.780951671177465, accuracy: 0.5133333333333333\n",
      "iteration no 44: Loss: 0.7798620166911729, accuracy: 0.5133333333333333\n",
      "iteration no 45: Loss: 0.7788303288738453, accuracy: 0.5133333333333333\n",
      "iteration no 46: Loss: 0.777852894005332, accuracy: 0.5133333333333333\n",
      "iteration no 47: Loss: 0.7769262824945681, accuracy: 0.5133333333333333\n",
      "iteration no 48: Loss: 0.7760473234610987, accuracy: 0.5133333333333333\n",
      "iteration no 49: Loss: 0.7752130819130993, accuracy: 0.5133333333333333\n",
      "iteration no 50: Loss: 0.7744208382254392, accuracy: 0.5133333333333333\n",
      "iteration no 51: Loss: 0.7736680696584992, accuracy: 0.5133333333333333\n",
      "iteration no 52: Loss: 0.7729524336905808, accuracy: 0.5133333333333333\n",
      "iteration no 53: Loss: 0.7722717529645561, accuracy: 0.5133333333333333\n",
      "iteration no 54: Loss: 0.7716240016735283, accuracy: 0.5133333333333333\n",
      "iteration no 55: Loss: 0.7710072932312254, accuracy: 0.5166666666666667\n",
      "iteration no 56: Loss: 0.7704198690910861, accuracy: 0.5166666666666667\n",
      "iteration no 57: Loss: 0.7698600885938867, accuracy: 0.5166666666666667\n",
      "iteration no 58: Loss: 0.7693264197376268, accuracy: 0.5166666666666667\n",
      "iteration no 59: Loss: 0.7688174307755259, accuracy: 0.5166666666666667\n",
      "iteration no 60: Loss: 0.7683317825586014, accuracy: 0.5166666666666667\n",
      "iteration no 61: Loss: 0.7678682215486157, accuracy: 0.5166666666666667\n",
      "iteration no 62: Loss: 0.7674255734353607, accuracy: 0.5166666666666667\n",
      "iteration no 63: Loss: 0.76700273729945, accuracy: 0.5166666666666667\n",
      "iteration no 64: Loss: 0.7665986802681275, accuracy: 0.5166666666666667\n",
      "iteration no 65: Loss: 0.7662124326171981, accuracy: 0.5166666666666667\n",
      "iteration no 66: Loss: 0.7658430832771297, accuracy: 0.5166666666666667\n",
      "iteration no 67: Loss: 0.765489775705748, accuracy: 0.5166666666666667\n",
      "iteration no 68: Loss: 0.765151704093822, accuracy: 0.5166666666666667\n",
      "iteration no 69: Loss: 0.7648281098732748, accuracy: 0.5166666666666667\n",
      "iteration no 70: Loss: 0.7645182785008092, accuracy: 0.5166666666666667\n",
      "iteration no 71: Loss: 0.7642215364924528, accuracy: 0.5166666666666667\n",
      "iteration no 72: Loss: 0.7639372486869472, accuracy: 0.5166666666666667\n",
      "iteration no 73: Loss: 0.7636648157180634, accuracy: 0.5166666666666667\n",
      "iteration no 74: Loss: 0.7634036716778521, accuracy: 0.5166666666666667\n",
      "iteration no 75: Loss: 0.7631532819545589, accuracy: 0.5166666666666667\n",
      "iteration no 76: Loss: 0.7629131412304756, accuracy: 0.5166666666666667\n",
      "iteration no 77: Loss: 0.7626827716263798, accuracy: 0.5133333333333333\n",
      "iteration no 78: Loss: 0.762461720980455, accuracy: 0.5133333333333333\n",
      "iteration no 79: Loss: 0.7622495612506941, accuracy: 0.5133333333333333\n",
      "iteration no 80: Loss: 0.7620458870307912, accuracy: 0.5133333333333333\n",
      "iteration no 81: Loss: 0.7618503141704239, accuracy: 0.5133333333333333\n",
      "iteration no 82: Loss: 0.7616624784916461, accuracy: 0.5133333333333333\n",
      "iteration no 83: Loss: 0.7614820345938342, accuracy: 0.5133333333333333\n",
      "iteration no 84: Loss: 0.7613086547402975, accuracy: 0.5133333333333333\n",
      "iteration no 85: Loss: 0.7611420278202568, accuracy: 0.5133333333333333\n",
      "iteration no 86: Loss: 0.7609818583804371, accuracy: 0.5133333333333333\n",
      "iteration no 87: Loss: 0.7608278657210088, accuracy: 0.5133333333333333\n",
      "iteration no 88: Loss: 0.7606797830510558, accuracy: 0.5133333333333333\n",
      "iteration no 89: Loss: 0.7605373566991495, accuracy: 0.5133333333333333\n",
      "iteration no 90: Loss: 0.7604003453749774, accuracy: 0.5133333333333333\n",
      "iteration no 91: Loss: 0.7602685194783015, accuracy: 0.5133333333333333\n",
      "iteration no 92: Loss: 0.7601416604518283, accuracy: 0.5133333333333333\n",
      "iteration no 93: Loss: 0.7600195601748452, accuracy: 0.5133333333333333\n",
      "iteration no 94: Loss: 0.7599020203947292, accuracy: 0.5133333333333333\n",
      "iteration no 95: Loss: 0.759788852193666, accuracy: 0.5133333333333333\n",
      "iteration no 96: Loss: 0.7596798754881167, accuracy: 0.5133333333333333\n",
      "iteration no 97: Loss: 0.7595749185587742, accuracy: 0.5133333333333333\n",
      "iteration no 98: Loss: 0.7594738176089144, accuracy: 0.5133333333333333\n",
      "iteration no 99: Loss: 0.7593764163492109, accuracy: 0.5133333333333333\n",
      "iteration no 100: Loss: 0.759282565607231, accuracy: 0.5133333333333333\n",
      "iteration no 101: Loss: 0.7591921229599579, accuracy: 0.5133333333333333\n",
      "iteration no 102: Loss: 0.7591049523878177, accuracy: 0.5166666666666667\n",
      "iteration no 103: Loss: 0.7590209239487891, accuracy: 0.5166666666666667\n",
      "iteration no 104: Loss: 0.7589399134712891, accuracy: 0.5166666666666667\n",
      "iteration no 105: Loss: 0.7588618022646176, accuracy: 0.5166666666666667\n",
      "iteration no 106: Loss: 0.7587864768458316, accuracy: 0.5166666666666667\n",
      "iteration no 107: Loss: 0.7587138286820029, accuracy: 0.5166666666666667\n",
      "iteration no 108: Loss: 0.7586437539468854, accuracy: 0.5166666666666667\n",
      "iteration no 109: Loss: 0.7585761532910862, accuracy: 0.5166666666666667\n",
      "iteration no 110: Loss: 0.7585109316249012, accuracy: 0.5166666666666667\n",
      "iteration no 111: Loss: 0.7584479979130299, accuracy: 0.5166666666666667\n",
      "iteration no 112: Loss: 0.758387264980441, accuracy: 0.5166666666666667\n",
      "iteration no 113: Loss: 0.75832864932871, accuracy: 0.5166666666666667\n",
      "iteration no 114: Loss: 0.7582720709621943, accuracy: 0.5166666666666667\n",
      "iteration no 115: Loss: 0.7582174532234555, accuracy: 0.5166666666666667\n",
      "iteration no 116: Loss: 0.7581647226373764, accuracy: 0.5166666666666667\n",
      "iteration no 117: Loss: 0.7581138087634619, accuracy: 0.5166666666666667\n",
      "iteration no 118: Loss: 0.7580646440558353, accuracy: 0.5166666666666667\n",
      "iteration no 119: Loss: 0.7580171637304899, accuracy: 0.5166666666666667\n",
      "iteration no 120: Loss: 0.757971305639367, accuracy: 0.5166666666666667\n",
      "iteration no 121: Loss: 0.7579270101508738, accuracy: 0.5166666666666667\n",
      "iteration no 122: Loss: 0.7578842200364682, accuracy: 0.5166666666666667\n",
      "iteration no 123: Loss: 0.7578428803629674, accuracy: 0.5166666666666667\n",
      "iteration no 124: Loss: 0.7578029383902606, accuracy: 0.5166666666666667\n",
      "iteration no 125: Loss: 0.7577643434741161, accuracy: 0.5166666666666667\n",
      "iteration no 126: Loss: 0.7577270469738066, accuracy: 0.5166666666666667\n",
      "iteration no 127: Loss: 0.7576910021642795, accuracy: 0.5166666666666667\n",
      "iteration no 128: Loss: 0.7576561641526284, accuracy: 0.5166666666666667\n",
      "iteration no 129: Loss: 0.7576224897986265, accuracy: 0.5166666666666667\n",
      "iteration no 130: Loss: 0.7575899376391022, accuracy: 0.5166666666666667\n",
      "iteration no 131: Loss: 0.7575584678159526, accuracy: 0.5166666666666667\n",
      "iteration no 132: Loss: 0.757528042007596, accuracy: 0.5166666666666667\n",
      "iteration no 133: Loss: 0.7574986233636836, accuracy: 0.5166666666666667\n",
      "iteration no 134: Loss: 0.7574701764428946, accuracy: 0.5133333333333333\n",
      "iteration no 135: Loss: 0.7574426671536555, accuracy: 0.5133333333333333\n",
      "iteration no 136: Loss: 0.7574160626976298, accuracy: 0.5133333333333333\n",
      "iteration no 137: Loss: 0.7573903315158321, accuracy: 0.5133333333333333\n",
      "iteration no 138: Loss: 0.7573654432372339, accuracy: 0.5133333333333333\n",
      "iteration no 139: Loss: 0.7573413686297311, accuracy: 0.5133333333333333\n",
      "iteration no 140: Loss: 0.7573180795533536, accuracy: 0.5133333333333333\n",
      "iteration no 141: Loss: 0.7572955489156032, accuracy: 0.5133333333333333\n",
      "iteration no 142: Loss: 0.7572737506288102, accuracy: 0.5133333333333333\n",
      "iteration no 143: Loss: 0.7572526595694108, accuracy: 0.5133333333333333\n",
      "iteration no 144: Loss: 0.7572322515390477, accuracy: 0.5133333333333333\n",
      "iteration no 145: Loss: 0.757212503227402, accuracy: 0.5133333333333333\n",
      "iteration no 146: Loss: 0.7571933921766734, accuracy: 0.5133333333333333\n",
      "iteration no 147: Loss: 0.7571748967476268, accuracy: 0.5133333333333333\n",
      "iteration no 148: Loss: 0.7571569960871278, accuracy: 0.5133333333333333\n",
      "iteration no 149: Loss: 0.757139670097096, accuracy: 0.5133333333333333\n",
      "iteration no 150: Loss: 0.7571228994048088, accuracy: 0.5133333333333333\n",
      "iteration no 151: Loss: 0.7571066653344865, accuracy: 0.5133333333333333\n",
      "iteration no 152: Loss: 0.757090949880102, accuracy: 0.5133333333333333\n",
      "iteration no 153: Loss: 0.7570757356793563, accuracy: 0.5133333333333333\n",
      "iteration no 154: Loss: 0.7570610059887605, accuracy: 0.5133333333333333\n",
      "iteration no 155: Loss: 0.7570467446597775, accuracy: 0.5133333333333333\n",
      "iteration no 156: Loss: 0.7570329361159713, accuracy: 0.5133333333333333\n",
      "iteration no 157: Loss: 0.7570195653311156, accuracy: 0.5133333333333333\n",
      "iteration no 158: Loss: 0.7570066178082216, accuracy: 0.5133333333333333\n",
      "iteration no 159: Loss: 0.7569940795594365, accuracy: 0.5133333333333333\n",
      "iteration no 160: Loss: 0.7569819370867791, accuracy: 0.5133333333333333\n",
      "iteration no 161: Loss: 0.7569701773636689, accuracy: 0.5133333333333333\n",
      "iteration no 162: Loss: 0.756958787817217, accuracy: 0.5133333333333333\n",
      "iteration no 163: Loss: 0.7569477563112411, accuracy: 0.5133333333333333\n",
      "iteration no 164: Loss: 0.7569370711299724, accuracy: 0.5133333333333333\n",
      "iteration no 165: Loss: 0.7569267209624273, accuracy: 0.5133333333333333\n",
      "iteration no 166: Loss: 0.7569166948874075, accuracy: 0.5133333333333333\n",
      "iteration no 167: Loss: 0.7569069823591069, accuracy: 0.5133333333333333\n",
      "iteration no 168: Loss: 0.7568975731932958, accuracy: 0.5133333333333333\n",
      "iteration no 169: Loss: 0.7568884575540546, accuracy: 0.5133333333333333\n",
      "iteration no 170: Loss: 0.7568796259410392, accuracy: 0.5133333333333333\n",
      "iteration no 171: Loss: 0.7568710691772486, accuracy: 0.5133333333333333\n",
      "iteration no 172: Loss: 0.7568627783972759, accuracy: 0.5133333333333333\n",
      "iteration no 173: Loss: 0.7568547450360225, accuracy: 0.5133333333333333\n",
      "iteration no 174: Loss: 0.7568469608178534, accuracy: 0.5133333333333333\n",
      "iteration no 175: Loss: 0.756839417746177, accuracy: 0.5133333333333333\n",
      "iteration no 176: Loss: 0.756832108093429, accuracy: 0.5133333333333333\n",
      "iteration no 177: Loss: 0.7568250243914445, accuracy: 0.5133333333333333\n",
      "iteration no 178: Loss: 0.7568181594222031, accuracy: 0.5133333333333333\n",
      "iteration no 179: Loss: 0.7568115062089288, accuracy: 0.5133333333333333\n",
      "iteration no 180: Loss: 0.7568050580075307, accuracy: 0.5133333333333333\n",
      "iteration no 181: Loss: 0.7567988082983726, accuracy: 0.5133333333333333\n",
      "iteration no 182: Loss: 0.7567927507783534, accuracy: 0.5133333333333333\n",
      "iteration no 183: Loss: 0.7567868793532893, accuracy: 0.5133333333333333\n",
      "iteration no 184: Loss: 0.7567811881305836, accuracy: 0.5133333333333333\n",
      "iteration no 185: Loss: 0.7567756714121733, accuracy: 0.5133333333333333\n",
      "iteration no 186: Loss: 0.7567703236877387, accuracy: 0.5133333333333333\n",
      "iteration no 187: Loss: 0.7567651396281692, accuracy: 0.5133333333333333\n",
      "iteration no 188: Loss: 0.7567601140792711, accuracy: 0.5133333333333333\n",
      "iteration no 189: Loss: 0.7567552420557113, accuracy: 0.5133333333333333\n",
      "iteration no 190: Loss: 0.7567505187351825, accuracy: 0.5133333333333333\n",
      "iteration no 191: Loss: 0.7567459394527865, accuracy: 0.5133333333333333\n",
      "iteration no 192: Loss: 0.7567414996956239, accuracy: 0.5133333333333333\n",
      "iteration no 193: Loss: 0.7567371950975809, accuracy: 0.5133333333333333\n",
      "iteration no 194: Loss: 0.7567330214343083, accuracy: 0.5133333333333333\n",
      "iteration no 195: Loss: 0.7567289746183835, accuracy: 0.5133333333333333\n",
      "iteration no 196: Loss: 0.7567250506946479, accuracy: 0.5133333333333333\n",
      "iteration no 197: Loss: 0.7567212458357134, accuracy: 0.5133333333333333\n",
      "iteration no 198: Loss: 0.7567175563376326, accuracy: 0.5133333333333333\n",
      "iteration no 199: Loss: 0.7567139786157235, accuracy: 0.5133333333333333\n",
      "iteration no 200: Loss: 0.7567105092005448, accuracy: 0.5166666666666667\n",
      "iteration no 1: Loss: 1.0961427586335915, accuracy: 0.5366666666666666\n",
      "iteration no 2: Loss: 1.065785691567856, accuracy: 0.5466666666666666\n",
      "iteration no 3: Loss: 1.0387505066998586, accuracy: 0.5433333333333333\n",
      "iteration no 4: Loss: 1.01463847967594, accuracy: 0.54\n",
      "iteration no 5: Loss: 0.993098035659822, accuracy: 0.5233333333333333\n",
      "iteration no 6: Loss: 0.9738192886570665, accuracy: 0.5166666666666667\n",
      "iteration no 7: Loss: 0.9565299024142456, accuracy: 0.5133333333333333\n",
      "iteration no 8: Loss: 0.9409913270520527, accuracy: 0.5166666666666667\n",
      "iteration no 9: Loss: 0.9269951733706407, accuracy: 0.5166666666666667\n",
      "iteration no 10: Loss: 0.9143597350266286, accuracy: 0.5166666666666667\n",
      "iteration no 11: Loss: 0.9029267262413107, accuracy: 0.5166666666666667\n",
      "iteration no 12: Loss: 0.8925582920787867, accuracy: 0.5166666666666667\n",
      "iteration no 13: Loss: 0.8831343212413582, accuracy: 0.5133333333333333\n",
      "iteration no 14: Loss: 0.8745500666393284, accuracy: 0.52\n",
      "iteration no 15: Loss: 0.8667140616557717, accuracy: 0.52\n",
      "iteration no 16: Loss: 0.8595463097867374, accuracy: 0.52\n",
      "iteration no 17: Loss: 0.8529767204596597, accuracy: 0.52\n",
      "iteration no 18: Loss: 0.84694376253042, accuracy: 0.52\n",
      "iteration no 19: Loss: 0.8413933078228617, accuracy: 0.5166666666666667\n",
      "iteration no 20: Loss: 0.8362776391160861, accuracy: 0.5166666666666667\n",
      "iteration no 21: Loss: 0.8315545995624216, accuracy: 0.5166666666666667\n",
      "iteration no 22: Loss: 0.8271868632393994, accuracy: 0.5166666666666667\n",
      "iteration no 23: Loss: 0.8231413091769376, accuracy: 0.5166666666666667\n",
      "iteration no 24: Loss: 0.8193884836382475, accuracy: 0.5166666666666667\n",
      "iteration no 25: Loss: 0.8159021376180654, accuracy: 0.52\n",
      "iteration no 26: Loss: 0.8126588284421282, accuracy: 0.52\n",
      "iteration no 27: Loss: 0.8096375760166435, accuracy: 0.52\n",
      "iteration no 28: Loss: 0.8068195657063092, accuracy: 0.52\n",
      "iteration no 29: Loss: 0.8041878910393503, accuracy: 0.5166666666666667\n",
      "iteration no 30: Loss: 0.8017273304741467, accuracy: 0.5166666666666667\n",
      "iteration no 31: Loss: 0.7994241533393571, accuracy: 0.5166666666666667\n",
      "iteration no 32: Loss: 0.797265950800854, accuracy: 0.5166666666666667\n",
      "iteration no 33: Loss: 0.7952414883346535, accuracy: 0.5166666666666667\n",
      "iteration no 34: Loss: 0.7933405767130891, accuracy: 0.5166666666666667\n",
      "iteration no 35: Loss: 0.7915539589570441, accuracy: 0.5166666666666667\n",
      "iteration no 36: Loss: 0.7898732110831538, accuracy: 0.5166666666666667\n",
      "iteration no 37: Loss: 0.7882906547925933, accuracy: 0.5166666666666667\n",
      "iteration no 38: Loss: 0.7867992805166764, accuracy: 0.5166666666666667\n",
      "iteration no 39: Loss: 0.785392679461884, accuracy: 0.5166666666666667\n",
      "iteration no 40: Loss: 0.7840649834896554, accuracy: 0.5166666666666667\n",
      "iteration no 41: Loss: 0.7828108118298764, accuracy: 0.5166666666666667\n",
      "iteration no 42: Loss: 0.7816252237660447, accuracy: 0.5133333333333333\n",
      "iteration no 43: Loss: 0.7805036765484982, accuracy: 0.5133333333333333\n",
      "iteration no 44: Loss: 0.7794419878930651, accuracy: 0.5133333333333333\n",
      "iteration no 45: Loss: 0.7784363025087442, accuracy: 0.5133333333333333\n",
      "iteration no 46: Loss: 0.7774830621718355, accuracy: 0.5133333333333333\n",
      "iteration no 47: Loss: 0.7765789789272066, accuracy: 0.5133333333333333\n",
      "iteration no 48: Loss: 0.7757210110517032, accuracy: 0.5166666666666667\n",
      "iteration no 49: Loss: 0.7749063414614453, accuracy: 0.5166666666666667\n",
      "iteration no 50: Loss: 0.7741323582850024, accuracy: 0.5166666666666667\n",
      "iteration no 51: Loss: 0.7733966373592068, accuracy: 0.5166666666666667\n",
      "iteration no 52: Loss: 0.7726969264344001, accuracy: 0.5166666666666667\n",
      "iteration no 53: Loss: 0.7720311309019394, accuracy: 0.5166666666666667\n",
      "iteration no 54: Loss: 0.771397300879365, accuracy: 0.5166666666666667\n",
      "iteration no 55: Loss: 0.770793619508254, accuracy: 0.5166666666666667\n",
      "iteration no 56: Loss: 0.770218392336859, accuracy: 0.5166666666666667\n",
      "iteration no 57: Loss: 0.7696700376745282, accuracy: 0.5166666666666667\n",
      "iteration no 58: Loss: 0.7691470778179038, accuracy: 0.5166666666666667\n",
      "iteration no 59: Loss: 0.7686481310602669, accuracy: 0.5166666666666667\n",
      "iteration no 60: Loss: 0.768171904405371, accuracy: 0.5166666666666667\n",
      "iteration no 61: Loss: 0.7677171869158395, accuracy: 0.5166666666666667\n",
      "iteration no 62: Loss: 0.7672828436338892, accuracy: 0.5166666666666667\n",
      "iteration no 63: Loss: 0.766867810018905, accuracy: 0.5166666666666667\n",
      "iteration no 64: Loss: 0.7664710868523478, accuracy: 0.5166666666666667\n",
      "iteration no 65: Loss: 0.7660917355657335, accuracy: 0.5166666666666667\n",
      "iteration no 66: Loss: 0.7657288739520796, accuracy: 0.5166666666666667\n",
      "iteration no 67: Loss: 0.7653816722253165, accuracy: 0.5166666666666667\n",
      "iteration no 68: Loss: 0.7650493493958228, accuracy: 0.5166666666666667\n",
      "iteration no 69: Loss: 0.7647311699334652, accuracy: 0.5166666666666667\n",
      "iteration no 70: Loss: 0.7644264406924128, accuracy: 0.5166666666666667\n",
      "iteration no 71: Loss: 0.7641345080745462, accuracy: 0.5166666666666667\n",
      "iteration no 72: Loss: 0.7638547554105688, accuracy: 0.5166666666666667\n",
      "iteration no 73: Loss: 0.7635866005399532, accuracy: 0.5166666666666667\n",
      "iteration no 74: Loss: 0.7633294935726868, accuracy: 0.5166666666666667\n",
      "iteration no 75: Loss: 0.7630829148173881, accuracy: 0.5166666666666667\n",
      "iteration no 76: Loss: 0.7628463728618374, accuracy: 0.5166666666666667\n",
      "iteration no 77: Loss: 0.7626194027932548, accuracy: 0.5166666666666667\n",
      "iteration no 78: Loss: 0.7624015645468422, accuracy: 0.5166666666666667\n",
      "iteration no 79: Loss: 0.7621924413721456, accuracy: 0.5133333333333333\n",
      "iteration no 80: Loss: 0.7619916384077488, accuracy: 0.5133333333333333\n",
      "iteration no 81: Loss: 0.7617987813556565, accuracy: 0.5133333333333333\n",
      "iteration no 82: Loss: 0.7616135152474925, accuracy: 0.5133333333333333\n",
      "iteration no 83: Loss: 0.7614355032953358, accuracy: 0.5133333333333333\n",
      "iteration no 84: Loss: 0.7612644258206342, accuracy: 0.5133333333333333\n",
      "iteration no 85: Loss: 0.7610999792552104, accuracy: 0.5133333333333333\n",
      "iteration no 86: Loss: 0.7609418752088823, accuracy: 0.5133333333333333\n",
      "iteration no 87: Loss: 0.7607898395986815, accuracy: 0.5133333333333333\n",
      "iteration no 88: Loss: 0.7606436118350788, accuracy: 0.5133333333333333\n",
      "iteration no 89: Loss: 0.7605029440610042, accuracy: 0.5133333333333333\n",
      "iteration no 90: Loss: 0.7603676004397981, accuracy: 0.5133333333333333\n",
      "iteration no 91: Loss: 0.760237356488542, accuracy: 0.5133333333333333\n",
      "iteration no 92: Loss: 0.7601119984535063, accuracy: 0.5133333333333333\n",
      "iteration no 93: Loss: 0.7599913227247177, accuracy: 0.5133333333333333\n",
      "iteration no 94: Loss: 0.759875135286879, accuracy: 0.5133333333333333\n",
      "iteration no 95: Loss: 0.7597632512040983, accuracy: 0.5133333333333333\n",
      "iteration no 96: Loss: 0.7596554941360802, accuracy: 0.5133333333333333\n",
      "iteration no 97: Loss: 0.759551695883616, accuracy: 0.5133333333333333\n",
      "iteration no 98: Loss: 0.7594516959613694, accuracy: 0.5133333333333333\n",
      "iteration no 99: Loss: 0.7593553411961167, accuracy: 0.5133333333333333\n",
      "iteration no 100: Loss: 0.7592624853487273, accuracy: 0.5166666666666667\n",
      "iteration no 101: Loss: 0.7591729887583105, accuracy: 0.5166666666666667\n",
      "iteration no 102: Loss: 0.7590867180070611, accuracy: 0.5166666666666667\n",
      "iteration no 103: Loss: 0.7590035456044503, accuracy: 0.5166666666666667\n",
      "iteration no 104: Loss: 0.7589233496895069, accuracy: 0.5166666666666667\n",
      "iteration no 105: Loss: 0.7588460137500209, accuracy: 0.5166666666666667\n",
      "iteration no 106: Loss: 0.7587714263575881, accuracy: 0.5166666666666667\n",
      "iteration no 107: Loss: 0.7586994809174923, accuracy: 0.5166666666666667\n",
      "iteration no 108: Loss: 0.7586300754324903, accuracy: 0.5166666666666667\n",
      "iteration no 109: Loss: 0.7585631122796306, accuracy: 0.5166666666666667\n",
      "iteration no 110: Loss: 0.7584984979992998, accuracy: 0.5166666666666667\n",
      "iteration no 111: Loss: 0.7584361430957423, accuracy: 0.5166666666666667\n",
      "iteration no 112: Loss: 0.7583759618483538, accuracy: 0.5166666666666667\n",
      "iteration no 113: Loss: 0.7583178721330965, accuracy: 0.5166666666666667\n",
      "iteration no 114: Loss: 0.7582617952534236, accuracy: 0.5166666666666667\n",
      "iteration no 115: Loss: 0.7582076557801498, accuracy: 0.5166666666666667\n",
      "iteration no 116: Loss: 0.758155381399732, accuracy: 0.5166666666666667\n",
      "iteration no 117: Loss: 0.7581049027704707, accuracy: 0.5166666666666667\n",
      "iteration no 118: Loss: 0.7580561533861624, accuracy: 0.5166666666666667\n",
      "iteration no 119: Loss: 0.7580090694467759, accuracy: 0.5166666666666667\n",
      "iteration no 120: Loss: 0.7579635897357454, accuracy: 0.5166666666666667\n",
      "iteration no 121: Loss: 0.7579196555034994, accuracy: 0.5166666666666667\n",
      "iteration no 122: Loss: 0.7578772103568742, accuracy: 0.5166666666666667\n",
      "iteration no 123: Loss: 0.7578362001540782, accuracy: 0.5166666666666667\n",
      "iteration no 124: Loss: 0.7577965729048932, accuracy: 0.5166666666666667\n",
      "iteration no 125: Loss: 0.7577582786758253, accuracy: 0.5166666666666667\n",
      "iteration no 126: Loss: 0.7577212694999258, accuracy: 0.5166666666666667\n",
      "iteration no 127: Loss: 0.7576854992910302, accuracy: 0.5166666666666667\n",
      "iteration no 128: Loss: 0.7576509237621708, accuracy: 0.5166666666666667\n",
      "iteration no 129: Loss: 0.7576175003479357, accuracy: 0.5166666666666667\n",
      "iteration no 130: Loss: 0.7575851881305649, accuracy: 0.5166666666666667\n",
      "iteration no 131: Loss: 0.7575539477695771, accuracy: 0.5133333333333333\n",
      "iteration no 132: Loss: 0.7575237414347452, accuracy: 0.5133333333333333\n",
      "iteration no 133: Loss: 0.7574945327422387, accuracy: 0.5133333333333333\n",
      "iteration no 134: Loss: 0.7574662866937678, accuracy: 0.5133333333333333\n",
      "iteration no 135: Loss: 0.7574389696185728, accuracy: 0.5133333333333333\n",
      "iteration no 136: Loss: 0.7574125491181091, accuracy: 0.5133333333333333\n",
      "iteration no 137: Loss: 0.7573869940132896, accuracy: 0.5133333333333333\n",
      "iteration no 138: Loss: 0.7573622742941533, accuracy: 0.5133333333333333\n",
      "iteration no 139: Loss: 0.7573383610718342, accuracy: 0.5133333333333333\n",
      "iteration no 140: Loss: 0.7573152265327165, accuracy: 0.5133333333333333\n",
      "iteration no 141: Loss: 0.7572928438946633, accuracy: 0.5133333333333333\n",
      "iteration no 142: Loss: 0.7572711873652145, accuracy: 0.5133333333333333\n",
      "iteration no 143: Loss: 0.7572502321016584, accuracy: 0.5133333333333333\n",
      "iteration no 144: Loss: 0.757229954172881, accuracy: 0.5133333333333333\n",
      "iteration no 145: Loss: 0.7572103305229054, accuracy: 0.5133333333333333\n",
      "iteration no 146: Loss: 0.7571913389360408, accuracy: 0.5133333333333333\n",
      "iteration no 147: Loss: 0.7571729580035599, accuracy: 0.5133333333333333\n",
      "iteration no 148: Loss: 0.7571551670918304, accuracy: 0.5133333333333333\n",
      "iteration no 149: Loss: 0.7571379463118335, accuracy: 0.5133333333333333\n",
      "iteration no 150: Loss: 0.757121276489998, accuracy: 0.5133333333333333\n",
      "iteration no 151: Loss: 0.7571051391402918, accuracy: 0.5133333333333333\n",
      "iteration no 152: Loss: 0.7570895164375065, accuracy: 0.5133333333333333\n",
      "iteration no 153: Loss: 0.7570743911916838, accuracy: 0.5133333333333333\n",
      "iteration no 154: Loss: 0.7570597468236239, accuracy: 0.5133333333333333\n",
      "iteration no 155: Loss: 0.7570455673414305, accuracy: 0.5133333333333333\n",
      "iteration no 156: Loss: 0.7570318373180426, accuracy: 0.5133333333333333\n",
      "iteration no 157: Loss: 0.757018541869704, accuracy: 0.5133333333333333\n",
      "iteration no 158: Loss: 0.7570056666353332, accuracy: 0.5133333333333333\n",
      "iteration no 159: Loss: 0.7569931977567471, accuracy: 0.5133333333333333\n",
      "iteration no 160: Loss: 0.7569811218597026, accuracy: 0.5133333333333333\n",
      "iteration no 161: Loss: 0.7569694260357175, accuracy: 0.5133333333333333\n",
      "iteration no 162: Loss: 0.7569580978246364, accuracy: 0.5133333333333333\n",
      "iteration no 163: Loss: 0.7569471251979071, accuracy: 0.5133333333333333\n",
      "iteration no 164: Loss: 0.7569364965425363, accuracy: 0.5133333333333333\n",
      "iteration no 165: Loss: 0.7569262006456946, accuracy: 0.5133333333333333\n",
      "iteration no 166: Loss: 0.7569162266799406, accuracy: 0.5133333333333333\n",
      "iteration no 167: Loss: 0.756906564189039, accuracy: 0.5133333333333333\n",
      "iteration no 168: Loss: 0.756897203074344, accuracy: 0.5133333333333333\n",
      "iteration no 169: Loss: 0.756888133581725, accuracy: 0.5133333333333333\n",
      "iteration no 170: Loss: 0.7568793462890115, accuracy: 0.5133333333333333\n",
      "iteration no 171: Loss: 0.7568708320939319, accuracy: 0.5133333333333333\n",
      "iteration no 172: Loss: 0.7568625822025281, accuracy: 0.5133333333333333\n",
      "iteration no 173: Loss: 0.7568545881180233, accuracy: 0.5133333333333333\n",
      "iteration no 174: Loss: 0.7568468416301246, accuracy: 0.5133333333333333\n",
      "iteration no 175: Loss: 0.7568393348047419, accuracy: 0.5133333333333333\n",
      "iteration no 176: Loss: 0.7568320599741035, accuracy: 0.5133333333333333\n",
      "iteration no 177: Loss: 0.7568250097272564, accuracy: 0.5133333333333333\n",
      "iteration no 178: Loss: 0.7568181769009279, accuracy: 0.5133333333333333\n",
      "iteration no 179: Loss: 0.7568115545707413, accuracy: 0.5133333333333333\n",
      "iteration no 180: Loss: 0.7568051360427642, accuracy: 0.5133333333333333\n",
      "iteration no 181: Loss: 0.7567989148453803, accuracy: 0.5133333333333333\n",
      "iteration no 182: Loss: 0.7567928847214679, accuracy: 0.5133333333333333\n",
      "iteration no 183: Loss: 0.7567870396208752, accuracy: 0.5133333333333333\n",
      "iteration no 184: Loss: 0.756781373693179, accuracy: 0.5133333333333333\n",
      "iteration no 185: Loss: 0.7567758812807133, accuracy: 0.5133333333333333\n",
      "iteration no 186: Loss: 0.756770556911862, accuracy: 0.5133333333333333\n",
      "iteration no 187: Loss: 0.7567653952945991, accuracy: 0.5133333333333333\n",
      "iteration no 188: Loss: 0.7567603913102693, accuracy: 0.5133333333333333\n",
      "iteration no 189: Loss: 0.7567555400076006, accuracy: 0.5133333333333333\n",
      "iteration no 190: Loss: 0.7567508365969359, accuracy: 0.5133333333333333\n",
      "iteration no 191: Loss: 0.7567462764446793, accuracy: 0.5133333333333333\n",
      "iteration no 192: Loss: 0.7567418550679436, accuracy: 0.5133333333333333\n",
      "iteration no 193: Loss: 0.7567375681293963, accuracy: 0.5133333333333333\n",
      "iteration no 194: Loss: 0.7567334114322909, accuracy: 0.5133333333333333\n",
      "iteration no 195: Loss: 0.7567293809156819, accuracy: 0.5133333333333333\n",
      "iteration no 196: Loss: 0.7567254726498104, accuracy: 0.5133333333333333\n",
      "iteration no 197: Loss: 0.7567216828316589, accuracy: 0.5133333333333333\n",
      "iteration no 198: Loss: 0.756718007780664, accuracy: 0.5133333333333333\n",
      "iteration no 199: Loss: 0.756714443934586, accuracy: 0.5133333333333333\n",
      "iteration no 200: Loss: 0.7567109878455239, accuracy: 0.5133333333333333\n",
      "iteration no 1: Loss: 1.0969361875117256, accuracy: 0.37\n",
      "iteration no 2: Loss: 1.0664782542427085, accuracy: 0.43666666666666665\n",
      "iteration no 3: Loss: 1.0393865434986405, accuracy: 0.47\n",
      "iteration no 4: Loss: 1.0152523383304595, accuracy: 0.48333333333333334\n",
      "iteration no 5: Loss: 0.993712520430889, accuracy: 0.4866666666666667\n",
      "iteration no 6: Loss: 0.9744474131657412, accuracy: 0.5\n",
      "iteration no 7: Loss: 0.9571774105704084, accuracy: 0.5033333333333333\n",
      "iteration no 8: Loss: 0.9416590146361438, accuracy: 0.5033333333333333\n",
      "iteration no 9: Loss: 0.9276807267825152, accuracy: 0.5066666666666667\n",
      "iteration no 10: Loss: 0.9150590769321084, accuracy: 0.5133333333333333\n",
      "iteration no 11: Loss: 0.9036349490789767, accuracy: 0.5166666666666667\n",
      "iteration no 12: Loss: 0.8932702765861369, accuracy: 0.5166666666666667\n",
      "iteration no 13: Loss: 0.8838451262032211, accuracy: 0.5166666666666667\n",
      "iteration no 14: Loss: 0.8752551581272432, accuracy: 0.52\n",
      "iteration no 15: Loss: 0.8674094328062659, accuracy: 0.5166666666666667\n",
      "iteration no 16: Loss: 0.8602285280136708, accuracy: 0.5133333333333333\n",
      "iteration no 17: Loss: 0.8536429281583929, accuracy: 0.5133333333333333\n",
      "iteration no 18: Loss: 0.8475916493439261, accuracy: 0.51\n",
      "iteration no 19: Loss: 0.842021066786906, accuracy: 0.5133333333333333\n",
      "iteration no 20: Loss: 0.8368839149211498, accuracy: 0.5133333333333333\n",
      "iteration no 21: Loss: 0.8321384343119229, accuracy: 0.5133333333333333\n",
      "iteration no 22: Loss: 0.827747643103069, accuracy: 0.5133333333333333\n",
      "iteration no 23: Loss: 0.8236787139812287, accuracy: 0.5133333333333333\n",
      "iteration no 24: Loss: 0.8199024405185542, accuracy: 0.5133333333333333\n",
      "iteration no 25: Loss: 0.816392779248441, accuracy: 0.5133333333333333\n",
      "iteration no 26: Loss: 0.8131264559632565, accuracy: 0.5133333333333333\n",
      "iteration no 27: Loss: 0.810082626535561, accuracy: 0.5133333333333333\n",
      "iteration no 28: Loss: 0.807242584094904, accuracy: 0.5133333333333333\n",
      "iteration no 29: Loss: 0.8045895056800897, accuracy: 0.5133333333333333\n",
      "iteration no 30: Loss: 0.8021082325678424, accuracy: 0.5133333333333333\n",
      "iteration no 31: Loss: 0.7997850793851066, accuracy: 0.5133333333333333\n",
      "iteration no 32: Loss: 0.7976076678716447, accuracy: 0.51\n",
      "iteration no 33: Loss: 0.7955647817959853, accuracy: 0.51\n",
      "iteration no 34: Loss: 0.7936462400613454, accuracy: 0.5133333333333333\n",
      "iteration no 35: Loss: 0.7918427854858908, accuracy: 0.5133333333333333\n",
      "iteration no 36: Loss: 0.7901459871178602, accuracy: 0.5133333333333333\n",
      "iteration no 37: Loss: 0.7885481542625041, accuracy: 0.5133333333333333\n",
      "iteration no 38: Loss: 0.7870422606643945, accuracy: 0.5133333333333333\n",
      "iteration no 39: Loss: 0.7856218775136334, accuracy: 0.5133333333333333\n",
      "iteration no 40: Loss: 0.7842811141346723, accuracy: 0.5166666666666667\n",
      "iteration no 41: Loss: 0.7830145653775121, accuracy: 0.5166666666666667\n",
      "iteration no 42: Loss: 0.7818172648676922, accuracy: 0.5166666666666667\n",
      "iteration no 43: Loss: 0.7806846433876374, accuracy: 0.5166666666666667\n",
      "iteration no 44: Loss: 0.7796124917608386, accuracy: 0.5166666666666667\n",
      "iteration no 45: Loss: 0.778596927694759, accuracy: 0.5166666666666667\n",
      "iteration no 46: Loss: 0.7776343661105091, accuracy: 0.5166666666666667\n",
      "iteration no 47: Loss: 0.7767214925491506, accuracy: 0.5166666666666667\n",
      "iteration no 48: Loss: 0.7758552392975221, accuracy: 0.5166666666666667\n",
      "iteration no 49: Loss: 0.7750327639221037, accuracy: 0.5166666666666667\n",
      "iteration no 50: Loss: 0.7742514299387089, accuracy: 0.5166666666666667\n",
      "iteration no 51: Loss: 0.7735087893797175, accuracy: 0.5166666666666667\n",
      "iteration no 52: Loss: 0.7728025670498699, accuracy: 0.5133333333333333\n",
      "iteration no 53: Loss: 0.7721306462870418, accuracy: 0.5133333333333333\n",
      "iteration no 54: Loss: 0.7714910560664558, accuracy: 0.5133333333333333\n",
      "iteration no 55: Loss: 0.7708819593059442, accuracy: 0.5133333333333333\n",
      "iteration no 56: Loss: 0.770301642246551, accuracy: 0.5133333333333333\n",
      "iteration no 57: Loss: 0.7697485047973193, accuracy: 0.5133333333333333\n",
      "iteration no 58: Loss: 0.7692210517458147, accuracy: 0.5133333333333333\n",
      "iteration no 59: Loss: 0.7687178847470698, accuracy: 0.5133333333333333\n",
      "iteration no 60: Loss: 0.7682376950133769, accuracy: 0.5133333333333333\n",
      "iteration no 61: Loss: 0.7677792566359258, accuracy: 0.5133333333333333\n",
      "iteration no 62: Loss: 0.7673414204768115, accuracy: 0.5133333333333333\n",
      "iteration no 63: Loss: 0.7669231085765618, accuracy: 0.5133333333333333\n",
      "iteration no 64: Loss: 0.7665233090281908, accuracy: 0.5133333333333333\n",
      "iteration no 65: Loss: 0.7661410712739427, accuracy: 0.5166666666666667\n",
      "iteration no 66: Loss: 0.7657755017854653, accuracy: 0.5166666666666667\n",
      "iteration no 67: Loss: 0.7654257600921952, accuracy: 0.5166666666666667\n",
      "iteration no 68: Loss: 0.7650910551263322, accuracy: 0.5166666666666667\n",
      "iteration no 69: Loss: 0.7647706418559637, accuracy: 0.5166666666666667\n",
      "iteration no 70: Loss: 0.7644638181807432, accuracy: 0.5166666666666667\n",
      "iteration no 71: Loss: 0.7641699220670453, accuracy: 0.5166666666666667\n",
      "iteration no 72: Loss: 0.7638883289017824, accuracy: 0.5166666666666667\n",
      "iteration no 73: Loss: 0.7636184490460676, accuracy: 0.5166666666666667\n",
      "iteration no 74: Loss: 0.763359725571718, accuracy: 0.5166666666666667\n",
      "iteration no 75: Loss: 0.7631116321651954, accuracy: 0.5166666666666667\n",
      "iteration no 76: Loss: 0.7628736711850219, accuracy: 0.5166666666666667\n",
      "iteration no 77: Loss: 0.7626453718600121, accuracy: 0.5166666666666667\n",
      "iteration no 78: Loss: 0.7624262886168114, accuracy: 0.5166666666666667\n",
      "iteration no 79: Loss: 0.7622159995262864, accuracy: 0.5133333333333333\n",
      "iteration no 80: Loss: 0.7620141048592488, accuracy: 0.5133333333333333\n",
      "iteration no 81: Loss: 0.7618202257428431, accuracy: 0.5133333333333333\n",
      "iteration no 82: Loss: 0.7616340029096935, accuracy: 0.5133333333333333\n",
      "iteration no 83: Loss: 0.7614550955325925, accuracy: 0.5133333333333333\n",
      "iteration no 84: Loss: 0.7612831801381461, accuracy: 0.5133333333333333\n",
      "iteration no 85: Loss: 0.7611179495933458, accuracy: 0.5133333333333333\n",
      "iteration no 86: Loss: 0.7609591121595536, accuracy: 0.5133333333333333\n",
      "iteration no 87: Loss: 0.7608063906088532, accuracy: 0.5133333333333333\n",
      "iteration no 88: Loss: 0.7606595213981342, accuracy: 0.5133333333333333\n",
      "iteration no 89: Loss: 0.760518253896665, accuracy: 0.5133333333333333\n",
      "iteration no 90: Loss: 0.7603823496632536, accuracy: 0.5133333333333333\n",
      "iteration no 91: Loss: 0.7602515817694137, accuracy: 0.5133333333333333\n",
      "iteration no 92: Loss: 0.7601257341652422, accuracy: 0.5133333333333333\n",
      "iteration no 93: Loss: 0.760004601084972, accuracy: 0.5133333333333333\n",
      "iteration no 94: Loss: 0.7598879864894121, accuracy: 0.5133333333333333\n",
      "iteration no 95: Loss: 0.7597757035426962, accuracy: 0.5133333333333333\n",
      "iteration no 96: Loss: 0.7596675741209684, accuracy: 0.5133333333333333\n",
      "iteration no 97: Loss: 0.7595634283508133, accuracy: 0.5133333333333333\n",
      "iteration no 98: Loss: 0.7594631041754049, accuracy: 0.5133333333333333\n",
      "iteration no 99: Loss: 0.759366446946506, accuracy: 0.5133333333333333\n",
      "iteration no 100: Loss: 0.7592733090405857, accuracy: 0.5133333333333333\n",
      "iteration no 101: Loss: 0.759183549497452, accuracy: 0.5133333333333333\n",
      "iteration no 102: Loss: 0.7590970336799193, accuracy: 0.5133333333333333\n",
      "iteration no 103: Loss: 0.7590136329531324, accuracy: 0.5133333333333333\n",
      "iteration no 104: Loss: 0.7589332243822763, accuracy: 0.5133333333333333\n",
      "iteration no 105: Loss: 0.7588556904474834, accuracy: 0.5166666666666667\n",
      "iteration no 106: Loss: 0.7587809187748447, accuracy: 0.5166666666666667\n",
      "iteration no 107: Loss: 0.7587088018825026, accuracy: 0.5166666666666667\n",
      "iteration no 108: Loss: 0.7586392369408761, accuracy: 0.5166666666666667\n",
      "iteration no 109: Loss: 0.7585721255461375, accuracy: 0.5166666666666667\n",
      "iteration no 110: Loss: 0.758507373506119, accuracy: 0.5166666666666667\n",
      "iteration no 111: Loss: 0.7584448906378845, accuracy: 0.5166666666666667\n",
      "iteration no 112: Loss: 0.7583845905762542, accuracy: 0.5166666666666667\n",
      "iteration no 113: Loss: 0.7583263905926206, accuracy: 0.5166666666666667\n",
      "iteration no 114: Loss: 0.758270211423431, accuracy: 0.5166666666666667\n",
      "iteration no 115: Loss: 0.7582159771077651, accuracy: 0.5166666666666667\n",
      "iteration no 116: Loss: 0.7581636148334637, accuracy: 0.5166666666666667\n",
      "iteration no 117: Loss: 0.758113054791307, accuracy: 0.5166666666666667\n",
      "iteration no 118: Loss: 0.7580642300367704, accuracy: 0.5166666666666667\n",
      "iteration no 119: Loss: 0.7580170763589159, accuracy: 0.5166666666666667\n",
      "iteration no 120: Loss: 0.7579715321560129, accuracy: 0.5166666666666667\n",
      "iteration no 121: Loss: 0.7579275383174939, accuracy: 0.5166666666666667\n",
      "iteration no 122: Loss: 0.7578850381118925, accuracy: 0.5166666666666667\n",
      "iteration no 123: Loss: 0.7578439770804197, accuracy: 0.5166666666666667\n",
      "iteration no 124: Loss: 0.7578043029358648, accuracy: 0.5166666666666667\n",
      "iteration no 125: Loss: 0.7577659654665223, accuracy: 0.5166666666666667\n",
      "iteration no 126: Loss: 0.7577289164448653, accuracy: 0.5166666666666667\n",
      "iteration no 127: Loss: 0.7576931095407056, accuracy: 0.5166666666666667\n",
      "iteration no 128: Loss: 0.7576585002385926, accuracy: 0.5166666666666667\n",
      "iteration no 129: Loss: 0.7576250457592232, accuracy: 0.5166666666666667\n",
      "iteration no 130: Loss: 0.7575927049846404, accuracy: 0.5166666666666667\n",
      "iteration no 131: Loss: 0.7575614383870253, accuracy: 0.5166666666666667\n",
      "iteration no 132: Loss: 0.757531207960881, accuracy: 0.5166666666666667\n",
      "iteration no 133: Loss: 0.7575019771584363, accuracy: 0.5166666666666667\n",
      "iteration no 134: Loss: 0.7574737108280933, accuracy: 0.5133333333333333\n",
      "iteration no 135: Loss: 0.7574463751557645, accuracy: 0.5133333333333333\n",
      "iteration no 136: Loss: 0.7574199376089431, accuracy: 0.5133333333333333\n",
      "iteration no 137: Loss: 0.7573943668833691, accuracy: 0.5133333333333333\n",
      "iteration no 138: Loss: 0.7573696328521579, accuracy: 0.5133333333333333\n",
      "iteration no 139: Loss: 0.7573457065172584, accuracy: 0.5133333333333333\n",
      "iteration no 140: Loss: 0.7573225599631318, accuracy: 0.5133333333333333\n",
      "iteration no 141: Loss: 0.7573001663125298, accuracy: 0.5133333333333333\n",
      "iteration no 142: Loss: 0.7572784996842737, accuracy: 0.5133333333333333\n",
      "iteration no 143: Loss: 0.7572575351529272, accuracy: 0.5133333333333333\n",
      "iteration no 144: Loss: 0.7572372487102741, accuracy: 0.5133333333333333\n",
      "iteration no 145: Loss: 0.7572176172285089, accuracy: 0.5133333333333333\n",
      "iteration no 146: Loss: 0.7571986184250575, accuracy: 0.5133333333333333\n",
      "iteration no 147: Loss: 0.7571802308289444, accuracy: 0.5133333333333333\n",
      "iteration no 148: Loss: 0.7571624337486365, accuracy: 0.5133333333333333\n",
      "iteration no 149: Loss: 0.7571452072412875, accuracy: 0.5133333333333333\n",
      "iteration no 150: Loss: 0.757128532083318, accuracy: 0.5133333333333333\n",
      "iteration no 151: Loss: 0.7571123897422649, accuracy: 0.5133333333333333\n",
      "iteration no 152: Loss: 0.7570967623498421, accuracy: 0.5133333333333333\n",
      "iteration no 153: Loss: 0.757081632676153, accuracy: 0.5133333333333333\n",
      "iteration no 154: Loss: 0.7570669841050018, accuracy: 0.5133333333333333\n",
      "iteration no 155: Loss: 0.7570528006102504, accuracy: 0.5133333333333333\n",
      "iteration no 156: Loss: 0.7570390667331736, accuracy: 0.5133333333333333\n",
      "iteration no 157: Loss: 0.7570257675607657, accuracy: 0.5133333333333333\n",
      "iteration no 158: Loss: 0.7570128887049535, accuracy: 0.5133333333333333\n",
      "iteration no 159: Loss: 0.7570004162826757, accuracy: 0.5133333333333333\n",
      "iteration no 160: Loss: 0.756988336896787, accuracy: 0.5133333333333333\n",
      "iteration no 161: Loss: 0.7569766376177518, accuracy: 0.5133333333333333\n",
      "iteration no 162: Loss: 0.756965305966088, accuracy: 0.5133333333333333\n",
      "iteration no 163: Loss: 0.7569543298955314, accuracy: 0.5133333333333333\n",
      "iteration no 164: Loss: 0.7569436977768849, accuracy: 0.5133333333333333\n",
      "iteration no 165: Loss: 0.7569333983825238, accuracy: 0.5133333333333333\n",
      "iteration no 166: Loss: 0.7569234208715262, accuracy: 0.5133333333333333\n",
      "iteration no 167: Loss: 0.7569137547754027, accuracy: 0.5133333333333333\n",
      "iteration no 168: Loss: 0.7569043899843986, accuracy: 0.5133333333333333\n",
      "iteration no 169: Loss: 0.7568953167343412, accuracy: 0.5133333333333333\n",
      "iteration no 170: Loss: 0.756886525594012, accuracy: 0.5133333333333333\n",
      "iteration no 171: Loss: 0.7568780074530184, accuracy: 0.5133333333333333\n",
      "iteration no 172: Loss: 0.7568697535101431, accuracy: 0.5133333333333333\n",
      "iteration no 173: Loss: 0.7568617552621528, accuracy: 0.5133333333333333\n",
      "iteration no 174: Loss: 0.7568540044930444, accuracy: 0.5133333333333333\n",
      "iteration no 175: Loss: 0.7568464932637109, accuracy: 0.5133333333333333\n",
      "iteration no 176: Loss: 0.7568392139020084, accuracy: 0.5133333333333333\n",
      "iteration no 177: Loss: 0.7568321589932081, accuracy: 0.5133333333333333\n",
      "iteration no 178: Loss: 0.7568253213708172, accuracy: 0.5133333333333333\n",
      "iteration no 179: Loss: 0.7568186941077517, accuracy: 0.5133333333333333\n",
      "iteration no 180: Loss: 0.7568122705078478, accuracy: 0.5133333333333333\n",
      "iteration no 181: Loss: 0.7568060440976974, accuracy: 0.5133333333333333\n",
      "iteration no 182: Loss: 0.756800008618793, accuracy: 0.5133333333333333\n",
      "iteration no 183: Loss: 0.7567941580199727, accuracy: 0.5133333333333333\n",
      "iteration no 184: Loss: 0.756788486450148, accuracy: 0.5133333333333333\n",
      "iteration no 185: Loss: 0.756782988251307, accuracy: 0.5133333333333333\n",
      "iteration no 186: Loss: 0.7567776579517796, accuracy: 0.5133333333333333\n",
      "iteration no 187: Loss: 0.7567724902597545, accuracy: 0.5133333333333333\n",
      "iteration no 188: Loss: 0.7567674800570374, accuracy: 0.5133333333333333\n",
      "iteration no 189: Loss: 0.7567626223930425, accuracy: 0.5133333333333333\n",
      "iteration no 190: Loss: 0.7567579124790047, accuracy: 0.5133333333333333\n",
      "iteration no 191: Loss: 0.756753345682407, accuracy: 0.5133333333333333\n",
      "iteration no 192: Loss: 0.756748917521612, accuracy: 0.5133333333333333\n",
      "iteration no 193: Loss: 0.7567446236606906, accuracy: 0.5133333333333333\n",
      "iteration no 194: Loss: 0.7567404599044403, accuracy: 0.5133333333333333\n",
      "iteration no 195: Loss: 0.756736422193584, accuracy: 0.5133333333333333\n",
      "iteration no 196: Loss: 0.7567325066001445, accuracy: 0.5133333333333333\n",
      "iteration no 197: Loss: 0.7567287093229861, accuracy: 0.5133333333333333\n",
      "iteration no 198: Loss: 0.7567250266835178, accuracy: 0.5133333333333333\n",
      "iteration no 199: Loss: 0.7567214551215503, accuracy: 0.5133333333333333\n",
      "iteration no 200: Loss: 0.7567179911913028, accuracy: 0.5166666666666667\n",
      "iteration no 1: Loss: 1.080844600839268, accuracy: 0.5133333333333333\n",
      "iteration no 2: Loss: 1.052099618704406, accuracy: 0.5166666666666667\n",
      "iteration no 3: Loss: 1.026506105143483, accuracy: 0.5166666666666667\n",
      "iteration no 4: Loss: 1.0036779478813747, accuracy: 0.5133333333333333\n",
      "iteration no 5: Loss: 0.9832783601928012, accuracy: 0.5066666666666667\n",
      "iteration no 6: Loss: 0.9650123313291703, accuracy: 0.5133333333333333\n",
      "iteration no 7: Loss: 0.9486215910933778, accuracy: 0.5166666666666667\n",
      "iteration no 8: Loss: 0.9338805017210783, accuracy: 0.5166666666666667\n",
      "iteration no 9: Loss: 0.9205923399851491, accuracy: 0.5233333333333333\n",
      "iteration no 10: Loss: 0.9085858412337701, accuracy: 0.5233333333333333\n",
      "iteration no 11: Loss: 0.8977120090553555, accuracy: 0.5233333333333333\n",
      "iteration no 12: Loss: 0.8878412178272195, accuracy: 0.5233333333333333\n",
      "iteration no 13: Loss: 0.8788606239077852, accuracy: 0.52\n",
      "iteration no 14: Loss: 0.8706718837256613, accuracy: 0.5166666666666667\n",
      "iteration no 15: Loss: 0.8631891631090924, accuracy: 0.52\n",
      "iteration no 16: Loss: 0.8563374137034726, accuracy: 0.5233333333333333\n",
      "iteration no 17: Loss: 0.850050888413784, accuracy: 0.5166666666666667\n",
      "iteration no 18: Loss: 0.8442718670870685, accuracy: 0.5133333333333333\n",
      "iteration no 19: Loss: 0.8389495648794711, accuracy: 0.5133333333333333\n",
      "iteration no 20: Loss: 0.8340391980262261, accuracy: 0.5133333333333333\n",
      "iteration no 21: Loss: 0.8295011844519179, accuracy: 0.5133333333333333\n",
      "iteration no 22: Loss: 0.8253004594559954, accuracy: 0.5133333333333333\n",
      "iteration no 23: Loss: 0.8214058893780652, accuracy: 0.5166666666666667\n",
      "iteration no 24: Loss: 0.8177897685849562, accuracy: 0.5166666666666667\n",
      "iteration no 25: Loss: 0.8144273872858553, accuracy: 0.5166666666666667\n",
      "iteration no 26: Loss: 0.8112966595683601, accuracy: 0.5166666666666667\n",
      "iteration no 27: Loss: 0.8083778026721974, accuracy: 0.5166666666666667\n",
      "iteration no 28: Loss: 0.8056530599031712, accuracy: 0.5166666666666667\n",
      "iteration no 29: Loss: 0.8031064607656055, accuracy: 0.5166666666666667\n",
      "iteration no 30: Loss: 0.8007236128850503, accuracy: 0.5166666666666667\n",
      "iteration no 31: Loss: 0.7984915211304223, accuracy: 0.5166666666666667\n",
      "iteration no 32: Loss: 0.7963984300495766, accuracy: 0.5166666666666667\n",
      "iteration no 33: Loss: 0.7944336863250766, accuracy: 0.5166666666666667\n",
      "iteration no 34: Loss: 0.7925876184554771, accuracy: 0.5166666666666667\n",
      "iteration no 35: Loss: 0.7908514312868568, accuracy: 0.5166666666666667\n",
      "iteration no 36: Loss: 0.7892171133724651, accuracy: 0.5166666666666667\n",
      "iteration no 37: Loss: 0.7876773554359722, accuracy: 0.5166666666666667\n",
      "iteration no 38: Loss: 0.7862254784649437, accuracy: 0.5166666666666667\n",
      "iteration no 39: Loss: 0.7848553701733939, accuracy: 0.5166666666666667\n",
      "iteration no 40: Loss: 0.7835614287518602, accuracy: 0.5166666666666667\n",
      "iteration no 41: Loss: 0.782338512975708, accuracy: 0.5166666666666667\n",
      "iteration no 42: Loss: 0.7811818978716425, accuracy: 0.5166666666666667\n",
      "iteration no 43: Loss: 0.7800872352523857, accuracy: 0.5166666666666667\n",
      "iteration no 44: Loss: 0.7790505185231846, accuracy: 0.5166666666666667\n",
      "iteration no 45: Loss: 0.7780680512438224, accuracy: 0.5133333333333333\n",
      "iteration no 46: Loss: 0.7771364189982327, accuracy: 0.5133333333333333\n",
      "iteration no 47: Loss: 0.7762524641824492, accuracy: 0.5133333333333333\n",
      "iteration no 48: Loss: 0.7754132633719544, accuracy: 0.5133333333333333\n",
      "iteration no 49: Loss: 0.7746161069727816, accuracy: 0.5133333333333333\n",
      "iteration no 50: Loss: 0.7738584808980205, accuracy: 0.5133333333333333\n",
      "iteration no 51: Loss: 0.7731380500435651, accuracy: 0.5133333333333333\n",
      "iteration no 52: Loss: 0.7724526433647905, accuracy: 0.5133333333333333\n",
      "iteration no 53: Loss: 0.7718002403799489, accuracy: 0.5133333333333333\n",
      "iteration no 54: Loss: 0.7711789589470063, accuracy: 0.5166666666666667\n",
      "iteration no 55: Loss: 0.7705870441788346, accuracy: 0.5166666666666667\n",
      "iteration no 56: Loss: 0.7700228583775044, accuracy: 0.5166666666666667\n",
      "iteration no 57: Loss: 0.7694848718822489, accuracy: 0.5166666666666667\n",
      "iteration no 58: Loss: 0.7689716547377319, accuracy: 0.5166666666666667\n",
      "iteration no 59: Loss: 0.7684818690998201, accuracy: 0.5166666666666667\n",
      "iteration no 60: Loss: 0.7680142623053132, accuracy: 0.5166666666666667\n",
      "iteration no 61: Loss: 0.7675676605402175, accuracy: 0.5166666666666667\n",
      "iteration no 62: Loss: 0.7671409630482903, accuracy: 0.5166666666666667\n",
      "iteration no 63: Loss: 0.7667331368278733, accuracy: 0.5166666666666667\n",
      "iteration no 64: Loss: 0.7663432117705884, accuracy: 0.5166666666666667\n",
      "iteration no 65: Loss: 0.765970276200363, accuracy: 0.5166666666666667\n",
      "iteration no 66: Loss: 0.7656134727755872, accuracy: 0.5166666666666667\n",
      "iteration no 67: Loss: 0.7652719947210486, accuracy: 0.5166666666666667\n",
      "iteration no 68: Loss: 0.7649450823596889, accuracy: 0.5166666666666667\n",
      "iteration no 69: Loss: 0.7646320199172549, accuracy: 0.5166666666666667\n",
      "iteration no 70: Loss: 0.7643321325755998, accuracy: 0.5166666666666667\n",
      "iteration no 71: Loss: 0.7640447837527943, accuracy: 0.5166666666666667\n",
      "iteration no 72: Loss: 0.7637693725903288, accuracy: 0.5166666666666667\n",
      "iteration no 73: Loss: 0.7635053316296071, accuracy: 0.5166666666666667\n",
      "iteration no 74: Loss: 0.7632521246616257, accuracy: 0.5166666666666667\n",
      "iteration no 75: Loss: 0.7630092447352639, accuracy: 0.5166666666666667\n",
      "iteration no 76: Loss: 0.7627762123109677, accuracy: 0.5166666666666667\n",
      "iteration no 77: Loss: 0.7625525735478482, accuracy: 0.5166666666666667\n",
      "iteration no 78: Loss: 0.7623378987133013, accuracy: 0.5166666666666667\n",
      "iteration no 79: Loss: 0.7621317807052554, accuracy: 0.5166666666666667\n",
      "iteration no 80: Loss: 0.76193383367804, accuracy: 0.5166666666666667\n",
      "iteration no 81: Loss: 0.76174369176367, accuracy: 0.5133333333333333\n",
      "iteration no 82: Loss: 0.7615610078810653, accuracy: 0.5133333333333333\n",
      "iteration no 83: Loss: 0.7613854526263796, accuracy: 0.5133333333333333\n",
      "iteration no 84: Loss: 0.7612167132382, accuracy: 0.5133333333333333\n",
      "iteration no 85: Loss: 0.7610544926319156, accuracy: 0.5133333333333333\n",
      "iteration no 86: Loss: 0.7608985084980386, accuracy: 0.5133333333333333\n",
      "iteration no 87: Loss: 0.7607484924596951, accuracy: 0.5133333333333333\n",
      "iteration no 88: Loss: 0.7606041892849067, accuracy: 0.5133333333333333\n",
      "iteration no 89: Loss: 0.7604653561496428, accuracy: 0.5133333333333333\n",
      "iteration no 90: Loss: 0.7603317619479505, accuracy: 0.5133333333333333\n",
      "iteration no 91: Loss: 0.7602031866457712, accuracy: 0.5133333333333333\n",
      "iteration no 92: Loss: 0.7600794206753267, accuracy: 0.5133333333333333\n",
      "iteration no 93: Loss: 0.7599602643671968, accuracy: 0.5133333333333333\n",
      "iteration no 94: Loss: 0.7598455274174524, accuracy: 0.5133333333333333\n",
      "iteration no 95: Loss: 0.7597350283873965, accuracy: 0.5133333333333333\n",
      "iteration no 96: Loss: 0.7596285942336732, accuracy: 0.5133333333333333\n",
      "iteration no 97: Loss: 0.7595260598666616, accuracy: 0.5133333333333333\n",
      "iteration no 98: Loss: 0.7594272677352393, accuracy: 0.5133333333333333\n",
      "iteration no 99: Loss: 0.7593320674361429, accuracy: 0.5133333333333333\n",
      "iteration no 100: Loss: 0.7592403153462839, accuracy: 0.5133333333333333\n",
      "iteration no 101: Loss: 0.7591518742765033, accuracy: 0.5166666666666667\n",
      "iteration no 102: Loss: 0.759066613145355, accuracy: 0.5166666666666667\n",
      "iteration no 103: Loss: 0.75898440667162, accuracy: 0.5166666666666667\n",
      "iteration no 104: Loss: 0.7589051350843359, accuracy: 0.5166666666666667\n",
      "iteration no 105: Loss: 0.7588286838492234, accuracy: 0.5166666666666667\n",
      "iteration no 106: Loss: 0.7587549434104678, accuracy: 0.5166666666666667\n",
      "iteration no 107: Loss: 0.7586838089468836, accuracy: 0.5166666666666667\n",
      "iteration no 108: Loss: 0.7586151801415661, accuracy: 0.5166666666666667\n",
      "iteration no 109: Loss: 0.7585489609641906, accuracy: 0.5166666666666667\n",
      "iteration no 110: Loss: 0.7584850594651764, accuracy: 0.5166666666666667\n",
      "iteration no 111: Loss: 0.758423387580996, accuracy: 0.5166666666666667\n",
      "iteration no 112: Loss: 0.7583638609499468, accuracy: 0.5166666666666667\n",
      "iteration no 113: Loss: 0.7583063987377571, accuracy: 0.5166666666666667\n",
      "iteration no 114: Loss: 0.758250923472436, accuracy: 0.5166666666666667\n",
      "iteration no 115: Loss: 0.7581973608878211, accuracy: 0.5166666666666667\n",
      "iteration no 116: Loss: 0.758145639775307, accuracy: 0.5166666666666667\n",
      "iteration no 117: Loss: 0.7580956918432762, accuracy: 0.5166666666666667\n",
      "iteration no 118: Loss: 0.7580474515837863, accuracy: 0.5166666666666667\n",
      "iteration no 119: Loss: 0.7580008561460913, accuracy: 0.5166666666666667\n",
      "iteration no 120: Loss: 0.7579558452166073, accuracy: 0.5166666666666667\n",
      "iteration no 121: Loss: 0.7579123609049526, accuracy: 0.5166666666666667\n",
      "iteration no 122: Loss: 0.7578703476357209, accuracy: 0.5166666666666667\n",
      "iteration no 123: Loss: 0.7578297520456634, accuracy: 0.5166666666666667\n",
      "iteration no 124: Loss: 0.7577905228859788, accuracy: 0.5166666666666667\n",
      "iteration no 125: Loss: 0.7577526109294263, accuracy: 0.5166666666666667\n",
      "iteration no 126: Loss: 0.7577159688819971, accuracy: 0.5166666666666667\n",
      "iteration no 127: Loss: 0.7576805512988936, accuracy: 0.5166666666666667\n",
      "iteration no 128: Loss: 0.7576463145045824, accuracy: 0.5133333333333333\n",
      "iteration no 129: Loss: 0.7576132165167011, accuracy: 0.5133333333333333\n",
      "iteration no 130: Loss: 0.7575812169736099, accuracy: 0.5133333333333333\n",
      "iteration no 131: Loss: 0.7575502770653953, accuracy: 0.5133333333333333\n",
      "iteration no 132: Loss: 0.7575203594681431, accuracy: 0.5133333333333333\n",
      "iteration no 133: Loss: 0.7574914282813042, accuracy: 0.5133333333333333\n",
      "iteration no 134: Loss: 0.7574634489679969, accuracy: 0.5133333333333333\n",
      "iteration no 135: Loss: 0.7574363882980875, accuracy: 0.5133333333333333\n",
      "iteration no 136: Loss: 0.7574102142939081, accuracy: 0.5133333333333333\n",
      "iteration no 137: Loss: 0.7573848961784742, accuracy: 0.5133333333333333\n",
      "iteration no 138: Loss: 0.7573604043260762, accuracy: 0.5133333333333333\n",
      "iteration no 139: Loss: 0.7573367102151217, accuracy: 0.5133333333333333\n",
      "iteration no 140: Loss: 0.7573137863831145, accuracy: 0.5133333333333333\n",
      "iteration no 141: Loss: 0.7572916063836652, accuracy: 0.5133333333333333\n",
      "iteration no 142: Loss: 0.7572701447454311, accuracy: 0.5133333333333333\n",
      "iteration no 143: Loss: 0.7572493769328851, accuracy: 0.5133333333333333\n",
      "iteration no 144: Loss: 0.7572292793088312, accuracy: 0.5133333333333333\n",
      "iteration no 145: Loss: 0.7572098290985719, accuracy: 0.5133333333333333\n",
      "iteration no 146: Loss: 0.7571910043556541, accuracy: 0.5133333333333333\n",
      "iteration no 147: Loss: 0.75717278392911, accuracy: 0.5133333333333333\n",
      "iteration no 148: Loss: 0.7571551474321275, accuracy: 0.5133333333333333\n",
      "iteration no 149: Loss: 0.7571380752120751, accuracy: 0.5133333333333333\n",
      "iteration no 150: Loss: 0.7571215483218201, accuracy: 0.5133333333333333\n",
      "iteration no 151: Loss: 0.7571055484922777, accuracy: 0.5133333333333333\n",
      "iteration no 152: Loss: 0.7570900581061311, accuracy: 0.5133333333333333\n",
      "iteration no 153: Loss: 0.7570750601726702, accuracy: 0.5133333333333333\n",
      "iteration no 154: Loss: 0.7570605383036941, accuracy: 0.5133333333333333\n",
      "iteration no 155: Loss: 0.7570464766904287, accuracy: 0.5133333333333333\n",
      "iteration no 156: Loss: 0.7570328600814117, accuracy: 0.5133333333333333\n",
      "iteration no 157: Loss: 0.7570196737613011, accuracy: 0.5133333333333333\n",
      "iteration no 158: Loss: 0.7570069035305645, accuracy: 0.5133333333333333\n",
      "iteration no 159: Loss: 0.7569945356860086, accuracy: 0.5133333333333333\n",
      "iteration no 160: Loss: 0.7569825570021099, accuracy: 0.5133333333333333\n",
      "iteration no 161: Loss: 0.7569709547131132, accuracy: 0.5133333333333333\n",
      "iteration no 162: Loss: 0.7569597164958587, accuracy: 0.5133333333333333\n",
      "iteration no 163: Loss: 0.7569488304533101, accuracy: 0.5133333333333333\n",
      "iteration no 164: Loss: 0.7569382850987476, accuracy: 0.5133333333333333\n",
      "iteration no 165: Loss: 0.7569280693406004, accuracy: 0.5133333333333333\n",
      "iteration no 166: Loss: 0.7569181724678875, accuracy: 0.5133333333333333\n",
      "iteration no 167: Loss: 0.7569085841362407, accuracy: 0.5133333333333333\n",
      "iteration no 168: Loss: 0.7568992943544857, accuracy: 0.5133333333333333\n",
      "iteration no 169: Loss: 0.7568902934717548, accuracy: 0.5133333333333333\n",
      "iteration no 170: Loss: 0.7568815721651102, accuracy: 0.5133333333333333\n",
      "iteration no 171: Loss: 0.7568731214276528, accuracy: 0.5133333333333333\n",
      "iteration no 172: Loss: 0.7568649325571007, accuracy: 0.5133333333333333\n",
      "iteration no 173: Loss: 0.7568569971448107, accuracy: 0.5133333333333333\n",
      "iteration no 174: Loss: 0.7568493070652298, accuracy: 0.5133333333333333\n",
      "iteration no 175: Loss: 0.7568418544657542, accuracy: 0.5133333333333333\n",
      "iteration no 176: Loss: 0.7568346317569821, accuracy: 0.5133333333333333\n",
      "iteration no 177: Loss: 0.75682763160334, accuracy: 0.5133333333333333\n",
      "iteration no 178: Loss: 0.7568208469140696, accuracy: 0.5133333333333333\n",
      "iteration no 179: Loss: 0.7568142708345599, accuracy: 0.5133333333333333\n",
      "iteration no 180: Loss: 0.7568078967380082, accuracy: 0.5133333333333333\n",
      "iteration no 181: Loss: 0.7568017182173987, accuracy: 0.5133333333333333\n",
      "iteration no 182: Loss: 0.7567957290777849, accuracy: 0.5133333333333333\n",
      "iteration no 183: Loss: 0.7567899233288615, accuracy: 0.5133333333333333\n",
      "iteration no 184: Loss: 0.7567842951778176, accuracy: 0.5133333333333333\n",
      "iteration no 185: Loss: 0.7567788390224557, accuracy: 0.5133333333333333\n",
      "iteration no 186: Loss: 0.7567735494445684, accuracy: 0.5133333333333333\n",
      "iteration no 187: Loss: 0.7567684212035622, accuracy: 0.5133333333333333\n",
      "iteration no 188: Loss: 0.7567634492303164, accuracy: 0.5133333333333333\n",
      "iteration no 189: Loss: 0.7567586286212703, accuracy: 0.5133333333333333\n",
      "iteration no 190: Loss: 0.7567539546327278, accuracy: 0.5133333333333333\n",
      "iteration no 191: Loss: 0.7567494226753706, accuracy: 0.5133333333333333\n",
      "iteration no 192: Loss: 0.7567450283089746, accuracy: 0.5133333333333333\n",
      "iteration no 193: Loss: 0.7567407672373161, accuracy: 0.5133333333333333\n",
      "iteration no 194: Loss: 0.7567366353032662, accuracy: 0.5133333333333333\n",
      "iteration no 195: Loss: 0.7567326284840612, accuracy: 0.5133333333333333\n",
      "iteration no 196: Loss: 0.7567287428867462, accuracy: 0.5133333333333333\n",
      "iteration no 197: Loss: 0.756724974743781, accuracy: 0.5133333333333333\n",
      "iteration no 198: Loss: 0.7567213204088056, accuracy: 0.5133333333333333\n",
      "iteration no 199: Loss: 0.7567177763525572, accuracy: 0.5133333333333333\n",
      "iteration no 200: Loss: 0.7567143391589336, accuracy: 0.5133333333333333\n",
      "25.8 ms  502 s per loop (mean  std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "# initialize the weights\n",
    "W = 0.1*np.random.randn(D,K)\n",
    "b = np.zeros((1,K))\n",
    "\n",
    "# X dimension = (N*K,D)\n",
    "# y dimension = (N*K,1)\n",
    "n_examples = X.shape[0]\n",
    "\n",
    "# declaring the hyperparameters\n",
    "step_size = 1\n",
    "reg = 1e-3\n",
    "iterations = 200\n",
    "\n",
    "# gradient descent loop\n",
    "for iter in range(iterations):\n",
    "\n",
    "    # calculate the scores\n",
    "    scores = np.dot(X, W) + b\n",
    "    \n",
    "    # apply softmax\n",
    "    exp_scores = np.exp(scores)\n",
    "    probs = exp_scores/np.sum(exp_scores, axis=1, keepdims=True)\n",
    "\n",
    "    # calculate loss\n",
    "    loss = -np.sum(np.log(probs[range(n_examples), y]))/n_examples + 0.5*reg*np.sum(W*W)\n",
    "\n",
    "    # calculate gradients\n",
    "    dscores = probs\n",
    "    dscores[range(n_examples), y] -= 1\n",
    "    dscores = dscores/n_examples \n",
    "\n",
    "    db = np.sum(dscores, axis=0, keepdims=True)\n",
    "    dW = np.dot(X.T, dscores) + reg*W \n",
    "\n",
    "    # perform gradient descent\n",
    "    W = W - step_size*dW\n",
    "    b = b - step_size*db\n",
    "\n",
    "    #check accuracy\n",
    "    scores = np.dot(X, W) + b\n",
    "    predictions = np.argmax(scores, axis=1)\n",
    "    accuracy = np.sum(predictions == y)/n_examples\n",
    "\n",
    "    print(f'iteration no {iter+1}: Loss: {loss}, accuracy: {accuracy}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<center><b>ACHIEVED ACCURACY OF 51.3% </b><center>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

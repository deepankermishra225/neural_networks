{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><b>TRAINING A NEURAL NETWORK USING NUMPY</b><br><center>\n",
    "<hr>\n",
    "<ul>\n",
    "<li>https://cs231n.github.io/neural-networks-case-study/#grad</li>\n",
    "<li>https://dustinstansbury.github.io/theclevermachine/derivation-backpropagation</li>\n",
    "<li>https://ai.plainenglish.io/gradient-descent-update-rule-for-multiclass-logistic-regression-4bf3033cac10</li>\n",
    "<li>https://blog.yani.ai/backpropagation/</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><b>GENERATING SAMPLE DATA</b></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "N = 100 # number of points per class \n",
    "D = 2 # dimensionality\n",
    "K = 3 # number of classes\n",
    "\n",
    "X = np.zeros((N*K,D))\n",
    "y = np.zeros(N*K, dtype='uint8')\n",
    "for j in range(K):\n",
    "  ix = range(N*j,N*(j+1))\n",
    "  r = np.linspace(0.0,1,N) # radius\n",
    "  t = np.linspace(j*4,(j+1)*4,N) + np.random.randn(N)*0.2 # theta\n",
    "  X[ix] = np.c_[r*np.sin(t), r*np.cos(t)]\n",
    "  y[ix] = j\n",
    "\n",
    "## visualize data\n",
    "data = {\n",
    "    'x1': X[:,0],\n",
    "    'x2': X[:,1],\n",
    "    'y': y\n",
    "}\n",
    "plt.rcParams['axes.facecolor'] = '#bea3c2'\n",
    "sns.scatterplot(x='x1', y='x2', hue='y', data=data)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><b>TRAINING A NEURAL NETWORK WITH ONE HIDDEN LAYER</b></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration no 1: Loss: 1.0987848703018153, accuracy: 0.19\n",
      "iteration no 2: Loss: 1.0986448475539188, accuracy: 0.39666666666666667\n",
      "iteration no 3: Loss: 1.0985036431928528, accuracy: 0.55\n",
      "iteration no 4: Loss: 1.0983668409767626, accuracy: 0.5766666666666667\n",
      "iteration no 5: Loss: 1.0982318120255623, accuracy: 0.5733333333333334\n",
      "iteration no 6: Loss: 1.0980952397470594, accuracy: 0.5766666666666667\n",
      "iteration no 7: Loss: 1.097958847518014, accuracy: 0.57\n",
      "iteration no 8: Loss: 1.0978177990312767, accuracy: 0.5566666666666666\n",
      "iteration no 9: Loss: 1.0976703760049353, accuracy: 0.5566666666666666\n",
      "iteration no 10: Loss: 1.0975144720236343, accuracy: 0.5433333333333333\n",
      "iteration no 11: Loss: 1.0973464782152982, accuracy: 0.5466666666666666\n",
      "iteration no 12: Loss: 1.09716450181197, accuracy: 0.5366666666666666\n",
      "iteration no 13: Loss: 1.096965330974789, accuracy: 0.5333333333333333\n",
      "iteration no 14: Loss: 1.0967469410733028, accuracy: 0.5266666666666666\n",
      "iteration no 15: Loss: 1.0965077800934944, accuracy: 0.53\n",
      "iteration no 16: Loss: 1.0962454224403897, accuracy: 0.53\n",
      "iteration no 17: Loss: 1.095956581559059, accuracy: 0.52\n",
      "iteration no 18: Loss: 1.0956373231422203, accuracy: 0.52\n",
      "iteration no 19: Loss: 1.0952853000673475, accuracy: 0.52\n",
      "iteration no 20: Loss: 1.0948963507280953, accuracy: 0.52\n",
      "iteration no 21: Loss: 1.0944675895703233, accuracy: 0.52\n",
      "iteration no 22: Loss: 1.093993399701576, accuracy: 0.5166666666666667\n",
      "iteration no 23: Loss: 1.0934693405125506, accuracy: 0.5166666666666667\n",
      "iteration no 24: Loss: 1.0928891310399313, accuracy: 0.5166666666666667\n",
      "iteration no 25: Loss: 1.0922486878026874, accuracy: 0.5133333333333333\n",
      "iteration no 26: Loss: 1.0915420721653493, accuracy: 0.5133333333333333\n",
      "iteration no 27: Loss: 1.0907626832162782, accuracy: 0.51\n",
      "iteration no 28: Loss: 1.0899009480643762, accuracy: 0.51\n",
      "iteration no 29: Loss: 1.0889476181992337, accuracy: 0.5033333333333333\n",
      "iteration no 30: Loss: 1.0878929814802454, accuracy: 0.5033333333333333\n",
      "iteration no 31: Loss: 1.08672703429525, accuracy: 0.5\n",
      "iteration no 32: Loss: 1.085442030427188, accuracy: 0.5\n",
      "iteration no 33: Loss: 1.0840278430174404, accuracy: 0.5033333333333333\n",
      "iteration no 34: Loss: 1.0824710376113715, accuracy: 0.5066666666666667\n",
      "iteration no 35: Loss: 1.0807583901379738, accuracy: 0.5066666666666667\n",
      "iteration no 36: Loss: 1.0788755649601482, accuracy: 0.5\n",
      "iteration no 37: Loss: 1.076808757994273, accuracy: 0.5\n",
      "iteration no 38: Loss: 1.0745416708332551, accuracy: 0.5\n",
      "iteration no 39: Loss: 1.0720580038086926, accuracy: 0.5\n",
      "iteration no 40: Loss: 1.0693405792266522, accuracy: 0.5033333333333333\n",
      "iteration no 41: Loss: 1.0663723671080942, accuracy: 0.5\n",
      "iteration no 42: Loss: 1.0631348882003322, accuracy: 0.5\n",
      "iteration no 43: Loss: 1.0596101022571647, accuracy: 0.5\n",
      "iteration no 44: Loss: 1.0557810064843123, accuracy: 0.5\n",
      "iteration no 45: Loss: 1.051630282598447, accuracy: 0.5\n",
      "iteration no 46: Loss: 1.0471422968983792, accuracy: 0.5\n",
      "iteration no 47: Loss: 1.0423024058156842, accuracy: 0.5033333333333333\n",
      "iteration no 48: Loss: 1.037098057512912, accuracy: 0.5066666666666667\n",
      "iteration no 49: Loss: 1.0315194914636314, accuracy: 0.5066666666666667\n",
      "iteration no 50: Loss: 1.0255611096195503, accuracy: 0.5033333333333333\n",
      "iteration no 51: Loss: 1.0192198896972489, accuracy: 0.5033333333333333\n",
      "iteration no 52: Loss: 1.012497232524489, accuracy: 0.51\n",
      "iteration no 53: Loss: 1.0053994781803697, accuracy: 0.5133333333333333\n",
      "iteration no 54: Loss: 0.9979382623545877, accuracy: 0.5133333333333333\n",
      "iteration no 55: Loss: 0.9901304671283429, accuracy: 0.5133333333333333\n",
      "iteration no 56: Loss: 0.9819991189729929, accuracy: 0.5133333333333333\n",
      "iteration no 57: Loss: 0.9735714512831646, accuracy: 0.5133333333333333\n",
      "iteration no 58: Loss: 0.9648807203790297, accuracy: 0.5166666666666667\n",
      "iteration no 59: Loss: 0.9559658984774358, accuracy: 0.5133333333333333\n",
      "iteration no 60: Loss: 0.9468700428967897, accuracy: 0.5133333333333333\n",
      "iteration no 61: Loss: 0.9376370607976301, accuracy: 0.5166666666666667\n",
      "iteration no 62: Loss: 0.9283127813855157, accuracy: 0.5166666666666667\n",
      "iteration no 63: Loss: 0.9189440031123605, accuracy: 0.52\n",
      "iteration no 64: Loss: 0.9095783565180737, accuracy: 0.5166666666666667\n",
      "iteration no 65: Loss: 0.9002624765921617, accuracy: 0.5233333333333333\n",
      "iteration no 66: Loss: 0.891041242146893, accuracy: 0.52\n",
      "iteration no 67: Loss: 0.8819555856740925, accuracy: 0.52\n",
      "iteration no 68: Loss: 0.8730430687018469, accuracy: 0.5233333333333333\n",
      "iteration no 69: Loss: 0.8643336748105961, accuracy: 0.5233333333333333\n",
      "iteration no 70: Loss: 0.8558587325829137, accuracy: 0.5166666666666667\n",
      "iteration no 71: Loss: 0.8476458609711849, accuracy: 0.51\n",
      "iteration no 72: Loss: 0.8397135451856426, accuracy: 0.51\n",
      "iteration no 73: Loss: 0.8320752383835462, accuracy: 0.51\n",
      "iteration no 74: Loss: 0.824748871374435, accuracy: 0.5066666666666667\n",
      "iteration no 75: Loss: 0.817743179975097, accuracy: 0.5066666666666667\n",
      "iteration no 76: Loss: 0.8110638883033627, accuracy: 0.5066666666666667\n",
      "iteration no 77: Loss: 0.8047123732270569, accuracy: 0.51\n",
      "iteration no 78: Loss: 0.798687642924804, accuracy: 0.5133333333333333\n",
      "iteration no 79: Loss: 0.7929865090678921, accuracy: 0.5133333333333333\n",
      "iteration no 80: Loss: 0.7876080850509172, accuracy: 0.5166666666666667\n",
      "iteration no 81: Loss: 0.7825416649105631, accuracy: 0.5166666666666667\n",
      "iteration no 82: Loss: 0.7777805852796722, accuracy: 0.5166666666666667\n",
      "iteration no 83: Loss: 0.773311129836146, accuracy: 0.5166666666666667\n",
      "iteration no 84: Loss: 0.769122762273566, accuracy: 0.5166666666666667\n",
      "iteration no 85: Loss: 0.7652064081527282, accuracy: 0.5166666666666667\n",
      "iteration no 86: Loss: 0.7615468355699531, accuracy: 0.5166666666666667\n",
      "iteration no 87: Loss: 0.7581334413722802, accuracy: 0.5166666666666667\n",
      "iteration no 88: Loss: 0.7549499509420913, accuracy: 0.52\n",
      "iteration no 89: Loss: 0.751980041876389, accuracy: 0.52\n",
      "iteration no 90: Loss: 0.7492119208957323, accuracy: 0.52\n",
      "iteration no 91: Loss: 0.746633123228894, accuracy: 0.52\n",
      "iteration no 92: Loss: 0.744228645239318, accuracy: 0.5233333333333333\n",
      "iteration no 93: Loss: 0.7419891493432894, accuracy: 0.5233333333333333\n",
      "iteration no 94: Loss: 0.7398994572442599, accuracy: 0.5233333333333333\n",
      "iteration no 95: Loss: 0.7379503997485835, accuracy: 0.5233333333333333\n",
      "iteration no 96: Loss: 0.7361301328587467, accuracy: 0.5233333333333333\n",
      "iteration no 97: Loss: 0.7344325569672834, accuracy: 0.5233333333333333\n",
      "iteration no 98: Loss: 0.732843805696036, accuracy: 0.5233333333333333\n",
      "iteration no 99: Loss: 0.7313573130503219, accuracy: 0.5233333333333333\n",
      "iteration no 100: Loss: 0.7299668017803502, accuracy: 0.5233333333333333\n",
      "iteration no 101: Loss: 0.7286643187024622, accuracy: 0.5233333333333333\n",
      "iteration no 102: Loss: 0.7274407562693013, accuracy: 0.5266666666666666\n",
      "iteration no 103: Loss: 0.7262887238088588, accuracy: 0.53\n",
      "iteration no 104: Loss: 0.7252034817206403, accuracy: 0.5333333333333333\n",
      "iteration no 105: Loss: 0.7241803041731143, accuracy: 0.5366666666666666\n",
      "iteration no 106: Loss: 0.7232113798119993, accuracy: 0.5366666666666666\n",
      "iteration no 107: Loss: 0.7222931117617658, accuracy: 0.54\n",
      "iteration no 108: Loss: 0.7214237538990721, accuracy: 0.54\n",
      "iteration no 109: Loss: 0.7205970253740166, accuracy: 0.54\n",
      "iteration no 110: Loss: 0.7198145495202116, accuracy: 0.54\n",
      "iteration no 111: Loss: 0.719070782205622, accuracy: 0.54\n",
      "iteration no 112: Loss: 0.7183651225097014, accuracy: 0.54\n",
      "iteration no 113: Loss: 0.7176931256137288, accuracy: 0.54\n",
      "iteration no 114: Loss: 0.7170520803605838, accuracy: 0.54\n",
      "iteration no 115: Loss: 0.7164403594226594, accuracy: 0.54\n",
      "iteration no 116: Loss: 0.7158565592557236, accuracy: 0.54\n",
      "iteration no 117: Loss: 0.7152961888847533, accuracy: 0.54\n",
      "iteration no 118: Loss: 0.7147595007618438, accuracy: 0.54\n",
      "iteration no 119: Loss: 0.7142404245515273, accuracy: 0.54\n",
      "iteration no 120: Loss: 0.7137358043101372, accuracy: 0.54\n",
      "iteration no 121: Loss: 0.7132486865555293, accuracy: 0.54\n",
      "iteration no 122: Loss: 0.7127757223601364, accuracy: 0.54\n",
      "iteration no 123: Loss: 0.7123178904746924, accuracy: 0.54\n",
      "iteration no 124: Loss: 0.7118761247905201, accuracy: 0.54\n",
      "iteration no 125: Loss: 0.7114512979728776, accuracy: 0.54\n",
      "iteration no 126: Loss: 0.7110391243429741, accuracy: 0.5433333333333333\n",
      "iteration no 127: Loss: 0.710639725775863, accuracy: 0.5433333333333333\n",
      "iteration no 128: Loss: 0.7102514570703242, accuracy: 0.5433333333333333\n",
      "iteration no 129: Loss: 0.7098732666590455, accuracy: 0.5466666666666666\n",
      "iteration no 130: Loss: 0.7095036999640976, accuracy: 0.55\n",
      "iteration no 131: Loss: 0.7091427592123043, accuracy: 0.55\n",
      "iteration no 132: Loss: 0.7087910298312304, accuracy: 0.55\n",
      "iteration no 133: Loss: 0.708446081518543, accuracy: 0.55\n",
      "iteration no 134: Loss: 0.7081063419501894, accuracy: 0.55\n",
      "iteration no 135: Loss: 0.707771825468821, accuracy: 0.55\n",
      "iteration no 136: Loss: 0.707442098818593, accuracy: 0.55\n",
      "iteration no 137: Loss: 0.7071184055773267, accuracy: 0.55\n",
      "iteration no 138: Loss: 0.7068000343401505, accuracy: 0.55\n",
      "iteration no 139: Loss: 0.7064856718210286, accuracy: 0.55\n",
      "iteration no 140: Loss: 0.70617454725029, accuracy: 0.55\n",
      "iteration no 141: Loss: 0.7058665242574088, accuracy: 0.55\n",
      "iteration no 142: Loss: 0.7055625786359961, accuracy: 0.55\n",
      "iteration no 143: Loss: 0.7052612658290588, accuracy: 0.55\n",
      "iteration no 144: Loss: 0.7049613504923427, accuracy: 0.55\n",
      "iteration no 145: Loss: 0.7046637399022382, accuracy: 0.55\n",
      "iteration no 146: Loss: 0.7043679751730451, accuracy: 0.55\n",
      "iteration no 147: Loss: 0.7040731960618108, accuracy: 0.55\n",
      "iteration no 148: Loss: 0.7037792843341937, accuracy: 0.55\n",
      "iteration no 149: Loss: 0.7034861228022817, accuracy: 0.55\n",
      "iteration no 150: Loss: 0.7031940783430783, accuracy: 0.55\n",
      "iteration no 151: Loss: 0.702902269775364, accuracy: 0.55\n",
      "iteration no 152: Loss: 0.7026122891178676, accuracy: 0.55\n",
      "iteration no 153: Loss: 0.7023249566023484, accuracy: 0.55\n",
      "iteration no 154: Loss: 0.7020389874147476, accuracy: 0.55\n",
      "iteration no 155: Loss: 0.7017537218386853, accuracy: 0.55\n",
      "iteration no 156: Loss: 0.7014697932007816, accuracy: 0.55\n",
      "iteration no 157: Loss: 0.7011874193581267, accuracy: 0.55\n",
      "iteration no 158: Loss: 0.7009085688919523, accuracy: 0.55\n",
      "iteration no 159: Loss: 0.700632399365235, accuracy: 0.55\n",
      "iteration no 160: Loss: 0.7003573247665623, accuracy: 0.55\n",
      "iteration no 161: Loss: 0.700083690642893, accuracy: 0.55\n",
      "iteration no 162: Loss: 0.6998115739443383, accuracy: 0.55\n",
      "iteration no 163: Loss: 0.6995433702190332, accuracy: 0.55\n",
      "iteration no 164: Loss: 0.6992761346195523, accuracy: 0.55\n",
      "iteration no 165: Loss: 0.6990096877101836, accuracy: 0.5533333333333333\n",
      "iteration no 166: Loss: 0.698744149764509, accuracy: 0.5533333333333333\n",
      "iteration no 167: Loss: 0.6984798163922395, accuracy: 0.5566666666666666\n",
      "iteration no 168: Loss: 0.6982173430963237, accuracy: 0.5566666666666666\n",
      "iteration no 169: Loss: 0.6979553329899129, accuracy: 0.5566666666666666\n",
      "iteration no 170: Loss: 0.6976944166322999, accuracy: 0.5566666666666666\n",
      "iteration no 171: Loss: 0.6974341247105198, accuracy: 0.5566666666666666\n",
      "iteration no 172: Loss: 0.6971741556365842, accuracy: 0.5566666666666666\n",
      "iteration no 173: Loss: 0.6969148587284957, accuracy: 0.5566666666666666\n",
      "iteration no 174: Loss: 0.6966567524290619, accuracy: 0.5566666666666666\n",
      "iteration no 175: Loss: 0.6963983723095355, accuracy: 0.5566666666666666\n",
      "iteration no 176: Loss: 0.696141941767939, accuracy: 0.5566666666666666\n",
      "iteration no 177: Loss: 0.6958880988358422, accuracy: 0.5566666666666666\n",
      "iteration no 178: Loss: 0.6956358161826979, accuracy: 0.5566666666666666\n",
      "iteration no 179: Loss: 0.6953842007149466, accuracy: 0.5566666666666666\n",
      "iteration no 180: Loss: 0.6951328105612835, accuracy: 0.5566666666666666\n",
      "iteration no 181: Loss: 0.6948826467478233, accuracy: 0.5566666666666666\n",
      "iteration no 182: Loss: 0.694632938028401, accuracy: 0.5566666666666666\n",
      "iteration no 183: Loss: 0.6943843769094503, accuracy: 0.5566666666666666\n",
      "iteration no 184: Loss: 0.6941373371084123, accuracy: 0.5566666666666666\n",
      "iteration no 185: Loss: 0.6938910988203212, accuracy: 0.5566666666666666\n",
      "iteration no 186: Loss: 0.6936451924187366, accuracy: 0.5566666666666666\n",
      "iteration no 187: Loss: 0.6933995573069417, accuracy: 0.5566666666666666\n",
      "iteration no 188: Loss: 0.69315514514275, accuracy: 0.5566666666666666\n",
      "iteration no 189: Loss: 0.6929113850250965, accuracy: 0.56\n",
      "iteration no 190: Loss: 0.6926690875072847, accuracy: 0.56\n",
      "iteration no 191: Loss: 0.6924273025079173, accuracy: 0.5633333333333334\n",
      "iteration no 192: Loss: 0.69218661285107, accuracy: 0.5633333333333334\n",
      "iteration no 193: Loss: 0.6919464400477099, accuracy: 0.5633333333333334\n",
      "iteration no 194: Loss: 0.6917063475794125, accuracy: 0.5633333333333334\n",
      "iteration no 195: Loss: 0.6914665734227398, accuracy: 0.5633333333333334\n",
      "iteration no 196: Loss: 0.691226779022359, accuracy: 0.5633333333333334\n",
      "iteration no 197: Loss: 0.6909873030661202, accuracy: 0.5633333333333334\n",
      "iteration no 198: Loss: 0.690747666133566, accuracy: 0.5633333333333334\n",
      "iteration no 199: Loss: 0.6905079457893698, accuracy: 0.5633333333333334\n",
      "iteration no 200: Loss: 0.6902689215546068, accuracy: 0.5633333333333334\n",
      "iteration no 201: Loss: 0.6900297199173705, accuracy: 0.5633333333333334\n",
      "iteration no 202: Loss: 0.6897904764917067, accuracy: 0.5633333333333334\n",
      "iteration no 203: Loss: 0.689551871074474, accuracy: 0.5633333333333334\n",
      "iteration no 204: Loss: 0.6893138296485147, accuracy: 0.5633333333333334\n",
      "iteration no 205: Loss: 0.6890757742374086, accuracy: 0.5633333333333334\n",
      "iteration no 206: Loss: 0.6888379975260266, accuracy: 0.5633333333333334\n",
      "iteration no 207: Loss: 0.6886005187381526, accuracy: 0.56\n",
      "iteration no 208: Loss: 0.6883627251536878, accuracy: 0.5633333333333334\n",
      "iteration no 209: Loss: 0.6881248662843114, accuracy: 0.56\n",
      "iteration no 210: Loss: 0.6878864187448621, accuracy: 0.5633333333333334\n",
      "iteration no 211: Loss: 0.6876477384560624, accuracy: 0.56\n",
      "iteration no 212: Loss: 0.6874092312579019, accuracy: 0.5633333333333334\n",
      "iteration no 213: Loss: 0.6871700999616588, accuracy: 0.5633333333333334\n",
      "iteration no 214: Loss: 0.686931028049356, accuracy: 0.5633333333333334\n",
      "iteration no 215: Loss: 0.6866913195154359, accuracy: 0.5633333333333334\n",
      "iteration no 216: Loss: 0.6864530665612814, accuracy: 0.5633333333333334\n",
      "iteration no 217: Loss: 0.686214871577592, accuracy: 0.5633333333333334\n",
      "iteration no 218: Loss: 0.685975700145897, accuracy: 0.5633333333333334\n",
      "iteration no 219: Loss: 0.6857365102304106, accuracy: 0.5633333333333334\n",
      "iteration no 220: Loss: 0.685497638860502, accuracy: 0.5666666666666667\n",
      "iteration no 221: Loss: 0.6852574768051439, accuracy: 0.57\n",
      "iteration no 222: Loss: 0.685016986203203, accuracy: 0.57\n",
      "iteration no 223: Loss: 0.6847764768016285, accuracy: 0.57\n",
      "iteration no 224: Loss: 0.6845349185556407, accuracy: 0.57\n",
      "iteration no 225: Loss: 0.6842926706889471, accuracy: 0.57\n",
      "iteration no 226: Loss: 0.6840505402758547, accuracy: 0.57\n",
      "iteration no 227: Loss: 0.6838082870534454, accuracy: 0.57\n",
      "iteration no 228: Loss: 0.6835656956185608, accuracy: 0.57\n",
      "iteration no 229: Loss: 0.68332229993435, accuracy: 0.57\n",
      "iteration no 230: Loss: 0.6830778350615523, accuracy: 0.57\n",
      "iteration no 231: Loss: 0.6828326677658576, accuracy: 0.5733333333333334\n",
      "iteration no 232: Loss: 0.6825873973627248, accuracy: 0.5733333333333334\n",
      "iteration no 233: Loss: 0.6823400758797653, accuracy: 0.5733333333333334\n",
      "iteration no 234: Loss: 0.682093126850423, accuracy: 0.5766666666666667\n",
      "iteration no 235: Loss: 0.6818445759380591, accuracy: 0.5766666666666667\n",
      "iteration no 236: Loss: 0.6815950824760248, accuracy: 0.5766666666666667\n",
      "iteration no 237: Loss: 0.6813453023254635, accuracy: 0.5766666666666667\n",
      "iteration no 238: Loss: 0.6810945888064465, accuracy: 0.5766666666666667\n",
      "iteration no 239: Loss: 0.6808417123465161, accuracy: 0.5766666666666667\n",
      "iteration no 240: Loss: 0.6805896365945965, accuracy: 0.5766666666666667\n",
      "iteration no 241: Loss: 0.680334852769088, accuracy: 0.5766666666666667\n",
      "iteration no 242: Loss: 0.6800805901946895, accuracy: 0.5766666666666667\n",
      "iteration no 243: Loss: 0.6798245862374408, accuracy: 0.5766666666666667\n",
      "iteration no 244: Loss: 0.6795672785534351, accuracy: 0.5766666666666667\n",
      "iteration no 245: Loss: 0.6793093696654477, accuracy: 0.5766666666666667\n",
      "iteration no 246: Loss: 0.679050981223545, accuracy: 0.5766666666666667\n",
      "iteration no 247: Loss: 0.6787896892432177, accuracy: 0.58\n",
      "iteration no 248: Loss: 0.6785279361668313, accuracy: 0.58\n",
      "iteration no 249: Loss: 0.6782652830632488, accuracy: 0.58\n",
      "iteration no 250: Loss: 0.6780005272756362, accuracy: 0.58\n",
      "iteration no 251: Loss: 0.6777352425357418, accuracy: 0.58\n",
      "iteration no 252: Loss: 0.6774691217710822, accuracy: 0.58\n",
      "iteration no 253: Loss: 0.6771996785248012, accuracy: 0.58\n",
      "iteration no 254: Loss: 0.6769302111281432, accuracy: 0.5833333333333334\n",
      "iteration no 255: Loss: 0.6766587700386894, accuracy: 0.5833333333333334\n",
      "iteration no 256: Loss: 0.6763864767858503, accuracy: 0.5866666666666667\n",
      "iteration no 257: Loss: 0.6761130815630364, accuracy: 0.5866666666666667\n",
      "iteration no 258: Loss: 0.6758383015179736, accuracy: 0.5866666666666667\n",
      "iteration no 259: Loss: 0.6755610829849085, accuracy: 0.5866666666666667\n",
      "iteration no 260: Loss: 0.6752834728695549, accuracy: 0.5866666666666667\n",
      "iteration no 261: Loss: 0.6750038168516993, accuracy: 0.5866666666666667\n",
      "iteration no 262: Loss: 0.6747229064011856, accuracy: 0.5866666666666667\n",
      "iteration no 263: Loss: 0.674443101055136, accuracy: 0.5866666666666667\n",
      "iteration no 264: Loss: 0.6741581802638779, accuracy: 0.5866666666666667\n",
      "iteration no 265: Loss: 0.6738710152616741, accuracy: 0.5866666666666667\n",
      "iteration no 266: Loss: 0.6735849582882286, accuracy: 0.5866666666666667\n",
      "iteration no 267: Loss: 0.6732963982887425, accuracy: 0.5866666666666667\n",
      "iteration no 268: Loss: 0.6730058796163702, accuracy: 0.5866666666666667\n",
      "iteration no 269: Loss: 0.6727156724815043, accuracy: 0.5866666666666667\n",
      "iteration no 270: Loss: 0.6724210910632641, accuracy: 0.5866666666666667\n",
      "iteration no 271: Loss: 0.6721251146233781, accuracy: 0.5866666666666667\n",
      "iteration no 272: Loss: 0.6718289113736977, accuracy: 0.5866666666666667\n",
      "iteration no 273: Loss: 0.6715291913573844, accuracy: 0.5866666666666667\n",
      "iteration no 274: Loss: 0.6712267906147958, accuracy: 0.5866666666666667\n",
      "iteration no 275: Loss: 0.670925364046198, accuracy: 0.5866666666666667\n",
      "iteration no 276: Loss: 0.6706195429341903, accuracy: 0.5866666666666667\n",
      "iteration no 277: Loss: 0.6703112006697112, accuracy: 0.5866666666666667\n",
      "iteration no 278: Loss: 0.6700043407140384, accuracy: 0.5866666666666667\n",
      "iteration no 279: Loss: 0.6696925052079323, accuracy: 0.5866666666666667\n",
      "iteration no 280: Loss: 0.6693793470996138, accuracy: 0.5866666666666667\n",
      "iteration no 281: Loss: 0.6690657967460746, accuracy: 0.5866666666666667\n",
      "iteration no 282: Loss: 0.6687488897910858, accuracy: 0.5866666666666667\n",
      "iteration no 283: Loss: 0.6684303230326966, accuracy: 0.5866666666666667\n",
      "iteration no 284: Loss: 0.6681094298142879, accuracy: 0.5866666666666667\n",
      "iteration no 285: Loss: 0.667787190461114, accuracy: 0.5866666666666667\n",
      "iteration no 286: Loss: 0.6674615841370526, accuracy: 0.5866666666666667\n",
      "iteration no 287: Loss: 0.6671341632481638, accuracy: 0.5866666666666667\n",
      "iteration no 288: Loss: 0.6668055702474693, accuracy: 0.5866666666666667\n",
      "iteration no 289: Loss: 0.6664735461026425, accuracy: 0.5866666666666667\n",
      "iteration no 290: Loss: 0.666139046428531, accuracy: 0.5866666666666667\n",
      "iteration no 291: Loss: 0.6658031666227929, accuracy: 0.5866666666666667\n",
      "iteration no 292: Loss: 0.6654659191068998, accuracy: 0.5866666666666667\n",
      "iteration no 293: Loss: 0.6651249467154351, accuracy: 0.5866666666666667\n",
      "iteration no 294: Loss: 0.6647816427482813, accuracy: 0.5866666666666667\n",
      "iteration no 295: Loss: 0.6644377072822836, accuracy: 0.5866666666666667\n",
      "iteration no 296: Loss: 0.6640901413156648, accuracy: 0.5866666666666667\n",
      "iteration no 297: Loss: 0.663740325603917, accuracy: 0.5866666666666667\n",
      "iteration no 298: Loss: 0.6633878316865219, accuracy: 0.5866666666666667\n",
      "iteration no 299: Loss: 0.6630331745359719, accuracy: 0.5866666666666667\n",
      "iteration no 300: Loss: 0.662676946891789, accuracy: 0.5866666666666667\n",
      "iteration no 301: Loss: 0.6623167333048736, accuracy: 0.59\n",
      "iteration no 302: Loss: 0.661954504688845, accuracy: 0.59\n",
      "iteration no 303: Loss: 0.6615902690770159, accuracy: 0.59\n",
      "iteration no 304: Loss: 0.661223098216927, accuracy: 0.59\n",
      "iteration no 305: Loss: 0.660853688224985, accuracy: 0.59\n",
      "iteration no 306: Loss: 0.6604823534153984, accuracy: 0.5933333333333334\n",
      "iteration no 307: Loss: 0.6601072280350518, accuracy: 0.5933333333333334\n",
      "iteration no 308: Loss: 0.6597300544705711, accuracy: 0.5966666666666667\n",
      "iteration no 309: Loss: 0.6593500189380813, accuracy: 0.5966666666666667\n",
      "iteration no 310: Loss: 0.6589677119590036, accuracy: 0.5966666666666667\n",
      "iteration no 311: Loss: 0.6585821597193751, accuracy: 0.5966666666666667\n",
      "iteration no 312: Loss: 0.6581943936786497, accuracy: 0.5966666666666667\n",
      "iteration no 313: Loss: 0.6578037908017026, accuracy: 0.5966666666666667\n",
      "iteration no 314: Loss: 0.6574112547395161, accuracy: 0.5966666666666667\n",
      "iteration no 315: Loss: 0.6570150789424665, accuracy: 0.5966666666666667\n",
      "iteration no 316: Loss: 0.6566174271706041, accuracy: 0.5966666666666667\n",
      "iteration no 317: Loss: 0.6562169040462772, accuracy: 0.6\n",
      "iteration no 318: Loss: 0.6558149964099217, accuracy: 0.6\n",
      "iteration no 319: Loss: 0.6554094887564949, accuracy: 0.6033333333333334\n",
      "iteration no 320: Loss: 0.6550018183664088, accuracy: 0.6\n",
      "iteration no 321: Loss: 0.6545902478094769, accuracy: 0.6033333333333334\n",
      "iteration no 322: Loss: 0.6541755227195969, accuracy: 0.6033333333333334\n",
      "iteration no 323: Loss: 0.653759556031728, accuracy: 0.6033333333333334\n",
      "iteration no 324: Loss: 0.6533386835876905, accuracy: 0.6033333333333334\n",
      "iteration no 325: Loss: 0.6529154517030984, accuracy: 0.6033333333333334\n",
      "iteration no 326: Loss: 0.6524880541839232, accuracy: 0.6033333333333334\n",
      "iteration no 327: Loss: 0.6520601595385583, accuracy: 0.6033333333333334\n",
      "iteration no 328: Loss: 0.6516282264450863, accuracy: 0.6033333333333334\n",
      "iteration no 329: Loss: 0.6511955480454532, accuracy: 0.6033333333333334\n",
      "iteration no 330: Loss: 0.6507569593732349, accuracy: 0.6033333333333334\n",
      "iteration no 331: Loss: 0.6503162812751638, accuracy: 0.6033333333333334\n",
      "iteration no 332: Loss: 0.6498729108754142, accuracy: 0.6033333333333334\n",
      "iteration no 333: Loss: 0.6494255720550154, accuracy: 0.6033333333333334\n",
      "iteration no 334: Loss: 0.6489777854697353, accuracy: 0.6033333333333334\n",
      "iteration no 335: Loss: 0.6485247802091887, accuracy: 0.6033333333333334\n",
      "iteration no 336: Loss: 0.6480710301217889, accuracy: 0.6033333333333334\n",
      "iteration no 337: Loss: 0.6476108494557832, accuracy: 0.6033333333333334\n",
      "iteration no 338: Loss: 0.647149478726961, accuracy: 0.6066666666666667\n",
      "iteration no 339: Loss: 0.6466836627565545, accuracy: 0.6133333333333333\n",
      "iteration no 340: Loss: 0.6462159138547043, accuracy: 0.6133333333333333\n",
      "iteration no 341: Loss: 0.6457445114768277, accuracy: 0.6133333333333333\n",
      "iteration no 342: Loss: 0.6452689711951086, accuracy: 0.6133333333333333\n",
      "iteration no 343: Loss: 0.6447901751974283, accuracy: 0.6133333333333333\n",
      "iteration no 344: Loss: 0.6443089251612512, accuracy: 0.6133333333333333\n",
      "iteration no 345: Loss: 0.6438239370584372, accuracy: 0.6166666666666667\n",
      "iteration no 346: Loss: 0.6433360379246449, accuracy: 0.62\n",
      "iteration no 347: Loss: 0.6428440963395484, accuracy: 0.62\n",
      "iteration no 348: Loss: 0.6423499494480833, accuracy: 0.62\n",
      "iteration no 349: Loss: 0.6418541644910839, accuracy: 0.62\n",
      "iteration no 350: Loss: 0.6413543804071475, accuracy: 0.62\n",
      "iteration no 351: Loss: 0.6408519618842552, accuracy: 0.62\n",
      "iteration no 352: Loss: 0.640345315755987, accuracy: 0.62\n",
      "iteration no 353: Loss: 0.6398362472810022, accuracy: 0.62\n",
      "iteration no 354: Loss: 0.6393266342668932, accuracy: 0.62\n",
      "iteration no 355: Loss: 0.6388138871231944, accuracy: 0.62\n",
      "iteration no 356: Loss: 0.6382992513905295, accuracy: 0.62\n",
      "iteration no 357: Loss: 0.6377784987733843, accuracy: 0.62\n",
      "iteration no 358: Loss: 0.6372569855214959, accuracy: 0.62\n",
      "iteration no 359: Loss: 0.6367330721331145, accuracy: 0.62\n",
      "iteration no 360: Loss: 0.6362082774993402, accuracy: 0.6233333333333333\n",
      "iteration no 361: Loss: 0.6356764506785079, accuracy: 0.6233333333333333\n",
      "iteration no 362: Loss: 0.6351432519124541, accuracy: 0.6233333333333333\n",
      "iteration no 363: Loss: 0.6346066341452299, accuracy: 0.6233333333333333\n",
      "iteration no 364: Loss: 0.6340670727347446, accuracy: 0.6233333333333333\n",
      "iteration no 365: Loss: 0.6335262599543183, accuracy: 0.6233333333333333\n",
      "iteration no 366: Loss: 0.6329839976618703, accuracy: 0.6266666666666667\n",
      "iteration no 367: Loss: 0.6324366070192248, accuracy: 0.6266666666666667\n",
      "iteration no 368: Loss: 0.6318873404802705, accuracy: 0.6266666666666667\n",
      "iteration no 369: Loss: 0.631336121482755, accuracy: 0.63\n",
      "iteration no 370: Loss: 0.6307801809589154, accuracy: 0.63\n",
      "iteration no 371: Loss: 0.6302213030666878, accuracy: 0.63\n",
      "iteration no 372: Loss: 0.6296598042406105, accuracy: 0.63\n",
      "iteration no 373: Loss: 0.629093720814228, accuracy: 0.63\n",
      "iteration no 374: Loss: 0.6285242222093899, accuracy: 0.63\n",
      "iteration no 375: Loss: 0.6279538030157554, accuracy: 0.63\n",
      "iteration no 376: Loss: 0.62737771223431, accuracy: 0.63\n",
      "iteration no 377: Loss: 0.626799283708839, accuracy: 0.63\n",
      "iteration no 378: Loss: 0.6262192225968448, accuracy: 0.63\n",
      "iteration no 379: Loss: 0.6256375791363387, accuracy: 0.6333333333333333\n",
      "iteration no 380: Loss: 0.6250526098519223, accuracy: 0.6366666666666667\n",
      "iteration no 381: Loss: 0.6244651107214584, accuracy: 0.6366666666666667\n",
      "iteration no 382: Loss: 0.6238753274812638, accuracy: 0.6366666666666667\n",
      "iteration no 383: Loss: 0.6232822959593057, accuracy: 0.6366666666666667\n",
      "iteration no 384: Loss: 0.6226873683669066, accuracy: 0.6366666666666667\n",
      "iteration no 385: Loss: 0.6220880007309566, accuracy: 0.6366666666666667\n",
      "iteration no 386: Loss: 0.6214854890047586, accuracy: 0.64\n",
      "iteration no 387: Loss: 0.6208817847215179, accuracy: 0.64\n",
      "iteration no 388: Loss: 0.6202748570568063, accuracy: 0.64\n",
      "iteration no 389: Loss: 0.6196644415927417, accuracy: 0.6433333333333333\n",
      "iteration no 390: Loss: 0.6190509877025318, accuracy: 0.64\n",
      "iteration no 391: Loss: 0.6184333836519619, accuracy: 0.6433333333333333\n",
      "iteration no 392: Loss: 0.6178143499621114, accuracy: 0.6433333333333333\n",
      "iteration no 393: Loss: 0.6171932243097953, accuracy: 0.6433333333333333\n",
      "iteration no 394: Loss: 0.6165690625751802, accuracy: 0.6466666666666666\n",
      "iteration no 395: Loss: 0.6159416783831761, accuracy: 0.6466666666666666\n",
      "iteration no 396: Loss: 0.6153110430448813, accuracy: 0.6466666666666666\n",
      "iteration no 397: Loss: 0.6146785502776934, accuracy: 0.6466666666666666\n",
      "iteration no 398: Loss: 0.6140422847639048, accuracy: 0.65\n",
      "iteration no 399: Loss: 0.6134024948118917, accuracy: 0.6533333333333333\n",
      "iteration no 400: Loss: 0.6127609535320685, accuracy: 0.6533333333333333\n",
      "iteration no 401: Loss: 0.6121155070012732, accuracy: 0.6533333333333333\n",
      "iteration no 402: Loss: 0.6114660203126346, accuracy: 0.6533333333333333\n",
      "iteration no 403: Loss: 0.6108142420547894, accuracy: 0.6533333333333333\n",
      "iteration no 404: Loss: 0.610160154976836, accuracy: 0.6533333333333333\n",
      "iteration no 405: Loss: 0.6095008852381406, accuracy: 0.6533333333333333\n",
      "iteration no 406: Loss: 0.6088431680952473, accuracy: 0.6533333333333333\n",
      "iteration no 407: Loss: 0.6081812267548214, accuracy: 0.6533333333333333\n",
      "iteration no 408: Loss: 0.6075170897425795, accuracy: 0.6533333333333333\n",
      "iteration no 409: Loss: 0.6068486508542884, accuracy: 0.6533333333333333\n",
      "iteration no 410: Loss: 0.6061779009793171, accuracy: 0.6566666666666666\n",
      "iteration no 411: Loss: 0.6055042212704413, accuracy: 0.6566666666666666\n",
      "iteration no 412: Loss: 0.6048293139881686, accuracy: 0.6566666666666666\n",
      "iteration no 413: Loss: 0.6041488318719493, accuracy: 0.6566666666666666\n",
      "iteration no 414: Loss: 0.6034687507655434, accuracy: 0.6566666666666666\n",
      "iteration no 415: Loss: 0.6027864305664419, accuracy: 0.6566666666666666\n",
      "iteration no 416: Loss: 0.6021033967662661, accuracy: 0.6566666666666666\n",
      "iteration no 417: Loss: 0.6014147228386526, accuracy: 0.6566666666666666\n",
      "iteration no 418: Loss: 0.6007254637764249, accuracy: 0.6566666666666666\n",
      "iteration no 419: Loss: 0.6000328883806186, accuracy: 0.66\n",
      "iteration no 420: Loss: 0.5993400589627429, accuracy: 0.6566666666666666\n",
      "iteration no 421: Loss: 0.5986382565603278, accuracy: 0.66\n",
      "iteration no 422: Loss: 0.597941087939755, accuracy: 0.66\n",
      "iteration no 423: Loss: 0.5972345399645145, accuracy: 0.6633333333333333\n",
      "iteration no 424: Loss: 0.5965291621852055, accuracy: 0.6633333333333333\n",
      "iteration no 425: Loss: 0.595814925520992, accuracy: 0.6633333333333333\n",
      "iteration no 426: Loss: 0.5951091403521707, accuracy: 0.6666666666666666\n",
      "iteration no 427: Loss: 0.5943877643263783, accuracy: 0.6666666666666666\n",
      "iteration no 428: Loss: 0.5936717079934904, accuracy: 0.6666666666666666\n",
      "iteration no 429: Loss: 0.5929546089496225, accuracy: 0.6666666666666666\n",
      "iteration no 430: Loss: 0.5922354134815581, accuracy: 0.6666666666666666\n",
      "iteration no 431: Loss: 0.5915097741529887, accuracy: 0.6666666666666666\n",
      "iteration no 432: Loss: 0.5907824673348147, accuracy: 0.6666666666666666\n",
      "iteration no 433: Loss: 0.5900543211505203, accuracy: 0.6666666666666666\n",
      "iteration no 434: Loss: 0.589323089955236, accuracy: 0.6666666666666666\n",
      "iteration no 435: Loss: 0.5885860079186929, accuracy: 0.6666666666666666\n",
      "iteration no 436: Loss: 0.5878528516752689, accuracy: 0.6666666666666666\n",
      "iteration no 437: Loss: 0.5871101099258242, accuracy: 0.6666666666666666\n",
      "iteration no 438: Loss: 0.5863696889921874, accuracy: 0.6666666666666666\n",
      "iteration no 439: Loss: 0.5856227391972257, accuracy: 0.6666666666666666\n",
      "iteration no 440: Loss: 0.5848756930311719, accuracy: 0.6666666666666666\n",
      "iteration no 441: Loss: 0.5841257033059071, accuracy: 0.6666666666666666\n",
      "iteration no 442: Loss: 0.5833709264040543, accuracy: 0.6666666666666666\n",
      "iteration no 443: Loss: 0.5826174376907758, accuracy: 0.6666666666666666\n",
      "iteration no 444: Loss: 0.58186527634568, accuracy: 0.6666666666666666\n",
      "iteration no 445: Loss: 0.5811020977316109, accuracy: 0.6666666666666666\n",
      "iteration no 446: Loss: 0.5803392740863289, accuracy: 0.6666666666666666\n",
      "iteration no 447: Loss: 0.5795751759659528, accuracy: 0.6666666666666666\n",
      "iteration no 448: Loss: 0.5788076392317528, accuracy: 0.6666666666666666\n",
      "iteration no 449: Loss: 0.5780414902504274, accuracy: 0.67\n",
      "iteration no 450: Loss: 0.5772703543121767, accuracy: 0.6666666666666666\n",
      "iteration no 451: Loss: 0.5764962286245526, accuracy: 0.67\n",
      "iteration no 452: Loss: 0.5757219561711098, accuracy: 0.6733333333333333\n",
      "iteration no 453: Loss: 0.5749396360244243, accuracy: 0.67\n",
      "iteration no 454: Loss: 0.5741627507242323, accuracy: 0.6766666666666666\n",
      "iteration no 455: Loss: 0.5733799645729513, accuracy: 0.6733333333333333\n",
      "iteration no 456: Loss: 0.5725959152241215, accuracy: 0.6766666666666666\n",
      "iteration no 457: Loss: 0.5718062072428505, accuracy: 0.6766666666666666\n",
      "iteration no 458: Loss: 0.571020105034851, accuracy: 0.6766666666666666\n",
      "iteration no 459: Loss: 0.5702305261732435, accuracy: 0.68\n",
      "iteration no 460: Loss: 0.569442331364506, accuracy: 0.68\n",
      "iteration no 461: Loss: 0.5686443824629538, accuracy: 0.6833333333333333\n",
      "iteration no 462: Loss: 0.5678513071908138, accuracy: 0.6766666666666666\n",
      "iteration no 463: Loss: 0.5670513442839828, accuracy: 0.6833333333333333\n",
      "iteration no 464: Loss: 0.5662543507260218, accuracy: 0.6766666666666666\n",
      "iteration no 465: Loss: 0.5654512638707226, accuracy: 0.6833333333333333\n",
      "iteration no 466: Loss: 0.5646448146369575, accuracy: 0.68\n",
      "iteration no 467: Loss: 0.5638356546283692, accuracy: 0.6833333333333333\n",
      "iteration no 468: Loss: 0.563031307087326, accuracy: 0.68\n",
      "iteration no 469: Loss: 0.5622215164807725, accuracy: 0.6833333333333333\n",
      "iteration no 470: Loss: 0.5614040340643572, accuracy: 0.6833333333333333\n",
      "iteration no 471: Loss: 0.5605888099763784, accuracy: 0.6833333333333333\n",
      "iteration no 472: Loss: 0.5597727730704748, accuracy: 0.6833333333333333\n",
      "iteration no 473: Loss: 0.5589499212580947, accuracy: 0.6866666666666666\n",
      "iteration no 474: Loss: 0.558134767740525, accuracy: 0.6833333333333333\n",
      "iteration no 475: Loss: 0.5573120308313327, accuracy: 0.6866666666666666\n",
      "iteration no 476: Loss: 0.5564908357160355, accuracy: 0.6866666666666666\n",
      "iteration no 477: Loss: 0.5556677276649483, accuracy: 0.6866666666666666\n",
      "iteration no 478: Loss: 0.5548363893486014, accuracy: 0.6866666666666666\n",
      "iteration no 479: Loss: 0.554006920763035, accuracy: 0.6866666666666666\n",
      "iteration no 480: Loss: 0.5531846384079472, accuracy: 0.6866666666666666\n",
      "iteration no 481: Loss: 0.5523570167189269, accuracy: 0.6866666666666666\n",
      "iteration no 482: Loss: 0.5515316626413698, accuracy: 0.6866666666666666\n",
      "iteration no 483: Loss: 0.5507027397539069, accuracy: 0.69\n",
      "iteration no 484: Loss: 0.5498770058234436, accuracy: 0.6866666666666666\n",
      "iteration no 485: Loss: 0.5490398725469853, accuracy: 0.6933333333333334\n",
      "iteration no 486: Loss: 0.5482188367561293, accuracy: 0.6866666666666666\n",
      "iteration no 487: Loss: 0.5473901054833561, accuracy: 0.6933333333333334\n",
      "iteration no 488: Loss: 0.5465715826411831, accuracy: 0.6866666666666666\n",
      "iteration no 489: Loss: 0.5457464300838599, accuracy: 0.6933333333333334\n",
      "iteration no 490: Loss: 0.5449340820654399, accuracy: 0.6866666666666666\n",
      "iteration no 491: Loss: 0.5441141745123566, accuracy: 0.6933333333333334\n",
      "iteration no 492: Loss: 0.5433023736372302, accuracy: 0.6866666666666666\n",
      "iteration no 493: Loss: 0.5424837108594688, accuracy: 0.6933333333333334\n",
      "iteration no 494: Loss: 0.5416952719413761, accuracy: 0.69\n",
      "iteration no 495: Loss: 0.540906262468153, accuracy: 0.7\n",
      "iteration no 496: Loss: 0.5401321044283082, accuracy: 0.69\n",
      "iteration no 497: Loss: 0.5393699394669441, accuracy: 0.7066666666666667\n",
      "iteration no 498: Loss: 0.5386199114955479, accuracy: 0.6966666666666667\n",
      "iteration no 499: Loss: 0.5379085067856039, accuracy: 0.72\n",
      "iteration no 500: Loss: 0.5372232567472691, accuracy: 0.7\n",
      "iteration no 501: Loss: 0.5365636743650017, accuracy: 0.73\n",
      "iteration no 502: Loss: 0.535954222318878, accuracy: 0.7033333333333334\n",
      "iteration no 503: Loss: 0.5354060983457918, accuracy: 0.7266666666666667\n",
      "iteration no 504: Loss: 0.5349450025578758, accuracy: 0.7\n",
      "iteration no 505: Loss: 0.5345397990288675, accuracy: 0.73\n",
      "iteration no 506: Loss: 0.5342367142826372, accuracy: 0.7066666666666667\n",
      "iteration no 507: Loss: 0.5339886704276796, accuracy: 0.7366666666666667\n",
      "iteration no 508: Loss: 0.533931250146072, accuracy: 0.7133333333333334\n",
      "iteration no 509: Loss: 0.5339125485502302, accuracy: 0.75\n",
      "iteration no 510: Loss: 0.5342222327146471, accuracy: 0.7166666666666667\n",
      "iteration no 511: Loss: 0.5344869238171689, accuracy: 0.7533333333333333\n",
      "iteration no 512: Loss: 0.535420552246376, accuracy: 0.7166666666666667\n",
      "iteration no 513: Loss: 0.5360875783225972, accuracy: 0.7733333333333333\n",
      "iteration no 514: Loss: 0.5377862767775129, accuracy: 0.7066666666666667\n",
      "iteration no 515: Loss: 0.538980755149905, accuracy: 0.78\n",
      "iteration no 516: Loss: 0.5418093073959482, accuracy: 0.7233333333333334\n",
      "iteration no 517: Loss: 0.5433052451562324, accuracy: 0.7766666666666666\n",
      "iteration no 518: Loss: 0.5476669257519072, accuracy: 0.7266666666666667\n",
      "iteration no 519: Loss: 0.5492240870348145, accuracy: 0.77\n",
      "iteration no 520: Loss: 0.5548878860953822, accuracy: 0.7333333333333333\n",
      "iteration no 521: Loss: 0.5559595984272729, accuracy: 0.7633333333333333\n",
      "iteration no 522: Loss: 0.5625008116801332, accuracy: 0.74\n",
      "iteration no 523: Loss: 0.5617908698379174, accuracy: 0.7633333333333333\n",
      "iteration no 524: Loss: 0.5683029919431645, accuracy: 0.7433333333333333\n",
      "iteration no 525: Loss: 0.5652519397667829, accuracy: 0.76\n",
      "iteration no 526: Loss: 0.5705756128347577, accuracy: 0.7433333333333333\n",
      "iteration no 527: Loss: 0.5651737554809843, accuracy: 0.7633333333333333\n",
      "iteration no 528: Loss: 0.5688846287979573, accuracy: 0.7433333333333333\n",
      "iteration no 529: Loss: 0.561735751955385, accuracy: 0.7633333333333333\n",
      "iteration no 530: Loss: 0.5641015175463345, accuracy: 0.74\n",
      "iteration no 531: Loss: 0.5563154691983163, accuracy: 0.7633333333333333\n",
      "iteration no 532: Loss: 0.5577071872749738, accuracy: 0.74\n",
      "iteration no 533: Loss: 0.5505206742804276, accuracy: 0.77\n",
      "iteration no 534: Loss: 0.551486575080018, accuracy: 0.7433333333333333\n",
      "iteration no 535: Loss: 0.5451573427448491, accuracy: 0.77\n",
      "iteration no 536: Loss: 0.5460717961866682, accuracy: 0.7466666666666667\n",
      "iteration no 537: Loss: 0.5403663269061033, accuracy: 0.7733333333333333\n",
      "iteration no 538: Loss: 0.541123052402112, accuracy: 0.7466666666666667\n",
      "iteration no 539: Loss: 0.5362270732540583, accuracy: 0.78\n",
      "iteration no 540: Loss: 0.5371505886587125, accuracy: 0.7566666666666667\n",
      "iteration no 541: Loss: 0.5328705854362548, accuracy: 0.78\n",
      "iteration no 542: Loss: 0.533905095869088, accuracy: 0.7566666666666667\n",
      "iteration no 543: Loss: 0.5300773403970422, accuracy: 0.7833333333333333\n",
      "iteration no 544: Loss: 0.5312814706068655, accuracy: 0.7533333333333333\n",
      "iteration no 545: Loss: 0.5277120548545245, accuracy: 0.7833333333333333\n",
      "iteration no 546: Loss: 0.5290046412866368, accuracy: 0.7566666666666667\n",
      "iteration no 547: Loss: 0.5256606860532773, accuracy: 0.7833333333333333\n",
      "iteration no 548: Loss: 0.5271221937784717, accuracy: 0.7566666666666667\n",
      "iteration no 549: Loss: 0.5240250040960805, accuracy: 0.7866666666666666\n",
      "iteration no 550: Loss: 0.5255786938004393, accuracy: 0.7566666666666667\n",
      "iteration no 551: Loss: 0.522561819356358, accuracy: 0.79\n",
      "iteration no 552: Loss: 0.5240709617819473, accuracy: 0.76\n",
      "iteration no 553: Loss: 0.5210570741036638, accuracy: 0.79\n",
      "iteration no 554: Loss: 0.5227661163576356, accuracy: 0.76\n",
      "iteration no 555: Loss: 0.5196835672820738, accuracy: 0.79\n",
      "iteration no 556: Loss: 0.5213703257515286, accuracy: 0.76\n",
      "iteration no 557: Loss: 0.5181870023518841, accuracy: 0.79\n",
      "iteration no 558: Loss: 0.5198861636888141, accuracy: 0.76\n",
      "iteration no 559: Loss: 0.5166444054867276, accuracy: 0.79\n",
      "iteration no 560: Loss: 0.5184930013540269, accuracy: 0.7666666666666667\n",
      "iteration no 561: Loss: 0.5151927261469986, accuracy: 0.79\n",
      "iteration no 562: Loss: 0.5169163433552457, accuracy: 0.7666666666666667\n",
      "iteration no 563: Loss: 0.5135084672417951, accuracy: 0.7933333333333333\n",
      "iteration no 564: Loss: 0.5153297522201069, accuracy: 0.77\n",
      "iteration no 565: Loss: 0.5119007665948201, accuracy: 0.7933333333333333\n",
      "iteration no 566: Loss: 0.5137647378590252, accuracy: 0.77\n",
      "iteration no 567: Loss: 0.5102106849814834, accuracy: 0.7933333333333333\n",
      "iteration no 568: Loss: 0.5120250341654068, accuracy: 0.7733333333333333\n",
      "iteration no 569: Loss: 0.508483136675657, accuracy: 0.7933333333333333\n",
      "iteration no 570: Loss: 0.5101271449985956, accuracy: 0.7733333333333333\n",
      "iteration no 571: Loss: 0.5066022909503206, accuracy: 0.7933333333333333\n",
      "iteration no 572: Loss: 0.5083195435107972, accuracy: 0.7766666666666666\n",
      "iteration no 573: Loss: 0.504809595486994, accuracy: 0.7933333333333333\n",
      "iteration no 574: Loss: 0.5064380157510959, accuracy: 0.78\n",
      "iteration no 575: Loss: 0.5030458567169281, accuracy: 0.7933333333333333\n",
      "iteration no 576: Loss: 0.5047622905488273, accuracy: 0.7833333333333333\n",
      "iteration no 577: Loss: 0.5013019007111873, accuracy: 0.7933333333333333\n",
      "iteration no 578: Loss: 0.5029941284850673, accuracy: 0.7866666666666666\n",
      "iteration no 579: Loss: 0.49971458254134227, accuracy: 0.7933333333333333\n",
      "iteration no 580: Loss: 0.5013177182385508, accuracy: 0.7866666666666666\n",
      "iteration no 581: Loss: 0.49778030246208943, accuracy: 0.7933333333333333\n",
      "iteration no 582: Loss: 0.4993476318697484, accuracy: 0.7866666666666666\n",
      "iteration no 583: Loss: 0.4959210297101992, accuracy: 0.7933333333333333\n",
      "iteration no 584: Loss: 0.4974233588807373, accuracy: 0.79\n",
      "iteration no 585: Loss: 0.49413836615586854, accuracy: 0.7966666666666666\n",
      "iteration no 586: Loss: 0.49552542170124686, accuracy: 0.79\n",
      "iteration no 587: Loss: 0.4922330828337965, accuracy: 0.7966666666666666\n",
      "iteration no 588: Loss: 0.4937907618235816, accuracy: 0.79\n",
      "iteration no 589: Loss: 0.490730139619737, accuracy: 0.7966666666666666\n",
      "iteration no 590: Loss: 0.49225742755193835, accuracy: 0.79\n",
      "iteration no 591: Loss: 0.48915507794734586, accuracy: 0.7966666666666666\n",
      "iteration no 592: Loss: 0.490720588682849, accuracy: 0.79\n",
      "iteration no 593: Loss: 0.4876068413712201, accuracy: 0.7966666666666666\n",
      "iteration no 594: Loss: 0.4891146252827413, accuracy: 0.79\n",
      "iteration no 595: Loss: 0.4859873468545861, accuracy: 0.7966666666666666\n",
      "iteration no 596: Loss: 0.48742382953676106, accuracy: 0.79\n",
      "iteration no 597: Loss: 0.48433020579496056, accuracy: 0.7966666666666666\n",
      "iteration no 598: Loss: 0.485827916038407, accuracy: 0.79\n",
      "iteration no 599: Loss: 0.4825630480769882, accuracy: 0.7966666666666666\n",
      "iteration no 600: Loss: 0.48396088389683745, accuracy: 0.7933333333333333\n",
      "iteration no 601: Loss: 0.48076559682402403, accuracy: 0.8\n",
      "iteration no 602: Loss: 0.48211996151758574, accuracy: 0.7933333333333333\n",
      "iteration no 603: Loss: 0.4790589112754469, accuracy: 0.8033333333333333\n",
      "iteration no 604: Loss: 0.4803779772844725, accuracy: 0.7933333333333333\n",
      "iteration no 605: Loss: 0.47713909945753397, accuracy: 0.8033333333333333\n",
      "iteration no 606: Loss: 0.47848114760199156, accuracy: 0.7966666666666666\n",
      "iteration no 607: Loss: 0.47539714070992534, accuracy: 0.8066666666666666\n",
      "iteration no 608: Loss: 0.47672425395217927, accuracy: 0.7966666666666666\n",
      "iteration no 609: Loss: 0.4735896870260347, accuracy: 0.8066666666666666\n",
      "iteration no 610: Loss: 0.4749184691591946, accuracy: 0.7966666666666666\n",
      "iteration no 611: Loss: 0.47178973273086217, accuracy: 0.8066666666666666\n",
      "iteration no 612: Loss: 0.47308572190253334, accuracy: 0.7966666666666666\n",
      "iteration no 613: Loss: 0.4700582044593841, accuracy: 0.8133333333333334\n",
      "iteration no 614: Loss: 0.47125291023061455, accuracy: 0.7966666666666666\n",
      "iteration no 615: Loss: 0.4682442351102059, accuracy: 0.8133333333333334\n",
      "iteration no 616: Loss: 0.4693906986176517, accuracy: 0.8033333333333333\n",
      "iteration no 617: Loss: 0.4662487463958165, accuracy: 0.8133333333333334\n",
      "iteration no 618: Loss: 0.4672339557027338, accuracy: 0.8033333333333333\n",
      "iteration no 619: Loss: 0.464181190039271, accuracy: 0.8133333333333334\n",
      "iteration no 620: Loss: 0.46508606753927084, accuracy: 0.8033333333333333\n",
      "iteration no 621: Loss: 0.4621680346490221, accuracy: 0.8133333333333334\n",
      "iteration no 622: Loss: 0.4630355313978085, accuracy: 0.8066666666666666\n",
      "iteration no 623: Loss: 0.46024728062505554, accuracy: 0.8133333333333334\n",
      "iteration no 624: Loss: 0.461185085317683, accuracy: 0.8066666666666666\n",
      "iteration no 625: Loss: 0.45840803757808307, accuracy: 0.8166666666666667\n",
      "iteration no 626: Loss: 0.4594661465855438, accuracy: 0.8066666666666666\n",
      "iteration no 627: Loss: 0.4568344990906314, accuracy: 0.82\n",
      "iteration no 628: Loss: 0.4578207345990573, accuracy: 0.81\n",
      "iteration no 629: Loss: 0.45518959865112135, accuracy: 0.82\n",
      "iteration no 630: Loss: 0.4562445497764327, accuracy: 0.81\n",
      "iteration no 631: Loss: 0.45370372631186956, accuracy: 0.8233333333333334\n",
      "iteration no 632: Loss: 0.4547201206142959, accuracy: 0.81\n",
      "iteration no 633: Loss: 0.45226747793883126, accuracy: 0.8233333333333334\n",
      "iteration no 634: Loss: 0.45326034570711393, accuracy: 0.8133333333333334\n",
      "iteration no 635: Loss: 0.4507263266670246, accuracy: 0.8233333333333334\n",
      "iteration no 636: Loss: 0.4517116325721974, accuracy: 0.8166666666666667\n",
      "iteration no 637: Loss: 0.44931822046437786, accuracy: 0.8233333333333334\n",
      "iteration no 638: Loss: 0.45016145095316434, accuracy: 0.82\n",
      "iteration no 639: Loss: 0.4477717475728628, accuracy: 0.8266666666666667\n",
      "iteration no 640: Loss: 0.44870186819389607, accuracy: 0.8166666666666667\n",
      "iteration no 641: Loss: 0.4463427143491171, accuracy: 0.83\n",
      "iteration no 642: Loss: 0.4470794622104557, accuracy: 0.82\n",
      "iteration no 643: Loss: 0.44480936274494526, accuracy: 0.83\n",
      "iteration no 644: Loss: 0.4454819938029995, accuracy: 0.82\n",
      "iteration no 645: Loss: 0.44328027322751445, accuracy: 0.8333333333333334\n",
      "iteration no 646: Loss: 0.44377196100263744, accuracy: 0.8233333333333334\n",
      "iteration no 647: Loss: 0.4416163337338416, accuracy: 0.8333333333333334\n",
      "iteration no 648: Loss: 0.4420213601900031, accuracy: 0.8233333333333334\n",
      "iteration no 649: Loss: 0.4400280841705129, accuracy: 0.83\n",
      "iteration no 650: Loss: 0.4403027429411507, accuracy: 0.8233333333333334\n",
      "iteration no 651: Loss: 0.4385537885888576, accuracy: 0.83\n",
      "iteration no 652: Loss: 0.4388039080856691, accuracy: 0.8233333333333334\n",
      "iteration no 653: Loss: 0.4373315942664181, accuracy: 0.83\n",
      "iteration no 654: Loss: 0.43746349033673504, accuracy: 0.8333333333333334\n",
      "iteration no 655: Loss: 0.4362084525315909, accuracy: 0.8333333333333334\n",
      "iteration no 656: Loss: 0.4360791864339809, accuracy: 0.83\n",
      "iteration no 657: Loss: 0.43506076232040036, accuracy: 0.8366666666666667\n",
      "iteration no 658: Loss: 0.43472944603695773, accuracy: 0.8366666666666667\n",
      "iteration no 659: Loss: 0.4341903119774961, accuracy: 0.8366666666666667\n",
      "iteration no 660: Loss: 0.4336979145775965, accuracy: 0.8433333333333334\n",
      "iteration no 661: Loss: 0.43350547758766134, accuracy: 0.8366666666666667\n",
      "iteration no 662: Loss: 0.43266397988477806, accuracy: 0.8433333333333334\n",
      "iteration no 663: Loss: 0.43296273253909207, accuracy: 0.8366666666666667\n",
      "iteration no 664: Loss: 0.4318195033114349, accuracy: 0.8433333333333334\n",
      "iteration no 665: Loss: 0.4328307106251205, accuracy: 0.8333333333333334\n",
      "iteration no 666: Loss: 0.43120032364992594, accuracy: 0.8433333333333334\n",
      "iteration no 667: Loss: 0.432773917835223, accuracy: 0.8366666666666667\n",
      "iteration no 668: Loss: 0.43076699663618717, accuracy: 0.8433333333333334\n",
      "iteration no 669: Loss: 0.43305312245089034, accuracy: 0.8333333333333334\n",
      "iteration no 670: Loss: 0.4305784316091467, accuracy: 0.84\n",
      "iteration no 671: Loss: 0.43357317571466736, accuracy: 0.8333333333333334\n",
      "iteration no 672: Loss: 0.430838176746305, accuracy: 0.84\n",
      "iteration no 673: Loss: 0.4345112789426531, accuracy: 0.84\n",
      "iteration no 674: Loss: 0.4317689632596734, accuracy: 0.84\n",
      "iteration no 675: Loss: 0.43590643471569573, accuracy: 0.8466666666666667\n",
      "iteration no 676: Loss: 0.43333244058444, accuracy: 0.8366666666666667\n",
      "iteration no 677: Loss: 0.4376920162030138, accuracy: 0.85\n",
      "iteration no 678: Loss: 0.43555060280663965, accuracy: 0.8366666666666667\n",
      "iteration no 679: Loss: 0.43947718828670274, accuracy: 0.8633333333333333\n",
      "iteration no 680: Loss: 0.43834810043368727, accuracy: 0.8333333333333334\n",
      "iteration no 681: Loss: 0.44114974753287334, accuracy: 0.8566666666666667\n",
      "iteration no 682: Loss: 0.4415290706937983, accuracy: 0.8366666666666667\n",
      "iteration no 683: Loss: 0.4426363355024815, accuracy: 0.8566666666666667\n",
      "iteration no 684: Loss: 0.44476370714469, accuracy: 0.8333333333333334\n",
      "iteration no 685: Loss: 0.4436435572805639, accuracy: 0.8533333333333334\n",
      "iteration no 686: Loss: 0.4475415458773896, accuracy: 0.83\n",
      "iteration no 687: Loss: 0.44407588447053176, accuracy: 0.85\n",
      "iteration no 688: Loss: 0.44930129944795993, accuracy: 0.83\n",
      "iteration no 689: Loss: 0.44356220579870026, accuracy: 0.8433333333333334\n",
      "iteration no 690: Loss: 0.44951586268479055, accuracy: 0.8366666666666667\n",
      "iteration no 691: Loss: 0.4416771709879548, accuracy: 0.8433333333333334\n",
      "iteration no 692: Loss: 0.44781889921730356, accuracy: 0.84\n",
      "iteration no 693: Loss: 0.4389438627710555, accuracy: 0.8433333333333334\n",
      "iteration no 694: Loss: 0.44487028206484724, accuracy: 0.8433333333333334\n",
      "iteration no 695: Loss: 0.4357218682987737, accuracy: 0.8533333333333334\n",
      "iteration no 696: Loss: 0.44120141914849764, accuracy: 0.85\n",
      "iteration no 697: Loss: 0.43235468602248583, accuracy: 0.85\n",
      "iteration no 698: Loss: 0.43721855148442595, accuracy: 0.85\n",
      "iteration no 699: Loss: 0.4287384888332473, accuracy: 0.8466666666666667\n",
      "iteration no 700: Loss: 0.43296988663322994, accuracy: 0.85\n",
      "iteration no 701: Loss: 0.4252324813706455, accuracy: 0.8466666666666667\n",
      "iteration no 702: Loss: 0.4289089867074176, accuracy: 0.8533333333333334\n",
      "iteration no 703: Loss: 0.42190390653023646, accuracy: 0.8466666666666667\n",
      "iteration no 704: Loss: 0.4251703882037473, accuracy: 0.8566666666666667\n",
      "iteration no 705: Loss: 0.418905779993963, accuracy: 0.8466666666666667\n",
      "iteration no 706: Loss: 0.4218137104149079, accuracy: 0.8566666666666667\n",
      "iteration no 707: Loss: 0.41611246724814926, accuracy: 0.85\n",
      "iteration no 708: Loss: 0.41875165750551724, accuracy: 0.8566666666666667\n",
      "iteration no 709: Loss: 0.4134918929930316, accuracy: 0.8566666666666667\n",
      "iteration no 710: Loss: 0.4157747369353414, accuracy: 0.8566666666666667\n",
      "iteration no 711: Loss: 0.41101176738687745, accuracy: 0.86\n",
      "iteration no 712: Loss: 0.4130279746559911, accuracy: 0.86\n",
      "iteration no 713: Loss: 0.4087511013626345, accuracy: 0.8666666666666667\n",
      "iteration no 714: Loss: 0.4105877525304299, accuracy: 0.86\n",
      "iteration no 715: Loss: 0.406741154848199, accuracy: 0.87\n",
      "iteration no 716: Loss: 0.40838227982845754, accuracy: 0.86\n",
      "iteration no 717: Loss: 0.40477965635530827, accuracy: 0.8733333333333333\n",
      "iteration no 718: Loss: 0.40624236215694887, accuracy: 0.86\n",
      "iteration no 719: Loss: 0.4029735504958018, accuracy: 0.8766666666666667\n",
      "iteration no 720: Loss: 0.40428225444016275, accuracy: 0.86\n",
      "iteration no 721: Loss: 0.4012869715431355, accuracy: 0.8766666666666667\n",
      "iteration no 722: Loss: 0.40245647457891925, accuracy: 0.86\n",
      "iteration no 723: Loss: 0.3997403223303996, accuracy: 0.8766666666666667\n",
      "iteration no 724: Loss: 0.40076155794154705, accuracy: 0.86\n",
      "iteration no 725: Loss: 0.39827365320061253, accuracy: 0.8733333333333333\n",
      "iteration no 726: Loss: 0.3991557195047322, accuracy: 0.86\n",
      "iteration no 727: Loss: 0.396782429216489, accuracy: 0.8733333333333333\n",
      "iteration no 728: Loss: 0.39753193378815294, accuracy: 0.86\n",
      "iteration no 729: Loss: 0.39545340883034136, accuracy: 0.8733333333333333\n",
      "iteration no 730: Loss: 0.39614423524516695, accuracy: 0.86\n",
      "iteration no 731: Loss: 0.39422574989927395, accuracy: 0.87\n",
      "iteration no 732: Loss: 0.3947552175102886, accuracy: 0.8633333333333333\n",
      "iteration no 733: Loss: 0.3929371193574902, accuracy: 0.87\n",
      "iteration no 734: Loss: 0.39331100642687145, accuracy: 0.8633333333333333\n",
      "iteration no 735: Loss: 0.3917862981898408, accuracy: 0.8733333333333333\n",
      "iteration no 736: Loss: 0.39208229841639586, accuracy: 0.8666666666666667\n",
      "iteration no 737: Loss: 0.39076415336419074, accuracy: 0.8733333333333333\n",
      "iteration no 738: Loss: 0.39089062688194864, accuracy: 0.8666666666666667\n",
      "iteration no 739: Loss: 0.3898059255251066, accuracy: 0.8733333333333333\n",
      "iteration no 740: Loss: 0.3898942725239372, accuracy: 0.8666666666666667\n",
      "iteration no 741: Loss: 0.3889753613739823, accuracy: 0.8733333333333333\n",
      "iteration no 742: Loss: 0.3888064795362613, accuracy: 0.8666666666666667\n",
      "iteration no 743: Loss: 0.3880548946576291, accuracy: 0.8766666666666667\n",
      "iteration no 744: Loss: 0.3877491363191314, accuracy: 0.87\n",
      "iteration no 745: Loss: 0.3872230549542429, accuracy: 0.88\n",
      "iteration no 746: Loss: 0.386854183997303, accuracy: 0.8666666666666667\n",
      "iteration no 747: Loss: 0.38651517921730366, accuracy: 0.88\n",
      "iteration no 748: Loss: 0.38580825609007746, accuracy: 0.8666666666666667\n",
      "iteration no 749: Loss: 0.38557314202108944, accuracy: 0.8733333333333333\n",
      "iteration no 750: Loss: 0.3847759762038896, accuracy: 0.8666666666666667\n",
      "iteration no 751: Loss: 0.3847037945309193, accuracy: 0.8733333333333333\n",
      "iteration no 752: Loss: 0.38368023228633336, accuracy: 0.8666666666666667\n",
      "iteration no 753: Loss: 0.3837759018009927, accuracy: 0.8733333333333333\n",
      "iteration no 754: Loss: 0.38270647161738497, accuracy: 0.8666666666666667\n",
      "iteration no 755: Loss: 0.3829422326881008, accuracy: 0.8666666666666667\n",
      "iteration no 756: Loss: 0.38160481145190417, accuracy: 0.8666666666666667\n",
      "iteration no 757: Loss: 0.3819331298244771, accuracy: 0.8766666666666667\n",
      "iteration no 758: Loss: 0.38052823978055295, accuracy: 0.8666666666666667\n",
      "iteration no 759: Loss: 0.3810789130800577, accuracy: 0.8766666666666667\n",
      "iteration no 760: Loss: 0.3796262400766032, accuracy: 0.87\n",
      "iteration no 761: Loss: 0.38026271648585813, accuracy: 0.8766666666666667\n",
      "iteration no 762: Loss: 0.37857610496244504, accuracy: 0.87\n",
      "iteration no 763: Loss: 0.37938281207792607, accuracy: 0.8766666666666667\n",
      "iteration no 764: Loss: 0.3777862639297687, accuracy: 0.87\n",
      "iteration no 765: Loss: 0.3786490482907456, accuracy: 0.8766666666666667\n",
      "iteration no 766: Loss: 0.37680572241685545, accuracy: 0.87\n",
      "iteration no 767: Loss: 0.377827126841119, accuracy: 0.8733333333333333\n",
      "iteration no 768: Loss: 0.37584193876316874, accuracy: 0.87\n",
      "iteration no 769: Loss: 0.37683220305942766, accuracy: 0.87\n",
      "iteration no 770: Loss: 0.3750980639109835, accuracy: 0.87\n",
      "iteration no 771: Loss: 0.37616917635206476, accuracy: 0.87\n",
      "iteration no 772: Loss: 0.37420553615545593, accuracy: 0.8666666666666667\n",
      "iteration no 773: Loss: 0.37527224142529264, accuracy: 0.87\n",
      "iteration no 774: Loss: 0.3732706628775271, accuracy: 0.8733333333333333\n",
      "iteration no 775: Loss: 0.37422229838772864, accuracy: 0.87\n",
      "iteration no 776: Loss: 0.37246859271848715, accuracy: 0.8733333333333333\n",
      "iteration no 777: Loss: 0.3733599423050286, accuracy: 0.87\n",
      "iteration no 778: Loss: 0.3714003491910435, accuracy: 0.8733333333333333\n",
      "iteration no 779: Loss: 0.3721736346826774, accuracy: 0.87\n",
      "iteration no 780: Loss: 0.3704065617918649, accuracy: 0.8766666666666667\n",
      "iteration no 781: Loss: 0.3710785371055476, accuracy: 0.8733333333333333\n",
      "iteration no 782: Loss: 0.3694692657596635, accuracy: 0.88\n",
      "iteration no 783: Loss: 0.3700236492110732, accuracy: 0.8733333333333333\n",
      "iteration no 784: Loss: 0.3683639431875616, accuracy: 0.8833333333333333\n",
      "iteration no 785: Loss: 0.3687434771502754, accuracy: 0.8733333333333333\n",
      "iteration no 786: Loss: 0.3673272211564835, accuracy: 0.8833333333333333\n",
      "iteration no 787: Loss: 0.36762445048557474, accuracy: 0.8766666666666667\n",
      "iteration no 788: Loss: 0.3662565612128797, accuracy: 0.8833333333333333\n",
      "iteration no 789: Loss: 0.36642146687236954, accuracy: 0.8766666666666667\n",
      "iteration no 790: Loss: 0.3651977549494477, accuracy: 0.8766666666666667\n",
      "iteration no 791: Loss: 0.36522115472102634, accuracy: 0.8766666666666667\n",
      "iteration no 792: Loss: 0.36416540755961374, accuracy: 0.8733333333333333\n",
      "iteration no 793: Loss: 0.36408462548958476, accuracy: 0.8766666666666667\n",
      "iteration no 794: Loss: 0.3630423098346437, accuracy: 0.8766666666666667\n",
      "iteration no 795: Loss: 0.36287960612374026, accuracy: 0.88\n",
      "iteration no 796: Loss: 0.36194945984583327, accuracy: 0.8766666666666667\n",
      "iteration no 797: Loss: 0.361684992249833, accuracy: 0.88\n",
      "iteration no 798: Loss: 0.3609554324357973, accuracy: 0.88\n",
      "iteration no 799: Loss: 0.3606325515100338, accuracy: 0.88\n",
      "iteration no 800: Loss: 0.359997844067716, accuracy: 0.88\n",
      "iteration no 801: Loss: 0.3596086980040383, accuracy: 0.88\n",
      "iteration no 802: Loss: 0.3589717313532765, accuracy: 0.8833333333333333\n",
      "iteration no 803: Loss: 0.3585565074011402, accuracy: 0.88\n",
      "iteration no 804: Loss: 0.35795195042584627, accuracy: 0.8833333333333333\n",
      "iteration no 805: Loss: 0.35749163029134473, accuracy: 0.88\n",
      "iteration no 806: Loss: 0.35699788693683354, accuracy: 0.8833333333333333\n",
      "iteration no 807: Loss: 0.35652758779418736, accuracy: 0.88\n",
      "iteration no 808: Loss: 0.3560125961430286, accuracy: 0.8866666666666667\n",
      "iteration no 809: Loss: 0.3555290913007256, accuracy: 0.88\n",
      "iteration no 810: Loss: 0.3551167359043472, accuracy: 0.89\n",
      "iteration no 811: Loss: 0.354636110870173, accuracy: 0.88\n",
      "iteration no 812: Loss: 0.3541825549727615, accuracy: 0.8833333333333333\n",
      "iteration no 813: Loss: 0.3536331753913945, accuracy: 0.88\n",
      "iteration no 814: Loss: 0.3531753733887771, accuracy: 0.8833333333333333\n",
      "iteration no 815: Loss: 0.35262782846354485, accuracy: 0.8766666666666667\n",
      "iteration no 816: Loss: 0.352215604432976, accuracy: 0.8833333333333333\n",
      "iteration no 817: Loss: 0.35168018845997334, accuracy: 0.8766666666666667\n",
      "iteration no 818: Loss: 0.3512877470905058, accuracy: 0.8833333333333333\n",
      "iteration no 819: Loss: 0.35075888932928323, accuracy: 0.8766666666666667\n",
      "iteration no 820: Loss: 0.3503677780453764, accuracy: 0.8833333333333333\n",
      "iteration no 821: Loss: 0.3498606866834169, accuracy: 0.8766666666666667\n",
      "iteration no 822: Loss: 0.34950736363985946, accuracy: 0.8833333333333333\n",
      "iteration no 823: Loss: 0.34899728706347777, accuracy: 0.8766666666666667\n",
      "iteration no 824: Loss: 0.34869390125888683, accuracy: 0.8833333333333333\n",
      "iteration no 825: Loss: 0.34822164998375515, accuracy: 0.8766666666666667\n",
      "iteration no 826: Loss: 0.3479456237657105, accuracy: 0.8833333333333333\n",
      "iteration no 827: Loss: 0.3474663258287397, accuracy: 0.88\n",
      "iteration no 828: Loss: 0.34721005689106255, accuracy: 0.88\n",
      "iteration no 829: Loss: 0.3467676001054817, accuracy: 0.8766666666666667\n",
      "iteration no 830: Loss: 0.34650311855166627, accuracy: 0.88\n",
      "iteration no 831: Loss: 0.34606353335741724, accuracy: 0.8733333333333333\n",
      "iteration no 832: Loss: 0.3458458391735876, accuracy: 0.8766666666666667\n",
      "iteration no 833: Loss: 0.3454276467917871, accuracy: 0.87\n",
      "iteration no 834: Loss: 0.34514351993074527, accuracy: 0.8733333333333333\n",
      "iteration no 835: Loss: 0.3447492088097239, accuracy: 0.87\n",
      "iteration no 836: Loss: 0.344517561863484, accuracy: 0.8733333333333333\n",
      "iteration no 837: Loss: 0.34417907959227473, accuracy: 0.87\n",
      "iteration no 838: Loss: 0.34390006785697275, accuracy: 0.8733333333333333\n",
      "iteration no 839: Loss: 0.3435827189080285, accuracy: 0.8733333333333333\n",
      "iteration no 840: Loss: 0.34331786379569995, accuracy: 0.8733333333333333\n",
      "iteration no 841: Loss: 0.3430128292733334, accuracy: 0.8733333333333333\n",
      "iteration no 842: Loss: 0.3427533105799334, accuracy: 0.8733333333333333\n",
      "iteration no 843: Loss: 0.34245460615954576, accuracy: 0.8733333333333333\n",
      "iteration no 844: Loss: 0.3421767877400387, accuracy: 0.8766666666666667\n",
      "iteration no 845: Loss: 0.34188808459885, accuracy: 0.8733333333333333\n",
      "iteration no 846: Loss: 0.34162444835689537, accuracy: 0.8766666666666667\n",
      "iteration no 847: Loss: 0.34133025628715796, accuracy: 0.8733333333333333\n",
      "iteration no 848: Loss: 0.34103978098687915, accuracy: 0.8766666666666667\n",
      "iteration no 849: Loss: 0.3407644912787966, accuracy: 0.8733333333333333\n",
      "iteration no 850: Loss: 0.34050156338190274, accuracy: 0.8766666666666667\n",
      "iteration no 851: Loss: 0.34021332435682106, accuracy: 0.8733333333333333\n",
      "iteration no 852: Loss: 0.3399327979379233, accuracy: 0.8766666666666667\n",
      "iteration no 853: Loss: 0.3396481686130237, accuracy: 0.8733333333333333\n",
      "iteration no 854: Loss: 0.3393762220381587, accuracy: 0.8766666666666667\n",
      "iteration no 855: Loss: 0.3390904648464734, accuracy: 0.8733333333333333\n",
      "iteration no 856: Loss: 0.3388146636989614, accuracy: 0.8766666666666667\n",
      "iteration no 857: Loss: 0.3385356076080814, accuracy: 0.8733333333333333\n",
      "iteration no 858: Loss: 0.33827461736841996, accuracy: 0.8766666666666667\n",
      "iteration no 859: Loss: 0.3380018795958723, accuracy: 0.8766666666666667\n",
      "iteration no 860: Loss: 0.33772314455455915, accuracy: 0.8766666666666667\n",
      "iteration no 861: Loss: 0.33744458127293764, accuracy: 0.8766666666666667\n",
      "iteration no 862: Loss: 0.33719068384771184, accuracy: 0.8766666666666667\n",
      "iteration no 863: Loss: 0.3369250531055818, accuracy: 0.8766666666666667\n",
      "iteration no 864: Loss: 0.33667646220709757, accuracy: 0.88\n",
      "iteration no 865: Loss: 0.3364204454480127, accuracy: 0.88\n",
      "iteration no 866: Loss: 0.3361443131475843, accuracy: 0.88\n",
      "iteration no 867: Loss: 0.3358820092220857, accuracy: 0.8833333333333333\n",
      "iteration no 868: Loss: 0.3356457323179125, accuracy: 0.8833333333333333\n",
      "iteration no 869: Loss: 0.3353923221063153, accuracy: 0.8833333333333333\n",
      "iteration no 870: Loss: 0.33515114049994454, accuracy: 0.8833333333333333\n",
      "iteration no 871: Loss: 0.3348965504285205, accuracy: 0.8866666666666667\n",
      "iteration no 872: Loss: 0.3346465279140717, accuracy: 0.8833333333333333\n",
      "iteration no 873: Loss: 0.33439926766139605, accuracy: 0.8866666666666667\n",
      "iteration no 874: Loss: 0.3341511994666917, accuracy: 0.88\n",
      "iteration no 875: Loss: 0.3339102380088461, accuracy: 0.8866666666666667\n",
      "iteration no 876: Loss: 0.3336735827875993, accuracy: 0.8833333333333333\n",
      "iteration no 877: Loss: 0.3334329332649132, accuracy: 0.8866666666666667\n",
      "iteration no 878: Loss: 0.3332214510159278, accuracy: 0.8833333333333333\n",
      "iteration no 879: Loss: 0.3329836988804067, accuracy: 0.8833333333333333\n",
      "iteration no 880: Loss: 0.33276331419327443, accuracy: 0.8833333333333333\n",
      "iteration no 881: Loss: 0.332529407752846, accuracy: 0.8833333333333333\n",
      "iteration no 882: Loss: 0.33231585170230354, accuracy: 0.8833333333333333\n",
      "iteration no 883: Loss: 0.3320899440953099, accuracy: 0.88\n",
      "iteration no 884: Loss: 0.33185192299696914, accuracy: 0.8833333333333333\n",
      "iteration no 885: Loss: 0.3316363082168761, accuracy: 0.88\n",
      "iteration no 886: Loss: 0.3314298182781385, accuracy: 0.8833333333333333\n",
      "iteration no 887: Loss: 0.3312139008642265, accuracy: 0.88\n",
      "iteration no 888: Loss: 0.33101793397977125, accuracy: 0.8833333333333333\n",
      "iteration no 889: Loss: 0.3308042819274768, accuracy: 0.88\n",
      "iteration no 890: Loss: 0.3306042612358663, accuracy: 0.8833333333333333\n",
      "iteration no 891: Loss: 0.33038791394110356, accuracy: 0.88\n",
      "iteration no 892: Loss: 0.33017033471808666, accuracy: 0.8833333333333333\n",
      "iteration no 893: Loss: 0.3299577718904794, accuracy: 0.88\n",
      "iteration no 894: Loss: 0.32973832039975925, accuracy: 0.8866666666666667\n",
      "iteration no 895: Loss: 0.32952065765600863, accuracy: 0.88\n",
      "iteration no 896: Loss: 0.32931476633375484, accuracy: 0.8866666666666667\n",
      "iteration no 897: Loss: 0.3291091009345979, accuracy: 0.88\n",
      "iteration no 898: Loss: 0.32890864480290255, accuracy: 0.8866666666666667\n",
      "iteration no 899: Loss: 0.328707340648815, accuracy: 0.8833333333333333\n",
      "iteration no 900: Loss: 0.3285125626135155, accuracy: 0.8866666666666667\n",
      "iteration no 901: Loss: 0.3283136697108707, accuracy: 0.8833333333333333\n",
      "iteration no 902: Loss: 0.3281190687982533, accuracy: 0.8866666666666667\n",
      "iteration no 903: Loss: 0.3279162652916991, accuracy: 0.8833333333333333\n",
      "iteration no 904: Loss: 0.32770972242607, accuracy: 0.8866666666666667\n",
      "iteration no 905: Loss: 0.3274997542963118, accuracy: 0.8833333333333333\n",
      "iteration no 906: Loss: 0.3272754698543016, accuracy: 0.8866666666666667\n",
      "iteration no 907: Loss: 0.32706849229128926, accuracy: 0.8866666666666667\n",
      "iteration no 908: Loss: 0.32686307728060193, accuracy: 0.89\n",
      "iteration no 909: Loss: 0.32666252242053917, accuracy: 0.8866666666666667\n",
      "iteration no 910: Loss: 0.32646380338688635, accuracy: 0.89\n",
      "iteration no 911: Loss: 0.32626132948042275, accuracy: 0.8866666666666667\n",
      "iteration no 912: Loss: 0.3260608980985501, accuracy: 0.89\n",
      "iteration no 913: Loss: 0.32585434561817356, accuracy: 0.8866666666666667\n",
      "iteration no 914: Loss: 0.3256577865358064, accuracy: 0.89\n",
      "iteration no 915: Loss: 0.3254696840197194, accuracy: 0.8866666666666667\n",
      "iteration no 916: Loss: 0.3252816186853776, accuracy: 0.89\n",
      "iteration no 917: Loss: 0.32509234681819865, accuracy: 0.8866666666666667\n",
      "iteration no 918: Loss: 0.3249151726239707, accuracy: 0.89\n",
      "iteration no 919: Loss: 0.32473342002851663, accuracy: 0.89\n",
      "iteration no 920: Loss: 0.32454932669720565, accuracy: 0.89\n",
      "iteration no 921: Loss: 0.32436440591438814, accuracy: 0.89\n",
      "iteration no 922: Loss: 0.32418540868770895, accuracy: 0.89\n",
      "iteration no 923: Loss: 0.32400382023666413, accuracy: 0.89\n",
      "iteration no 924: Loss: 0.32382846802984055, accuracy: 0.89\n",
      "iteration no 925: Loss: 0.32364990053488935, accuracy: 0.89\n",
      "iteration no 926: Loss: 0.3234765592857403, accuracy: 0.89\n",
      "iteration no 927: Loss: 0.3232978030457847, accuracy: 0.89\n",
      "iteration no 928: Loss: 0.32313070206715505, accuracy: 0.89\n",
      "iteration no 929: Loss: 0.3229542559538457, accuracy: 0.8866666666666667\n",
      "iteration no 930: Loss: 0.3227696207710587, accuracy: 0.89\n",
      "iteration no 931: Loss: 0.3225925068520172, accuracy: 0.8866666666666667\n",
      "iteration no 932: Loss: 0.32241496897318506, accuracy: 0.89\n",
      "iteration no 933: Loss: 0.3222428760891964, accuracy: 0.8866666666666667\n",
      "iteration no 934: Loss: 0.3220769655658567, accuracy: 0.89\n",
      "iteration no 935: Loss: 0.32190364938350796, accuracy: 0.8866666666666667\n",
      "iteration no 936: Loss: 0.32173968682544174, accuracy: 0.89\n",
      "iteration no 937: Loss: 0.3215697906566579, accuracy: 0.8866666666666667\n",
      "iteration no 938: Loss: 0.32140392048521715, accuracy: 0.89\n",
      "iteration no 939: Loss: 0.32123605511484077, accuracy: 0.8866666666666667\n",
      "iteration no 940: Loss: 0.32107780086946874, accuracy: 0.89\n",
      "iteration no 941: Loss: 0.32091792642919686, accuracy: 0.8866666666666667\n",
      "iteration no 942: Loss: 0.3207659684892015, accuracy: 0.89\n",
      "iteration no 943: Loss: 0.320601433599934, accuracy: 0.8866666666666667\n",
      "iteration no 944: Loss: 0.3204443057513139, accuracy: 0.89\n",
      "iteration no 945: Loss: 0.32028412453580074, accuracy: 0.8866666666666667\n",
      "iteration no 946: Loss: 0.3201272950380294, accuracy: 0.8866666666666667\n",
      "iteration no 947: Loss: 0.319965056202897, accuracy: 0.8866666666666667\n",
      "iteration no 948: Loss: 0.3198148999676904, accuracy: 0.89\n",
      "iteration no 949: Loss: 0.31965940809934124, accuracy: 0.8866666666666667\n",
      "iteration no 950: Loss: 0.31950502440213413, accuracy: 0.89\n",
      "iteration no 951: Loss: 0.3193436378669148, accuracy: 0.8866666666666667\n",
      "iteration no 952: Loss: 0.31918925507724744, accuracy: 0.8866666666666667\n",
      "iteration no 953: Loss: 0.31902597292228924, accuracy: 0.8866666666666667\n",
      "iteration no 954: Loss: 0.31887158796106996, accuracy: 0.8866666666666667\n",
      "iteration no 955: Loss: 0.3187178166573139, accuracy: 0.8866666666666667\n",
      "iteration no 956: Loss: 0.318568435038649, accuracy: 0.8866666666666667\n",
      "iteration no 957: Loss: 0.31841344967414903, accuracy: 0.8866666666666667\n",
      "iteration no 958: Loss: 0.3182674861370318, accuracy: 0.8866666666666667\n",
      "iteration no 959: Loss: 0.31811457117939923, accuracy: 0.89\n",
      "iteration no 960: Loss: 0.3179654831034918, accuracy: 0.8866666666666667\n",
      "iteration no 961: Loss: 0.3178130903117127, accuracy: 0.8866666666666667\n",
      "iteration no 962: Loss: 0.3176653882876811, accuracy: 0.8866666666666667\n",
      "iteration no 963: Loss: 0.3175143426367332, accuracy: 0.8866666666666667\n",
      "iteration no 964: Loss: 0.3173682719382174, accuracy: 0.8866666666666667\n",
      "iteration no 965: Loss: 0.31721835365227374, accuracy: 0.8866666666666667\n",
      "iteration no 966: Loss: 0.31707649116545256, accuracy: 0.8866666666666667\n",
      "iteration no 967: Loss: 0.3169269118881379, accuracy: 0.8866666666666667\n",
      "iteration no 968: Loss: 0.3167812421891489, accuracy: 0.8866666666666667\n",
      "iteration no 969: Loss: 0.31663068352983453, accuracy: 0.8866666666666667\n",
      "iteration no 970: Loss: 0.31648416447301686, accuracy: 0.8866666666666667\n",
      "iteration no 971: Loss: 0.3163386872133891, accuracy: 0.8866666666666667\n",
      "iteration no 972: Loss: 0.31619345898542156, accuracy: 0.8866666666666667\n",
      "iteration no 973: Loss: 0.3160458549118612, accuracy: 0.8866666666666667\n",
      "iteration no 974: Loss: 0.3159015811373731, accuracy: 0.8866666666666667\n",
      "iteration no 975: Loss: 0.31575715224412315, accuracy: 0.8866666666666667\n",
      "iteration no 976: Loss: 0.31561663717972627, accuracy: 0.8866666666666667\n",
      "iteration no 977: Loss: 0.3154750422071074, accuracy: 0.8866666666666667\n",
      "iteration no 978: Loss: 0.3153363756065671, accuracy: 0.8866666666666667\n",
      "iteration no 979: Loss: 0.31519373341433987, accuracy: 0.8866666666666667\n",
      "iteration no 980: Loss: 0.3150570022689785, accuracy: 0.8866666666666667\n",
      "iteration no 981: Loss: 0.31491807995788984, accuracy: 0.8866666666666667\n",
      "iteration no 982: Loss: 0.31478149058584054, accuracy: 0.8866666666666667\n",
      "iteration no 983: Loss: 0.314642150096733, accuracy: 0.8866666666666667\n",
      "iteration no 984: Loss: 0.3145133691974008, accuracy: 0.8866666666666667\n",
      "iteration no 985: Loss: 0.31437560827371014, accuracy: 0.8866666666666667\n",
      "iteration no 986: Loss: 0.31424182101406345, accuracy: 0.8866666666666667\n",
      "iteration no 987: Loss: 0.3141055325654877, accuracy: 0.8866666666666667\n",
      "iteration no 988: Loss: 0.3139756179608056, accuracy: 0.8866666666666667\n",
      "iteration no 989: Loss: 0.31384083469338275, accuracy: 0.8866666666666667\n",
      "iteration no 990: Loss: 0.3137133738720339, accuracy: 0.8866666666666667\n",
      "iteration no 991: Loss: 0.3135791404949497, accuracy: 0.8866666666666667\n",
      "iteration no 992: Loss: 0.31344624229113893, accuracy: 0.8866666666666667\n",
      "iteration no 993: Loss: 0.31331505223755185, accuracy: 0.8866666666666667\n",
      "iteration no 994: Loss: 0.3131887455102488, accuracy: 0.89\n",
      "iteration no 995: Loss: 0.3130582259682998, accuracy: 0.8866666666666667\n",
      "iteration no 996: Loss: 0.3129346655797762, accuracy: 0.89\n",
      "iteration no 997: Loss: 0.31280449820865774, accuracy: 0.8866666666666667\n",
      "iteration no 998: Loss: 0.312680728855855, accuracy: 0.89\n",
      "iteration no 999: Loss: 0.3125530948817702, accuracy: 0.8866666666666667\n",
      "iteration no 1000: Loss: 0.3124302579654184, accuracy: 0.89\n",
      "iteration no 1001: Loss: 0.312301540104406, accuracy: 0.8866666666666667\n",
      "iteration no 1002: Loss: 0.3121797788521691, accuracy: 0.89\n",
      "iteration no 1003: Loss: 0.31205303237614523, accuracy: 0.8866666666666667\n",
      "iteration no 1004: Loss: 0.3119309769679026, accuracy: 0.89\n",
      "iteration no 1005: Loss: 0.3118054287949299, accuracy: 0.8866666666666667\n",
      "iteration no 1006: Loss: 0.31168406713282293, accuracy: 0.89\n",
      "iteration no 1007: Loss: 0.31156004795566306, accuracy: 0.8866666666666667\n",
      "iteration no 1008: Loss: 0.311439105073986, accuracy: 0.89\n",
      "iteration no 1009: Loss: 0.31131791602505243, accuracy: 0.8866666666666667\n",
      "iteration no 1010: Loss: 0.3111971706204777, accuracy: 0.8933333333333333\n",
      "iteration no 1011: Loss: 0.3110750198197864, accuracy: 0.8866666666666667\n",
      "iteration no 1012: Loss: 0.3109547729759546, accuracy: 0.8966666666666666\n",
      "iteration no 1013: Loss: 0.31083622737695876, accuracy: 0.8866666666666667\n",
      "iteration no 1014: Loss: 0.3107177885766262, accuracy: 0.9\n",
      "iteration no 1015: Loss: 0.31060089935094776, accuracy: 0.89\n",
      "iteration no 1016: Loss: 0.31048395855358973, accuracy: 0.9033333333333333\n",
      "iteration no 1017: Loss: 0.31036887377364764, accuracy: 0.89\n",
      "iteration no 1018: Loss: 0.3102536874596466, accuracy: 0.9033333333333333\n",
      "iteration no 1019: Loss: 0.31014004620573055, accuracy: 0.89\n",
      "iteration no 1020: Loss: 0.3100230326475324, accuracy: 0.9033333333333333\n",
      "iteration no 1021: Loss: 0.3099068165250244, accuracy: 0.8933333333333333\n",
      "iteration no 1022: Loss: 0.30979253731547773, accuracy: 0.9033333333333333\n",
      "iteration no 1023: Loss: 0.30968144130261405, accuracy: 0.8933333333333333\n",
      "iteration no 1024: Loss: 0.30956752997447307, accuracy: 0.9033333333333333\n",
      "iteration no 1025: Loss: 0.3094542255441328, accuracy: 0.8933333333333333\n",
      "iteration no 1026: Loss: 0.3093402866973104, accuracy: 0.9033333333333333\n",
      "iteration no 1027: Loss: 0.3092314397242473, accuracy: 0.8933333333333333\n",
      "iteration no 1028: Loss: 0.309118301666488, accuracy: 0.9033333333333333\n",
      "iteration no 1029: Loss: 0.30900339992237846, accuracy: 0.8966666666666666\n",
      "iteration no 1030: Loss: 0.3088920623454334, accuracy: 0.9033333333333333\n",
      "iteration no 1031: Loss: 0.3087835202540161, accuracy: 0.8966666666666666\n",
      "iteration no 1032: Loss: 0.3086741549765677, accuracy: 0.9033333333333333\n",
      "iteration no 1033: Loss: 0.30856484186983035, accuracy: 0.8966666666666666\n",
      "iteration no 1034: Loss: 0.30845461728784035, accuracy: 0.9033333333333333\n",
      "iteration no 1035: Loss: 0.3083492643525834, accuracy: 0.8966666666666666\n",
      "iteration no 1036: Loss: 0.3082389364963295, accuracy: 0.9033333333333333\n",
      "iteration no 1037: Loss: 0.3081296297865783, accuracy: 0.8966666666666666\n",
      "iteration no 1038: Loss: 0.30802299613829587, accuracy: 0.9033333333333333\n",
      "iteration no 1039: Loss: 0.3079161706177248, accuracy: 0.8966666666666666\n",
      "iteration no 1040: Loss: 0.3078090299193209, accuracy: 0.9033333333333333\n",
      "iteration no 1041: Loss: 0.3077048575872629, accuracy: 0.8966666666666666\n",
      "iteration no 1042: Loss: 0.30760004852522305, accuracy: 0.9033333333333333\n",
      "iteration no 1043: Loss: 0.3074957679637623, accuracy: 0.8966666666666666\n",
      "iteration no 1044: Loss: 0.3073898877017347, accuracy: 0.9033333333333333\n",
      "iteration no 1045: Loss: 0.3072859570460812, accuracy: 0.8966666666666666\n",
      "iteration no 1046: Loss: 0.30718029776457806, accuracy: 0.9033333333333333\n",
      "iteration no 1047: Loss: 0.3070772224743777, accuracy: 0.9\n",
      "iteration no 1048: Loss: 0.30697345406223864, accuracy: 0.9033333333333333\n",
      "iteration no 1049: Loss: 0.30687351098317267, accuracy: 0.9\n",
      "iteration no 1050: Loss: 0.3067690198165547, accuracy: 0.9066666666666666\n",
      "iteration no 1051: Loss: 0.3066652722793658, accuracy: 0.9\n",
      "iteration no 1052: Loss: 0.306562114990559, accuracy: 0.9066666666666666\n",
      "iteration no 1053: Loss: 0.30646178836251686, accuracy: 0.9\n",
      "iteration no 1054: Loss: 0.30636005991786797, accuracy: 0.9066666666666666\n",
      "iteration no 1055: Loss: 0.3062618452550308, accuracy: 0.9\n",
      "iteration no 1056: Loss: 0.3061596792850053, accuracy: 0.9066666666666666\n",
      "iteration no 1057: Loss: 0.3060595589009441, accuracy: 0.9\n",
      "iteration no 1058: Loss: 0.30595970971122965, accuracy: 0.9066666666666666\n",
      "iteration no 1059: Loss: 0.3058624211154238, accuracy: 0.9\n",
      "iteration no 1060: Loss: 0.3057611947591478, accuracy: 0.9066666666666666\n",
      "iteration no 1061: Loss: 0.3056646025310397, accuracy: 0.9\n",
      "iteration no 1062: Loss: 0.3055652564704988, accuracy: 0.9066666666666666\n",
      "iteration no 1063: Loss: 0.30546951076061785, accuracy: 0.9\n",
      "iteration no 1064: Loss: 0.3053681814078115, accuracy: 0.9066666666666666\n",
      "iteration no 1065: Loss: 0.30527011392539716, accuracy: 0.9\n",
      "iteration no 1066: Loss: 0.3051720284946084, accuracy: 0.9066666666666666\n",
      "iteration no 1067: Loss: 0.3050768140182726, accuracy: 0.9\n",
      "iteration no 1068: Loss: 0.30497876052018275, accuracy: 0.9066666666666666\n",
      "iteration no 1069: Loss: 0.304884327109743, accuracy: 0.9\n",
      "iteration no 1070: Loss: 0.3047865327156092, accuracy: 0.9066666666666666\n",
      "iteration no 1071: Loss: 0.30469399977994094, accuracy: 0.9\n",
      "iteration no 1072: Loss: 0.30459804464028645, accuracy: 0.9066666666666666\n",
      "iteration no 1073: Loss: 0.3045034593002137, accuracy: 0.9033333333333333\n",
      "iteration no 1074: Loss: 0.304407505174546, accuracy: 0.9066666666666666\n",
      "iteration no 1075: Loss: 0.30431554680452655, accuracy: 0.9\n",
      "iteration no 1076: Loss: 0.3042214550491112, accuracy: 0.9066666666666666\n",
      "iteration no 1077: Loss: 0.3041306246446443, accuracy: 0.9\n",
      "iteration no 1078: Loss: 0.3040362466959526, accuracy: 0.9066666666666666\n",
      "iteration no 1079: Loss: 0.3039441735381283, accuracy: 0.9033333333333333\n",
      "iteration no 1080: Loss: 0.3038515433617145, accuracy: 0.9066666666666666\n",
      "iteration no 1081: Loss: 0.3037613759531339, accuracy: 0.9033333333333333\n",
      "iteration no 1082: Loss: 0.30366990813640693, accuracy: 0.91\n",
      "iteration no 1083: Loss: 0.3035783737733767, accuracy: 0.9033333333333333\n",
      "iteration no 1084: Loss: 0.30348553051140925, accuracy: 0.91\n",
      "iteration no 1085: Loss: 0.30339636119001784, accuracy: 0.9033333333333333\n",
      "iteration no 1086: Loss: 0.303304724620028, accuracy: 0.91\n",
      "iteration no 1087: Loss: 0.3032153755602135, accuracy: 0.9066666666666666\n",
      "iteration no 1088: Loss: 0.3031225638617376, accuracy: 0.91\n",
      "iteration no 1089: Loss: 0.30303140791328254, accuracy: 0.9066666666666666\n",
      "iteration no 1090: Loss: 0.30293962075746184, accuracy: 0.91\n",
      "iteration no 1091: Loss: 0.3028508836837936, accuracy: 0.9066666666666666\n",
      "iteration no 1092: Loss: 0.30276009021926836, accuracy: 0.91\n",
      "iteration no 1093: Loss: 0.30267232428895613, accuracy: 0.9066666666666666\n",
      "iteration no 1094: Loss: 0.30258092207716414, accuracy: 0.91\n",
      "iteration no 1095: Loss: 0.30249341742774816, accuracy: 0.9066666666666666\n",
      "iteration no 1096: Loss: 0.3024040928643385, accuracy: 0.91\n",
      "iteration no 1097: Loss: 0.3023183434125214, accuracy: 0.9066666666666666\n",
      "iteration no 1098: Loss: 0.302227472894454, accuracy: 0.91\n",
      "iteration no 1099: Loss: 0.30213724762700084, accuracy: 0.9066666666666666\n",
      "iteration no 1100: Loss: 0.30204608856044246, accuracy: 0.91\n",
      "iteration no 1101: Loss: 0.30195761238647306, accuracy: 0.91\n",
      "iteration no 1102: Loss: 0.3018672055828107, accuracy: 0.91\n",
      "iteration no 1103: Loss: 0.3017805339669931, accuracy: 0.91\n",
      "iteration no 1104: Loss: 0.3016897399175022, accuracy: 0.91\n",
      "iteration no 1105: Loss: 0.3016028539902588, accuracy: 0.91\n",
      "iteration no 1106: Loss: 0.3015129728378414, accuracy: 0.91\n",
      "iteration no 1107: Loss: 0.301426973252657, accuracy: 0.91\n",
      "iteration no 1108: Loss: 0.3013354479530594, accuracy: 0.91\n",
      "iteration no 1109: Loss: 0.30124755694619354, accuracy: 0.91\n",
      "iteration no 1110: Loss: 0.3011566027440411, accuracy: 0.91\n",
      "iteration no 1111: Loss: 0.301070632063239, accuracy: 0.91\n",
      "iteration no 1112: Loss: 0.30098000230336047, accuracy: 0.91\n",
      "iteration no 1113: Loss: 0.30089411523281173, accuracy: 0.91\n",
      "iteration no 1114: Loss: 0.3008048152101134, accuracy: 0.91\n",
      "iteration no 1115: Loss: 0.30071988226237717, accuracy: 0.91\n",
      "iteration no 1116: Loss: 0.3006306982588882, accuracy: 0.91\n",
      "iteration no 1117: Loss: 0.30054602096815214, accuracy: 0.91\n",
      "iteration no 1118: Loss: 0.30045738074655043, accuracy: 0.91\n",
      "iteration no 1119: Loss: 0.30037367464956516, accuracy: 0.91\n",
      "iteration no 1120: Loss: 0.30028691826056403, accuracy: 0.91\n",
      "iteration no 1121: Loss: 0.3002015599313648, accuracy: 0.91\n",
      "iteration no 1122: Loss: 0.30011502958736225, accuracy: 0.91\n",
      "iteration no 1123: Loss: 0.3000320449196361, accuracy: 0.91\n",
      "iteration no 1124: Loss: 0.29994624624394817, accuracy: 0.91\n",
      "iteration no 1125: Loss: 0.29986268525503934, accuracy: 0.91\n",
      "iteration no 1126: Loss: 0.29977903289406865, accuracy: 0.91\n",
      "iteration no 1127: Loss: 0.2996980141182364, accuracy: 0.91\n",
      "iteration no 1128: Loss: 0.29961353589257755, accuracy: 0.91\n",
      "iteration no 1129: Loss: 0.29953363303481034, accuracy: 0.91\n",
      "iteration no 1130: Loss: 0.2994497510768964, accuracy: 0.91\n",
      "iteration no 1131: Loss: 0.2993690271327255, accuracy: 0.91\n",
      "iteration no 1132: Loss: 0.2992853718069328, accuracy: 0.91\n",
      "iteration no 1133: Loss: 0.29920629868325627, accuracy: 0.91\n",
      "iteration no 1134: Loss: 0.299123220220279, accuracy: 0.9133333333333333\n",
      "iteration no 1135: Loss: 0.29904385272067086, accuracy: 0.91\n",
      "iteration no 1136: Loss: 0.2989621194352123, accuracy: 0.9166666666666666\n",
      "iteration no 1137: Loss: 0.298882549352004, accuracy: 0.91\n",
      "iteration no 1138: Loss: 0.298798775492007, accuracy: 0.9166666666666666\n",
      "iteration no 1139: Loss: 0.29871860206259326, accuracy: 0.91\n",
      "iteration no 1140: Loss: 0.29863619104106454, accuracy: 0.9166666666666666\n",
      "iteration no 1141: Loss: 0.2985539044009963, accuracy: 0.91\n",
      "iteration no 1142: Loss: 0.29846904944925634, accuracy: 0.9166666666666666\n",
      "iteration no 1143: Loss: 0.29838770341072307, accuracy: 0.91\n",
      "iteration no 1144: Loss: 0.2983034724488312, accuracy: 0.9166666666666666\n",
      "iteration no 1145: Loss: 0.2982214203589203, accuracy: 0.91\n",
      "iteration no 1146: Loss: 0.29813723035248435, accuracy: 0.9133333333333333\n",
      "iteration no 1147: Loss: 0.2980542106605356, accuracy: 0.91\n",
      "iteration no 1148: Loss: 0.2979719664557635, accuracy: 0.9166666666666666\n",
      "iteration no 1149: Loss: 0.2978906554173976, accuracy: 0.91\n",
      "iteration no 1150: Loss: 0.2978081524768223, accuracy: 0.9166666666666666\n",
      "iteration no 1151: Loss: 0.297728627766147, accuracy: 0.91\n",
      "iteration no 1152: Loss: 0.2976475632505193, accuracy: 0.9166666666666666\n",
      "iteration no 1153: Loss: 0.29756880873832364, accuracy: 0.91\n",
      "iteration no 1154: Loss: 0.2974868422696121, accuracy: 0.9166666666666666\n",
      "iteration no 1155: Loss: 0.2974078767542898, accuracy: 0.9133333333333333\n",
      "iteration no 1156: Loss: 0.2973282563500749, accuracy: 0.9166666666666666\n",
      "iteration no 1157: Loss: 0.2972492748273513, accuracy: 0.9133333333333333\n",
      "iteration no 1158: Loss: 0.29716985806000273, accuracy: 0.9166666666666666\n",
      "iteration no 1159: Loss: 0.29709303030396006, accuracy: 0.9166666666666666\n",
      "iteration no 1160: Loss: 0.29701356908718063, accuracy: 0.9166666666666666\n",
      "iteration no 1161: Loss: 0.29693642725725256, accuracy: 0.9166666666666666\n",
      "iteration no 1162: Loss: 0.29685942058535925, accuracy: 0.9166666666666666\n",
      "iteration no 1163: Loss: 0.29678226526640017, accuracy: 0.9166666666666666\n",
      "iteration no 1164: Loss: 0.29670386847169, accuracy: 0.9166666666666666\n",
      "iteration no 1165: Loss: 0.2966277885111096, accuracy: 0.9166666666666666\n",
      "iteration no 1166: Loss: 0.2965501852276761, accuracy: 0.9166666666666666\n",
      "iteration no 1167: Loss: 0.29647457752043893, accuracy: 0.9166666666666666\n",
      "iteration no 1168: Loss: 0.29639907308307173, accuracy: 0.9166666666666666\n",
      "iteration no 1169: Loss: 0.29632598145670264, accuracy: 0.92\n",
      "iteration no 1170: Loss: 0.2962521101885449, accuracy: 0.9166666666666666\n",
      "iteration no 1171: Loss: 0.2961795106760929, accuracy: 0.92\n",
      "iteration no 1172: Loss: 0.29610508471304, accuracy: 0.92\n",
      "iteration no 1173: Loss: 0.2960320808787328, accuracy: 0.92\n",
      "iteration no 1174: Loss: 0.2959592872086173, accuracy: 0.92\n",
      "iteration no 1175: Loss: 0.2958878671453426, accuracy: 0.92\n",
      "iteration no 1176: Loss: 0.29581477757766494, accuracy: 0.92\n",
      "iteration no 1177: Loss: 0.2957434660206566, accuracy: 0.92\n",
      "iteration no 1178: Loss: 0.29567076879230436, accuracy: 0.92\n",
      "iteration no 1179: Loss: 0.29560008848639296, accuracy: 0.92\n",
      "iteration no 1180: Loss: 0.29552763218683664, accuracy: 0.92\n",
      "iteration no 1181: Loss: 0.29545452725188515, accuracy: 0.92\n",
      "iteration no 1182: Loss: 0.2953749907133677, accuracy: 0.92\n",
      "iteration no 1183: Loss: 0.2952965689207567, accuracy: 0.92\n",
      "iteration no 1184: Loss: 0.2952189633052024, accuracy: 0.92\n",
      "iteration no 1185: Loss: 0.29514108426945407, accuracy: 0.9233333333333333\n",
      "iteration no 1186: Loss: 0.2950652548036214, accuracy: 0.92\n",
      "iteration no 1187: Loss: 0.29498849474844235, accuracy: 0.9233333333333333\n",
      "iteration no 1188: Loss: 0.2949093280686157, accuracy: 0.92\n",
      "iteration no 1189: Loss: 0.2948005452251258, accuracy: 0.92\n",
      "iteration no 1190: Loss: 0.2946951098381297, accuracy: 0.92\n",
      "iteration no 1191: Loss: 0.2945909297091332, accuracy: 0.92\n",
      "iteration no 1192: Loss: 0.2944886745638732, accuracy: 0.9166666666666666\n",
      "iteration no 1193: Loss: 0.2943895994378555, accuracy: 0.92\n",
      "iteration no 1194: Loss: 0.2942974402845252, accuracy: 0.9166666666666666\n",
      "iteration no 1195: Loss: 0.2942061927090951, accuracy: 0.92\n",
      "iteration no 1196: Loss: 0.29411731698438937, accuracy: 0.92\n",
      "iteration no 1197: Loss: 0.2940264470828896, accuracy: 0.92\n",
      "iteration no 1198: Loss: 0.29394035191989665, accuracy: 0.92\n",
      "iteration no 1199: Loss: 0.2938504868826732, accuracy: 0.92\n",
      "iteration no 1200: Loss: 0.2937635693975504, accuracy: 0.92\n",
      "iteration no 1201: Loss: 0.29367842744514777, accuracy: 0.92\n",
      "iteration no 1202: Loss: 0.2935949837219104, accuracy: 0.92\n",
      "iteration no 1203: Loss: 0.2935117357555907, accuracy: 0.92\n",
      "iteration no 1204: Loss: 0.2934308979161983, accuracy: 0.9233333333333333\n",
      "iteration no 1205: Loss: 0.29335237487022603, accuracy: 0.9233333333333333\n",
      "iteration no 1206: Loss: 0.29327469659030586, accuracy: 0.9233333333333333\n",
      "iteration no 1207: Loss: 0.2931975141879223, accuracy: 0.9233333333333333\n",
      "iteration no 1208: Loss: 0.29312177863365607, accuracy: 0.9233333333333333\n",
      "iteration no 1209: Loss: 0.2930452612743772, accuracy: 0.9233333333333333\n",
      "iteration no 1210: Loss: 0.29296943505935474, accuracy: 0.9233333333333333\n",
      "iteration no 1211: Loss: 0.29289372341464137, accuracy: 0.9233333333333333\n",
      "iteration no 1212: Loss: 0.2928194896176994, accuracy: 0.9233333333333333\n",
      "iteration no 1213: Loss: 0.2927444701426305, accuracy: 0.9233333333333333\n",
      "iteration no 1214: Loss: 0.29267175642354853, accuracy: 0.9233333333333333\n",
      "iteration no 1215: Loss: 0.29259955063106186, accuracy: 0.9233333333333333\n",
      "iteration no 1216: Loss: 0.2925299208912242, accuracy: 0.9233333333333333\n",
      "iteration no 1217: Loss: 0.2924602547438243, accuracy: 0.9233333333333333\n",
      "iteration no 1218: Loss: 0.2923930246939717, accuracy: 0.9233333333333333\n",
      "iteration no 1219: Loss: 0.292323522348147, accuracy: 0.9233333333333333\n",
      "iteration no 1220: Loss: 0.2922547591359784, accuracy: 0.9233333333333333\n",
      "iteration no 1221: Loss: 0.2921862785360957, accuracy: 0.9233333333333333\n",
      "iteration no 1222: Loss: 0.29212006857321116, accuracy: 0.9233333333333333\n",
      "iteration no 1223: Loss: 0.29205153916197835, accuracy: 0.9233333333333333\n",
      "iteration no 1224: Loss: 0.291983280672455, accuracy: 0.9233333333333333\n",
      "iteration no 1225: Loss: 0.29191563606018733, accuracy: 0.9233333333333333\n",
      "iteration no 1226: Loss: 0.29185010120134625, accuracy: 0.9233333333333333\n",
      "iteration no 1227: Loss: 0.29178256193760455, accuracy: 0.9233333333333333\n",
      "iteration no 1228: Loss: 0.29171557514235746, accuracy: 0.9233333333333333\n",
      "iteration no 1229: Loss: 0.2916489689939617, accuracy: 0.9266666666666666\n",
      "iteration no 1230: Loss: 0.2915830859478402, accuracy: 0.9233333333333333\n",
      "iteration no 1231: Loss: 0.29151682549101104, accuracy: 0.9266666666666666\n",
      "iteration no 1232: Loss: 0.29145139991859814, accuracy: 0.9233333333333333\n",
      "iteration no 1233: Loss: 0.2913856320657111, accuracy: 0.9266666666666666\n",
      "iteration no 1234: Loss: 0.2913205858830895, accuracy: 0.9233333333333333\n",
      "iteration no 1235: Loss: 0.2912583911205723, accuracy: 0.9266666666666666\n",
      "iteration no 1236: Loss: 0.2911967780819732, accuracy: 0.9233333333333333\n",
      "iteration no 1237: Loss: 0.2911368000111265, accuracy: 0.9266666666666666\n",
      "iteration no 1238: Loss: 0.29107469626744603, accuracy: 0.9233333333333333\n",
      "iteration no 1239: Loss: 0.2910124536751144, accuracy: 0.9266666666666666\n",
      "iteration no 1240: Loss: 0.29095686567192347, accuracy: 0.9233333333333333\n",
      "iteration no 1241: Loss: 0.2908944385573709, accuracy: 0.9266666666666666\n",
      "iteration no 1242: Loss: 0.29083502389380445, accuracy: 0.9233333333333333\n",
      "iteration no 1243: Loss: 0.2907773193614453, accuracy: 0.9266666666666666\n",
      "iteration no 1244: Loss: 0.2907165673528041, accuracy: 0.9233333333333333\n",
      "iteration no 1245: Loss: 0.2906548218853033, accuracy: 0.9266666666666666\n",
      "iteration no 1246: Loss: 0.29059738066728097, accuracy: 0.9233333333333333\n",
      "iteration no 1247: Loss: 0.2905397781111873, accuracy: 0.9266666666666666\n",
      "iteration no 1248: Loss: 0.2904789908807655, accuracy: 0.9233333333333333\n",
      "iteration no 1249: Loss: 0.2904201052642194, accuracy: 0.93\n",
      "iteration no 1250: Loss: 0.29036148726521654, accuracy: 0.9233333333333333\n",
      "iteration no 1251: Loss: 0.29030141087787753, accuracy: 0.93\n",
      "iteration no 1252: Loss: 0.29024643901296965, accuracy: 0.9233333333333333\n",
      "iteration no 1253: Loss: 0.29018902567244875, accuracy: 0.93\n",
      "iteration no 1254: Loss: 0.290129979735517, accuracy: 0.9233333333333333\n",
      "iteration no 1255: Loss: 0.2900711089538772, accuracy: 0.93\n",
      "iteration no 1256: Loss: 0.29001449135725044, accuracy: 0.9233333333333333\n",
      "iteration no 1257: Loss: 0.289956833797152, accuracy: 0.93\n",
      "iteration no 1258: Loss: 0.28990170861423153, accuracy: 0.9233333333333333\n",
      "iteration no 1259: Loss: 0.28984602979153745, accuracy: 0.93\n",
      "iteration no 1260: Loss: 0.28979185541101893, accuracy: 0.9233333333333333\n",
      "iteration no 1261: Loss: 0.28973545254159844, accuracy: 0.93\n",
      "iteration no 1262: Loss: 0.28967832437544594, accuracy: 0.9233333333333333\n",
      "iteration no 1263: Loss: 0.28961993246868495, accuracy: 0.93\n",
      "iteration no 1264: Loss: 0.28956505106962577, accuracy: 0.9233333333333333\n",
      "iteration no 1265: Loss: 0.28951081275592944, accuracy: 0.93\n",
      "iteration no 1266: Loss: 0.28945702172576526, accuracy: 0.9233333333333333\n",
      "iteration no 1267: Loss: 0.28940142634478544, accuracy: 0.93\n",
      "iteration no 1268: Loss: 0.28934697776448437, accuracy: 0.9233333333333333\n",
      "iteration no 1269: Loss: 0.2892890916594242, accuracy: 0.93\n",
      "iteration no 1270: Loss: 0.2892348610928483, accuracy: 0.9233333333333333\n",
      "iteration no 1271: Loss: 0.2891776667370801, accuracy: 0.93\n",
      "iteration no 1272: Loss: 0.28912417794259915, accuracy: 0.9233333333333333\n",
      "iteration no 1273: Loss: 0.2890700466607152, accuracy: 0.93\n",
      "iteration no 1274: Loss: 0.2890176678110741, accuracy: 0.9233333333333333\n",
      "iteration no 1275: Loss: 0.28896283529293276, accuracy: 0.93\n",
      "iteration no 1276: Loss: 0.28890929168114077, accuracy: 0.93\n",
      "iteration no 1277: Loss: 0.28885264409422, accuracy: 0.93\n",
      "iteration no 1278: Loss: 0.28880052730097344, accuracy: 0.93\n",
      "iteration no 1279: Loss: 0.28874494922332233, accuracy: 0.93\n",
      "iteration no 1280: Loss: 0.28869317237547687, accuracy: 0.93\n",
      "iteration no 1281: Loss: 0.2886396626378586, accuracy: 0.93\n",
      "iteration no 1282: Loss: 0.28858718263810174, accuracy: 0.93\n",
      "iteration no 1283: Loss: 0.28853407922073915, accuracy: 0.93\n",
      "iteration no 1284: Loss: 0.2884813085774068, accuracy: 0.93\n",
      "iteration no 1285: Loss: 0.2884267599434767, accuracy: 0.93\n",
      "iteration no 1286: Loss: 0.28837581853837885, accuracy: 0.9333333333333333\n",
      "iteration no 1287: Loss: 0.28832084750221204, accuracy: 0.93\n",
      "iteration no 1288: Loss: 0.2882694780254115, accuracy: 0.9333333333333333\n",
      "iteration no 1289: Loss: 0.2882187237166954, accuracy: 0.9333333333333333\n",
      "iteration no 1290: Loss: 0.28816623579847567, accuracy: 0.9333333333333333\n",
      "iteration no 1291: Loss: 0.28811458088974745, accuracy: 0.9333333333333333\n",
      "iteration no 1292: Loss: 0.28806433754320937, accuracy: 0.9333333333333333\n",
      "iteration no 1293: Loss: 0.28801088886011356, accuracy: 0.9333333333333333\n",
      "iteration no 1294: Loss: 0.2879605313996513, accuracy: 0.9333333333333333\n",
      "iteration no 1295: Loss: 0.2879075746323338, accuracy: 0.9333333333333333\n",
      "iteration no 1296: Loss: 0.28785581935237897, accuracy: 0.9333333333333333\n",
      "iteration no 1297: Loss: 0.28780665005733375, accuracy: 0.9333333333333333\n",
      "iteration no 1298: Loss: 0.2877578612812238, accuracy: 0.9333333333333333\n",
      "iteration no 1299: Loss: 0.2877077246484364, accuracy: 0.9333333333333333\n",
      "iteration no 1300: Loss: 0.2876576694327035, accuracy: 0.9333333333333333\n",
      "iteration no 1301: Loss: 0.28760468150725294, accuracy: 0.9333333333333333\n",
      "iteration no 1302: Loss: 0.28755412571965927, accuracy: 0.9333333333333333\n",
      "iteration no 1303: Loss: 0.28750242474018817, accuracy: 0.9333333333333333\n",
      "iteration no 1304: Loss: 0.28745155936993877, accuracy: 0.9333333333333333\n",
      "iteration no 1305: Loss: 0.287400900820747, accuracy: 0.9333333333333333\n",
      "iteration no 1306: Loss: 0.28735185317166795, accuracy: 0.9333333333333333\n",
      "iteration no 1307: Loss: 0.2873059017074704, accuracy: 0.9366666666666666\n",
      "iteration no 1308: Loss: 0.2872590474157587, accuracy: 0.9333333333333333\n",
      "iteration no 1309: Loss: 0.28720963191184096, accuracy: 0.9366666666666666\n",
      "iteration no 1310: Loss: 0.2871617329465919, accuracy: 0.9333333333333333\n",
      "iteration no 1311: Loss: 0.2871108219267217, accuracy: 0.9366666666666666\n",
      "iteration no 1312: Loss: 0.28706382488631005, accuracy: 0.9333333333333333\n",
      "iteration no 1313: Loss: 0.2870128603200675, accuracy: 0.9366666666666666\n",
      "iteration no 1314: Loss: 0.28696595969015665, accuracy: 0.9366666666666666\n",
      "iteration no 1315: Loss: 0.2869164233450233, accuracy: 0.9366666666666666\n",
      "iteration no 1316: Loss: 0.28686952895603895, accuracy: 0.9366666666666666\n",
      "iteration no 1317: Loss: 0.28682425092178365, accuracy: 0.9366666666666666\n",
      "iteration no 1318: Loss: 0.2867754748105298, accuracy: 0.9366666666666666\n",
      "iteration no 1319: Loss: 0.2867300289996374, accuracy: 0.9366666666666666\n",
      "iteration no 1320: Loss: 0.2866828271242393, accuracy: 0.9366666666666666\n",
      "iteration no 1321: Loss: 0.28663454930875226, accuracy: 0.9366666666666666\n",
      "iteration no 1322: Loss: 0.2865866417258, accuracy: 0.9366666666666666\n",
      "iteration no 1323: Loss: 0.2865397992478148, accuracy: 0.9366666666666666\n",
      "iteration no 1324: Loss: 0.2864924297327539, accuracy: 0.9366666666666666\n",
      "iteration no 1325: Loss: 0.2864438103835625, accuracy: 0.9366666666666666\n",
      "iteration no 1326: Loss: 0.28639699445954653, accuracy: 0.9366666666666666\n",
      "iteration no 1327: Loss: 0.2863500344773177, accuracy: 0.9366666666666666\n",
      "iteration no 1328: Loss: 0.2863057031811917, accuracy: 0.9366666666666666\n",
      "iteration no 1329: Loss: 0.2862649831340516, accuracy: 0.9366666666666666\n",
      "iteration no 1330: Loss: 0.2862186870484875, accuracy: 0.9366666666666666\n",
      "iteration no 1331: Loss: 0.2861723656724837, accuracy: 0.9366666666666666\n",
      "iteration no 1332: Loss: 0.2861273435710421, accuracy: 0.9366666666666666\n",
      "iteration no 1333: Loss: 0.28608050364068005, accuracy: 0.9366666666666666\n",
      "iteration no 1334: Loss: 0.2860360207858535, accuracy: 0.9366666666666666\n",
      "iteration no 1335: Loss: 0.2859891495211561, accuracy: 0.9366666666666666\n",
      "iteration no 1336: Loss: 0.28594433087693655, accuracy: 0.9366666666666666\n",
      "iteration no 1337: Loss: 0.2858980983708387, accuracy: 0.9366666666666666\n",
      "iteration no 1338: Loss: 0.2858552449233696, accuracy: 0.9366666666666666\n",
      "iteration no 1339: Loss: 0.285810621882651, accuracy: 0.9366666666666666\n",
      "iteration no 1340: Loss: 0.2857679908013166, accuracy: 0.9366666666666666\n",
      "iteration no 1341: Loss: 0.2857280793701113, accuracy: 0.9366666666666666\n",
      "iteration no 1342: Loss: 0.28568303589642957, accuracy: 0.9366666666666666\n",
      "iteration no 1343: Loss: 0.28563824587350606, accuracy: 0.9366666666666666\n",
      "iteration no 1344: Loss: 0.2855936224196356, accuracy: 0.9366666666666666\n",
      "iteration no 1345: Loss: 0.28554886909518046, accuracy: 0.9366666666666666\n",
      "iteration no 1346: Loss: 0.2855059167502305, accuracy: 0.9366666666666666\n",
      "iteration no 1347: Loss: 0.285461625679346, accuracy: 0.9366666666666666\n",
      "iteration no 1348: Loss: 0.28541909783548713, accuracy: 0.9366666666666666\n",
      "iteration no 1349: Loss: 0.2853754723655133, accuracy: 0.9366666666666666\n",
      "iteration no 1350: Loss: 0.28533251584090874, accuracy: 0.9366666666666666\n",
      "iteration no 1351: Loss: 0.28529076948802273, accuracy: 0.9366666666666666\n",
      "iteration no 1352: Loss: 0.2852494005955032, accuracy: 0.9366666666666666\n",
      "iteration no 1353: Loss: 0.2852081960730292, accuracy: 0.9366666666666666\n",
      "iteration no 1354: Loss: 0.28516542166050474, accuracy: 0.9366666666666666\n",
      "iteration no 1355: Loss: 0.28512184575397864, accuracy: 0.9366666666666666\n",
      "iteration no 1356: Loss: 0.28507897711089114, accuracy: 0.9366666666666666\n",
      "iteration no 1357: Loss: 0.285036041179192, accuracy: 0.9366666666666666\n",
      "iteration no 1358: Loss: 0.28499396294990026, accuracy: 0.9366666666666666\n",
      "iteration no 1359: Loss: 0.28495188952160655, accuracy: 0.9366666666666666\n",
      "iteration no 1360: Loss: 0.28491035805151854, accuracy: 0.9366666666666666\n",
      "iteration no 1361: Loss: 0.28487093801058566, accuracy: 0.9366666666666666\n",
      "iteration no 1362: Loss: 0.28482909240403537, accuracy: 0.9366666666666666\n",
      "iteration no 1363: Loss: 0.2847896211971407, accuracy: 0.9366666666666666\n",
      "iteration no 1364: Loss: 0.2847487862449268, accuracy: 0.9366666666666666\n",
      "iteration no 1365: Loss: 0.284706611748512, accuracy: 0.9366666666666666\n",
      "iteration no 1366: Loss: 0.2846643319313138, accuracy: 0.9366666666666666\n",
      "iteration no 1367: Loss: 0.2846235367600651, accuracy: 0.9366666666666666\n",
      "iteration no 1368: Loss: 0.28458243617501333, accuracy: 0.9366666666666666\n",
      "iteration no 1369: Loss: 0.28454104164826133, accuracy: 0.94\n",
      "iteration no 1370: Loss: 0.2844999506652866, accuracy: 0.9366666666666666\n",
      "iteration no 1371: Loss: 0.28445893727237326, accuracy: 0.94\n",
      "iteration no 1372: Loss: 0.2844181084364737, accuracy: 0.9366666666666666\n",
      "iteration no 1373: Loss: 0.2843798005031412, accuracy: 0.94\n",
      "iteration no 1374: Loss: 0.28433930438812444, accuracy: 0.9366666666666666\n",
      "iteration no 1375: Loss: 0.28430262822322055, accuracy: 0.94\n",
      "iteration no 1376: Loss: 0.28426198979269846, accuracy: 0.9366666666666666\n",
      "iteration no 1377: Loss: 0.28422212551693293, accuracy: 0.94\n",
      "iteration no 1378: Loss: 0.28418104410961886, accuracy: 0.9366666666666666\n",
      "iteration no 1379: Loss: 0.28414114862958056, accuracy: 0.94\n",
      "iteration no 1380: Loss: 0.28410041105664996, accuracy: 0.9366666666666666\n",
      "iteration no 1381: Loss: 0.28406091272560313, accuracy: 0.94\n",
      "iteration no 1382: Loss: 0.28402026991948015, accuracy: 0.9366666666666666\n",
      "iteration no 1383: Loss: 0.2839802371213375, accuracy: 0.94\n",
      "iteration no 1384: Loss: 0.28394091692970774, accuracy: 0.9366666666666666\n",
      "iteration no 1385: Loss: 0.28390295285630374, accuracy: 0.94\n",
      "iteration no 1386: Loss: 0.283863206113979, accuracy: 0.9366666666666666\n",
      "iteration no 1387: Loss: 0.2838252940361645, accuracy: 0.94\n",
      "iteration no 1388: Loss: 0.28378712065820433, accuracy: 0.9366666666666666\n",
      "iteration no 1389: Loss: 0.2837521278331507, accuracy: 0.94\n",
      "iteration no 1390: Loss: 0.2837123551969715, accuracy: 0.9366666666666666\n",
      "iteration no 1391: Loss: 0.28367331364440895, accuracy: 0.94\n",
      "iteration no 1392: Loss: 0.2836327586046207, accuracy: 0.9366666666666666\n",
      "iteration no 1393: Loss: 0.2835941683722113, accuracy: 0.94\n",
      "iteration no 1394: Loss: 0.2835549761539427, accuracy: 0.94\n",
      "iteration no 1395: Loss: 0.28351727790057446, accuracy: 0.94\n",
      "iteration no 1396: Loss: 0.28347717124607674, accuracy: 0.94\n",
      "iteration no 1397: Loss: 0.2834395252951849, accuracy: 0.94\n",
      "iteration no 1398: Loss: 0.2834008537100427, accuracy: 0.94\n",
      "iteration no 1399: Loss: 0.28336324320152506, accuracy: 0.94\n",
      "iteration no 1400: Loss: 0.28332483851894563, accuracy: 0.94\n",
      "iteration no 1401: Loss: 0.2832871478276334, accuracy: 0.94\n",
      "iteration no 1402: Loss: 0.28324977775912813, accuracy: 0.94\n",
      "iteration no 1403: Loss: 0.28321470670321053, accuracy: 0.94\n",
      "iteration no 1404: Loss: 0.28317676869108355, accuracy: 0.94\n",
      "iteration no 1405: Loss: 0.28313901306599554, accuracy: 0.94\n",
      "iteration no 1406: Loss: 0.2831011019907383, accuracy: 0.94\n",
      "iteration no 1407: Loss: 0.2830639010367138, accuracy: 0.94\n",
      "iteration no 1408: Loss: 0.2830256585665716, accuracy: 0.94\n",
      "iteration no 1409: Loss: 0.2829872879299684, accuracy: 0.94\n",
      "iteration no 1410: Loss: 0.28294998919694636, accuracy: 0.94\n",
      "iteration no 1411: Loss: 0.2829153702628442, accuracy: 0.94\n",
      "iteration no 1412: Loss: 0.282877452584848, accuracy: 0.94\n",
      "iteration no 1413: Loss: 0.2828418900010415, accuracy: 0.94\n",
      "iteration no 1414: Loss: 0.28280416264288805, accuracy: 0.94\n",
      "iteration no 1415: Loss: 0.28276723155344213, accuracy: 0.94\n",
      "iteration no 1416: Loss: 0.282730829895534, accuracy: 0.94\n",
      "iteration no 1417: Loss: 0.28269766833228865, accuracy: 0.94\n",
      "iteration no 1418: Loss: 0.2826610419820772, accuracy: 0.94\n",
      "iteration no 1419: Loss: 0.28262449707443954, accuracy: 0.94\n",
      "iteration no 1420: Loss: 0.2825870570232139, accuracy: 0.94\n",
      "iteration no 1421: Loss: 0.2825495933349083, accuracy: 0.94\n",
      "iteration no 1422: Loss: 0.2825132534150562, accuracy: 0.94\n",
      "iteration no 1423: Loss: 0.2824771456169657, accuracy: 0.9433333333333334\n",
      "iteration no 1424: Loss: 0.2824413880409357, accuracy: 0.94\n",
      "iteration no 1425: Loss: 0.2824075919452571, accuracy: 0.9433333333333334\n",
      "iteration no 1426: Loss: 0.2823712722314046, accuracy: 0.94\n",
      "iteration no 1427: Loss: 0.28233628884347994, accuracy: 0.9433333333333334\n",
      "iteration no 1428: Loss: 0.28230031376443937, accuracy: 0.94\n",
      "iteration no 1429: Loss: 0.2822648419214259, accuracy: 0.9433333333333334\n",
      "iteration no 1430: Loss: 0.2822302097763436, accuracy: 0.94\n",
      "iteration no 1431: Loss: 0.28219890878819975, accuracy: 0.9433333333333334\n",
      "iteration no 1432: Loss: 0.28216247056986016, accuracy: 0.94\n",
      "iteration no 1433: Loss: 0.282129034145169, accuracy: 0.9433333333333334\n",
      "iteration no 1434: Loss: 0.28209195961511524, accuracy: 0.94\n",
      "iteration no 1435: Loss: 0.2820580340309486, accuracy: 0.9433333333333334\n",
      "iteration no 1436: Loss: 0.2820234628921212, accuracy: 0.94\n",
      "iteration no 1437: Loss: 0.2819897976678322, accuracy: 0.9433333333333334\n",
      "iteration no 1438: Loss: 0.2819538531120068, accuracy: 0.94\n",
      "iteration no 1439: Loss: 0.28191985115657126, accuracy: 0.9433333333333334\n",
      "iteration no 1440: Loss: 0.2818837877883473, accuracy: 0.94\n",
      "iteration no 1441: Loss: 0.28184920354715975, accuracy: 0.9433333333333334\n",
      "iteration no 1442: Loss: 0.2818144591645204, accuracy: 0.94\n",
      "iteration no 1443: Loss: 0.2817802147725389, accuracy: 0.9433333333333334\n",
      "iteration no 1444: Loss: 0.2817464716229122, accuracy: 0.9433333333333334\n",
      "iteration no 1445: Loss: 0.281714326105096, accuracy: 0.9433333333333334\n",
      "iteration no 1446: Loss: 0.2816804089706081, accuracy: 0.94\n",
      "iteration no 1447: Loss: 0.28164523271712244, accuracy: 0.9433333333333334\n",
      "iteration no 1448: Loss: 0.2816113874023288, accuracy: 0.9433333333333334\n",
      "iteration no 1449: Loss: 0.2815765570741138, accuracy: 0.9433333333333334\n",
      "iteration no 1450: Loss: 0.28154455548021184, accuracy: 0.94\n",
      "iteration no 1451: Loss: 0.28151274739003235, accuracy: 0.9433333333333334\n",
      "iteration no 1452: Loss: 0.28147803173869135, accuracy: 0.9433333333333334\n",
      "iteration no 1453: Loss: 0.2814490845667169, accuracy: 0.9433333333333334\n",
      "iteration no 1454: Loss: 0.2814150816929286, accuracy: 0.94\n",
      "iteration no 1455: Loss: 0.2813793199361895, accuracy: 0.9433333333333334\n",
      "iteration no 1456: Loss: 0.28134597121957006, accuracy: 0.9433333333333334\n",
      "iteration no 1457: Loss: 0.28131719818438794, accuracy: 0.9433333333333334\n",
      "iteration no 1458: Loss: 0.28128309515645994, accuracy: 0.94\n",
      "iteration no 1459: Loss: 0.28124606255598195, accuracy: 0.9433333333333334\n",
      "iteration no 1460: Loss: 0.2812110274321112, accuracy: 0.9433333333333334\n",
      "iteration no 1461: Loss: 0.28117503912403913, accuracy: 0.9433333333333334\n",
      "iteration no 1462: Loss: 0.2811419623777696, accuracy: 0.9433333333333334\n",
      "iteration no 1463: Loss: 0.28110987086195516, accuracy: 0.9433333333333334\n",
      "iteration no 1464: Loss: 0.2810750963113171, accuracy: 0.9433333333333334\n",
      "iteration no 1465: Loss: 0.28104092603606584, accuracy: 0.9433333333333334\n",
      "iteration no 1466: Loss: 0.2810052755209748, accuracy: 0.9433333333333334\n",
      "iteration no 1467: Loss: 0.28097512654419654, accuracy: 0.9433333333333334\n",
      "iteration no 1468: Loss: 0.2809408620432429, accuracy: 0.94\n",
      "iteration no 1469: Loss: 0.28090541424213744, accuracy: 0.9433333333333334\n",
      "iteration no 1470: Loss: 0.28087132876665, accuracy: 0.9433333333333334\n",
      "iteration no 1471: Loss: 0.2808395500696755, accuracy: 0.9433333333333334\n",
      "iteration no 1472: Loss: 0.28080629501379595, accuracy: 0.9433333333333334\n",
      "iteration no 1473: Loss: 0.2807696122454189, accuracy: 0.9433333333333334\n",
      "iteration no 1474: Loss: 0.2807357793912329, accuracy: 0.9433333333333334\n",
      "iteration no 1475: Loss: 0.28070432231847897, accuracy: 0.9433333333333334\n",
      "iteration no 1476: Loss: 0.28067184105342957, accuracy: 0.9433333333333334\n",
      "iteration no 1477: Loss: 0.28063672171161685, accuracy: 0.9433333333333334\n",
      "iteration no 1478: Loss: 0.28060376105544876, accuracy: 0.9433333333333334\n",
      "iteration no 1479: Loss: 0.28056850015335955, accuracy: 0.9433333333333334\n",
      "iteration no 1480: Loss: 0.28053893689355563, accuracy: 0.9433333333333334\n",
      "iteration no 1481: Loss: 0.2805041666859864, accuracy: 0.9433333333333334\n",
      "iteration no 1482: Loss: 0.2804682297843184, accuracy: 0.9433333333333334\n",
      "iteration no 1483: Loss: 0.28043349652188115, accuracy: 0.9433333333333334\n",
      "iteration no 1484: Loss: 0.2804018492607927, accuracy: 0.9433333333333334\n",
      "iteration no 1485: Loss: 0.28036798565134846, accuracy: 0.9433333333333334\n",
      "iteration no 1486: Loss: 0.28033524656696845, accuracy: 0.9433333333333334\n",
      "iteration no 1487: Loss: 0.2803002168902436, accuracy: 0.9433333333333334\n",
      "iteration no 1488: Loss: 0.2802690213014055, accuracy: 0.9433333333333334\n",
      "iteration no 1489: Loss: 0.2802385330332618, accuracy: 0.9466666666666667\n",
      "iteration no 1490: Loss: 0.28020485427272085, accuracy: 0.9433333333333334\n",
      "iteration no 1491: Loss: 0.2801728592634366, accuracy: 0.9466666666666667\n",
      "iteration no 1492: Loss: 0.2801394923644426, accuracy: 0.9433333333333334\n",
      "iteration no 1493: Loss: 0.2801081139433783, accuracy: 0.9466666666666667\n",
      "iteration no 1494: Loss: 0.280075413313016, accuracy: 0.9433333333333334\n",
      "iteration no 1495: Loss: 0.2800399708177982, accuracy: 0.9433333333333334\n",
      "iteration no 1496: Loss: 0.2800079379511768, accuracy: 0.9433333333333334\n",
      "iteration no 1497: Loss: 0.2799771329205943, accuracy: 0.9466666666666667\n",
      "iteration no 1498: Loss: 0.27994549450229456, accuracy: 0.9433333333333334\n",
      "iteration no 1499: Loss: 0.2799101242536003, accuracy: 0.9466666666666667\n",
      "iteration no 1500: Loss: 0.2798807330189457, accuracy: 0.9433333333333334\n",
      "iteration no 1501: Loss: 0.27985166053674987, accuracy: 0.9466666666666667\n",
      "iteration no 1502: Loss: 0.27981970683192453, accuracy: 0.9433333333333334\n",
      "iteration no 1503: Loss: 0.279785101054539, accuracy: 0.9466666666666667\n",
      "iteration no 1504: Loss: 0.27975416390891694, accuracy: 0.9433333333333334\n",
      "iteration no 1505: Loss: 0.27972158437161865, accuracy: 0.9466666666666667\n",
      "iteration no 1506: Loss: 0.2796949537974248, accuracy: 0.9433333333333334\n",
      "iteration no 1507: Loss: 0.2796631934059844, accuracy: 0.9466666666666667\n",
      "iteration no 1508: Loss: 0.27963268894469484, accuracy: 0.9433333333333334\n",
      "iteration no 1509: Loss: 0.2796005155741535, accuracy: 0.9466666666666667\n",
      "iteration no 1510: Loss: 0.27957198838103214, accuracy: 0.9433333333333334\n",
      "iteration no 1511: Loss: 0.27954166532731545, accuracy: 0.9466666666666667\n",
      "iteration no 1512: Loss: 0.27951226432019716, accuracy: 0.9433333333333334\n",
      "iteration no 1513: Loss: 0.2794782250354723, accuracy: 0.9466666666666667\n",
      "iteration no 1514: Loss: 0.27944942008114004, accuracy: 0.9433333333333334\n",
      "iteration no 1515: Loss: 0.27942083378743354, accuracy: 0.9466666666666667\n",
      "iteration no 1516: Loss: 0.2793905069561371, accuracy: 0.9433333333333334\n",
      "iteration no 1517: Loss: 0.27935941105784984, accuracy: 0.9466666666666667\n",
      "iteration no 1518: Loss: 0.27932839036901125, accuracy: 0.9433333333333334\n",
      "iteration no 1519: Loss: 0.27930076945664906, accuracy: 0.9466666666666667\n",
      "iteration no 1520: Loss: 0.2792708263784006, accuracy: 0.9433333333333334\n",
      "iteration no 1521: Loss: 0.279237215161939, accuracy: 0.9466666666666667\n",
      "iteration no 1522: Loss: 0.27920959978497456, accuracy: 0.9433333333333334\n",
      "iteration no 1523: Loss: 0.27918232841682866, accuracy: 0.9466666666666667\n",
      "iteration no 1524: Loss: 0.27915334691579247, accuracy: 0.9433333333333334\n",
      "iteration no 1525: Loss: 0.2791185887879882, accuracy: 0.9466666666666667\n",
      "iteration no 1526: Loss: 0.2790866053864549, accuracy: 0.9433333333333334\n",
      "iteration no 1527: Loss: 0.2790534005944778, accuracy: 0.9466666666666667\n",
      "iteration no 1528: Loss: 0.2790296345916498, accuracy: 0.9433333333333334\n",
      "iteration no 1529: Loss: 0.27899803829982955, accuracy: 0.9466666666666667\n",
      "iteration no 1530: Loss: 0.27896942164996064, accuracy: 0.9433333333333334\n",
      "iteration no 1531: Loss: 0.27893662733903574, accuracy: 0.9466666666666667\n",
      "iteration no 1532: Loss: 0.2789088617555664, accuracy: 0.9433333333333334\n",
      "iteration no 1533: Loss: 0.27888076339798135, accuracy: 0.9466666666666667\n",
      "iteration no 1534: Loss: 0.27884860617481905, accuracy: 0.9433333333333334\n",
      "iteration no 1535: Loss: 0.27881823125902816, accuracy: 0.9466666666666667\n",
      "iteration no 1536: Loss: 0.27878812828764593, accuracy: 0.9433333333333334\n",
      "iteration no 1537: Loss: 0.2787638265034293, accuracy: 0.9466666666666667\n",
      "iteration no 1538: Loss: 0.2787337718154254, accuracy: 0.9433333333333334\n",
      "iteration no 1539: Loss: 0.27870602500600483, accuracy: 0.9466666666666667\n",
      "iteration no 1540: Loss: 0.27867733426360985, accuracy: 0.9433333333333334\n",
      "iteration no 1541: Loss: 0.27865170710829495, accuracy: 0.9466666666666667\n",
      "iteration no 1542: Loss: 0.27862209909590996, accuracy: 0.9433333333333334\n",
      "iteration no 1543: Loss: 0.2785909949823968, accuracy: 0.9466666666666667\n",
      "iteration no 1544: Loss: 0.2785619661420051, accuracy: 0.9433333333333334\n",
      "iteration no 1545: Loss: 0.2785354181336003, accuracy: 0.9466666666666667\n",
      "iteration no 1546: Loss: 0.2785069835974915, accuracy: 0.9433333333333334\n",
      "iteration no 1547: Loss: 0.27847583174704327, accuracy: 0.9466666666666667\n",
      "iteration no 1548: Loss: 0.2784464535917112, accuracy: 0.9433333333333334\n",
      "iteration no 1549: Loss: 0.27841477076153737, accuracy: 0.9466666666666667\n",
      "iteration no 1550: Loss: 0.2783895656545663, accuracy: 0.9433333333333334\n",
      "iteration no 1551: Loss: 0.27836108025615536, accuracy: 0.9466666666666667\n",
      "iteration no 1552: Loss: 0.27833089388836, accuracy: 0.9433333333333334\n",
      "iteration no 1553: Loss: 0.27830193231705336, accuracy: 0.9466666666666667\n",
      "iteration no 1554: Loss: 0.2782744199504811, accuracy: 0.9433333333333334\n",
      "iteration no 1555: Loss: 0.27824895569124797, accuracy: 0.9466666666666667\n",
      "iteration no 1556: Loss: 0.2782201219217072, accuracy: 0.9433333333333334\n",
      "iteration no 1557: Loss: 0.27818946087289975, accuracy: 0.9466666666666667\n",
      "iteration no 1558: Loss: 0.27816254477643415, accuracy: 0.9433333333333334\n",
      "iteration no 1559: Loss: 0.27813713162089787, accuracy: 0.9466666666666667\n",
      "iteration no 1560: Loss: 0.2781111245509403, accuracy: 0.9433333333333334\n",
      "iteration no 1561: Loss: 0.27808095984008124, accuracy: 0.9466666666666667\n",
      "iteration no 1562: Loss: 0.2780512187229581, accuracy: 0.9466666666666667\n",
      "iteration no 1563: Loss: 0.27802631827208246, accuracy: 0.9466666666666667\n",
      "iteration no 1564: Loss: 0.277996178382303, accuracy: 0.9466666666666667\n",
      "iteration no 1565: Loss: 0.2779666867321552, accuracy: 0.9466666666666667\n",
      "iteration no 1566: Loss: 0.27793850522683394, accuracy: 0.9466666666666667\n",
      "iteration no 1567: Loss: 0.2779140503022819, accuracy: 0.9466666666666667\n",
      "iteration no 1568: Loss: 0.27788664164923055, accuracy: 0.9466666666666667\n",
      "iteration no 1569: Loss: 0.2778572088583003, accuracy: 0.9466666666666667\n",
      "iteration no 1570: Loss: 0.2778318685618832, accuracy: 0.9466666666666667\n",
      "iteration no 1571: Loss: 0.2778014473158628, accuracy: 0.9466666666666667\n",
      "iteration no 1572: Loss: 0.2777771202949805, accuracy: 0.9466666666666667\n",
      "iteration no 1573: Loss: 0.2777495445637746, accuracy: 0.9466666666666667\n",
      "iteration no 1574: Loss: 0.27772169264019725, accuracy: 0.9466666666666667\n",
      "iteration no 1575: Loss: 0.27769202993855624, accuracy: 0.9466666666666667\n",
      "iteration no 1576: Loss: 0.2776654473312494, accuracy: 0.9466666666666667\n",
      "iteration no 1577: Loss: 0.2776387398038873, accuracy: 0.9466666666666667\n",
      "iteration no 1578: Loss: 0.2776115755646801, accuracy: 0.9466666666666667\n",
      "iteration no 1579: Loss: 0.2775825522643622, accuracy: 0.9466666666666667\n",
      "iteration no 1580: Loss: 0.2775549319953236, accuracy: 0.9466666666666667\n",
      "iteration no 1581: Loss: 0.27752802935209636, accuracy: 0.9466666666666667\n",
      "iteration no 1582: Loss: 0.2774988818652711, accuracy: 0.9466666666666667\n",
      "iteration no 1583: Loss: 0.27746854074646604, accuracy: 0.9466666666666667\n",
      "iteration no 1584: Loss: 0.27744424920804817, accuracy: 0.9466666666666667\n",
      "iteration no 1585: Loss: 0.2774210320380987, accuracy: 0.9466666666666667\n",
      "iteration no 1586: Loss: 0.27739221604410935, accuracy: 0.9466666666666667\n",
      "iteration no 1587: Loss: 0.2773613887042219, accuracy: 0.9466666666666667\n",
      "iteration no 1588: Loss: 0.2773318003684854, accuracy: 0.9466666666666667\n",
      "iteration no 1589: Loss: 0.2773071300833013, accuracy: 0.9466666666666667\n",
      "iteration no 1590: Loss: 0.2772806057805208, accuracy: 0.95\n",
      "iteration no 1591: Loss: 0.27725251970233644, accuracy: 0.9466666666666667\n",
      "iteration no 1592: Loss: 0.2772241485040787, accuracy: 0.95\n",
      "iteration no 1593: Loss: 0.2771994571025269, accuracy: 0.9466666666666667\n",
      "iteration no 1594: Loss: 0.2771717418916055, accuracy: 0.95\n",
      "iteration no 1595: Loss: 0.2771451962533859, accuracy: 0.9466666666666667\n",
      "iteration no 1596: Loss: 0.2771161571329799, accuracy: 0.95\n",
      "iteration no 1597: Loss: 0.27708685853857856, accuracy: 0.9466666666666667\n",
      "iteration no 1598: Loss: 0.27706396654492704, accuracy: 0.95\n",
      "iteration no 1599: Loss: 0.2770359082601056, accuracy: 0.9466666666666667\n",
      "iteration no 1600: Loss: 0.2770110142779669, accuracy: 0.95\n",
      "iteration no 1601: Loss: 0.2769834998479698, accuracy: 0.9466666666666667\n",
      "iteration no 1602: Loss: 0.2769596522994866, accuracy: 0.95\n",
      "iteration no 1603: Loss: 0.27693330629233465, accuracy: 0.9466666666666667\n",
      "iteration no 1604: Loss: 0.2769066679314598, accuracy: 0.95\n",
      "iteration no 1605: Loss: 0.27688070663805764, accuracy: 0.9466666666666667\n",
      "iteration no 1606: Loss: 0.2768556259800896, accuracy: 0.95\n",
      "iteration no 1607: Loss: 0.27683131889656415, accuracy: 0.9466666666666667\n",
      "iteration no 1608: Loss: 0.2768057731519367, accuracy: 0.95\n",
      "iteration no 1609: Loss: 0.2767779992397689, accuracy: 0.95\n",
      "iteration no 1610: Loss: 0.2767487617559191, accuracy: 0.95\n",
      "iteration no 1611: Loss: 0.2767260309447102, accuracy: 0.95\n",
      "iteration no 1612: Loss: 0.2767014805431399, accuracy: 0.95\n",
      "iteration no 1613: Loss: 0.2766741203495658, accuracy: 0.95\n",
      "iteration no 1614: Loss: 0.2766454311542832, accuracy: 0.95\n",
      "iteration no 1615: Loss: 0.27661847970622294, accuracy: 0.9533333333333334\n",
      "iteration no 1616: Loss: 0.27659667183291425, accuracy: 0.95\n",
      "iteration no 1617: Loss: 0.2765685550775572, accuracy: 0.9533333333333334\n",
      "iteration no 1618: Loss: 0.27654483935577023, accuracy: 0.95\n",
      "iteration no 1619: Loss: 0.27651789263650334, accuracy: 0.9533333333333334\n",
      "iteration no 1620: Loss: 0.2764933491105383, accuracy: 0.9533333333333334\n",
      "iteration no 1621: Loss: 0.27646906733033344, accuracy: 0.9533333333333334\n",
      "iteration no 1622: Loss: 0.2764464812471835, accuracy: 0.95\n",
      "iteration no 1623: Loss: 0.27641952536559955, accuracy: 0.9533333333333334\n",
      "iteration no 1624: Loss: 0.27639243025308813, accuracy: 0.9533333333333334\n",
      "iteration no 1625: Loss: 0.27636956484725783, accuracy: 0.9533333333333334\n",
      "iteration no 1626: Loss: 0.27634496281475407, accuracy: 0.9533333333333334\n",
      "iteration no 1627: Loss: 0.2763187323312917, accuracy: 0.9533333333333334\n",
      "iteration no 1628: Loss: 0.27629322562762554, accuracy: 0.9533333333333334\n",
      "iteration no 1629: Loss: 0.27627180429470616, accuracy: 0.9533333333333334\n",
      "iteration no 1630: Loss: 0.2762460335013856, accuracy: 0.9533333333333334\n",
      "iteration no 1631: Loss: 0.2762195340813033, accuracy: 0.9533333333333334\n",
      "iteration no 1632: Loss: 0.27619329228030354, accuracy: 0.9533333333333334\n",
      "iteration no 1633: Loss: 0.27616461308356716, accuracy: 0.9533333333333334\n",
      "iteration no 1634: Loss: 0.2761414469058431, accuracy: 0.9533333333333334\n",
      "iteration no 1635: Loss: 0.27611724685424155, accuracy: 0.9533333333333334\n",
      "iteration no 1636: Loss: 0.2760912947129551, accuracy: 0.9533333333333334\n",
      "iteration no 1637: Loss: 0.2760612551714166, accuracy: 0.9533333333333334\n",
      "iteration no 1638: Loss: 0.27603651084507297, accuracy: 0.9533333333333334\n",
      "iteration no 1639: Loss: 0.2760108764766098, accuracy: 0.9533333333333334\n",
      "iteration no 1640: Loss: 0.27598671209646564, accuracy: 0.9533333333333334\n",
      "iteration no 1641: Loss: 0.275957467689004, accuracy: 0.9533333333333334\n",
      "iteration no 1642: Loss: 0.2759305752020888, accuracy: 0.9533333333333334\n",
      "iteration no 1643: Loss: 0.2759059122988404, accuracy: 0.9533333333333334\n",
      "iteration no 1644: Loss: 0.275882435734071, accuracy: 0.9533333333333334\n",
      "iteration no 1645: Loss: 0.2758552013517255, accuracy: 0.9533333333333334\n",
      "iteration no 1646: Loss: 0.2758270250474338, accuracy: 0.9533333333333334\n",
      "iteration no 1647: Loss: 0.2758031744346412, accuracy: 0.9533333333333334\n",
      "iteration no 1648: Loss: 0.2757792592867234, accuracy: 0.9533333333333334\n",
      "iteration no 1649: Loss: 0.2757520800651964, accuracy: 0.9533333333333334\n",
      "iteration no 1650: Loss: 0.2757245265409261, accuracy: 0.9533333333333334\n",
      "iteration no 1651: Loss: 0.275697446597933, accuracy: 0.9533333333333334\n",
      "iteration no 1652: Loss: 0.2756718434963792, accuracy: 0.9533333333333334\n",
      "iteration no 1653: Loss: 0.27564664554065793, accuracy: 0.9533333333333334\n",
      "iteration no 1654: Loss: 0.27562002096078025, accuracy: 0.9533333333333334\n",
      "iteration no 1655: Loss: 0.27559343656369234, accuracy: 0.9533333333333334\n",
      "iteration no 1656: Loss: 0.2755721335714527, accuracy: 0.9533333333333334\n",
      "iteration no 1657: Loss: 0.2755507152980927, accuracy: 0.9533333333333334\n",
      "iteration no 1658: Loss: 0.2755244171367066, accuracy: 0.9533333333333334\n",
      "iteration no 1659: Loss: 0.2754979789505283, accuracy: 0.9533333333333334\n",
      "iteration no 1660: Loss: 0.2754748382667071, accuracy: 0.9533333333333334\n",
      "iteration no 1661: Loss: 0.2754537546453292, accuracy: 0.9533333333333334\n",
      "iteration no 1662: Loss: 0.2754258593558308, accuracy: 0.9533333333333334\n",
      "iteration no 1663: Loss: 0.27539856003501073, accuracy: 0.9533333333333334\n",
      "iteration no 1664: Loss: 0.2753725580124281, accuracy: 0.9533333333333334\n",
      "iteration no 1665: Loss: 0.2753492549605506, accuracy: 0.9533333333333334\n",
      "iteration no 1666: Loss: 0.2753228848813677, accuracy: 0.9533333333333334\n",
      "iteration no 1667: Loss: 0.2752965933749467, accuracy: 0.9533333333333334\n",
      "iteration no 1668: Loss: 0.2752718605313432, accuracy: 0.9533333333333334\n",
      "iteration no 1669: Loss: 0.275248988452585, accuracy: 0.9533333333333334\n",
      "iteration no 1670: Loss: 0.2752256452478914, accuracy: 0.9533333333333334\n",
      "iteration no 1671: Loss: 0.27519875298709096, accuracy: 0.9533333333333334\n",
      "iteration no 1672: Loss: 0.275172695130306, accuracy: 0.9533333333333334\n",
      "iteration no 1673: Loss: 0.27514749530745986, accuracy: 0.9533333333333334\n",
      "iteration no 1674: Loss: 0.2751250597448265, accuracy: 0.9533333333333334\n",
      "iteration no 1675: Loss: 0.27510013919272397, accuracy: 0.9533333333333334\n",
      "iteration no 1676: Loss: 0.27507340268073793, accuracy: 0.9533333333333334\n",
      "iteration no 1677: Loss: 0.2750480547444008, accuracy: 0.9533333333333334\n",
      "iteration no 1678: Loss: 0.2750268918536244, accuracy: 0.9533333333333334\n",
      "iteration no 1679: Loss: 0.2750046532161986, accuracy: 0.9533333333333334\n",
      "iteration no 1680: Loss: 0.27497853039113523, accuracy: 0.9533333333333334\n",
      "iteration no 1681: Loss: 0.2749519976237011, accuracy: 0.9533333333333334\n",
      "iteration no 1682: Loss: 0.274930311981706, accuracy: 0.9533333333333334\n",
      "iteration no 1683: Loss: 0.2749087434179939, accuracy: 0.9533333333333334\n",
      "iteration no 1684: Loss: 0.2748822706643796, accuracy: 0.9533333333333334\n",
      "iteration no 1685: Loss: 0.27485625863688545, accuracy: 0.9533333333333334\n",
      "iteration no 1686: Loss: 0.27483359438280497, accuracy: 0.9533333333333334\n",
      "iteration no 1687: Loss: 0.2748137747996073, accuracy: 0.9566666666666667\n",
      "iteration no 1688: Loss: 0.27478682743993327, accuracy: 0.9533333333333334\n",
      "iteration no 1689: Loss: 0.2747603025370006, accuracy: 0.9533333333333334\n",
      "iteration no 1690: Loss: 0.27473631864150244, accuracy: 0.9533333333333334\n",
      "iteration no 1691: Loss: 0.2747186920910135, accuracy: 0.9566666666666667\n",
      "iteration no 1692: Loss: 0.27469230778561293, accuracy: 0.9533333333333334\n",
      "iteration no 1693: Loss: 0.27466595659256515, accuracy: 0.9533333333333334\n",
      "iteration no 1694: Loss: 0.2746396160291724, accuracy: 0.9533333333333334\n",
      "iteration no 1695: Loss: 0.2746176477699776, accuracy: 0.9566666666666667\n",
      "iteration no 1696: Loss: 0.2745918151930206, accuracy: 0.9533333333333334\n",
      "iteration no 1697: Loss: 0.2745661015143657, accuracy: 0.9533333333333334\n",
      "iteration no 1698: Loss: 0.2745415110304057, accuracy: 0.9533333333333334\n",
      "iteration no 1699: Loss: 0.2745167694251746, accuracy: 0.9533333333333334\n",
      "iteration no 1700: Loss: 0.2744959785065017, accuracy: 0.9533333333333334\n",
      "iteration no 1701: Loss: 0.2744711502642375, accuracy: 0.9533333333333334\n",
      "iteration no 1702: Loss: 0.2744463634886788, accuracy: 0.9533333333333334\n",
      "iteration no 1703: Loss: 0.274421863533315, accuracy: 0.9533333333333334\n",
      "iteration no 1704: Loss: 0.2744030848804012, accuracy: 0.9533333333333334\n",
      "iteration no 1705: Loss: 0.27438144472628667, accuracy: 0.9566666666666667\n",
      "iteration no 1706: Loss: 0.27435633465557935, accuracy: 0.9533333333333334\n",
      "iteration no 1707: Loss: 0.27433329910849646, accuracy: 0.9566666666666667\n",
      "iteration no 1708: Loss: 0.2743112145962851, accuracy: 0.9533333333333334\n",
      "iteration no 1709: Loss: 0.27429203748358055, accuracy: 0.9566666666666667\n",
      "iteration no 1710: Loss: 0.27426891885922794, accuracy: 0.9533333333333334\n",
      "iteration no 1711: Loss: 0.2742441070078822, accuracy: 0.9566666666666667\n",
      "iteration no 1712: Loss: 0.274221701618947, accuracy: 0.9533333333333334\n",
      "iteration no 1713: Loss: 0.274205526381004, accuracy: 0.9566666666666667\n",
      "iteration no 1714: Loss: 0.2741785121091821, accuracy: 0.9533333333333334\n",
      "iteration no 1715: Loss: 0.2741549160540633, accuracy: 0.9566666666666667\n",
      "iteration no 1716: Loss: 0.27412911077975516, accuracy: 0.9533333333333334\n",
      "iteration no 1717: Loss: 0.27410782110943155, accuracy: 0.9566666666666667\n",
      "iteration no 1718: Loss: 0.27408424699646083, accuracy: 0.9533333333333334\n",
      "iteration no 1719: Loss: 0.27405971107909455, accuracy: 0.9566666666666667\n",
      "iteration no 1720: Loss: 0.2740361033147982, accuracy: 0.9533333333333334\n",
      "iteration no 1721: Loss: 0.2740124071547911, accuracy: 0.9566666666666667\n",
      "iteration no 1722: Loss: 0.27399344736645953, accuracy: 0.9533333333333334\n",
      "iteration no 1723: Loss: 0.2739703831307719, accuracy: 0.9566666666666667\n",
      "iteration no 1724: Loss: 0.2739477129213872, accuracy: 0.9533333333333334\n",
      "iteration no 1725: Loss: 0.27392395027299465, accuracy: 0.9566666666666667\n",
      "iteration no 1726: Loss: 0.2739039877100329, accuracy: 0.9533333333333334\n",
      "iteration no 1727: Loss: 0.27388503509357176, accuracy: 0.9566666666666667\n",
      "iteration no 1728: Loss: 0.27386130288713617, accuracy: 0.9533333333333334\n",
      "iteration no 1729: Loss: 0.27383849126428517, accuracy: 0.9566666666666667\n",
      "iteration no 1730: Loss: 0.27381623461769256, accuracy: 0.9533333333333334\n",
      "iteration no 1731: Loss: 0.27379972133879693, accuracy: 0.9566666666666667\n",
      "iteration no 1732: Loss: 0.2737751584239346, accuracy: 0.9533333333333334\n",
      "iteration no 1733: Loss: 0.2737510095715498, accuracy: 0.9566666666666667\n",
      "iteration no 1734: Loss: 0.2737284452686255, accuracy: 0.9533333333333334\n",
      "iteration no 1735: Loss: 0.27370638792917024, accuracy: 0.9566666666666667\n",
      "iteration no 1736: Loss: 0.2736851305156546, accuracy: 0.9533333333333334\n",
      "iteration no 1737: Loss: 0.2736604560723302, accuracy: 0.9566666666666667\n",
      "iteration no 1738: Loss: 0.27363816472544383, accuracy: 0.9533333333333334\n",
      "iteration no 1739: Loss: 0.2736176930937886, accuracy: 0.9566666666666667\n",
      "iteration no 1740: Loss: 0.2735982742099911, accuracy: 0.9533333333333334\n",
      "iteration no 1741: Loss: 0.2735767604414409, accuracy: 0.9566666666666667\n",
      "iteration no 1742: Loss: 0.273554463404155, accuracy: 0.9533333333333334\n",
      "iteration no 1743: Loss: 0.27353304988528326, accuracy: 0.9566666666666667\n",
      "iteration no 1744: Loss: 0.27351139918521816, accuracy: 0.9533333333333334\n",
      "iteration no 1745: Loss: 0.27349144115320706, accuracy: 0.9566666666666667\n",
      "iteration no 1746: Loss: 0.27346796139191, accuracy: 0.9533333333333334\n",
      "iteration no 1747: Loss: 0.2734453208951406, accuracy: 0.9566666666666667\n",
      "iteration no 1748: Loss: 0.2734261555910481, accuracy: 0.9533333333333334\n",
      "iteration no 1749: Loss: 0.273407593829341, accuracy: 0.9566666666666667\n",
      "iteration no 1750: Loss: 0.2733855468066421, accuracy: 0.9533333333333334\n",
      "iteration no 1751: Loss: 0.2733621407350601, accuracy: 0.9566666666666667\n",
      "iteration no 1752: Loss: 0.27334251220007516, accuracy: 0.9533333333333334\n",
      "iteration no 1753: Loss: 0.27331960987307874, accuracy: 0.9566666666666667\n",
      "iteration no 1754: Loss: 0.2732980609822, accuracy: 0.9566666666666667\n",
      "iteration no 1755: Loss: 0.273274862411155, accuracy: 0.9566666666666667\n",
      "iteration no 1756: Loss: 0.2732540003394945, accuracy: 0.9566666666666667\n",
      "iteration no 1757: Loss: 0.2732330700934163, accuracy: 0.9566666666666667\n",
      "iteration no 1758: Loss: 0.27321475230345077, accuracy: 0.9533333333333334\n",
      "iteration no 1759: Loss: 0.27319451487193913, accuracy: 0.9566666666666667\n",
      "iteration no 1760: Loss: 0.27317278891171004, accuracy: 0.9533333333333334\n",
      "iteration no 1761: Loss: 0.2731535727647593, accuracy: 0.9566666666666667\n",
      "iteration no 1762: Loss: 0.27313283550613654, accuracy: 0.9533333333333334\n",
      "iteration no 1763: Loss: 0.2731126296197946, accuracy: 0.9566666666666667\n",
      "iteration no 1764: Loss: 0.27308959459973503, accuracy: 0.9533333333333334\n",
      "iteration no 1765: Loss: 0.27306887049755846, accuracy: 0.9566666666666667\n",
      "iteration no 1766: Loss: 0.2730483661665941, accuracy: 0.9533333333333334\n",
      "iteration no 1767: Loss: 0.2730278424296821, accuracy: 0.9566666666666667\n",
      "iteration no 1768: Loss: 0.2730075605493806, accuracy: 0.9533333333333334\n",
      "iteration no 1769: Loss: 0.2729870831634876, accuracy: 0.9566666666666667\n",
      "iteration no 1770: Loss: 0.2729661203508434, accuracy: 0.9533333333333334\n",
      "iteration no 1771: Loss: 0.2729468109089039, accuracy: 0.9566666666666667\n",
      "iteration no 1772: Loss: 0.27292446073294646, accuracy: 0.9533333333333334\n",
      "iteration no 1773: Loss: 0.27290433007985293, accuracy: 0.9566666666666667\n",
      "iteration no 1774: Loss: 0.2728833217227223, accuracy: 0.9533333333333334\n",
      "iteration no 1775: Loss: 0.2728646672608328, accuracy: 0.9566666666666667\n",
      "iteration no 1776: Loss: 0.2728435384744498, accuracy: 0.9533333333333334\n",
      "iteration no 1777: Loss: 0.272824796743446, accuracy: 0.9566666666666667\n",
      "iteration no 1778: Loss: 0.2728020997454593, accuracy: 0.9533333333333334\n",
      "iteration no 1779: Loss: 0.27278363376450004, accuracy: 0.9566666666666667\n",
      "iteration no 1780: Loss: 0.2727597146246858, accuracy: 0.9566666666666667\n",
      "iteration no 1781: Loss: 0.27274001554929606, accuracy: 0.9566666666666667\n",
      "iteration no 1782: Loss: 0.27271706576883004, accuracy: 0.9566666666666667\n",
      "iteration no 1783: Loss: 0.27269908208742644, accuracy: 0.9566666666666667\n",
      "iteration no 1784: Loss: 0.2726762805645671, accuracy: 0.9566666666666667\n",
      "iteration no 1785: Loss: 0.2726574570133664, accuracy: 0.9566666666666667\n",
      "iteration no 1786: Loss: 0.2726362887014251, accuracy: 0.9566666666666667\n",
      "iteration no 1787: Loss: 0.2726145817428371, accuracy: 0.9566666666666667\n",
      "iteration no 1788: Loss: 0.2725932943869668, accuracy: 0.9566666666666667\n",
      "iteration no 1789: Loss: 0.2725717159236548, accuracy: 0.9566666666666667\n",
      "iteration no 1790: Loss: 0.272550291635681, accuracy: 0.9566666666666667\n",
      "iteration no 1791: Loss: 0.27252904965545377, accuracy: 0.9566666666666667\n",
      "iteration no 1792: Loss: 0.27251010990889213, accuracy: 0.9566666666666667\n",
      "iteration no 1793: Loss: 0.2724891607085583, accuracy: 0.9566666666666667\n",
      "iteration no 1794: Loss: 0.27247018299132547, accuracy: 0.9566666666666667\n",
      "iteration no 1795: Loss: 0.2724486749585716, accuracy: 0.9566666666666667\n",
      "iteration no 1796: Loss: 0.2724304491770654, accuracy: 0.9566666666666667\n",
      "iteration no 1797: Loss: 0.2724115501610326, accuracy: 0.9566666666666667\n",
      "iteration no 1798: Loss: 0.27239285217975484, accuracy: 0.9566666666666667\n",
      "iteration no 1799: Loss: 0.27237290997683195, accuracy: 0.9566666666666667\n",
      "iteration no 1800: Loss: 0.2723513081522161, accuracy: 0.9566666666666667\n",
      "iteration no 1801: Loss: 0.27233414357004737, accuracy: 0.9566666666666667\n",
      "iteration no 1802: Loss: 0.27231383560239775, accuracy: 0.9566666666666667\n",
      "iteration no 1803: Loss: 0.2722936512583457, accuracy: 0.9566666666666667\n",
      "iteration no 1804: Loss: 0.27227238668931, accuracy: 0.9566666666666667\n",
      "iteration no 1805: Loss: 0.2722535855567185, accuracy: 0.9566666666666667\n",
      "iteration no 1806: Loss: 0.27223413990122025, accuracy: 0.9566666666666667\n",
      "iteration no 1807: Loss: 0.27221315378834327, accuracy: 0.9566666666666667\n",
      "iteration no 1808: Loss: 0.2721912148312573, accuracy: 0.9566666666666667\n",
      "iteration no 1809: Loss: 0.2721702986598799, accuracy: 0.9566666666666667\n",
      "iteration no 1810: Loss: 0.2721504985732061, accuracy: 0.9566666666666667\n",
      "iteration no 1811: Loss: 0.2721315280801229, accuracy: 0.9566666666666667\n",
      "iteration no 1812: Loss: 0.2721099069901357, accuracy: 0.9566666666666667\n",
      "iteration no 1813: Loss: 0.2720887331432515, accuracy: 0.9566666666666667\n",
      "iteration no 1814: Loss: 0.2720703203059888, accuracy: 0.9566666666666667\n",
      "iteration no 1815: Loss: 0.2720512675938393, accuracy: 0.9566666666666667\n",
      "iteration no 1816: Loss: 0.27203017055580747, accuracy: 0.9566666666666667\n",
      "iteration no 1817: Loss: 0.2720098399898081, accuracy: 0.9566666666666667\n",
      "iteration no 1818: Loss: 0.2719916880204667, accuracy: 0.9566666666666667\n",
      "iteration no 1819: Loss: 0.2719750410188223, accuracy: 0.9566666666666667\n",
      "iteration no 1820: Loss: 0.2719533426296169, accuracy: 0.9566666666666667\n",
      "iteration no 1821: Loss: 0.271931559594353, accuracy: 0.9566666666666667\n",
      "iteration no 1822: Loss: 0.2719127161538082, accuracy: 0.9566666666666667\n",
      "iteration no 1823: Loss: 0.27189676588429323, accuracy: 0.9566666666666667\n",
      "iteration no 1824: Loss: 0.2718739266211987, accuracy: 0.9566666666666667\n",
      "iteration no 1825: Loss: 0.27185303059486926, accuracy: 0.9566666666666667\n",
      "iteration no 1826: Loss: 0.27183492049750524, accuracy: 0.9566666666666667\n",
      "iteration no 1827: Loss: 0.2718186123964245, accuracy: 0.9566666666666667\n",
      "iteration no 1828: Loss: 0.2717971320651404, accuracy: 0.9566666666666667\n",
      "iteration no 1829: Loss: 0.2717751306982438, accuracy: 0.9566666666666667\n",
      "iteration no 1830: Loss: 0.271756110601847, accuracy: 0.9566666666666667\n",
      "iteration no 1831: Loss: 0.2717416532082334, accuracy: 0.9566666666666667\n",
      "iteration no 1832: Loss: 0.27171940234908487, accuracy: 0.9566666666666667\n",
      "iteration no 1833: Loss: 0.2716998368896745, accuracy: 0.9566666666666667\n",
      "iteration no 1834: Loss: 0.27167952207466517, accuracy: 0.9566666666666667\n",
      "iteration no 1835: Loss: 0.27166562953667206, accuracy: 0.9566666666666667\n",
      "iteration no 1836: Loss: 0.2716422266943239, accuracy: 0.9566666666666667\n",
      "iteration no 1837: Loss: 0.2716197905525057, accuracy: 0.9566666666666667\n",
      "iteration no 1838: Loss: 0.2715986252760017, accuracy: 0.9566666666666667\n",
      "iteration no 1839: Loss: 0.2715822686408026, accuracy: 0.9566666666666667\n",
      "iteration no 1840: Loss: 0.2715622532243797, accuracy: 0.9566666666666667\n",
      "iteration no 1841: Loss: 0.2715383979346447, accuracy: 0.9566666666666667\n",
      "iteration no 1842: Loss: 0.2715189310684499, accuracy: 0.9566666666666667\n",
      "iteration no 1843: Loss: 0.271503308460283, accuracy: 0.9566666666666667\n",
      "iteration no 1844: Loss: 0.27148078234212475, accuracy: 0.9566666666666667\n",
      "iteration no 1845: Loss: 0.27145871761014423, accuracy: 0.9566666666666667\n",
      "iteration no 1846: Loss: 0.2714378606204149, accuracy: 0.9566666666666667\n",
      "iteration no 1847: Loss: 0.2714235799591769, accuracy: 0.9566666666666667\n",
      "iteration no 1848: Loss: 0.2714010506190773, accuracy: 0.9566666666666667\n",
      "iteration no 1849: Loss: 0.2713796821946447, accuracy: 0.9566666666666667\n",
      "iteration no 1850: Loss: 0.27135864369914303, accuracy: 0.9566666666666667\n",
      "iteration no 1851: Loss: 0.2713447429301219, accuracy: 0.9566666666666667\n",
      "iteration no 1852: Loss: 0.2713205823077303, accuracy: 0.9566666666666667\n",
      "iteration no 1853: Loss: 0.2712984654512849, accuracy: 0.9566666666666667\n",
      "iteration no 1854: Loss: 0.27127763914421815, accuracy: 0.9566666666666667\n",
      "iteration no 1855: Loss: 0.2712614380560743, accuracy: 0.9566666666666667\n",
      "iteration no 1856: Loss: 0.2712385886552305, accuracy: 0.9566666666666667\n",
      "iteration no 1857: Loss: 0.27121648283041616, accuracy: 0.9566666666666667\n",
      "iteration no 1858: Loss: 0.2711972171397938, accuracy: 0.9566666666666667\n",
      "iteration no 1859: Loss: 0.2711783293483403, accuracy: 0.9566666666666667\n",
      "iteration no 1860: Loss: 0.2711592569035076, accuracy: 0.9566666666666667\n",
      "iteration no 1861: Loss: 0.2711373884076685, accuracy: 0.9566666666666667\n",
      "iteration no 1862: Loss: 0.27111847421856466, accuracy: 0.9566666666666667\n",
      "iteration no 1863: Loss: 0.2710994396099073, accuracy: 0.9566666666666667\n",
      "iteration no 1864: Loss: 0.2710803903095038, accuracy: 0.9566666666666667\n",
      "iteration no 1865: Loss: 0.2710593089661412, accuracy: 0.9566666666666667\n",
      "iteration no 1866: Loss: 0.2710398291204423, accuracy: 0.9566666666666667\n",
      "iteration no 1867: Loss: 0.271020746485806, accuracy: 0.9566666666666667\n",
      "iteration no 1868: Loss: 0.2710020736737711, accuracy: 0.9566666666666667\n",
      "iteration no 1869: Loss: 0.2709810500716566, accuracy: 0.9566666666666667\n",
      "iteration no 1870: Loss: 0.2709621958734383, accuracy: 0.9566666666666667\n",
      "iteration no 1871: Loss: 0.2709425245066519, accuracy: 0.9566666666666667\n",
      "iteration no 1872: Loss: 0.2709238140698884, accuracy: 0.9566666666666667\n",
      "iteration no 1873: Loss: 0.27090331090289305, accuracy: 0.9566666666666667\n",
      "iteration no 1874: Loss: 0.2708841834641902, accuracy: 0.9566666666666667\n",
      "iteration no 1875: Loss: 0.2708642952926995, accuracy: 0.9566666666666667\n",
      "iteration no 1876: Loss: 0.2708451902871718, accuracy: 0.9566666666666667\n",
      "iteration no 1877: Loss: 0.27082309807282695, accuracy: 0.9566666666666667\n",
      "iteration no 1878: Loss: 0.27080384295956916, accuracy: 0.9566666666666667\n",
      "iteration no 1879: Loss: 0.27078266344824914, accuracy: 0.9566666666666667\n",
      "iteration no 1880: Loss: 0.2707638696833856, accuracy: 0.9566666666666667\n",
      "iteration no 1881: Loss: 0.2707425446277436, accuracy: 0.9566666666666667\n",
      "iteration no 1882: Loss: 0.2707224035584215, accuracy: 0.9566666666666667\n",
      "iteration no 1883: Loss: 0.2707027418863647, accuracy: 0.9566666666666667\n",
      "iteration no 1884: Loss: 0.27068266226085014, accuracy: 0.9566666666666667\n",
      "iteration no 1885: Loss: 0.2706631902415237, accuracy: 0.9566666666666667\n",
      "iteration no 1886: Loss: 0.27064252073211253, accuracy: 0.9566666666666667\n",
      "iteration no 1887: Loss: 0.27062289543620693, accuracy: 0.9566666666666667\n",
      "iteration no 1888: Loss: 0.27060340576364006, accuracy: 0.9566666666666667\n",
      "iteration no 1889: Loss: 0.2705837821800174, accuracy: 0.9566666666666667\n",
      "iteration no 1890: Loss: 0.27056318499425586, accuracy: 0.9566666666666667\n",
      "iteration no 1891: Loss: 0.27054461559511805, accuracy: 0.9566666666666667\n",
      "iteration no 1892: Loss: 0.27052544352354446, accuracy: 0.9566666666666667\n",
      "iteration no 1893: Loss: 0.2705052943235547, accuracy: 0.9566666666666667\n",
      "iteration no 1894: Loss: 0.2704853369024503, accuracy: 0.96\n",
      "iteration no 1895: Loss: 0.27046604785904155, accuracy: 0.9566666666666667\n",
      "iteration no 1896: Loss: 0.2704482214939772, accuracy: 0.96\n",
      "iteration no 1897: Loss: 0.27042761919543395, accuracy: 0.9566666666666667\n",
      "iteration no 1898: Loss: 0.27040763338776086, accuracy: 0.96\n",
      "iteration no 1899: Loss: 0.27038735261655084, accuracy: 0.9566666666666667\n",
      "iteration no 1900: Loss: 0.2703690802800455, accuracy: 0.96\n",
      "iteration no 1901: Loss: 0.270349304592803, accuracy: 0.9566666666666667\n",
      "iteration no 1902: Loss: 0.2703281997592078, accuracy: 0.96\n",
      "iteration no 1903: Loss: 0.2703071893848392, accuracy: 0.9566666666666667\n",
      "iteration no 1904: Loss: 0.270288892188863, accuracy: 0.96\n",
      "iteration no 1905: Loss: 0.2702680299096693, accuracy: 0.9566666666666667\n",
      "iteration no 1906: Loss: 0.2702471801108233, accuracy: 0.9566666666666667\n",
      "iteration no 1907: Loss: 0.2702287988856039, accuracy: 0.9566666666666667\n",
      "iteration no 1908: Loss: 0.27021011147699425, accuracy: 0.9566666666666667\n",
      "iteration no 1909: Loss: 0.27019062904637603, accuracy: 0.9566666666666667\n",
      "iteration no 1910: Loss: 0.27016960338213725, accuracy: 0.9566666666666667\n",
      "iteration no 1911: Loss: 0.27015241286163466, accuracy: 0.9566666666666667\n",
      "iteration no 1912: Loss: 0.27013469620037384, accuracy: 0.96\n",
      "iteration no 1913: Loss: 0.270115047572658, accuracy: 0.9566666666666667\n",
      "iteration no 1914: Loss: 0.2700943054729742, accuracy: 0.96\n",
      "iteration no 1915: Loss: 0.2700784559271525, accuracy: 0.9566666666666667\n",
      "iteration no 1916: Loss: 0.2700608662497215, accuracy: 0.96\n",
      "iteration no 1917: Loss: 0.27004154041580775, accuracy: 0.9566666666666667\n",
      "iteration no 1918: Loss: 0.27002089611834984, accuracy: 0.96\n",
      "iteration no 1919: Loss: 0.27000597492537376, accuracy: 0.9566666666666667\n",
      "iteration no 1920: Loss: 0.2699881280898554, accuracy: 0.96\n",
      "iteration no 1921: Loss: 0.2699677233526506, accuracy: 0.9566666666666667\n",
      "iteration no 1922: Loss: 0.2699476591422872, accuracy: 0.96\n",
      "iteration no 1923: Loss: 0.26993309433632007, accuracy: 0.9566666666666667\n",
      "iteration no 1924: Loss: 0.2699151629034528, accuracy: 0.96\n",
      "iteration no 1925: Loss: 0.26989451012005133, accuracy: 0.9566666666666667\n",
      "iteration no 1926: Loss: 0.26987491378704126, accuracy: 0.96\n",
      "iteration no 1927: Loss: 0.2698583492787988, accuracy: 0.9566666666666667\n",
      "iteration no 1928: Loss: 0.26984022155912313, accuracy: 0.96\n",
      "iteration no 1929: Loss: 0.26981914377974514, accuracy: 0.9566666666666667\n",
      "iteration no 1930: Loss: 0.2698010914024419, accuracy: 0.96\n",
      "iteration no 1931: Loss: 0.26978520101856385, accuracy: 0.96\n",
      "iteration no 1932: Loss: 0.26976392040318103, accuracy: 0.96\n",
      "iteration no 1933: Loss: 0.2697452688552219, accuracy: 0.96\n",
      "iteration no 1934: Loss: 0.2697298857152772, accuracy: 0.96\n",
      "iteration no 1935: Loss: 0.2697106670148509, accuracy: 0.96\n",
      "iteration no 1936: Loss: 0.2696905180627871, accuracy: 0.96\n",
      "iteration no 1937: Loss: 0.2696729001240288, accuracy: 0.96\n",
      "iteration no 1938: Loss: 0.26965761755328704, accuracy: 0.96\n",
      "iteration no 1939: Loss: 0.2696366800709477, accuracy: 0.96\n",
      "iteration no 1940: Loss: 0.26961656060622285, accuracy: 0.96\n",
      "iteration no 1941: Loss: 0.26960099912183, accuracy: 0.96\n",
      "iteration no 1942: Loss: 0.2695812539281022, accuracy: 0.96\n",
      "iteration no 1943: Loss: 0.269561768970036, accuracy: 0.96\n",
      "iteration no 1944: Loss: 0.2695438466893935, accuracy: 0.96\n",
      "iteration no 1945: Loss: 0.2695270692608574, accuracy: 0.96\n",
      "iteration no 1946: Loss: 0.2695079058946992, accuracy: 0.96\n",
      "iteration no 1947: Loss: 0.2694882037287989, accuracy: 0.96\n",
      "iteration no 1948: Loss: 0.26947245476878695, accuracy: 0.96\n",
      "iteration no 1949: Loss: 0.2694528402797256, accuracy: 0.96\n",
      "iteration no 1950: Loss: 0.2694313017126467, accuracy: 0.96\n",
      "iteration no 1951: Loss: 0.26941429159360447, accuracy: 0.96\n",
      "iteration no 1952: Loss: 0.26939459911842933, accuracy: 0.96\n",
      "iteration no 1953: Loss: 0.26937511114852025, accuracy: 0.96\n",
      "iteration no 1954: Loss: 0.26935524457308785, accuracy: 0.96\n",
      "iteration no 1955: Loss: 0.2693367505855773, accuracy: 0.96\n",
      "iteration no 1956: Loss: 0.2693173303192037, accuracy: 0.96\n",
      "iteration no 1957: Loss: 0.2692973507614825, accuracy: 0.96\n",
      "iteration no 1958: Loss: 0.26928056725453187, accuracy: 0.96\n",
      "iteration no 1959: Loss: 0.26926020892851577, accuracy: 0.96\n",
      "iteration no 1960: Loss: 0.26924149293736827, accuracy: 0.96\n",
      "iteration no 1961: Loss: 0.2692248786361529, accuracy: 0.96\n",
      "iteration no 1962: Loss: 0.2692028459127735, accuracy: 0.96\n",
      "iteration no 1963: Loss: 0.26918407378438236, accuracy: 0.96\n",
      "iteration no 1964: Loss: 0.2691690173442196, accuracy: 0.96\n",
      "iteration no 1965: Loss: 0.26914966460066797, accuracy: 0.96\n",
      "iteration no 1966: Loss: 0.2691286504835475, accuracy: 0.96\n",
      "iteration no 1967: Loss: 0.2691130618571977, accuracy: 0.96\n",
      "iteration no 1968: Loss: 0.2690973596823131, accuracy: 0.96\n",
      "iteration no 1969: Loss: 0.26907458446009497, accuracy: 0.96\n",
      "iteration no 1970: Loss: 0.2690569805016473, accuracy: 0.96\n",
      "iteration no 1971: Loss: 0.2690382256018586, accuracy: 0.96\n",
      "iteration no 1972: Loss: 0.2690184372171286, accuracy: 0.96\n",
      "iteration no 1973: Loss: 0.2690013918152021, accuracy: 0.96\n",
      "iteration no 1974: Loss: 0.2689833887087418, accuracy: 0.96\n",
      "iteration no 1975: Loss: 0.268965304062668, accuracy: 0.96\n",
      "iteration no 1976: Loss: 0.2689459248803057, accuracy: 0.96\n",
      "iteration no 1977: Loss: 0.26892907903127516, accuracy: 0.96\n",
      "iteration no 1978: Loss: 0.26891173520101375, accuracy: 0.96\n",
      "iteration no 1979: Loss: 0.26889101695919565, accuracy: 0.96\n",
      "iteration no 1980: Loss: 0.2688760608634175, accuracy: 0.96\n",
      "iteration no 1981: Loss: 0.268855249423511, accuracy: 0.96\n",
      "iteration no 1982: Loss: 0.26883570187440564, accuracy: 0.96\n",
      "iteration no 1983: Loss: 0.2688200179032721, accuracy: 0.96\n",
      "iteration no 1984: Loss: 0.26880277620561444, accuracy: 0.96\n",
      "iteration no 1985: Loss: 0.26878367104723616, accuracy: 0.96\n",
      "iteration no 1986: Loss: 0.26876202248081354, accuracy: 0.96\n",
      "iteration no 1987: Loss: 0.2687489019306615, accuracy: 0.96\n",
      "iteration no 1988: Loss: 0.2687260598554169, accuracy: 0.96\n",
      "iteration no 1989: Loss: 0.26870841224337744, accuracy: 0.96\n",
      "iteration no 1990: Loss: 0.26869351358157906, accuracy: 0.96\n",
      "iteration no 1991: Loss: 0.26867363335018907, accuracy: 0.96\n",
      "iteration no 1992: Loss: 0.26865527075357853, accuracy: 0.96\n",
      "iteration no 1993: Loss: 0.26863864799902515, accuracy: 0.96\n",
      "iteration no 1994: Loss: 0.26861872340009474, accuracy: 0.96\n",
      "iteration no 1995: Loss: 0.26860211128889144, accuracy: 0.96\n",
      "iteration no 1996: Loss: 0.2685834589473334, accuracy: 0.96\n",
      "iteration no 1997: Loss: 0.2685657418253844, accuracy: 0.96\n",
      "iteration no 1998: Loss: 0.2685470447798883, accuracy: 0.96\n",
      "iteration no 1999: Loss: 0.2685306529049561, accuracy: 0.96\n",
      "iteration no 2000: Loss: 0.268512436153397, accuracy: 0.96\n",
      "iteration no 2001: Loss: 0.2684930026145066, accuracy: 0.96\n",
      "iteration no 2002: Loss: 0.2684779364706689, accuracy: 0.96\n",
      "iteration no 2003: Loss: 0.26845855383361256, accuracy: 0.96\n",
      "iteration no 2004: Loss: 0.26844047160324075, accuracy: 0.96\n",
      "iteration no 2005: Loss: 0.2684238143448083, accuracy: 0.96\n",
      "iteration no 2006: Loss: 0.2684050311082246, accuracy: 0.96\n",
      "iteration no 2007: Loss: 0.26838886332845613, accuracy: 0.96\n",
      "iteration no 2008: Loss: 0.26836982446793034, accuracy: 0.96\n",
      "iteration no 2009: Loss: 0.2683502161578578, accuracy: 0.96\n",
      "iteration no 2010: Loss: 0.2683326883718512, accuracy: 0.96\n",
      "iteration no 2011: Loss: 0.26831398917603766, accuracy: 0.96\n",
      "iteration no 2012: Loss: 0.26829555504529157, accuracy: 0.96\n",
      "iteration no 2013: Loss: 0.26827725799439, accuracy: 0.96\n",
      "iteration no 2014: Loss: 0.26826207054164536, accuracy: 0.96\n",
      "iteration no 2015: Loss: 0.268240256394896, accuracy: 0.96\n",
      "iteration no 2016: Loss: 0.2682215097514798, accuracy: 0.96\n",
      "iteration no 2017: Loss: 0.2682033244662204, accuracy: 0.96\n",
      "iteration no 2018: Loss: 0.26818317512878803, accuracy: 0.96\n",
      "iteration no 2019: Loss: 0.2681664724138714, accuracy: 0.96\n",
      "iteration no 2020: Loss: 0.26814689005041287, accuracy: 0.96\n",
      "iteration no 2021: Loss: 0.26812836225131226, accuracy: 0.96\n",
      "iteration no 2022: Loss: 0.2681084449745767, accuracy: 0.96\n",
      "iteration no 2023: Loss: 0.2680888830137686, accuracy: 0.96\n",
      "iteration no 2024: Loss: 0.26806827933929206, accuracy: 0.96\n",
      "iteration no 2025: Loss: 0.26805401757250413, accuracy: 0.96\n",
      "iteration no 2026: Loss: 0.26803247644796424, accuracy: 0.96\n",
      "iteration no 2027: Loss: 0.2680111752235632, accuracy: 0.96\n",
      "iteration no 2028: Loss: 0.2679962176188408, accuracy: 0.96\n",
      "iteration no 2029: Loss: 0.2679753726315402, accuracy: 0.96\n",
      "iteration no 2030: Loss: 0.2679559791244345, accuracy: 0.96\n",
      "iteration no 2031: Loss: 0.26793736256027517, accuracy: 0.96\n",
      "iteration no 2032: Loss: 0.2679205769910257, accuracy: 0.96\n",
      "iteration no 2033: Loss: 0.2678990566149079, accuracy: 0.96\n",
      "iteration no 2034: Loss: 0.26788113125568275, accuracy: 0.96\n",
      "iteration no 2035: Loss: 0.2678611207764171, accuracy: 0.96\n",
      "iteration no 2036: Loss: 0.26784227836970265, accuracy: 0.96\n",
      "iteration no 2037: Loss: 0.2678251462556886, accuracy: 0.96\n",
      "iteration no 2038: Loss: 0.2678058067770234, accuracy: 0.96\n",
      "iteration no 2039: Loss: 0.2677867519211382, accuracy: 0.96\n",
      "iteration no 2040: Loss: 0.2677687153777829, accuracy: 0.96\n",
      "iteration no 2041: Loss: 0.26774867720228895, accuracy: 0.96\n",
      "iteration no 2042: Loss: 0.26772739441975624, accuracy: 0.96\n",
      "iteration no 2043: Loss: 0.26771181389386334, accuracy: 0.96\n",
      "iteration no 2044: Loss: 0.2676919536142607, accuracy: 0.96\n",
      "iteration no 2045: Loss: 0.2676715621247592, accuracy: 0.96\n",
      "iteration no 2046: Loss: 0.2676538230042159, accuracy: 0.96\n",
      "iteration no 2047: Loss: 0.26763479774139487, accuracy: 0.96\n",
      "iteration no 2048: Loss: 0.2676141352936028, accuracy: 0.96\n",
      "iteration no 2049: Loss: 0.26759752461561703, accuracy: 0.96\n",
      "iteration no 2050: Loss: 0.26757808847094433, accuracy: 0.96\n",
      "iteration no 2051: Loss: 0.26755812376809246, accuracy: 0.96\n",
      "iteration no 2052: Loss: 0.2675402115183088, accuracy: 0.96\n",
      "iteration no 2053: Loss: 0.26752095092860767, accuracy: 0.96\n",
      "iteration no 2054: Loss: 0.26750283408633396, accuracy: 0.96\n",
      "iteration no 2055: Loss: 0.26748428786354583, accuracy: 0.96\n",
      "iteration no 2056: Loss: 0.26746353465244926, accuracy: 0.96\n",
      "iteration no 2057: Loss: 0.2674470728491572, accuracy: 0.96\n",
      "iteration no 2058: Loss: 0.26742718751813954, accuracy: 0.96\n",
      "iteration no 2059: Loss: 0.2674059222697547, accuracy: 0.96\n",
      "iteration no 2060: Loss: 0.26738981145299473, accuracy: 0.96\n",
      "iteration no 2061: Loss: 0.26737108359115014, accuracy: 0.96\n",
      "iteration no 2062: Loss: 0.2673490366490678, accuracy: 0.96\n",
      "iteration no 2063: Loss: 0.2673306833203931, accuracy: 0.96\n",
      "iteration no 2064: Loss: 0.2673102463377268, accuracy: 0.96\n",
      "iteration no 2065: Loss: 0.2672872386825233, accuracy: 0.96\n",
      "iteration no 2066: Loss: 0.26726520327191705, accuracy: 0.96\n",
      "iteration no 2067: Loss: 0.267243267270142, accuracy: 0.96\n",
      "iteration no 2068: Loss: 0.2672226047903875, accuracy: 0.96\n",
      "iteration no 2069: Loss: 0.2672019662080903, accuracy: 0.96\n",
      "iteration no 2070: Loss: 0.267179286224576, accuracy: 0.96\n",
      "iteration no 2071: Loss: 0.2671604468469999, accuracy: 0.96\n",
      "iteration no 2072: Loss: 0.26713831266518306, accuracy: 0.96\n",
      "iteration no 2073: Loss: 0.2671161316120415, accuracy: 0.96\n",
      "iteration no 2074: Loss: 0.26709944144219305, accuracy: 0.96\n",
      "iteration no 2075: Loss: 0.26707575509370335, accuracy: 0.96\n",
      "iteration no 2076: Loss: 0.2670544363245428, accuracy: 0.96\n",
      "iteration no 2077: Loss: 0.2670359517057874, accuracy: 0.96\n",
      "iteration no 2078: Loss: 0.2670139499041855, accuracy: 0.96\n",
      "iteration no 2079: Loss: 0.26699446479017774, accuracy: 0.96\n",
      "iteration no 2080: Loss: 0.2669735345260563, accuracy: 0.96\n",
      "iteration no 2081: Loss: 0.26695259021632295, accuracy: 0.96\n",
      "iteration no 2082: Loss: 0.2669320405021419, accuracy: 0.96\n",
      "iteration no 2083: Loss: 0.2669099755997397, accuracy: 0.96\n",
      "iteration no 2084: Loss: 0.266889129172764, accuracy: 0.96\n",
      "iteration no 2085: Loss: 0.2668717880437347, accuracy: 0.96\n",
      "iteration no 2086: Loss: 0.26684882653496095, accuracy: 0.96\n",
      "iteration no 2087: Loss: 0.26682785921790086, accuracy: 0.96\n",
      "iteration no 2088: Loss: 0.2668086808500691, accuracy: 0.96\n",
      "iteration no 2089: Loss: 0.2667864096473675, accuracy: 0.96\n",
      "iteration no 2090: Loss: 0.2667671584723861, accuracy: 0.96\n",
      "iteration no 2091: Loss: 0.26674799730082444, accuracy: 0.96\n",
      "iteration no 2092: Loss: 0.2667252259244099, accuracy: 0.96\n",
      "iteration no 2093: Loss: 0.26670741238216755, accuracy: 0.96\n",
      "iteration no 2094: Loss: 0.26668677317261125, accuracy: 0.96\n",
      "iteration no 2095: Loss: 0.2666645737871079, accuracy: 0.96\n",
      "iteration no 2096: Loss: 0.2666453307793073, accuracy: 0.96\n",
      "iteration no 2097: Loss: 0.26662393187379396, accuracy: 0.96\n",
      "iteration no 2098: Loss: 0.2666046356856173, accuracy: 0.96\n",
      "iteration no 2099: Loss: 0.26658409523138604, accuracy: 0.96\n",
      "iteration no 2100: Loss: 0.2665636959247021, accuracy: 0.96\n",
      "iteration no 2101: Loss: 0.26654475432436486, accuracy: 0.96\n",
      "iteration no 2102: Loss: 0.2665223074513413, accuracy: 0.96\n",
      "iteration no 2103: Loss: 0.2665001890609868, accuracy: 0.96\n",
      "iteration no 2104: Loss: 0.26648407974545496, accuracy: 0.96\n",
      "iteration no 2105: Loss: 0.2664610169702582, accuracy: 0.96\n",
      "iteration no 2106: Loss: 0.2664404635857239, accuracy: 0.96\n",
      "iteration no 2107: Loss: 0.26642167930093924, accuracy: 0.96\n",
      "iteration no 2108: Loss: 0.2663966671939698, accuracy: 0.96\n",
      "iteration no 2109: Loss: 0.26637442618386353, accuracy: 0.96\n",
      "iteration no 2110: Loss: 0.2663510370123537, accuracy: 0.96\n",
      "iteration no 2111: Loss: 0.26632691087663823, accuracy: 0.96\n",
      "iteration no 2112: Loss: 0.266305556397994, accuracy: 0.96\n",
      "iteration no 2113: Loss: 0.2662810443464602, accuracy: 0.96\n",
      "iteration no 2114: Loss: 0.26625917326908777, accuracy: 0.96\n",
      "iteration no 2115: Loss: 0.2662356698386005, accuracy: 0.96\n",
      "iteration no 2116: Loss: 0.26621021352701657, accuracy: 0.96\n",
      "iteration no 2117: Loss: 0.2661906217227568, accuracy: 0.96\n",
      "iteration no 2118: Loss: 0.26616640680334414, accuracy: 0.96\n",
      "iteration no 2119: Loss: 0.26614097142522736, accuracy: 0.96\n",
      "iteration no 2120: Loss: 0.2661241322215516, accuracy: 0.96\n",
      "iteration no 2121: Loss: 0.2660982236293594, accuracy: 0.96\n",
      "iteration no 2122: Loss: 0.2660741089167665, accuracy: 0.96\n",
      "iteration no 2123: Loss: 0.2660547679138945, accuracy: 0.96\n",
      "iteration no 2124: Loss: 0.26602916657554193, accuracy: 0.96\n",
      "iteration no 2125: Loss: 0.26600757068313297, accuracy: 0.96\n",
      "iteration no 2126: Loss: 0.2659867429292347, accuracy: 0.96\n",
      "iteration no 2127: Loss: 0.2659627086440877, accuracy: 0.96\n",
      "iteration no 2128: Loss: 0.26594244096296593, accuracy: 0.96\n",
      "iteration no 2129: Loss: 0.26592059981900323, accuracy: 0.96\n",
      "iteration no 2130: Loss: 0.26589750842192433, accuracy: 0.96\n",
      "iteration no 2131: Loss: 0.26587872414195973, accuracy: 0.96\n",
      "iteration no 2132: Loss: 0.26585507091619504, accuracy: 0.96\n",
      "iteration no 2133: Loss: 0.2658333337775968, accuracy: 0.96\n",
      "iteration no 2134: Loss: 0.26581415336229564, accuracy: 0.96\n",
      "iteration no 2135: Loss: 0.26578972911108417, accuracy: 0.96\n",
      "iteration no 2136: Loss: 0.26577018748932213, accuracy: 0.96\n",
      "iteration no 2137: Loss: 0.2657474263666973, accuracy: 0.96\n",
      "iteration no 2138: Loss: 0.2657227743042829, accuracy: 0.96\n",
      "iteration no 2139: Loss: 0.2657054988108549, accuracy: 0.96\n",
      "iteration no 2140: Loss: 0.26568170855617684, accuracy: 0.96\n",
      "iteration no 2141: Loss: 0.2656594851303656, accuracy: 0.96\n",
      "iteration no 2142: Loss: 0.2656420306404103, accuracy: 0.96\n",
      "iteration no 2143: Loss: 0.26561768432370236, accuracy: 0.96\n",
      "iteration no 2144: Loss: 0.2655966715889194, accuracy: 0.96\n",
      "iteration no 2145: Loss: 0.2655762591262586, accuracy: 0.96\n",
      "iteration no 2146: Loss: 0.26555405668927323, accuracy: 0.96\n",
      "iteration no 2147: Loss: 0.26553418093503284, accuracy: 0.96\n",
      "iteration no 2148: Loss: 0.26551364778395414, accuracy: 0.96\n",
      "iteration no 2149: Loss: 0.2654903953334824, accuracy: 0.96\n",
      "iteration no 2150: Loss: 0.26547086934704334, accuracy: 0.96\n",
      "iteration no 2151: Loss: 0.26544978308290024, accuracy: 0.96\n",
      "iteration no 2152: Loss: 0.2654284849957377, accuracy: 0.96\n",
      "iteration no 2153: Loss: 0.26540765781852044, accuracy: 0.96\n",
      "iteration no 2154: Loss: 0.2653843231019941, accuracy: 0.96\n",
      "iteration no 2155: Loss: 0.26536544065133805, accuracy: 0.96\n",
      "iteration no 2156: Loss: 0.26534320596530575, accuracy: 0.96\n",
      "iteration no 2157: Loss: 0.26532133934692886, accuracy: 0.96\n",
      "iteration no 2158: Loss: 0.26530645827296506, accuracy: 0.96\n",
      "iteration no 2159: Loss: 0.2652846778922351, accuracy: 0.96\n",
      "iteration no 2160: Loss: 0.26526704016182956, accuracy: 0.96\n",
      "iteration no 2161: Loss: 0.26524727533652587, accuracy: 0.96\n",
      "iteration no 2162: Loss: 0.26522682059368863, accuracy: 0.96\n",
      "iteration no 2163: Loss: 0.2652114060110562, accuracy: 0.96\n",
      "iteration no 2164: Loss: 0.2651921043939236, accuracy: 0.96\n",
      "iteration no 2165: Loss: 0.26517406895707474, accuracy: 0.96\n",
      "iteration no 2166: Loss: 0.2651564393929892, accuracy: 0.96\n",
      "iteration no 2167: Loss: 0.26513685764111466, accuracy: 0.96\n",
      "iteration no 2168: Loss: 0.26512047958900153, accuracy: 0.96\n",
      "iteration no 2169: Loss: 0.2651028903001026, accuracy: 0.96\n",
      "iteration no 2170: Loss: 0.2650840073358812, accuracy: 0.96\n",
      "iteration no 2171: Loss: 0.26506562796642524, accuracy: 0.96\n",
      "iteration no 2172: Loss: 0.26504539727759785, accuracy: 0.96\n",
      "iteration no 2173: Loss: 0.2650303158539667, accuracy: 0.96\n",
      "iteration no 2174: Loss: 0.26501060846394264, accuracy: 0.96\n",
      "iteration no 2175: Loss: 0.2649929869581449, accuracy: 0.96\n",
      "iteration no 2176: Loss: 0.264977226871198, accuracy: 0.96\n",
      "iteration no 2177: Loss: 0.26495499870991956, accuracy: 0.96\n",
      "iteration no 2178: Loss: 0.2649400029437672, accuracy: 0.96\n",
      "iteration no 2179: Loss: 0.26491978932430155, accuracy: 0.96\n",
      "iteration no 2180: Loss: 0.2649002596699037, accuracy: 0.96\n",
      "iteration no 2181: Loss: 0.2648838293234316, accuracy: 0.96\n",
      "iteration no 2182: Loss: 0.2648632149888879, accuracy: 0.96\n",
      "iteration no 2183: Loss: 0.2648464530069027, accuracy: 0.96\n",
      "iteration no 2184: Loss: 0.26482965163699523, accuracy: 0.96\n",
      "iteration no 2185: Loss: 0.2648109360146889, accuracy: 0.96\n",
      "iteration no 2186: Loss: 0.26479157395118774, accuracy: 0.96\n",
      "iteration no 2187: Loss: 0.2647736640817998, accuracy: 0.96\n",
      "iteration no 2188: Loss: 0.26475559029114837, accuracy: 0.96\n",
      "iteration no 2189: Loss: 0.2647369731526597, accuracy: 0.96\n",
      "iteration no 2190: Loss: 0.26471890387902064, accuracy: 0.96\n",
      "iteration no 2191: Loss: 0.26469782125612307, accuracy: 0.96\n",
      "iteration no 2192: Loss: 0.26467869918205, accuracy: 0.96\n",
      "iteration no 2193: Loss: 0.2646614534561466, accuracy: 0.96\n",
      "iteration no 2194: Loss: 0.26463912294867126, accuracy: 0.96\n",
      "iteration no 2195: Loss: 0.26462395477029754, accuracy: 0.96\n",
      "iteration no 2196: Loss: 0.26460278919032887, accuracy: 0.96\n",
      "iteration no 2197: Loss: 0.2645845346305331, accuracy: 0.96\n",
      "iteration no 2198: Loss: 0.26456698876422097, accuracy: 0.96\n",
      "iteration no 2199: Loss: 0.26454646694007317, accuracy: 0.96\n",
      "iteration no 2200: Loss: 0.26452934131814276, accuracy: 0.96\n",
      "iteration no 2201: Loss: 0.2645089264732052, accuracy: 0.96\n",
      "iteration no 2202: Loss: 0.2644931533853747, accuracy: 0.96\n",
      "iteration no 2203: Loss: 0.26447329353419174, accuracy: 0.96\n",
      "iteration no 2204: Loss: 0.2644562339002761, accuracy: 0.96\n",
      "iteration no 2205: Loss: 0.2644353181950937, accuracy: 0.96\n",
      "iteration no 2206: Loss: 0.26441887463249075, accuracy: 0.96\n",
      "iteration no 2207: Loss: 0.2643993822442586, accuracy: 0.96\n",
      "iteration no 2208: Loss: 0.26438098693914036, accuracy: 0.96\n",
      "iteration no 2209: Loss: 0.2643647490742177, accuracy: 0.96\n",
      "iteration no 2210: Loss: 0.2643435360465178, accuracy: 0.96\n",
      "iteration no 2211: Loss: 0.2643289971958369, accuracy: 0.96\n",
      "iteration no 2212: Loss: 0.2643089655484068, accuracy: 0.96\n",
      "iteration no 2213: Loss: 0.2642914011610633, accuracy: 0.96\n",
      "iteration no 2214: Loss: 0.26427527230537007, accuracy: 0.96\n",
      "iteration no 2215: Loss: 0.2642550424386057, accuracy: 0.96\n",
      "iteration no 2216: Loss: 0.26424010451221086, accuracy: 0.96\n",
      "iteration no 2217: Loss: 0.2642194070756514, accuracy: 0.96\n",
      "iteration no 2218: Loss: 0.2642041648710149, accuracy: 0.96\n",
      "iteration no 2219: Loss: 0.26418498166751964, accuracy: 0.96\n",
      "iteration no 2220: Loss: 0.2641695643123844, accuracy: 0.96\n",
      "iteration no 2221: Loss: 0.26414944218343944, accuracy: 0.96\n",
      "iteration no 2222: Loss: 0.2641338210091507, accuracy: 0.96\n",
      "iteration no 2223: Loss: 0.26411474757711106, accuracy: 0.96\n",
      "iteration no 2224: Loss: 0.2640980217996844, accuracy: 0.96\n",
      "iteration no 2225: Loss: 0.26408105972444196, accuracy: 0.96\n",
      "iteration no 2226: Loss: 0.2640622900269588, accuracy: 0.96\n",
      "iteration no 2227: Loss: 0.26404853730579575, accuracy: 0.96\n",
      "iteration no 2228: Loss: 0.26402671294672175, accuracy: 0.96\n",
      "iteration no 2229: Loss: 0.264011691968271, accuracy: 0.96\n",
      "iteration no 2230: Loss: 0.26399330735770926, accuracy: 0.96\n",
      "iteration no 2231: Loss: 0.2639744476791927, accuracy: 0.96\n",
      "iteration no 2232: Loss: 0.26395946274933996, accuracy: 0.96\n",
      "iteration no 2233: Loss: 0.26394020528530565, accuracy: 0.96\n",
      "iteration no 2234: Loss: 0.263923894697904, accuracy: 0.96\n",
      "iteration no 2235: Loss: 0.2639047583487972, accuracy: 0.96\n",
      "iteration no 2236: Loss: 0.2638884363111231, accuracy: 0.96\n",
      "iteration no 2237: Loss: 0.26387066097703643, accuracy: 0.96\n",
      "iteration no 2238: Loss: 0.2638529118830549, accuracy: 0.96\n",
      "iteration no 2239: Loss: 0.26383564351022265, accuracy: 0.96\n",
      "iteration no 2240: Loss: 0.26381793533235987, accuracy: 0.96\n",
      "iteration no 2241: Loss: 0.26380122779808757, accuracy: 0.96\n",
      "iteration no 2242: Loss: 0.2637829760194046, accuracy: 0.96\n",
      "iteration no 2243: Loss: 0.26376948024165636, accuracy: 0.96\n",
      "iteration no 2244: Loss: 0.2637483436256273, accuracy: 0.96\n",
      "iteration no 2245: Loss: 0.26373579524818536, accuracy: 0.96\n",
      "iteration no 2246: Loss: 0.26371277160971446, accuracy: 0.96\n",
      "iteration no 2247: Loss: 0.26369682612907885, accuracy: 0.96\n",
      "iteration no 2248: Loss: 0.263678338189525, accuracy: 0.96\n",
      "iteration no 2249: Loss: 0.26365923917668954, accuracy: 0.96\n",
      "iteration no 2250: Loss: 0.2636439285095287, accuracy: 0.96\n",
      "iteration no 2251: Loss: 0.2636239027443999, accuracy: 0.96\n",
      "iteration no 2252: Loss: 0.2636083572599891, accuracy: 0.96\n",
      "iteration no 2253: Loss: 0.2635885388758099, accuracy: 0.96\n",
      "iteration no 2254: Loss: 0.2635725744084386, accuracy: 0.96\n",
      "iteration no 2255: Loss: 0.2635548948493466, accuracy: 0.96\n",
      "iteration no 2256: Loss: 0.26353777800611233, accuracy: 0.96\n",
      "iteration no 2257: Loss: 0.26351798084859046, accuracy: 0.96\n",
      "iteration no 2258: Loss: 0.26350171479339324, accuracy: 0.96\n",
      "iteration no 2259: Loss: 0.2634828083153612, accuracy: 0.96\n",
      "iteration no 2260: Loss: 0.26346658133908446, accuracy: 0.96\n",
      "iteration no 2261: Loss: 0.2634482220942754, accuracy: 0.96\n",
      "iteration no 2262: Loss: 0.26342993720645347, accuracy: 0.96\n",
      "iteration no 2263: Loss: 0.2634148887408456, accuracy: 0.96\n",
      "iteration no 2264: Loss: 0.26339474381539213, accuracy: 0.96\n",
      "iteration no 2265: Loss: 0.26338140749067074, accuracy: 0.96\n",
      "iteration no 2266: Loss: 0.26336080856713195, accuracy: 0.96\n",
      "iteration no 2267: Loss: 0.26334567837135325, accuracy: 0.96\n",
      "iteration no 2268: Loss: 0.2633252506120277, accuracy: 0.96\n",
      "iteration no 2269: Loss: 0.26330670966826064, accuracy: 0.96\n",
      "iteration no 2270: Loss: 0.2632903331501386, accuracy: 0.96\n",
      "iteration no 2271: Loss: 0.26327129065905186, accuracy: 0.96\n",
      "iteration no 2272: Loss: 0.2632539172570912, accuracy: 0.96\n",
      "iteration no 2273: Loss: 0.2632361702402637, accuracy: 0.96\n",
      "iteration no 2274: Loss: 0.2632189024327472, accuracy: 0.96\n",
      "iteration no 2275: Loss: 0.26320085178561126, accuracy: 0.96\n",
      "iteration no 2276: Loss: 0.26318404106334675, accuracy: 0.96\n",
      "iteration no 2277: Loss: 0.2631658995111624, accuracy: 0.96\n",
      "iteration no 2278: Loss: 0.2631507210295209, accuracy: 0.96\n",
      "iteration no 2279: Loss: 0.2631309545285325, accuracy: 0.96\n",
      "iteration no 2280: Loss: 0.2631155156659977, accuracy: 0.96\n",
      "iteration no 2281: Loss: 0.2630959833068708, accuracy: 0.96\n",
      "iteration no 2282: Loss: 0.2630781594930732, accuracy: 0.96\n",
      "iteration no 2283: Loss: 0.26306079854083386, accuracy: 0.96\n",
      "iteration no 2284: Loss: 0.2630419908654149, accuracy: 0.96\n",
      "iteration no 2285: Loss: 0.2630265914278954, accuracy: 0.96\n",
      "iteration no 2286: Loss: 0.2630051804211068, accuracy: 0.96\n",
      "iteration no 2287: Loss: 0.26299194586836994, accuracy: 0.96\n",
      "iteration no 2288: Loss: 0.26297107397756275, accuracy: 0.96\n",
      "iteration no 2289: Loss: 0.26295624282311497, accuracy: 0.96\n",
      "iteration no 2290: Loss: 0.2629355151862539, accuracy: 0.96\n",
      "iteration no 2291: Loss: 0.2629209177541849, accuracy: 0.96\n",
      "iteration no 2292: Loss: 0.2629014160521144, accuracy: 0.96\n",
      "iteration no 2293: Loss: 0.26288601356848457, accuracy: 0.96\n",
      "iteration no 2294: Loss: 0.2628663557120178, accuracy: 0.96\n",
      "iteration no 2295: Loss: 0.26285119165870274, accuracy: 0.96\n",
      "iteration no 2296: Loss: 0.2628318215214466, accuracy: 0.96\n",
      "iteration no 2297: Loss: 0.26281563038140515, accuracy: 0.96\n",
      "iteration no 2298: Loss: 0.2627976407120408, accuracy: 0.96\n",
      "iteration no 2299: Loss: 0.26277993650152065, accuracy: 0.96\n",
      "iteration no 2300: Loss: 0.2627643023378302, accuracy: 0.96\n",
      "iteration no 2301: Loss: 0.26274616579119864, accuracy: 0.9633333333333334\n",
      "iteration no 2302: Loss: 0.26273227893858664, accuracy: 0.96\n",
      "iteration no 2303: Loss: 0.26271209016812336, accuracy: 0.9633333333333334\n",
      "iteration no 2304: Loss: 0.26269935470941586, accuracy: 0.96\n",
      "iteration no 2305: Loss: 0.26267961887169605, accuracy: 0.9633333333333334\n",
      "iteration no 2306: Loss: 0.2626641057069417, accuracy: 0.96\n",
      "iteration no 2307: Loss: 0.2626437579757142, accuracy: 0.9633333333333334\n",
      "iteration no 2308: Loss: 0.2626275268904459, accuracy: 0.96\n",
      "iteration no 2309: Loss: 0.26261037220777056, accuracy: 0.9633333333333334\n",
      "iteration no 2310: Loss: 0.26259275354527967, accuracy: 0.9633333333333334\n",
      "iteration no 2311: Loss: 0.2625758277512724, accuracy: 0.9633333333333334\n",
      "iteration no 2312: Loss: 0.26255926895866427, accuracy: 0.9633333333333334\n",
      "iteration no 2313: Loss: 0.262541469874978, accuracy: 0.9633333333333334\n",
      "iteration no 2314: Loss: 0.2625254731004599, accuracy: 0.9633333333333334\n",
      "iteration no 2315: Loss: 0.26250713762160055, accuracy: 0.9633333333333334\n",
      "iteration no 2316: Loss: 0.2624919618061823, accuracy: 0.9633333333333334\n",
      "iteration no 2317: Loss: 0.262474639328153, accuracy: 0.9633333333333334\n",
      "iteration no 2318: Loss: 0.2624572969719301, accuracy: 0.9633333333333334\n",
      "iteration no 2319: Loss: 0.26244267781482694, accuracy: 0.96\n",
      "iteration no 2320: Loss: 0.2624252269304278, accuracy: 0.9633333333333334\n",
      "iteration no 2321: Loss: 0.2624109266201488, accuracy: 0.9633333333333334\n",
      "iteration no 2322: Loss: 0.26239103744791115, accuracy: 0.9633333333333334\n",
      "iteration no 2323: Loss: 0.2623778962035982, accuracy: 0.96\n",
      "iteration no 2324: Loss: 0.26235708045743905, accuracy: 0.9633333333333334\n",
      "iteration no 2325: Loss: 0.2623423379853467, accuracy: 0.9633333333333334\n",
      "iteration no 2326: Loss: 0.2623222011085105, accuracy: 0.9633333333333334\n",
      "iteration no 2327: Loss: 0.26230563427240416, accuracy: 0.9633333333333334\n",
      "iteration no 2328: Loss: 0.26228836975651093, accuracy: 0.9633333333333334\n",
      "iteration no 2329: Loss: 0.2622696902022716, accuracy: 0.9633333333333334\n",
      "iteration no 2330: Loss: 0.2622550211325342, accuracy: 0.9633333333333334\n",
      "iteration no 2331: Loss: 0.2622357973122865, accuracy: 0.9633333333333334\n",
      "iteration no 2332: Loss: 0.26222086120964894, accuracy: 0.9633333333333334\n",
      "iteration no 2333: Loss: 0.2622021233890667, accuracy: 0.9633333333333334\n",
      "iteration no 2334: Loss: 0.2621868039852398, accuracy: 0.9633333333333334\n",
      "iteration no 2335: Loss: 0.2621682752137871, accuracy: 0.9633333333333334\n",
      "iteration no 2336: Loss: 0.2621532346006962, accuracy: 0.9633333333333334\n",
      "iteration no 2337: Loss: 0.26213466938606367, accuracy: 0.9633333333333334\n",
      "iteration no 2338: Loss: 0.2621195896465793, accuracy: 0.9633333333333334\n",
      "iteration no 2339: Loss: 0.26210082303727905, accuracy: 0.9633333333333334\n",
      "iteration no 2340: Loss: 0.2620871043772767, accuracy: 0.9633333333333334\n",
      "iteration no 2341: Loss: 0.262066368751006, accuracy: 0.9633333333333334\n",
      "iteration no 2342: Loss: 0.2620542951447317, accuracy: 0.9633333333333334\n",
      "iteration no 2343: Loss: 0.2620340074298222, accuracy: 0.9633333333333334\n",
      "iteration no 2344: Loss: 0.26202155521575377, accuracy: 0.9633333333333334\n",
      "iteration no 2345: Loss: 0.261999087187902, accuracy: 0.9633333333333334\n",
      "iteration no 2346: Loss: 0.2619878872297513, accuracy: 0.9633333333333334\n",
      "iteration no 2347: Loss: 0.26196712964473945, accuracy: 0.9633333333333334\n",
      "iteration no 2348: Loss: 0.2619525000007067, accuracy: 0.9633333333333334\n",
      "iteration no 2349: Loss: 0.26193065772720064, accuracy: 0.9633333333333334\n",
      "iteration no 2350: Loss: 0.2619166748815705, accuracy: 0.9633333333333334\n",
      "iteration no 2351: Loss: 0.2618969809139774, accuracy: 0.9633333333333334\n",
      "iteration no 2352: Loss: 0.26188014561756706, accuracy: 0.9633333333333334\n",
      "iteration no 2353: Loss: 0.2618622438805386, accuracy: 0.9633333333333334\n",
      "iteration no 2354: Loss: 0.2618448799762582, accuracy: 0.9633333333333334\n",
      "iteration no 2355: Loss: 0.2618285486666452, accuracy: 0.9633333333333334\n",
      "iteration no 2356: Loss: 0.26181066764242744, accuracy: 0.9633333333333334\n",
      "iteration no 2357: Loss: 0.26179434149588293, accuracy: 0.9633333333333334\n",
      "iteration no 2358: Loss: 0.2617765181659276, accuracy: 0.9633333333333334\n",
      "iteration no 2359: Loss: 0.2617600953147727, accuracy: 0.9633333333333334\n",
      "iteration no 2360: Loss: 0.2617424896630654, accuracy: 0.9633333333333334\n",
      "iteration no 2361: Loss: 0.2617257089434672, accuracy: 0.9633333333333334\n",
      "iteration no 2362: Loss: 0.2617090829305864, accuracy: 0.9633333333333334\n",
      "iteration no 2363: Loss: 0.26169127444058937, accuracy: 0.9633333333333334\n",
      "iteration no 2364: Loss: 0.26167598516242463, accuracy: 0.9633333333333334\n",
      "iteration no 2365: Loss: 0.2616584880753695, accuracy: 0.9633333333333334\n",
      "iteration no 2366: Loss: 0.2616440847072745, accuracy: 0.9633333333333334\n",
      "iteration no 2367: Loss: 0.26162629150776434, accuracy: 0.9633333333333334\n",
      "iteration no 2368: Loss: 0.26161050930697954, accuracy: 0.9633333333333334\n",
      "iteration no 2369: Loss: 0.26159323982138133, accuracy: 0.9633333333333334\n",
      "iteration no 2370: Loss: 0.2615763455380622, accuracy: 0.9633333333333334\n",
      "iteration no 2371: Loss: 0.2615595066582548, accuracy: 0.9633333333333334\n",
      "iteration no 2372: Loss: 0.2615411547042822, accuracy: 0.9633333333333334\n",
      "iteration no 2373: Loss: 0.2615254066979032, accuracy: 0.9633333333333334\n",
      "iteration no 2374: Loss: 0.26150748898620496, accuracy: 0.9633333333333334\n",
      "iteration no 2375: Loss: 0.2614914709699344, accuracy: 0.9633333333333334\n",
      "iteration no 2376: Loss: 0.2614733729614832, accuracy: 0.9633333333333334\n",
      "iteration no 2377: Loss: 0.2614571041554746, accuracy: 0.9633333333333334\n",
      "iteration no 2378: Loss: 0.2614403548090266, accuracy: 0.9633333333333334\n",
      "iteration no 2379: Loss: 0.26142155424798, accuracy: 0.9633333333333334\n",
      "iteration no 2380: Loss: 0.2614056105606776, accuracy: 0.9633333333333334\n",
      "iteration no 2381: Loss: 0.26138626842833723, accuracy: 0.9633333333333334\n",
      "iteration no 2382: Loss: 0.26137145684703456, accuracy: 0.9633333333333334\n",
      "iteration no 2383: Loss: 0.26135233502314825, accuracy: 0.9633333333333334\n",
      "iteration no 2384: Loss: 0.2613374634144353, accuracy: 0.9633333333333334\n",
      "iteration no 2385: Loss: 0.2613181154457129, accuracy: 0.9633333333333334\n",
      "iteration no 2386: Loss: 0.2613045488752751, accuracy: 0.9633333333333334\n",
      "iteration no 2387: Loss: 0.26128453438970767, accuracy: 0.9633333333333334\n",
      "iteration no 2388: Loss: 0.2612698293703777, accuracy: 0.9633333333333334\n",
      "iteration no 2389: Loss: 0.2612502230163912, accuracy: 0.9633333333333334\n",
      "iteration no 2390: Loss: 0.2612376718035858, accuracy: 0.9633333333333334\n",
      "iteration no 2391: Loss: 0.26121835258258674, accuracy: 0.9633333333333334\n",
      "iteration no 2392: Loss: 0.26120487281496163, accuracy: 0.9633333333333334\n",
      "iteration no 2393: Loss: 0.26118511100525355, accuracy: 0.9633333333333334\n",
      "iteration no 2394: Loss: 0.26117219287772236, accuracy: 0.9633333333333334\n",
      "iteration no 2395: Loss: 0.2611521614556285, accuracy: 0.9633333333333334\n",
      "iteration no 2396: Loss: 0.26113807895333574, accuracy: 0.9633333333333334\n",
      "iteration no 2397: Loss: 0.26111772107912845, accuracy: 0.9633333333333334\n",
      "iteration no 2398: Loss: 0.2611041258231329, accuracy: 0.9633333333333334\n",
      "iteration no 2399: Loss: 0.2610841249851633, accuracy: 0.9633333333333334\n",
      "iteration no 2400: Loss: 0.2610689784029864, accuracy: 0.9633333333333334\n",
      "iteration no 2401: Loss: 0.26104952383147173, accuracy: 0.9633333333333334\n",
      "iteration no 2402: Loss: 0.26103500589896167, accuracy: 0.9633333333333334\n",
      "iteration no 2403: Loss: 0.261016724769723, accuracy: 0.9633333333333334\n",
      "iteration no 2404: Loss: 0.26099996793334757, accuracy: 0.9633333333333334\n",
      "iteration no 2405: Loss: 0.26098216158489246, accuracy: 0.9633333333333334\n",
      "iteration no 2406: Loss: 0.2609670152395648, accuracy: 0.9633333333333334\n",
      "iteration no 2407: Loss: 0.260947900592302, accuracy: 0.9633333333333334\n",
      "iteration no 2408: Loss: 0.2609319118448464, accuracy: 0.9633333333333334\n",
      "iteration no 2409: Loss: 0.26091415923859895, accuracy: 0.9633333333333334\n",
      "iteration no 2410: Loss: 0.2608991864977861, accuracy: 0.9633333333333334\n",
      "iteration no 2411: Loss: 0.2608797136586879, accuracy: 0.9633333333333334\n",
      "iteration no 2412: Loss: 0.2608632237011928, accuracy: 0.9633333333333334\n",
      "iteration no 2413: Loss: 0.26084574249367914, accuracy: 0.9633333333333334\n",
      "iteration no 2414: Loss: 0.26082713205130853, accuracy: 0.9633333333333334\n",
      "iteration no 2415: Loss: 0.2608091247577459, accuracy: 0.9633333333333334\n",
      "iteration no 2416: Loss: 0.26079237969928276, accuracy: 0.9633333333333334\n",
      "iteration no 2417: Loss: 0.2607768718459386, accuracy: 0.9633333333333334\n",
      "iteration no 2418: Loss: 0.26075992174347107, accuracy: 0.9633333333333334\n",
      "iteration no 2419: Loss: 0.26074485501739414, accuracy: 0.9633333333333334\n",
      "iteration no 2420: Loss: 0.2607279997426531, accuracy: 0.9633333333333334\n",
      "iteration no 2421: Loss: 0.260715305307089, accuracy: 0.9633333333333334\n",
      "iteration no 2422: Loss: 0.26069732113415306, accuracy: 0.9633333333333334\n",
      "iteration no 2423: Loss: 0.2606848837588406, accuracy: 0.9633333333333334\n",
      "iteration no 2424: Loss: 0.26066534603023356, accuracy: 0.9633333333333334\n",
      "iteration no 2425: Loss: 0.26065152336403774, accuracy: 0.9633333333333334\n",
      "iteration no 2426: Loss: 0.26063060290788737, accuracy: 0.9633333333333334\n",
      "iteration no 2427: Loss: 0.2606176752786267, accuracy: 0.9633333333333334\n",
      "iteration no 2428: Loss: 0.2605980542705047, accuracy: 0.9633333333333334\n",
      "iteration no 2429: Loss: 0.26058329607315955, accuracy: 0.9633333333333334\n",
      "iteration no 2430: Loss: 0.26056614811317075, accuracy: 0.9633333333333334\n",
      "iteration no 2431: Loss: 0.26055047081391, accuracy: 0.9633333333333334\n",
      "iteration no 2432: Loss: 0.2605346762014821, accuracy: 0.9633333333333334\n",
      "iteration no 2433: Loss: 0.26051698650903804, accuracy: 0.9633333333333334\n",
      "iteration no 2434: Loss: 0.2605036533982702, accuracy: 0.9633333333333334\n",
      "iteration no 2435: Loss: 0.2604841474928091, accuracy: 0.9633333333333334\n",
      "iteration no 2436: Loss: 0.26047203152370857, accuracy: 0.9633333333333334\n",
      "iteration no 2437: Loss: 0.2604518824781323, accuracy: 0.9633333333333334\n",
      "iteration no 2438: Loss: 0.2604390657345839, accuracy: 0.9633333333333334\n",
      "iteration no 2439: Loss: 0.2604199497403632, accuracy: 0.9633333333333334\n",
      "iteration no 2440: Loss: 0.2604059898366549, accuracy: 0.9633333333333334\n",
      "iteration no 2441: Loss: 0.2603902951738695, accuracy: 0.9633333333333334\n",
      "iteration no 2442: Loss: 0.2603732252376959, accuracy: 0.9633333333333334\n",
      "iteration no 2443: Loss: 0.26035842325551006, accuracy: 0.9633333333333334\n",
      "iteration no 2444: Loss: 0.26033998084756754, accuracy: 0.9633333333333334\n",
      "iteration no 2445: Loss: 0.26032766362740994, accuracy: 0.9633333333333334\n",
      "iteration no 2446: Loss: 0.2603081777523398, accuracy: 0.9633333333333334\n",
      "iteration no 2447: Loss: 0.26029608065638926, accuracy: 0.9633333333333334\n",
      "iteration no 2448: Loss: 0.26027871942344877, accuracy: 0.9633333333333334\n",
      "iteration no 2449: Loss: 0.26026483999776395, accuracy: 0.9633333333333334\n",
      "iteration no 2450: Loss: 0.2602480509830525, accuracy: 0.9633333333333334\n",
      "iteration no 2451: Loss: 0.2602341508654993, accuracy: 0.9633333333333334\n",
      "iteration no 2452: Loss: 0.2602167682272589, accuracy: 0.9633333333333334\n",
      "iteration no 2453: Loss: 0.26020167418520357, accuracy: 0.9633333333333334\n",
      "iteration no 2454: Loss: 0.26018416679967205, accuracy: 0.9633333333333334\n",
      "iteration no 2455: Loss: 0.2601694198081136, accuracy: 0.9633333333333334\n",
      "iteration no 2456: Loss: 0.2601520479061456, accuracy: 0.9633333333333334\n",
      "iteration no 2457: Loss: 0.2601363426451138, accuracy: 0.9633333333333334\n",
      "iteration no 2458: Loss: 0.26011768926798967, accuracy: 0.9633333333333334\n",
      "iteration no 2459: Loss: 0.26010387092598053, accuracy: 0.9633333333333334\n",
      "iteration no 2460: Loss: 0.26008477693877624, accuracy: 0.9633333333333334\n",
      "iteration no 2461: Loss: 0.26007297263580675, accuracy: 0.9633333333333334\n",
      "iteration no 2462: Loss: 0.2600529946418869, accuracy: 0.9633333333333334\n",
      "iteration no 2463: Loss: 0.2600405943652369, accuracy: 0.9633333333333334\n",
      "iteration no 2464: Loss: 0.2600221500288848, accuracy: 0.9633333333333334\n",
      "iteration no 2465: Loss: 0.2600066566736051, accuracy: 0.9633333333333334\n",
      "iteration no 2466: Loss: 0.2599910459674749, accuracy: 0.9633333333333334\n",
      "iteration no 2467: Loss: 0.25997315383091407, accuracy: 0.9633333333333334\n",
      "iteration no 2468: Loss: 0.2599607708637727, accuracy: 0.9633333333333334\n",
      "iteration no 2469: Loss: 0.2599418496760247, accuracy: 0.9633333333333334\n",
      "iteration no 2470: Loss: 0.25992886173171115, accuracy: 0.9633333333333334\n",
      "iteration no 2471: Loss: 0.25991145694408235, accuracy: 0.9633333333333334\n",
      "iteration no 2472: Loss: 0.259896918989414, accuracy: 0.9633333333333334\n",
      "iteration no 2473: Loss: 0.2598805837106894, accuracy: 0.9633333333333334\n",
      "iteration no 2474: Loss: 0.2598657721177193, accuracy: 0.9633333333333334\n",
      "iteration no 2475: Loss: 0.2598483852286426, accuracy: 0.9633333333333334\n",
      "iteration no 2476: Loss: 0.2598351225176395, accuracy: 0.9633333333333334\n",
      "iteration no 2477: Loss: 0.2598174908497063, accuracy: 0.9633333333333334\n",
      "iteration no 2478: Loss: 0.25980440440121266, accuracy: 0.9633333333333334\n",
      "iteration no 2479: Loss: 0.25978800855468803, accuracy: 0.9633333333333334\n",
      "iteration no 2480: Loss: 0.2597732061923116, accuracy: 0.9633333333333334\n",
      "iteration no 2481: Loss: 0.259758500029203, accuracy: 0.9633333333333334\n",
      "iteration no 2482: Loss: 0.2597412155267364, accuracy: 0.9633333333333334\n",
      "iteration no 2483: Loss: 0.25972876480106316, accuracy: 0.9633333333333334\n",
      "iteration no 2484: Loss: 0.25970991740774696, accuracy: 0.9633333333333334\n",
      "iteration no 2485: Loss: 0.2596979742470175, accuracy: 0.9633333333333334\n",
      "iteration no 2486: Loss: 0.25968079791503346, accuracy: 0.9633333333333334\n",
      "iteration no 2487: Loss: 0.2596667569935494, accuracy: 0.9633333333333334\n",
      "iteration no 2488: Loss: 0.2596505360904863, accuracy: 0.9633333333333334\n",
      "iteration no 2489: Loss: 0.2596368677283463, accuracy: 0.9633333333333334\n",
      "iteration no 2490: Loss: 0.2596192375836218, accuracy: 0.9633333333333334\n",
      "iteration no 2491: Loss: 0.25960443782469333, accuracy: 0.9633333333333334\n",
      "iteration no 2492: Loss: 0.25958535632710966, accuracy: 0.9633333333333334\n",
      "iteration no 2493: Loss: 0.25957161979136695, accuracy: 0.9633333333333334\n",
      "iteration no 2494: Loss: 0.2595530400696725, accuracy: 0.9633333333333334\n",
      "iteration no 2495: Loss: 0.2595385871658966, accuracy: 0.9633333333333334\n",
      "iteration no 2496: Loss: 0.2595218119659756, accuracy: 0.9633333333333334\n",
      "iteration no 2497: Loss: 0.2595073000036561, accuracy: 0.9633333333333334\n",
      "iteration no 2498: Loss: 0.2594930810182171, accuracy: 0.9633333333333334\n",
      "iteration no 2499: Loss: 0.25947593217544646, accuracy: 0.9633333333333334\n",
      "iteration no 2500: Loss: 0.25946169017625226, accuracy: 0.9633333333333334\n",
      "iteration no 2501: Loss: 0.2594446560484889, accuracy: 0.9633333333333334\n",
      "iteration no 2502: Loss: 0.2594331806736263, accuracy: 0.9633333333333334\n",
      "iteration no 2503: Loss: 0.25941331280948704, accuracy: 0.9633333333333334\n",
      "iteration no 2504: Loss: 0.259401604431808, accuracy: 0.9633333333333334\n",
      "iteration no 2505: Loss: 0.25938320579909735, accuracy: 0.9633333333333334\n",
      "iteration no 2506: Loss: 0.2593699744124066, accuracy: 0.9633333333333334\n",
      "iteration no 2507: Loss: 0.2593537512370125, accuracy: 0.9633333333333334\n",
      "iteration no 2508: Loss: 0.25933706150612457, accuracy: 0.9633333333333334\n",
      "iteration no 2509: Loss: 0.259324115169456, accuracy: 0.9633333333333334\n",
      "iteration no 2510: Loss: 0.2593066577390323, accuracy: 0.9633333333333334\n",
      "iteration no 2511: Loss: 0.2592942244245487, accuracy: 0.9633333333333334\n",
      "iteration no 2512: Loss: 0.25927520984281605, accuracy: 0.9633333333333334\n",
      "iteration no 2513: Loss: 0.25926357788762794, accuracy: 0.9633333333333334\n",
      "iteration no 2514: Loss: 0.25924532997924615, accuracy: 0.9633333333333334\n",
      "iteration no 2515: Loss: 0.25923274632073257, accuracy: 0.9633333333333334\n",
      "iteration no 2516: Loss: 0.259216502455575, accuracy: 0.9633333333333334\n",
      "iteration no 2517: Loss: 0.2592012809930452, accuracy: 0.9633333333333334\n",
      "iteration no 2518: Loss: 0.259187292173789, accuracy: 0.9633333333333334\n",
      "iteration no 2519: Loss: 0.25917072115444817, accuracy: 0.9633333333333334\n",
      "iteration no 2520: Loss: 0.2591559722886155, accuracy: 0.9633333333333334\n",
      "iteration no 2521: Loss: 0.2591407524118718, accuracy: 0.9633333333333334\n",
      "iteration no 2522: Loss: 0.2591266613065965, accuracy: 0.9633333333333334\n",
      "iteration no 2523: Loss: 0.259110989479108, accuracy: 0.9633333333333334\n",
      "iteration no 2524: Loss: 0.2590964182399373, accuracy: 0.9633333333333334\n",
      "iteration no 2525: Loss: 0.25908201213758897, accuracy: 0.9633333333333334\n",
      "iteration no 2526: Loss: 0.25906774598339993, accuracy: 0.9633333333333334\n",
      "iteration no 2527: Loss: 0.25905126675689677, accuracy: 0.9633333333333334\n",
      "iteration no 2528: Loss: 0.2590385390815677, accuracy: 0.9633333333333334\n",
      "iteration no 2529: Loss: 0.2590199163397479, accuracy: 0.9633333333333334\n",
      "iteration no 2530: Loss: 0.259007528721619, accuracy: 0.9633333333333334\n",
      "iteration no 2531: Loss: 0.2589893795420363, accuracy: 0.9633333333333334\n",
      "iteration no 2532: Loss: 0.2589767862911523, accuracy: 0.9633333333333334\n",
      "iteration no 2533: Loss: 0.25896030012741345, accuracy: 0.9633333333333334\n",
      "iteration no 2534: Loss: 0.25894617914130635, accuracy: 0.9633333333333334\n",
      "iteration no 2535: Loss: 0.2589303928933273, accuracy: 0.9633333333333334\n",
      "iteration no 2536: Loss: 0.2589159661413158, accuracy: 0.9633333333333334\n",
      "iteration no 2537: Loss: 0.2589021526525139, accuracy: 0.9633333333333334\n",
      "iteration no 2538: Loss: 0.2588840382233108, accuracy: 0.9633333333333334\n",
      "iteration no 2539: Loss: 0.2588730270856236, accuracy: 0.9633333333333334\n",
      "iteration no 2540: Loss: 0.25885472961694606, accuracy: 0.9633333333333334\n",
      "iteration no 2541: Loss: 0.2588433494290412, accuracy: 0.9633333333333334\n",
      "iteration no 2542: Loss: 0.2588253084461358, accuracy: 0.9633333333333334\n",
      "iteration no 2543: Loss: 0.2588142790047839, accuracy: 0.9633333333333334\n",
      "iteration no 2544: Loss: 0.2587969469090753, accuracy: 0.9633333333333334\n",
      "iteration no 2545: Loss: 0.2587851575821881, accuracy: 0.9633333333333334\n",
      "iteration no 2546: Loss: 0.2587690114381732, accuracy: 0.9633333333333334\n",
      "iteration no 2547: Loss: 0.2587543444455932, accuracy: 0.9633333333333334\n",
      "iteration no 2548: Loss: 0.25874149456647483, accuracy: 0.9633333333333334\n",
      "iteration no 2549: Loss: 0.2587242956852984, accuracy: 0.9633333333333334\n",
      "iteration no 2550: Loss: 0.25871182350115784, accuracy: 0.9633333333333334\n",
      "iteration no 2551: Loss: 0.25869461869206367, accuracy: 0.9633333333333334\n",
      "iteration no 2552: Loss: 0.2586825087841384, accuracy: 0.9633333333333334\n",
      "iteration no 2553: Loss: 0.258665668633893, accuracy: 0.9633333333333334\n",
      "iteration no 2554: Loss: 0.2586535708830745, accuracy: 0.9633333333333334\n",
      "iteration no 2555: Loss: 0.25863833995467017, accuracy: 0.9633333333333334\n",
      "iteration no 2556: Loss: 0.2586251613837152, accuracy: 0.9633333333333334\n",
      "iteration no 2557: Loss: 0.2586104758065109, accuracy: 0.9633333333333334\n",
      "iteration no 2558: Loss: 0.2585980770229693, accuracy: 0.9633333333333334\n",
      "iteration no 2559: Loss: 0.2585833093920964, accuracy: 0.9633333333333334\n",
      "iteration no 2560: Loss: 0.25856760565847225, accuracy: 0.9633333333333334\n",
      "iteration no 2561: Loss: 0.25855270506365735, accuracy: 0.9633333333333334\n",
      "iteration no 2562: Loss: 0.25853781546627735, accuracy: 0.9633333333333334\n",
      "iteration no 2563: Loss: 0.25852437767449304, accuracy: 0.9633333333333334\n",
      "iteration no 2564: Loss: 0.2585098794439199, accuracy: 0.9633333333333334\n",
      "iteration no 2565: Loss: 0.2584958176748769, accuracy: 0.9633333333333334\n",
      "iteration no 2566: Loss: 0.258481631988024, accuracy: 0.9633333333333334\n",
      "iteration no 2567: Loss: 0.2584692864501137, accuracy: 0.9633333333333334\n",
      "iteration no 2568: Loss: 0.25845405270165533, accuracy: 0.9633333333333334\n",
      "iteration no 2569: Loss: 0.2584406284985656, accuracy: 0.9633333333333334\n",
      "iteration no 2570: Loss: 0.25842594385733564, accuracy: 0.9633333333333334\n",
      "iteration no 2571: Loss: 0.2584142094103848, accuracy: 0.9633333333333334\n",
      "iteration no 2572: Loss: 0.25839654893478825, accuracy: 0.9633333333333334\n",
      "iteration no 2573: Loss: 0.2583868002924918, accuracy: 0.9633333333333334\n",
      "iteration no 2574: Loss: 0.2583688844896954, accuracy: 0.9633333333333334\n",
      "iteration no 2575: Loss: 0.2583579397784469, accuracy: 0.9633333333333334\n",
      "iteration no 2576: Loss: 0.25834097911398524, accuracy: 0.9633333333333334\n",
      "iteration no 2577: Loss: 0.2583290253770036, accuracy: 0.9633333333333334\n",
      "iteration no 2578: Loss: 0.2583136162036793, accuracy: 0.9633333333333334\n",
      "iteration no 2579: Loss: 0.2583000927235492, accuracy: 0.9633333333333334\n",
      "iteration no 2580: Loss: 0.25828813463057027, accuracy: 0.9633333333333334\n",
      "iteration no 2581: Loss: 0.25827228813186254, accuracy: 0.9633333333333334\n",
      "iteration no 2582: Loss: 0.2582598781221936, accuracy: 0.9633333333333334\n",
      "iteration no 2583: Loss: 0.2582447534967989, accuracy: 0.9633333333333334\n",
      "iteration no 2584: Loss: 0.25823407226255235, accuracy: 0.9633333333333334\n",
      "iteration no 2585: Loss: 0.25821681347322273, accuracy: 0.9633333333333334\n",
      "iteration no 2586: Loss: 0.2582049552048166, accuracy: 0.9633333333333334\n",
      "iteration no 2587: Loss: 0.2581898644074605, accuracy: 0.9633333333333334\n",
      "iteration no 2588: Loss: 0.2581768009775292, accuracy: 0.9633333333333334\n",
      "iteration no 2589: Loss: 0.25816254005097133, accuracy: 0.9633333333333334\n",
      "iteration no 2590: Loss: 0.2581492236053833, accuracy: 0.9633333333333334\n",
      "iteration no 2591: Loss: 0.2581361977809492, accuracy: 0.9633333333333334\n",
      "iteration no 2592: Loss: 0.25812306636159343, accuracy: 0.9633333333333334\n",
      "iteration no 2593: Loss: 0.25811137625884784, accuracy: 0.9633333333333334\n",
      "iteration no 2594: Loss: 0.25809478353186843, accuracy: 0.9633333333333334\n",
      "iteration no 2595: Loss: 0.25808266680977177, accuracy: 0.9633333333333334\n",
      "iteration no 2596: Loss: 0.25806806384121433, accuracy: 0.9633333333333334\n",
      "iteration no 2597: Loss: 0.25805491319768115, accuracy: 0.9633333333333334\n",
      "iteration no 2598: Loss: 0.2580392491941535, accuracy: 0.9633333333333334\n",
      "iteration no 2599: Loss: 0.25802589231645295, accuracy: 0.9633333333333334\n",
      "iteration no 2600: Loss: 0.25801354682655375, accuracy: 0.9633333333333334\n",
      "iteration no 2601: Loss: 0.257998990040619, accuracy: 0.9633333333333334\n",
      "iteration no 2602: Loss: 0.2579853322042238, accuracy: 0.9633333333333334\n",
      "iteration no 2603: Loss: 0.25797230995498255, accuracy: 0.9633333333333334\n",
      "iteration no 2604: Loss: 0.2579593237292886, accuracy: 0.9633333333333334\n",
      "iteration no 2605: Loss: 0.25794547944480434, accuracy: 0.9633333333333334\n",
      "iteration no 2606: Loss: 0.2579321043296235, accuracy: 0.9633333333333334\n",
      "iteration no 2607: Loss: 0.2579183492312584, accuracy: 0.9633333333333334\n",
      "iteration no 2608: Loss: 0.25790416244651043, accuracy: 0.9633333333333334\n",
      "iteration no 2609: Loss: 0.2578921721789399, accuracy: 0.9633333333333334\n",
      "iteration no 2610: Loss: 0.25787752312881174, accuracy: 0.9633333333333334\n",
      "iteration no 2611: Loss: 0.2578655435013866, accuracy: 0.9633333333333334\n",
      "iteration no 2612: Loss: 0.25785068819812396, accuracy: 0.9633333333333334\n",
      "iteration no 2613: Loss: 0.25784148189700884, accuracy: 0.9633333333333334\n",
      "iteration no 2614: Loss: 0.25782480258776413, accuracy: 0.9633333333333334\n",
      "iteration no 2615: Loss: 0.2578139893801369, accuracy: 0.9633333333333334\n",
      "iteration no 2616: Loss: 0.2577989181701475, accuracy: 0.9633333333333334\n",
      "iteration no 2617: Loss: 0.25778667675494354, accuracy: 0.9633333333333334\n",
      "iteration no 2618: Loss: 0.25777247374826495, accuracy: 0.9633333333333334\n",
      "iteration no 2619: Loss: 0.25775803094805927, accuracy: 0.9633333333333334\n",
      "iteration no 2620: Loss: 0.25774534670517485, accuracy: 0.9633333333333334\n",
      "iteration no 2621: Loss: 0.2577305241604222, accuracy: 0.9633333333333334\n",
      "iteration no 2622: Loss: 0.25771992938762733, accuracy: 0.9633333333333334\n",
      "iteration no 2623: Loss: 0.2577035467852404, accuracy: 0.9633333333333334\n",
      "iteration no 2624: Loss: 0.25769415263832635, accuracy: 0.9633333333333334\n",
      "iteration no 2625: Loss: 0.2576789694297883, accuracy: 0.9633333333333334\n",
      "iteration no 2626: Loss: 0.25766760946060724, accuracy: 0.9633333333333334\n",
      "iteration no 2627: Loss: 0.25765415129889446, accuracy: 0.9633333333333334\n",
      "iteration no 2628: Loss: 0.25764052545660476, accuracy: 0.9633333333333334\n",
      "iteration no 2629: Loss: 0.25762930294581204, accuracy: 0.9633333333333334\n",
      "iteration no 2630: Loss: 0.2576137470365112, accuracy: 0.9633333333333334\n",
      "iteration no 2631: Loss: 0.2576011764055723, accuracy: 0.9633333333333334\n",
      "iteration no 2632: Loss: 0.25758804425632414, accuracy: 0.9633333333333334\n",
      "iteration no 2633: Loss: 0.25757365389533116, accuracy: 0.9633333333333334\n",
      "iteration no 2634: Loss: 0.257560007002062, accuracy: 0.9633333333333334\n",
      "iteration no 2635: Loss: 0.2575454579342925, accuracy: 0.9633333333333334\n",
      "iteration no 2636: Loss: 0.25753428082674995, accuracy: 0.9633333333333334\n",
      "iteration no 2637: Loss: 0.2575172942039323, accuracy: 0.9633333333333334\n",
      "iteration no 2638: Loss: 0.25750684151878533, accuracy: 0.9633333333333334\n",
      "iteration no 2639: Loss: 0.2574894922915218, accuracy: 0.9633333333333334\n",
      "iteration no 2640: Loss: 0.2574775488046509, accuracy: 0.9633333333333334\n",
      "iteration no 2641: Loss: 0.25746320700880804, accuracy: 0.9633333333333334\n",
      "iteration no 2642: Loss: 0.2574500223470506, accuracy: 0.9633333333333334\n",
      "iteration no 2643: Loss: 0.2574353294222713, accuracy: 0.9633333333333334\n",
      "iteration no 2644: Loss: 0.2574216826110954, accuracy: 0.9633333333333334\n",
      "iteration no 2645: Loss: 0.25740912836679813, accuracy: 0.9633333333333334\n",
      "iteration no 2646: Loss: 0.2573933940571419, accuracy: 0.9633333333333334\n",
      "iteration no 2647: Loss: 0.2573818777258727, accuracy: 0.9633333333333334\n",
      "iteration no 2648: Loss: 0.25736980661813147, accuracy: 0.9633333333333334\n",
      "iteration no 2649: Loss: 0.2573546420617701, accuracy: 0.9633333333333334\n",
      "iteration no 2650: Loss: 0.25734241767677246, accuracy: 0.9633333333333334\n",
      "iteration no 2651: Loss: 0.25732669354416143, accuracy: 0.9633333333333334\n",
      "iteration no 2652: Loss: 0.2573171431729513, accuracy: 0.9633333333333334\n",
      "iteration no 2653: Loss: 0.25729980758989535, accuracy: 0.9633333333333334\n",
      "iteration no 2654: Loss: 0.2572879320484857, accuracy: 0.9633333333333334\n",
      "iteration no 2655: Loss: 0.25727294584701843, accuracy: 0.9633333333333334\n",
      "iteration no 2656: Loss: 0.25725837614596103, accuracy: 0.9633333333333334\n",
      "iteration no 2657: Loss: 0.25724635249768235, accuracy: 0.9633333333333334\n",
      "iteration no 2658: Loss: 0.2572334211775788, accuracy: 0.9633333333333334\n",
      "iteration no 2659: Loss: 0.2572203355926068, accuracy: 0.9633333333333334\n",
      "iteration no 2660: Loss: 0.257207447020461, accuracy: 0.9633333333333334\n",
      "iteration no 2661: Loss: 0.2571937415803426, accuracy: 0.9633333333333334\n",
      "iteration no 2662: Loss: 0.2571840099461361, accuracy: 0.9633333333333334\n",
      "iteration no 2663: Loss: 0.2571662617102337, accuracy: 0.9633333333333334\n",
      "iteration no 2664: Loss: 0.25715775132441515, accuracy: 0.9633333333333334\n",
      "iteration no 2665: Loss: 0.2571407053087333, accuracy: 0.9633333333333334\n",
      "iteration no 2666: Loss: 0.2571287744177736, accuracy: 0.9633333333333334\n",
      "iteration no 2667: Loss: 0.25711470087331234, accuracy: 0.9633333333333334\n",
      "iteration no 2668: Loss: 0.25710147075339773, accuracy: 0.9633333333333334\n",
      "iteration no 2669: Loss: 0.2570882735162115, accuracy: 0.9633333333333334\n",
      "iteration no 2670: Loss: 0.2570752838149789, accuracy: 0.9633333333333334\n",
      "iteration no 2671: Loss: 0.25706258548675054, accuracy: 0.9633333333333334\n",
      "iteration no 2672: Loss: 0.25704920131353304, accuracy: 0.9633333333333334\n",
      "iteration no 2673: Loss: 0.2570352781192185, accuracy: 0.9633333333333334\n",
      "iteration no 2674: Loss: 0.25702514499375057, accuracy: 0.9633333333333334\n",
      "iteration no 2675: Loss: 0.25700929038438897, accuracy: 0.9633333333333334\n",
      "iteration no 2676: Loss: 0.2569972166464528, accuracy: 0.9633333333333334\n",
      "iteration no 2677: Loss: 0.2569853207888856, accuracy: 0.9633333333333334\n",
      "iteration no 2678: Loss: 0.256970013897348, accuracy: 0.9633333333333334\n",
      "iteration no 2679: Loss: 0.25695949061532697, accuracy: 0.9633333333333334\n",
      "iteration no 2680: Loss: 0.2569444764488962, accuracy: 0.9633333333333334\n",
      "iteration no 2681: Loss: 0.25693307939944865, accuracy: 0.9633333333333334\n",
      "iteration no 2682: Loss: 0.25691921474123697, accuracy: 0.9633333333333334\n",
      "iteration no 2683: Loss: 0.25690732614203515, accuracy: 0.9633333333333334\n",
      "iteration no 2684: Loss: 0.2568938870748926, accuracy: 0.9633333333333334\n",
      "iteration no 2685: Loss: 0.25688104250318494, accuracy: 0.9633333333333334\n",
      "iteration no 2686: Loss: 0.2568692570873566, accuracy: 0.9633333333333334\n",
      "iteration no 2687: Loss: 0.25685512218387674, accuracy: 0.9633333333333334\n",
      "iteration no 2688: Loss: 0.25684124679991394, accuracy: 0.9633333333333334\n",
      "iteration no 2689: Loss: 0.2568309095536958, accuracy: 0.9633333333333334\n",
      "iteration no 2690: Loss: 0.2568166829775806, accuracy: 0.9633333333333334\n",
      "iteration no 2691: Loss: 0.25680432925346747, accuracy: 0.9633333333333334\n",
      "iteration no 2692: Loss: 0.25679304104061507, accuracy: 0.9633333333333334\n",
      "iteration no 2693: Loss: 0.2567777020706076, accuracy: 0.9633333333333334\n",
      "iteration no 2694: Loss: 0.2567684457505759, accuracy: 0.9633333333333334\n",
      "iteration no 2695: Loss: 0.2567525408906414, accuracy: 0.9633333333333334\n",
      "iteration no 2696: Loss: 0.2567406671024333, accuracy: 0.9633333333333334\n",
      "iteration no 2697: Loss: 0.25672832022934444, accuracy: 0.9633333333333334\n",
      "iteration no 2698: Loss: 0.25671371801397985, accuracy: 0.9633333333333334\n",
      "iteration no 2699: Loss: 0.2567036294198426, accuracy: 0.9633333333333334\n",
      "iteration no 2700: Loss: 0.25668973160107306, accuracy: 0.9633333333333334\n",
      "iteration no 2701: Loss: 0.2566769310688818, accuracy: 0.9633333333333334\n",
      "iteration no 2702: Loss: 0.2566657797081032, accuracy: 0.9633333333333334\n",
      "iteration no 2703: Loss: 0.2566525912532401, accuracy: 0.9633333333333334\n",
      "iteration no 2704: Loss: 0.25663935662489773, accuracy: 0.9633333333333334\n",
      "iteration no 2705: Loss: 0.25662889681730006, accuracy: 0.9633333333333334\n",
      "iteration no 2706: Loss: 0.2566149578653911, accuracy: 0.9633333333333334\n",
      "iteration no 2707: Loss: 0.25660363662332375, accuracy: 0.9633333333333334\n",
      "iteration no 2708: Loss: 0.2565918481468281, accuracy: 0.9633333333333334\n",
      "iteration no 2709: Loss: 0.25657792314694433, accuracy: 0.9633333333333334\n",
      "iteration no 2710: Loss: 0.2565689972668407, accuracy: 0.9633333333333334\n",
      "iteration no 2711: Loss: 0.2565534160826196, accuracy: 0.9633333333333334\n",
      "iteration no 2712: Loss: 0.2565424181837697, accuracy: 0.9633333333333334\n",
      "iteration no 2713: Loss: 0.2565302803461118, accuracy: 0.9633333333333334\n",
      "iteration no 2714: Loss: 0.2565164483587034, accuracy: 0.9633333333333334\n",
      "iteration no 2715: Loss: 0.25650513714550843, accuracy: 0.9633333333333334\n",
      "iteration no 2716: Loss: 0.25649298432846257, accuracy: 0.9633333333333334\n",
      "iteration no 2717: Loss: 0.25648046836323446, accuracy: 0.9633333333333334\n",
      "iteration no 2718: Loss: 0.2564679012583069, accuracy: 0.9633333333333334\n",
      "iteration no 2719: Loss: 0.2564566276392955, accuracy: 0.9633333333333334\n",
      "iteration no 2720: Loss: 0.25644438586909213, accuracy: 0.9633333333333334\n",
      "iteration no 2721: Loss: 0.2564322135156028, accuracy: 0.9633333333333334\n",
      "iteration no 2722: Loss: 0.256418716987218, accuracy: 0.9633333333333334\n",
      "iteration no 2723: Loss: 0.2564082431678269, accuracy: 0.9633333333333334\n",
      "iteration no 2724: Loss: 0.256397115847235, accuracy: 0.9633333333333334\n",
      "iteration no 2725: Loss: 0.25638212046239867, accuracy: 0.9633333333333334\n",
      "iteration no 2726: Loss: 0.25637231015774864, accuracy: 0.9633333333333334\n",
      "iteration no 2727: Loss: 0.2563582805692467, accuracy: 0.9633333333333334\n",
      "iteration no 2728: Loss: 0.25634700113994013, accuracy: 0.9633333333333334\n",
      "iteration no 2729: Loss: 0.25633433785679516, accuracy: 0.9633333333333334\n",
      "iteration no 2730: Loss: 0.2563207466425257, accuracy: 0.9633333333333334\n",
      "iteration no 2731: Loss: 0.25631067956910814, accuracy: 0.9633333333333334\n",
      "iteration no 2732: Loss: 0.2562972431497075, accuracy: 0.9633333333333334\n",
      "iteration no 2733: Loss: 0.25628534753180254, accuracy: 0.9633333333333334\n",
      "iteration no 2734: Loss: 0.25627444845032965, accuracy: 0.9666666666666667\n",
      "iteration no 2735: Loss: 0.25626202004210574, accuracy: 0.9633333333333334\n",
      "iteration no 2736: Loss: 0.25624949010105147, accuracy: 0.9633333333333334\n",
      "iteration no 2737: Loss: 0.2562367805623602, accuracy: 0.9633333333333334\n",
      "iteration no 2738: Loss: 0.25622428693473437, accuracy: 0.9633333333333334\n",
      "iteration no 2739: Loss: 0.25621259194783624, accuracy: 0.9633333333333334\n",
      "iteration no 2740: Loss: 0.25620139887713167, accuracy: 0.9633333333333334\n",
      "iteration no 2741: Loss: 0.25618729637290527, accuracy: 0.9633333333333334\n",
      "iteration no 2742: Loss: 0.2561758951003865, accuracy: 0.9633333333333334\n",
      "iteration no 2743: Loss: 0.25616474790104327, accuracy: 0.9633333333333334\n",
      "iteration no 2744: Loss: 0.2561505628621335, accuracy: 0.9633333333333334\n",
      "iteration no 2745: Loss: 0.25614067349201936, accuracy: 0.9633333333333334\n",
      "iteration no 2746: Loss: 0.25612703210285365, accuracy: 0.9633333333333334\n",
      "iteration no 2747: Loss: 0.2561153902138151, accuracy: 0.9633333333333334\n",
      "iteration no 2748: Loss: 0.25610481430091697, accuracy: 0.97\n",
      "iteration no 2749: Loss: 0.2560912367313481, accuracy: 0.9633333333333334\n",
      "iteration no 2750: Loss: 0.256079885892094, accuracy: 0.97\n",
      "iteration no 2751: Loss: 0.25606756863145186, accuracy: 0.9633333333333334\n",
      "iteration no 2752: Loss: 0.2560554743560946, accuracy: 0.9633333333333334\n",
      "iteration no 2753: Loss: 0.2560427407676294, accuracy: 0.9633333333333334\n",
      "iteration no 2754: Loss: 0.2560311971654503, accuracy: 0.97\n",
      "iteration no 2755: Loss: 0.2560194388217828, accuracy: 0.9633333333333334\n",
      "iteration no 2756: Loss: 0.25600669261264875, accuracy: 0.97\n",
      "iteration no 2757: Loss: 0.2559956656644976, accuracy: 0.9633333333333334\n",
      "iteration no 2758: Loss: 0.25598180443798546, accuracy: 0.9633333333333334\n",
      "iteration no 2759: Loss: 0.25597186024093316, accuracy: 0.9633333333333334\n",
      "iteration no 2760: Loss: 0.2559606832835195, accuracy: 0.97\n",
      "iteration no 2761: Loss: 0.2559453598648341, accuracy: 0.9633333333333334\n",
      "iteration no 2762: Loss: 0.25593682929388095, accuracy: 0.97\n",
      "iteration no 2763: Loss: 0.2559217449689748, accuracy: 0.9633333333333334\n",
      "iteration no 2764: Loss: 0.2559066798637931, accuracy: 0.97\n",
      "iteration no 2765: Loss: 0.25589472417042974, accuracy: 0.97\n",
      "iteration no 2766: Loss: 0.2558792886886288, accuracy: 0.9633333333333334\n",
      "iteration no 2767: Loss: 0.25586703725084003, accuracy: 0.97\n",
      "iteration no 2768: Loss: 0.2558536187210597, accuracy: 0.97\n",
      "iteration no 2769: Loss: 0.2558389070541592, accuracy: 0.97\n",
      "iteration no 2770: Loss: 0.2558257929127611, accuracy: 0.97\n",
      "iteration no 2771: Loss: 0.2558130078305099, accuracy: 0.97\n",
      "iteration no 2772: Loss: 0.25579830458069014, accuracy: 0.97\n",
      "iteration no 2773: Loss: 0.2557846224226873, accuracy: 0.97\n",
      "iteration no 2774: Loss: 0.25577251083825053, accuracy: 0.97\n",
      "iteration no 2775: Loss: 0.2557574909190094, accuracy: 0.97\n",
      "iteration no 2776: Loss: 0.25574486685619413, accuracy: 0.97\n",
      "iteration no 2777: Loss: 0.2557317704246661, accuracy: 0.97\n",
      "iteration no 2778: Loss: 0.25571680298319854, accuracy: 0.9666666666666667\n",
      "iteration no 2779: Loss: 0.2557063535941655, accuracy: 0.97\n",
      "iteration no 2780: Loss: 0.25569080767458047, accuracy: 0.97\n",
      "iteration no 2781: Loss: 0.2556771857156564, accuracy: 0.97\n",
      "iteration no 2782: Loss: 0.2556650161341598, accuracy: 0.97\n",
      "iteration no 2783: Loss: 0.2556511722854656, accuracy: 0.97\n",
      "iteration no 2784: Loss: 0.2556393278194382, accuracy: 0.97\n",
      "iteration no 2785: Loss: 0.25562424415037216, accuracy: 0.97\n",
      "iteration no 2786: Loss: 0.25561101472917563, accuracy: 0.97\n",
      "iteration no 2787: Loss: 0.25559934474418156, accuracy: 0.97\n",
      "iteration no 2788: Loss: 0.2555847971218587, accuracy: 0.97\n",
      "iteration no 2789: Loss: 0.2555716760552757, accuracy: 0.97\n",
      "iteration no 2790: Loss: 0.2555594478606141, accuracy: 0.97\n",
      "iteration no 2791: Loss: 0.2555460963583557, accuracy: 0.97\n",
      "iteration no 2792: Loss: 0.255532406680874, accuracy: 0.97\n",
      "iteration no 2793: Loss: 0.25552041838889095, accuracy: 0.97\n",
      "iteration no 2794: Loss: 0.25550648848960417, accuracy: 0.97\n",
      "iteration no 2795: Loss: 0.255493296818359, accuracy: 0.97\n",
      "iteration no 2796: Loss: 0.25548250920015997, accuracy: 0.97\n",
      "iteration no 2797: Loss: 0.2554669809349288, accuracy: 0.97\n",
      "iteration no 2798: Loss: 0.2554544800503813, accuracy: 0.97\n",
      "iteration no 2799: Loss: 0.25544328389896376, accuracy: 0.97\n",
      "iteration no 2800: Loss: 0.2554280415184583, accuracy: 0.97\n",
      "iteration no 2801: Loss: 0.2554173549215952, accuracy: 0.97\n",
      "iteration no 2802: Loss: 0.25540374250196995, accuracy: 0.97\n",
      "iteration no 2803: Loss: 0.25538899168278584, accuracy: 0.97\n",
      "iteration no 2804: Loss: 0.2553786493839526, accuracy: 0.97\n",
      "iteration no 2805: Loss: 0.25536519878754477, accuracy: 0.97\n",
      "iteration no 2806: Loss: 0.25535195934135013, accuracy: 0.97\n",
      "iteration no 2807: Loss: 0.25534005597403103, accuracy: 0.97\n",
      "iteration no 2808: Loss: 0.2553270888486343, accuracy: 0.97\n",
      "iteration no 2809: Loss: 0.25531417858035205, accuracy: 0.97\n",
      "iteration no 2810: Loss: 0.25530267631332476, accuracy: 0.97\n",
      "iteration no 2811: Loss: 0.25529023883804025, accuracy: 0.97\n",
      "iteration no 2812: Loss: 0.25527557486479646, accuracy: 0.97\n",
      "iteration no 2813: Loss: 0.25526574097320065, accuracy: 0.97\n",
      "iteration no 2814: Loss: 0.25525289096832565, accuracy: 0.97\n",
      "iteration no 2815: Loss: 0.25523772784819704, accuracy: 0.97\n",
      "iteration no 2816: Loss: 0.2552279932952194, accuracy: 0.97\n",
      "iteration no 2817: Loss: 0.25521457066189424, accuracy: 0.97\n",
      "iteration no 2818: Loss: 0.2552010150350262, accuracy: 0.97\n",
      "iteration no 2819: Loss: 0.2551917989682826, accuracy: 0.97\n",
      "iteration no 2820: Loss: 0.2551767112333162, accuracy: 0.97\n",
      "iteration no 2821: Loss: 0.2551646741083965, accuracy: 0.97\n",
      "iteration no 2822: Loss: 0.2551550459635672, accuracy: 0.97\n",
      "iteration no 2823: Loss: 0.2551401296439112, accuracy: 0.97\n",
      "iteration no 2824: Loss: 0.25512852957729876, accuracy: 0.97\n",
      "iteration no 2825: Loss: 0.25511656131992855, accuracy: 0.97\n",
      "iteration no 2826: Loss: 0.25510363732279173, accuracy: 0.97\n",
      "iteration no 2827: Loss: 0.2550919977172157, accuracy: 0.97\n",
      "iteration no 2828: Loss: 0.2550794479455043, accuracy: 0.97\n",
      "iteration no 2829: Loss: 0.2550668937371372, accuracy: 0.97\n",
      "iteration no 2830: Loss: 0.2550541992118216, accuracy: 0.97\n",
      "iteration no 2831: Loss: 0.2550426368509959, accuracy: 0.97\n",
      "iteration no 2832: Loss: 0.25503072507992103, accuracy: 0.97\n",
      "iteration no 2833: Loss: 0.2550172004348781, accuracy: 0.97\n",
      "iteration no 2834: Loss: 0.2550062492685339, accuracy: 0.97\n",
      "iteration no 2835: Loss: 0.25499359524284804, accuracy: 0.97\n",
      "iteration no 2836: Loss: 0.2549801005947625, accuracy: 0.97\n",
      "iteration no 2837: Loss: 0.25496981423828313, accuracy: 0.97\n",
      "iteration no 2838: Loss: 0.2549568138836245, accuracy: 0.97\n",
      "iteration no 2839: Loss: 0.25494311868012826, accuracy: 0.97\n",
      "iteration no 2840: Loss: 0.2549342467965362, accuracy: 0.97\n",
      "iteration no 2841: Loss: 0.25492017896024727, accuracy: 0.97\n",
      "iteration no 2842: Loss: 0.2549073656530696, accuracy: 0.97\n",
      "iteration no 2843: Loss: 0.2548971385570005, accuracy: 0.97\n",
      "iteration no 2844: Loss: 0.2548835221189374, accuracy: 0.97\n",
      "iteration no 2845: Loss: 0.254871225326653, accuracy: 0.97\n",
      "iteration no 2846: Loss: 0.25486023121137447, accuracy: 0.97\n",
      "iteration no 2847: Loss: 0.25484716183687023, accuracy: 0.97\n",
      "iteration no 2848: Loss: 0.25483539637029634, accuracy: 0.97\n",
      "iteration no 2849: Loss: 0.2548237342753467, accuracy: 0.97\n",
      "iteration no 2850: Loss: 0.25481130663109886, accuracy: 0.97\n",
      "iteration no 2851: Loss: 0.254799834219019, accuracy: 0.97\n",
      "iteration no 2852: Loss: 0.25478666564792984, accuracy: 0.97\n",
      "iteration no 2853: Loss: 0.25477543068953484, accuracy: 0.97\n",
      "iteration no 2854: Loss: 0.2547636969273339, accuracy: 0.97\n",
      "iteration no 2855: Loss: 0.2547505207307583, accuracy: 0.97\n",
      "iteration no 2856: Loss: 0.25474021499402333, accuracy: 0.97\n",
      "iteration no 2857: Loss: 0.25472700332236564, accuracy: 0.97\n",
      "iteration no 2858: Loss: 0.25471414885267707, accuracy: 0.97\n",
      "iteration no 2859: Loss: 0.2547060570207408, accuracy: 0.97\n",
      "iteration no 2860: Loss: 0.2546911927462073, accuracy: 0.97\n",
      "iteration no 2861: Loss: 0.25467821221391745, accuracy: 0.97\n",
      "iteration no 2862: Loss: 0.2546690956628698, accuracy: 0.97\n",
      "iteration no 2863: Loss: 0.2546561702386496, accuracy: 0.97\n",
      "iteration no 2864: Loss: 0.2546435951061148, accuracy: 0.97\n",
      "iteration no 2865: Loss: 0.2546321089099475, accuracy: 0.97\n",
      "iteration no 2866: Loss: 0.2546200168911839, accuracy: 0.97\n",
      "iteration no 2867: Loss: 0.2546084136800606, accuracy: 0.97\n",
      "iteration no 2868: Loss: 0.25459603869808645, accuracy: 0.97\n",
      "iteration no 2869: Loss: 0.25458481160919255, accuracy: 0.97\n",
      "iteration no 2870: Loss: 0.2545731414706125, accuracy: 0.97\n",
      "iteration no 2871: Loss: 0.2545609055544086, accuracy: 0.97\n",
      "iteration no 2872: Loss: 0.2545502731970002, accuracy: 0.97\n",
      "iteration no 2873: Loss: 0.2545376677349378, accuracy: 0.97\n",
      "iteration no 2874: Loss: 0.2545254946445217, accuracy: 0.97\n",
      "iteration no 2875: Loss: 0.25451398006137443, accuracy: 0.97\n",
      "iteration no 2876: Loss: 0.2545031526350481, accuracy: 0.97\n",
      "iteration no 2877: Loss: 0.2544909591959759, accuracy: 0.97\n",
      "iteration no 2878: Loss: 0.25447813726889257, accuracy: 0.97\n",
      "iteration no 2879: Loss: 0.25446888195061657, accuracy: 0.97\n",
      "iteration no 2880: Loss: 0.25445584394909226, accuracy: 0.97\n",
      "iteration no 2881: Loss: 0.2544425778011171, accuracy: 0.97\n",
      "iteration no 2882: Loss: 0.25443386540934876, accuracy: 0.97\n",
      "iteration no 2883: Loss: 0.2544205271430716, accuracy: 0.97\n",
      "iteration no 2884: Loss: 0.25440850113531516, accuracy: 0.97\n",
      "iteration no 2885: Loss: 0.2543985039772576, accuracy: 0.97\n",
      "iteration no 2886: Loss: 0.2543861170716871, accuracy: 0.97\n",
      "iteration no 2887: Loss: 0.2543738061183909, accuracy: 0.97\n",
      "iteration no 2888: Loss: 0.2543623875115582, accuracy: 0.97\n",
      "iteration no 2889: Loss: 0.2543518284593364, accuracy: 0.97\n",
      "iteration no 2890: Loss: 0.2543395123123103, accuracy: 0.97\n",
      "iteration no 2891: Loss: 0.2543271383276282, accuracy: 0.97\n",
      "iteration no 2892: Loss: 0.2543167485236797, accuracy: 0.97\n",
      "iteration no 2893: Loss: 0.2543055911030834, accuracy: 0.97\n",
      "iteration no 2894: Loss: 0.2542932577499397, accuracy: 0.97\n",
      "iteration no 2895: Loss: 0.2542810288391502, accuracy: 0.97\n",
      "iteration no 2896: Loss: 0.25427009180183946, accuracy: 0.97\n",
      "iteration no 2897: Loss: 0.25425965153839325, accuracy: 0.97\n",
      "iteration no 2898: Loss: 0.254246286374031, accuracy: 0.97\n",
      "iteration no 2899: Loss: 0.25423600127533885, accuracy: 0.97\n",
      "iteration no 2900: Loss: 0.2542256217551759, accuracy: 0.97\n",
      "iteration no 2901: Loss: 0.25421242848393866, accuracy: 0.97\n",
      "iteration no 2902: Loss: 0.25420101388542976, accuracy: 0.97\n",
      "iteration no 2903: Loss: 0.25419093007300464, accuracy: 0.97\n",
      "iteration no 2904: Loss: 0.25417822868706047, accuracy: 0.97\n",
      "iteration no 2905: Loss: 0.2541669765382987, accuracy: 0.97\n",
      "iteration no 2906: Loss: 0.2541561363425091, accuracy: 0.97\n",
      "iteration no 2907: Loss: 0.25414480307030024, accuracy: 0.97\n",
      "iteration no 2908: Loss: 0.2541323911210141, accuracy: 0.97\n",
      "iteration no 2909: Loss: 0.2541207316057405, accuracy: 0.97\n",
      "iteration no 2910: Loss: 0.254111004746423, accuracy: 0.97\n",
      "iteration no 2911: Loss: 0.25409822743665195, accuracy: 0.97\n",
      "iteration no 2912: Loss: 0.25408792569565575, accuracy: 0.97\n",
      "iteration no 2913: Loss: 0.2540758515161563, accuracy: 0.97\n",
      "iteration no 2914: Loss: 0.254065103927828, accuracy: 0.97\n",
      "iteration no 2915: Loss: 0.25405426146315424, accuracy: 0.97\n",
      "iteration no 2916: Loss: 0.25404104367203206, accuracy: 0.97\n",
      "iteration no 2917: Loss: 0.2540307160450817, accuracy: 0.97\n",
      "iteration no 2918: Loss: 0.25402048342797307, accuracy: 0.97\n",
      "iteration no 2919: Loss: 0.2540085083562374, accuracy: 0.97\n",
      "iteration no 2920: Loss: 0.25399607000624863, accuracy: 0.97\n",
      "iteration no 2921: Loss: 0.25398624936724384, accuracy: 0.97\n",
      "iteration no 2922: Loss: 0.253975274212783, accuracy: 0.97\n",
      "iteration no 2923: Loss: 0.2539630575446766, accuracy: 0.97\n",
      "iteration no 2924: Loss: 0.25395141207955974, accuracy: 0.97\n",
      "iteration no 2925: Loss: 0.25394089961793864, accuracy: 0.97\n",
      "iteration no 2926: Loss: 0.2539285286801534, accuracy: 0.97\n",
      "iteration no 2927: Loss: 0.253918388724929, accuracy: 0.97\n",
      "iteration no 2928: Loss: 0.25390600232538785, accuracy: 0.97\n",
      "iteration no 2929: Loss: 0.2538958412317303, accuracy: 0.97\n",
      "iteration no 2930: Loss: 0.25388403814958327, accuracy: 0.97\n",
      "iteration no 2931: Loss: 0.25387279327117485, accuracy: 0.97\n",
      "iteration no 2932: Loss: 0.25386105791348734, accuracy: 0.97\n",
      "iteration no 2933: Loss: 0.25384987160095246, accuracy: 0.97\n",
      "iteration no 2934: Loss: 0.25383950133304933, accuracy: 0.97\n",
      "iteration no 2935: Loss: 0.25382660369943943, accuracy: 0.97\n",
      "iteration no 2936: Loss: 0.25381598463041094, accuracy: 0.97\n",
      "iteration no 2937: Loss: 0.25380657633910597, accuracy: 0.97\n",
      "iteration no 2938: Loss: 0.2537933861285119, accuracy: 0.97\n",
      "iteration no 2939: Loss: 0.25378147326071565, accuracy: 0.97\n",
      "iteration no 2940: Loss: 0.253771918317622, accuracy: 0.97\n",
      "iteration no 2941: Loss: 0.2537608154154212, accuracy: 0.97\n",
      "iteration no 2942: Loss: 0.2537483561692383, accuracy: 0.97\n",
      "iteration no 2943: Loss: 0.25373691039975793, accuracy: 0.97\n",
      "iteration no 2944: Loss: 0.25372714924820655, accuracy: 0.97\n",
      "iteration no 2945: Loss: 0.25371564532357926, accuracy: 0.97\n",
      "iteration no 2946: Loss: 0.2537045298573689, accuracy: 0.97\n",
      "iteration no 2947: Loss: 0.25369229842894003, accuracy: 0.97\n",
      "iteration no 2948: Loss: 0.25368169755043224, accuracy: 0.97\n",
      "iteration no 2949: Loss: 0.2536718401172957, accuracy: 0.97\n",
      "iteration no 2950: Loss: 0.25365940710511137, accuracy: 0.97\n",
      "iteration no 2951: Loss: 0.25364767358337326, accuracy: 0.97\n",
      "iteration no 2952: Loss: 0.2536374660912559, accuracy: 0.97\n",
      "iteration no 2953: Loss: 0.2536282473541888, accuracy: 0.97\n",
      "iteration no 2954: Loss: 0.2536153068381012, accuracy: 0.97\n",
      "iteration no 2955: Loss: 0.253602392840238, accuracy: 0.97\n",
      "iteration no 2956: Loss: 0.2535944599424689, accuracy: 0.97\n",
      "iteration no 2957: Loss: 0.2535823376725306, accuracy: 0.97\n",
      "iteration no 2958: Loss: 0.25357031979182215, accuracy: 0.97\n",
      "iteration no 2959: Loss: 0.2535594918313653, accuracy: 0.97\n",
      "iteration no 2960: Loss: 0.2535493501411586, accuracy: 0.97\n",
      "iteration no 2961: Loss: 0.253537340129115, accuracy: 0.97\n",
      "iteration no 2962: Loss: 0.2535261374798716, accuracy: 0.97\n",
      "iteration no 2963: Loss: 0.2535156785373393, accuracy: 0.97\n",
      "iteration no 2964: Loss: 0.2535045285814944, accuracy: 0.97\n",
      "iteration no 2965: Loss: 0.253493484610456, accuracy: 0.97\n",
      "iteration no 2966: Loss: 0.2534823518515253, accuracy: 0.97\n",
      "iteration no 2967: Loss: 0.25347135979196417, accuracy: 0.97\n",
      "iteration no 2968: Loss: 0.2534614791558089, accuracy: 0.97\n",
      "iteration no 2969: Loss: 0.2534488383866306, accuracy: 0.97\n",
      "iteration no 2970: Loss: 0.2534379633436608, accuracy: 0.97\n",
      "iteration no 2971: Loss: 0.2534275878982097, accuracy: 0.97\n",
      "iteration no 2972: Loss: 0.2534175028117597, accuracy: 0.97\n",
      "iteration no 2973: Loss: 0.2534050362856237, accuracy: 0.97\n",
      "iteration no 2974: Loss: 0.2533941916502139, accuracy: 0.97\n",
      "iteration no 2975: Loss: 0.2533847085650149, accuracy: 0.97\n",
      "iteration no 2976: Loss: 0.253372977226418, accuracy: 0.97\n",
      "iteration no 2977: Loss: 0.25336185241652703, accuracy: 0.97\n",
      "iteration no 2978: Loss: 0.253351282991374, accuracy: 0.97\n",
      "iteration no 2979: Loss: 0.2533406418125484, accuracy: 0.97\n",
      "iteration no 2980: Loss: 0.2533299865154837, accuracy: 0.97\n",
      "iteration no 2981: Loss: 0.25331893770874725, accuracy: 0.97\n",
      "iteration no 2982: Loss: 0.2533079504554546, accuracy: 0.97\n",
      "iteration no 2983: Loss: 0.2532968710589511, accuracy: 0.97\n",
      "iteration no 2984: Loss: 0.2532883945893836, accuracy: 0.97\n",
      "iteration no 2985: Loss: 0.2532760515763569, accuracy: 0.97\n",
      "iteration no 2986: Loss: 0.2532640594514173, accuracy: 0.97\n",
      "iteration no 2987: Loss: 0.25325531919315475, accuracy: 0.97\n",
      "iteration no 2988: Loss: 0.2532449709974497, accuracy: 0.97\n",
      "iteration no 2989: Loss: 0.2532334003149289, accuracy: 0.97\n",
      "iteration no 2990: Loss: 0.2532222588737876, accuracy: 0.97\n",
      "iteration no 2991: Loss: 0.253212437852398, accuracy: 0.97\n",
      "iteration no 2992: Loss: 0.2532024782975964, accuracy: 0.97\n",
      "iteration no 2993: Loss: 0.2531918479885377, accuracy: 0.97\n",
      "iteration no 2994: Loss: 0.2531812854601986, accuracy: 0.97\n",
      "iteration no 2995: Loss: 0.25316953601529923, accuracy: 0.97\n",
      "iteration no 2996: Loss: 0.2531612028074497, accuracy: 0.97\n",
      "iteration no 2997: Loss: 0.25314986014896307, accuracy: 0.97\n",
      "iteration no 2998: Loss: 0.2531394419251912, accuracy: 0.97\n",
      "iteration no 2999: Loss: 0.25312826347310047, accuracy: 0.97\n",
      "iteration no 3000: Loss: 0.2531198719408415, accuracy: 0.97\n",
      "iteration no 3001: Loss: 0.25310878903470074, accuracy: 0.97\n",
      "iteration no 3002: Loss: 0.2530972281716647, accuracy: 0.97\n",
      "iteration no 3003: Loss: 0.2530881122353836, accuracy: 0.97\n",
      "iteration no 3004: Loss: 0.25307719991341554, accuracy: 0.97\n",
      "iteration no 3005: Loss: 0.2530673910929031, accuracy: 0.97\n",
      "iteration no 3006: Loss: 0.25305744893460236, accuracy: 0.97\n",
      "iteration no 3007: Loss: 0.2530464711012941, accuracy: 0.97\n",
      "iteration no 3008: Loss: 0.2530357061601921, accuracy: 0.97\n",
      "iteration no 3009: Loss: 0.25302689291293357, accuracy: 0.97\n",
      "iteration no 3010: Loss: 0.25301662024232063, accuracy: 0.97\n",
      "iteration no 3011: Loss: 0.2530048555815173, accuracy: 0.97\n",
      "iteration no 3012: Loss: 0.2529950775741712, accuracy: 0.97\n",
      "iteration no 3013: Loss: 0.2529854750681066, accuracy: 0.97\n",
      "iteration no 3014: Loss: 0.25297539795031937, accuracy: 0.97\n",
      "iteration no 3015: Loss: 0.2529650729015212, accuracy: 0.97\n",
      "iteration no 3016: Loss: 0.2529548007642058, accuracy: 0.97\n",
      "iteration no 3017: Loss: 0.25294428084985887, accuracy: 0.97\n",
      "iteration no 3018: Loss: 0.2529339151513887, accuracy: 0.97\n",
      "iteration no 3019: Loss: 0.2529250860497139, accuracy: 0.97\n",
      "iteration no 3020: Loss: 0.25291374155140484, accuracy: 0.97\n",
      "iteration no 3021: Loss: 0.2529030465229055, accuracy: 0.97\n",
      "iteration no 3022: Loss: 0.2528938056319451, accuracy: 0.97\n",
      "iteration no 3023: Loss: 0.2528839933432045, accuracy: 0.97\n",
      "iteration no 3024: Loss: 0.25287345137644807, accuracy: 0.97\n",
      "iteration no 3025: Loss: 0.2528626806909706, accuracy: 0.97\n",
      "iteration no 3026: Loss: 0.2528533166871411, accuracy: 0.97\n",
      "iteration no 3027: Loss: 0.25284308406945233, accuracy: 0.97\n",
      "iteration no 3028: Loss: 0.2528325035423199, accuracy: 0.97\n",
      "iteration no 3029: Loss: 0.25282324121665445, accuracy: 0.97\n",
      "iteration no 3030: Loss: 0.25281259439963794, accuracy: 0.97\n",
      "iteration no 3031: Loss: 0.2528031287563176, accuracy: 0.97\n",
      "iteration no 3032: Loss: 0.2527933032851271, accuracy: 0.97\n",
      "iteration no 3033: Loss: 0.25278340300866486, accuracy: 0.97\n",
      "iteration no 3034: Loss: 0.25277267262060366, accuracy: 0.97\n",
      "iteration no 3035: Loss: 0.25276315007803635, accuracy: 0.97\n",
      "iteration no 3036: Loss: 0.25275405298118864, accuracy: 0.97\n",
      "iteration no 3037: Loss: 0.25274440324159686, accuracy: 0.97\n",
      "iteration no 3038: Loss: 0.25273342911595675, accuracy: 0.97\n",
      "iteration no 3039: Loss: 0.2527241499623682, accuracy: 0.97\n",
      "iteration no 3040: Loss: 0.25271421361302426, accuracy: 0.97\n",
      "iteration no 3041: Loss: 0.25270495256577186, accuracy: 0.97\n",
      "iteration no 3042: Loss: 0.2526953721542879, accuracy: 0.97\n",
      "iteration no 3043: Loss: 0.25268626336128946, accuracy: 0.97\n",
      "iteration no 3044: Loss: 0.25267602358207086, accuracy: 0.97\n",
      "iteration no 3045: Loss: 0.25266530987053537, accuracy: 0.97\n",
      "iteration no 3046: Loss: 0.25265823535152004, accuracy: 0.97\n",
      "iteration no 3047: Loss: 0.252647574871807, accuracy: 0.97\n",
      "iteration no 3048: Loss: 0.2526381474051551, accuracy: 0.97\n",
      "iteration no 3049: Loss: 0.25262764509761404, accuracy: 0.97\n",
      "iteration no 3050: Loss: 0.25261909580660485, accuracy: 0.97\n",
      "iteration no 3051: Loss: 0.2526104638726738, accuracy: 0.97\n",
      "iteration no 3052: Loss: 0.252599589180052, accuracy: 0.97\n",
      "iteration no 3053: Loss: 0.2525907389320357, accuracy: 0.97\n",
      "iteration no 3054: Loss: 0.2525806719481427, accuracy: 0.97\n",
      "iteration no 3055: Loss: 0.2525716853152978, accuracy: 0.97\n",
      "iteration no 3056: Loss: 0.2525623225542551, accuracy: 0.97\n",
      "iteration no 3057: Loss: 0.2525540070103381, accuracy: 0.97\n",
      "iteration no 3058: Loss: 0.252543117004131, accuracy: 0.97\n",
      "iteration no 3059: Loss: 0.2525335014999779, accuracy: 0.97\n",
      "iteration no 3060: Loss: 0.2525247290823336, accuracy: 0.97\n",
      "iteration no 3061: Loss: 0.252516600268813, accuracy: 0.97\n",
      "iteration no 3062: Loss: 0.25250628051647367, accuracy: 0.97\n",
      "iteration no 3063: Loss: 0.25249559753223505, accuracy: 0.97\n",
      "iteration no 3064: Loss: 0.25248713788128613, accuracy: 0.97\n",
      "iteration no 3065: Loss: 0.25247785807617473, accuracy: 0.97\n",
      "iteration no 3066: Loss: 0.25246969945587566, accuracy: 0.97\n",
      "iteration no 3067: Loss: 0.25245940471797124, accuracy: 0.97\n",
      "iteration no 3068: Loss: 0.25245087111277337, accuracy: 0.97\n",
      "iteration no 3069: Loss: 0.2524403009927243, accuracy: 0.97\n",
      "iteration no 3070: Loss: 0.25243142485907294, accuracy: 0.97\n",
      "iteration no 3071: Loss: 0.25242286787051427, accuracy: 0.97\n",
      "iteration no 3072: Loss: 0.25241365581074093, accuracy: 0.97\n",
      "iteration no 3073: Loss: 0.25240379267724483, accuracy: 0.97\n",
      "iteration no 3074: Loss: 0.25239353318312685, accuracy: 0.97\n",
      "iteration no 3075: Loss: 0.25238622970127556, accuracy: 0.97\n",
      "iteration no 3076: Loss: 0.2523767087690484, accuracy: 0.97\n",
      "iteration no 3077: Loss: 0.2523673146465488, accuracy: 0.97\n",
      "iteration no 3078: Loss: 0.2523577593577217, accuracy: 0.97\n",
      "iteration no 3079: Loss: 0.25234931114038184, accuracy: 0.97\n",
      "iteration no 3080: Loss: 0.25234068103936746, accuracy: 0.97\n",
      "iteration no 3081: Loss: 0.2523304981223025, accuracy: 0.97\n",
      "iteration no 3082: Loss: 0.25232135882304135, accuracy: 0.97\n",
      "iteration no 3083: Loss: 0.2523120507062705, accuracy: 0.97\n",
      "iteration no 3084: Loss: 0.2523039833284043, accuracy: 0.97\n",
      "iteration no 3085: Loss: 0.25229453934115054, accuracy: 0.97\n",
      "iteration no 3086: Loss: 0.25228609905936406, accuracy: 0.97\n",
      "iteration no 3087: Loss: 0.2522758087082593, accuracy: 0.97\n",
      "iteration no 3088: Loss: 0.2522665901882772, accuracy: 0.97\n",
      "iteration no 3089: Loss: 0.25225774053761757, accuracy: 0.97\n",
      "iteration no 3090: Loss: 0.25225095967152467, accuracy: 0.97\n",
      "iteration no 3091: Loss: 0.2522404421761085, accuracy: 0.97\n",
      "iteration no 3092: Loss: 0.2522305193522876, accuracy: 0.97\n",
      "iteration no 3093: Loss: 0.25222147835821673, accuracy: 0.97\n",
      "iteration no 3094: Loss: 0.25221290620780035, accuracy: 0.97\n",
      "iteration no 3095: Loss: 0.25220492345472545, accuracy: 0.97\n",
      "iteration no 3096: Loss: 0.2521949031921155, accuracy: 0.97\n",
      "iteration no 3097: Loss: 0.25218630882604964, accuracy: 0.97\n",
      "iteration no 3098: Loss: 0.2521765308958313, accuracy: 0.97\n",
      "iteration no 3099: Loss: 0.2521678846679801, accuracy: 0.97\n",
      "iteration no 3100: Loss: 0.2521589092787069, accuracy: 0.97\n",
      "iteration no 3101: Loss: 0.2521503737837675, accuracy: 0.97\n",
      "iteration no 3102: Loss: 0.25214141633943804, accuracy: 0.97\n",
      "iteration no 3103: Loss: 0.2521314726539193, accuracy: 0.97\n",
      "iteration no 3104: Loss: 0.25212296405100365, accuracy: 0.97\n",
      "iteration no 3105: Loss: 0.2521147848805225, accuracy: 0.97\n",
      "iteration no 3106: Loss: 0.2521057576783009, accuracy: 0.97\n",
      "iteration no 3107: Loss: 0.2520955683100431, accuracy: 0.97\n",
      "iteration no 3108: Loss: 0.25208738910356626, accuracy: 0.97\n",
      "iteration no 3109: Loss: 0.25207917110130557, accuracy: 0.97\n",
      "iteration no 3110: Loss: 0.2520707117010038, accuracy: 0.97\n",
      "iteration no 3111: Loss: 0.2520606494050617, accuracy: 0.97\n",
      "iteration no 3112: Loss: 0.2520515952997822, accuracy: 0.97\n",
      "iteration no 3113: Loss: 0.25204322492559894, accuracy: 0.97\n",
      "iteration no 3114: Loss: 0.25203478584457756, accuracy: 0.97\n",
      "iteration no 3115: Loss: 0.25202646756478086, accuracy: 0.97\n",
      "iteration no 3116: Loss: 0.2520169432570979, accuracy: 0.97\n",
      "iteration no 3117: Loss: 0.2520083786201276, accuracy: 0.97\n",
      "iteration no 3118: Loss: 0.2519983078883021, accuracy: 0.97\n",
      "iteration no 3119: Loss: 0.25199046034460015, accuracy: 0.97\n",
      "iteration no 3120: Loss: 0.25198226148714586, accuracy: 0.97\n",
      "iteration no 3121: Loss: 0.25197334984297837, accuracy: 0.97\n",
      "iteration no 3122: Loss: 0.2519637481649674, accuracy: 0.97\n",
      "iteration no 3123: Loss: 0.25195425069214505, accuracy: 0.97\n",
      "iteration no 3124: Loss: 0.2519466606155924, accuracy: 0.97\n",
      "iteration no 3125: Loss: 0.25193846048441493, accuracy: 0.97\n",
      "iteration no 3126: Loss: 0.2519293031795373, accuracy: 0.97\n",
      "iteration no 3127: Loss: 0.25191948233995004, accuracy: 0.97\n",
      "iteration no 3128: Loss: 0.25191165159356615, accuracy: 0.97\n",
      "iteration no 3129: Loss: 0.25190226965347695, accuracy: 0.97\n",
      "iteration no 3130: Loss: 0.2518942132061798, accuracy: 0.97\n",
      "iteration no 3131: Loss: 0.2518848946241127, accuracy: 0.97\n",
      "iteration no 3132: Loss: 0.251876267830127, accuracy: 0.97\n",
      "iteration no 3133: Loss: 0.25186760469585145, accuracy: 0.97\n",
      "iteration no 3134: Loss: 0.2518581534831286, accuracy: 0.97\n",
      "iteration no 3135: Loss: 0.25185040942769754, accuracy: 0.97\n",
      "iteration no 3136: Loss: 0.2518411078939355, accuracy: 0.97\n",
      "iteration no 3137: Loss: 0.25183231443651144, accuracy: 0.97\n",
      "iteration no 3138: Loss: 0.2518227823498515, accuracy: 0.97\n",
      "iteration no 3139: Loss: 0.25181470707868475, accuracy: 0.97\n",
      "iteration no 3140: Loss: 0.25180769892468297, accuracy: 0.97\n",
      "iteration no 3141: Loss: 0.2517976593790582, accuracy: 0.97\n",
      "iteration no 3142: Loss: 0.2517882259430648, accuracy: 0.97\n",
      "iteration no 3143: Loss: 0.2517790932397053, accuracy: 0.97\n",
      "iteration no 3144: Loss: 0.2517710479239027, accuracy: 0.97\n",
      "iteration no 3145: Loss: 0.2517626729898247, accuracy: 0.97\n",
      "iteration no 3146: Loss: 0.2517546673768546, accuracy: 0.97\n",
      "iteration no 3147: Loss: 0.2517450474742762, accuracy: 0.97\n",
      "iteration no 3148: Loss: 0.2517365893232347, accuracy: 0.97\n",
      "iteration no 3149: Loss: 0.25172701530132097, accuracy: 0.97\n",
      "iteration no 3150: Loss: 0.2517183371113212, accuracy: 0.97\n",
      "iteration no 3151: Loss: 0.251710430623897, accuracy: 0.97\n",
      "iteration no 3152: Loss: 0.2517020662764531, accuracy: 0.97\n",
      "iteration no 3153: Loss: 0.25169330397547895, accuracy: 0.97\n",
      "iteration no 3154: Loss: 0.25168362552815066, accuracy: 0.97\n",
      "iteration no 3155: Loss: 0.2516759198470289, accuracy: 0.97\n",
      "iteration no 3156: Loss: 0.2516681653309321, accuracy: 0.97\n",
      "iteration no 3157: Loss: 0.2516589712992332, accuracy: 0.97\n",
      "iteration no 3158: Loss: 0.25164935657272974, accuracy: 0.97\n",
      "iteration no 3159: Loss: 0.25164126365162387, accuracy: 0.97\n",
      "iteration no 3160: Loss: 0.2516328923586936, accuracy: 0.97\n",
      "iteration no 3161: Loss: 0.25162504823449067, accuracy: 0.97\n",
      "iteration no 3162: Loss: 0.2516159731801065, accuracy: 0.97\n",
      "iteration no 3163: Loss: 0.251606755715167, accuracy: 0.97\n",
      "iteration no 3164: Loss: 0.25159881650518306, accuracy: 0.97\n",
      "iteration no 3165: Loss: 0.25158950626872195, accuracy: 0.97\n",
      "iteration no 3166: Loss: 0.2515825689090304, accuracy: 0.97\n",
      "iteration no 3167: Loss: 0.25157327051442746, accuracy: 0.97\n",
      "iteration no 3168: Loss: 0.2515650157653221, accuracy: 0.97\n",
      "iteration no 3169: Loss: 0.25155592199213905, accuracy: 0.97\n",
      "iteration no 3170: Loss: 0.2515471301927203, accuracy: 0.97\n",
      "iteration no 3171: Loss: 0.25153923563494224, accuracy: 0.97\n",
      "iteration no 3172: Loss: 0.2515318715935079, accuracy: 0.97\n",
      "iteration no 3173: Loss: 0.2515228966848617, accuracy: 0.97\n",
      "iteration no 3174: Loss: 0.2515132610545487, accuracy: 0.97\n",
      "iteration no 3175: Loss: 0.251504929800226, accuracy: 0.97\n",
      "iteration no 3176: Loss: 0.2514966336957094, accuracy: 0.97\n",
      "iteration no 3177: Loss: 0.25148882333647105, accuracy: 0.97\n",
      "iteration no 3178: Loss: 0.25148044174035245, accuracy: 0.97\n",
      "iteration no 3179: Loss: 0.2514717085975925, accuracy: 0.97\n",
      "iteration no 3180: Loss: 0.2514639019840719, accuracy: 0.97\n",
      "iteration no 3181: Loss: 0.25145509156233037, accuracy: 0.97\n",
      "iteration no 3182: Loss: 0.25144720432401835, accuracy: 0.97\n",
      "iteration no 3183: Loss: 0.2514385567455435, accuracy: 0.97\n",
      "iteration no 3184: Loss: 0.2514305973400809, accuracy: 0.97\n",
      "iteration no 3185: Loss: 0.2514218990000018, accuracy: 0.97\n",
      "iteration no 3186: Loss: 0.251413500147454, accuracy: 0.97\n",
      "iteration no 3187: Loss: 0.2514045865405944, accuracy: 0.97\n",
      "iteration no 3188: Loss: 0.25139694073774865, accuracy: 0.97\n",
      "iteration no 3189: Loss: 0.2513888062000005, accuracy: 0.97\n",
      "iteration no 3190: Loss: 0.2513794203814289, accuracy: 0.97\n",
      "iteration no 3191: Loss: 0.25137101075231805, accuracy: 0.97\n",
      "iteration no 3192: Loss: 0.2513616182449817, accuracy: 0.97\n",
      "iteration no 3193: Loss: 0.2513552558406852, accuracy: 0.97\n",
      "iteration no 3194: Loss: 0.25134657119533294, accuracy: 0.97\n",
      "iteration no 3195: Loss: 0.2513375311422592, accuracy: 0.97\n",
      "iteration no 3196: Loss: 0.2513281832113771, accuracy: 0.97\n",
      "iteration no 3197: Loss: 0.25131997691294433, accuracy: 0.97\n",
      "iteration no 3198: Loss: 0.2513113103088683, accuracy: 0.97\n",
      "iteration no 3199: Loss: 0.25130284513099116, accuracy: 0.97\n",
      "iteration no 3200: Loss: 0.25129512473229565, accuracy: 0.97\n",
      "iteration no 3201: Loss: 0.25128580473846773, accuracy: 0.97\n",
      "iteration no 3202: Loss: 0.2512779291731821, accuracy: 0.97\n",
      "iteration no 3203: Loss: 0.25126840447334314, accuracy: 0.97\n",
      "iteration no 3204: Loss: 0.2512597398897849, accuracy: 0.97\n",
      "iteration no 3205: Loss: 0.2512511595476231, accuracy: 0.97\n",
      "iteration no 3206: Loss: 0.2512436449255506, accuracy: 0.97\n",
      "iteration no 3207: Loss: 0.2512341474148184, accuracy: 0.97\n",
      "iteration no 3208: Loss: 0.25122435925179293, accuracy: 0.97\n",
      "iteration no 3209: Loss: 0.25121524083348423, accuracy: 0.97\n",
      "iteration no 3210: Loss: 0.25120631744989735, accuracy: 0.97\n",
      "iteration no 3211: Loss: 0.2511983001644269, accuracy: 0.97\n",
      "iteration no 3212: Loss: 0.2511890828361775, accuracy: 0.97\n",
      "iteration no 3213: Loss: 0.2511801905789544, accuracy: 0.97\n",
      "iteration no 3214: Loss: 0.25117142069772125, accuracy: 0.97\n",
      "iteration no 3215: Loss: 0.2511625477216203, accuracy: 0.97\n",
      "iteration no 3216: Loss: 0.2511531410933868, accuracy: 0.97\n",
      "iteration no 3217: Loss: 0.2511444948689961, accuracy: 0.97\n",
      "iteration no 3218: Loss: 0.2511360172572208, accuracy: 0.97\n",
      "iteration no 3219: Loss: 0.2511269975007649, accuracy: 0.97\n",
      "iteration no 3220: Loss: 0.25111799833949866, accuracy: 0.97\n",
      "iteration no 3221: Loss: 0.2511086749126655, accuracy: 0.97\n",
      "iteration no 3222: Loss: 0.2510998287166296, accuracy: 0.97\n",
      "iteration no 3223: Loss: 0.25109181394240754, accuracy: 0.97\n",
      "iteration no 3224: Loss: 0.25108350183683276, accuracy: 0.97\n",
      "iteration no 3225: Loss: 0.2510740519977637, accuracy: 0.97\n",
      "iteration no 3226: Loss: 0.25106466849705683, accuracy: 0.97\n",
      "iteration no 3227: Loss: 0.2510560374002141, accuracy: 0.97\n",
      "iteration no 3228: Loss: 0.25104690439885907, accuracy: 0.97\n",
      "iteration no 3229: Loss: 0.25103925136169447, accuracy: 0.97\n",
      "iteration no 3230: Loss: 0.2510297893912056, accuracy: 0.97\n",
      "iteration no 3231: Loss: 0.25102088339389483, accuracy: 0.97\n",
      "iteration no 3232: Loss: 0.2510120815519664, accuracy: 0.97\n",
      "iteration no 3233: Loss: 0.2510028655417022, accuracy: 0.97\n",
      "iteration no 3234: Loss: 0.25099367471923684, accuracy: 0.97\n",
      "iteration no 3235: Loss: 0.2509851969169543, accuracy: 0.97\n",
      "iteration no 3236: Loss: 0.25097775139774403, accuracy: 0.9733333333333334\n",
      "iteration no 3237: Loss: 0.2509683110029679, accuracy: 0.97\n",
      "iteration no 3238: Loss: 0.25095936663766094, accuracy: 0.97\n",
      "iteration no 3239: Loss: 0.2509497204206455, accuracy: 0.9733333333333334\n",
      "iteration no 3240: Loss: 0.250941352598553, accuracy: 0.97\n",
      "iteration no 3241: Loss: 0.2509330868228305, accuracy: 0.9733333333333334\n",
      "iteration no 3242: Loss: 0.2509251875431785, accuracy: 0.9733333333333334\n",
      "iteration no 3243: Loss: 0.2509156176309015, accuracy: 0.9733333333333334\n",
      "iteration no 3244: Loss: 0.2509064933111691, accuracy: 0.9733333333333334\n",
      "iteration no 3245: Loss: 0.25089849928538266, accuracy: 0.9733333333333334\n",
      "iteration no 3246: Loss: 0.25088909425579253, accuracy: 0.9733333333333334\n",
      "iteration no 3247: Loss: 0.25088087928077063, accuracy: 0.9733333333333334\n",
      "iteration no 3248: Loss: 0.2508719607125175, accuracy: 0.9733333333333334\n",
      "iteration no 3249: Loss: 0.2508634866316547, accuracy: 0.97\n",
      "iteration no 3250: Loss: 0.2508544834264315, accuracy: 0.9733333333333334\n",
      "iteration no 3251: Loss: 0.25084577953401005, accuracy: 0.97\n",
      "iteration no 3252: Loss: 0.25083680640688777, accuracy: 0.9733333333333334\n",
      "iteration no 3253: Loss: 0.2508278956052829, accuracy: 0.9733333333333334\n",
      "iteration no 3254: Loss: 0.2508206217764876, accuracy: 0.9733333333333334\n",
      "iteration no 3255: Loss: 0.25081137870695924, accuracy: 0.9733333333333334\n",
      "iteration no 3256: Loss: 0.2508024127461732, accuracy: 0.9733333333333334\n",
      "iteration no 3257: Loss: 0.25079291634330236, accuracy: 0.9733333333333334\n",
      "iteration no 3258: Loss: 0.2507849214342747, accuracy: 0.9733333333333334\n",
      "iteration no 3259: Loss: 0.2507762418746935, accuracy: 0.9733333333333334\n",
      "iteration no 3260: Loss: 0.25076860254569316, accuracy: 0.9733333333333334\n",
      "iteration no 3261: Loss: 0.2507596244823831, accuracy: 0.9733333333333334\n",
      "iteration no 3262: Loss: 0.2507503946595661, accuracy: 0.9733333333333334\n",
      "iteration no 3263: Loss: 0.2507419291279128, accuracy: 0.9733333333333334\n",
      "iteration no 3264: Loss: 0.25073301317590047, accuracy: 0.9733333333333334\n",
      "iteration no 3265: Loss: 0.2507242642234989, accuracy: 0.9733333333333334\n",
      "iteration no 3266: Loss: 0.2507136596832029, accuracy: 0.9733333333333334\n",
      "iteration no 3267: Loss: 0.25070449859301885, accuracy: 0.9733333333333334\n",
      "iteration no 3268: Loss: 0.25069420203617276, accuracy: 0.9733333333333334\n",
      "iteration no 3269: Loss: 0.2506840942882835, accuracy: 0.9733333333333334\n",
      "iteration no 3270: Loss: 0.25067305320723954, accuracy: 0.9733333333333334\n",
      "iteration no 3271: Loss: 0.2506628901102151, accuracy: 0.9733333333333334\n",
      "iteration no 3272: Loss: 0.25065232128386206, accuracy: 0.9733333333333334\n",
      "iteration no 3273: Loss: 0.25064278382108585, accuracy: 0.9733333333333334\n",
      "iteration no 3274: Loss: 0.25063367394732194, accuracy: 0.9733333333333334\n",
      "iteration no 3275: Loss: 0.25062294015130226, accuracy: 0.9733333333333334\n",
      "iteration no 3276: Loss: 0.25061286276710504, accuracy: 0.9733333333333334\n",
      "iteration no 3277: Loss: 0.25060231791395643, accuracy: 0.9733333333333334\n",
      "iteration no 3278: Loss: 0.2505925325546945, accuracy: 0.9733333333333334\n",
      "iteration no 3279: Loss: 0.2505825513826127, accuracy: 0.9733333333333334\n",
      "iteration no 3280: Loss: 0.2505731442759668, accuracy: 0.9733333333333334\n",
      "iteration no 3281: Loss: 0.2505629345298292, accuracy: 0.9733333333333334\n",
      "iteration no 3282: Loss: 0.25055427807959024, accuracy: 0.9733333333333334\n",
      "iteration no 3283: Loss: 0.2505434872236143, accuracy: 0.9733333333333334\n",
      "iteration no 3284: Loss: 0.2505348625547716, accuracy: 0.9733333333333334\n",
      "iteration no 3285: Loss: 0.25052491962140155, accuracy: 0.9733333333333334\n",
      "iteration no 3286: Loss: 0.2505153213732346, accuracy: 0.9733333333333334\n",
      "iteration no 3287: Loss: 0.2505053972773491, accuracy: 0.9733333333333334\n",
      "iteration no 3288: Loss: 0.25049544697280246, accuracy: 0.9733333333333334\n",
      "iteration no 3289: Loss: 0.2504860495061956, accuracy: 0.9733333333333334\n",
      "iteration no 3290: Loss: 0.2504769550726631, accuracy: 0.9733333333333334\n",
      "iteration no 3291: Loss: 0.25046798105480733, accuracy: 0.9733333333333334\n",
      "iteration no 3292: Loss: 0.25045742835153467, accuracy: 0.9733333333333334\n",
      "iteration no 3293: Loss: 0.25044809690854974, accuracy: 0.9733333333333334\n",
      "iteration no 3294: Loss: 0.25043845069880016, accuracy: 0.9733333333333334\n",
      "iteration no 3295: Loss: 0.25042858093227327, accuracy: 0.9733333333333334\n",
      "iteration no 3296: Loss: 0.2504197749980879, accuracy: 0.9733333333333334\n",
      "iteration no 3297: Loss: 0.25040967007733983, accuracy: 0.9733333333333334\n",
      "iteration no 3298: Loss: 0.25040088292807006, accuracy: 0.9733333333333334\n",
      "iteration no 3299: Loss: 0.25039116839518943, accuracy: 0.9733333333333334\n",
      "iteration no 3300: Loss: 0.25038155062122514, accuracy: 0.9733333333333334\n",
      "iteration no 3301: Loss: 0.2503718955055789, accuracy: 0.9733333333333334\n",
      "iteration no 3302: Loss: 0.25036254763234145, accuracy: 0.9733333333333334\n",
      "iteration no 3303: Loss: 0.25035344065656245, accuracy: 0.9733333333333334\n",
      "iteration no 3304: Loss: 0.25034336103629684, accuracy: 0.9733333333333334\n",
      "iteration no 3305: Loss: 0.25033423367919805, accuracy: 0.9733333333333334\n",
      "iteration no 3306: Loss: 0.25032449793724126, accuracy: 0.9733333333333334\n",
      "iteration no 3307: Loss: 0.25031597016762686, accuracy: 0.9733333333333334\n",
      "iteration no 3308: Loss: 0.25030677372662563, accuracy: 0.9733333333333334\n",
      "iteration no 3309: Loss: 0.2502967302966694, accuracy: 0.9733333333333334\n",
      "iteration no 3310: Loss: 0.2502877304201967, accuracy: 0.9733333333333334\n",
      "iteration no 3311: Loss: 0.2502777559027567, accuracy: 0.9733333333333334\n",
      "iteration no 3312: Loss: 0.2502685955290563, accuracy: 0.9733333333333334\n",
      "iteration no 3313: Loss: 0.25025986936936995, accuracy: 0.9733333333333334\n",
      "iteration no 3314: Loss: 0.2502502867963942, accuracy: 0.9733333333333334\n",
      "iteration no 3315: Loss: 0.2502415641844292, accuracy: 0.9733333333333334\n",
      "iteration no 3316: Loss: 0.2502321551170048, accuracy: 0.9733333333333334\n",
      "iteration no 3317: Loss: 0.25022276236037155, accuracy: 0.9733333333333334\n",
      "iteration no 3318: Loss: 0.250213364635246, accuracy: 0.9733333333333334\n",
      "iteration no 3319: Loss: 0.250204124312594, accuracy: 0.9733333333333334\n",
      "iteration no 3320: Loss: 0.25019499980180226, accuracy: 0.9733333333333334\n",
      "iteration no 3321: Loss: 0.25018505482959913, accuracy: 0.9733333333333334\n",
      "iteration no 3322: Loss: 0.25017611231580084, accuracy: 0.9733333333333334\n",
      "iteration no 3323: Loss: 0.2501667277883553, accuracy: 0.9733333333333334\n",
      "iteration no 3324: Loss: 0.250158566263411, accuracy: 0.9733333333333334\n",
      "iteration no 3325: Loss: 0.25014971532188024, accuracy: 0.9733333333333334\n",
      "iteration no 3326: Loss: 0.2501397170793359, accuracy: 0.9733333333333334\n",
      "iteration no 3327: Loss: 0.25013094010554215, accuracy: 0.9733333333333334\n",
      "iteration no 3328: Loss: 0.25012121759543937, accuracy: 0.9733333333333334\n",
      "iteration no 3329: Loss: 0.2501123150377963, accuracy: 0.9733333333333334\n",
      "iteration no 3330: Loss: 0.25010345605013706, accuracy: 0.9733333333333334\n",
      "iteration no 3331: Loss: 0.25009386899693037, accuracy: 0.9733333333333334\n",
      "iteration no 3332: Loss: 0.2500856494515117, accuracy: 0.9733333333333334\n",
      "iteration no 3333: Loss: 0.2500760762505497, accuracy: 0.9733333333333334\n",
      "iteration no 3334: Loss: 0.2500670327851569, accuracy: 0.9733333333333334\n",
      "iteration no 3335: Loss: 0.25005805011261795, accuracy: 0.9733333333333334\n",
      "iteration no 3336: Loss: 0.25004926505399505, accuracy: 0.9733333333333334\n",
      "iteration no 3337: Loss: 0.25004022924804975, accuracy: 0.9733333333333334\n",
      "iteration no 3338: Loss: 0.25003041881346355, accuracy: 0.9733333333333334\n",
      "iteration no 3339: Loss: 0.250021758486069, accuracy: 0.9733333333333334\n",
      "iteration no 3340: Loss: 0.2500117772532993, accuracy: 0.9733333333333334\n",
      "iteration no 3341: Loss: 0.2500033344606757, accuracy: 0.9733333333333334\n",
      "iteration no 3342: Loss: 0.24999475095269166, accuracy: 0.9733333333333334\n",
      "iteration no 3343: Loss: 0.24998467080640396, accuracy: 0.9733333333333334\n",
      "iteration no 3344: Loss: 0.24997543918996745, accuracy: 0.9733333333333334\n",
      "iteration no 3345: Loss: 0.24996561639031517, accuracy: 0.9733333333333334\n",
      "iteration no 3346: Loss: 0.24995660877075382, accuracy: 0.9733333333333334\n",
      "iteration no 3347: Loss: 0.2499478002383359, accuracy: 0.9733333333333334\n",
      "iteration no 3348: Loss: 0.2499381266005526, accuracy: 0.9733333333333334\n",
      "iteration no 3349: Loss: 0.2499290752194263, accuracy: 0.9733333333333334\n",
      "iteration no 3350: Loss: 0.249919513055171, accuracy: 0.9733333333333334\n",
      "iteration no 3351: Loss: 0.24991121433732183, accuracy: 0.9733333333333334\n",
      "iteration no 3352: Loss: 0.2499015466375762, accuracy: 0.9733333333333334\n",
      "iteration no 3353: Loss: 0.24989302131376892, accuracy: 0.9733333333333334\n",
      "iteration no 3354: Loss: 0.24988344205238472, accuracy: 0.9733333333333334\n",
      "iteration no 3355: Loss: 0.24987382362097144, accuracy: 0.9733333333333334\n",
      "iteration no 3356: Loss: 0.2498649184797104, accuracy: 0.9733333333333334\n",
      "iteration no 3357: Loss: 0.24985554559070633, accuracy: 0.9733333333333334\n",
      "iteration no 3358: Loss: 0.24984733870625064, accuracy: 0.9733333333333334\n",
      "iteration no 3359: Loss: 0.24983725028402098, accuracy: 0.9733333333333334\n",
      "iteration no 3360: Loss: 0.2498284138562555, accuracy: 0.9733333333333334\n",
      "iteration no 3361: Loss: 0.24981955806605632, accuracy: 0.9733333333333334\n",
      "iteration no 3362: Loss: 0.2498105362227866, accuracy: 0.9733333333333334\n",
      "iteration no 3363: Loss: 0.24980229616249167, accuracy: 0.9733333333333334\n",
      "iteration no 3364: Loss: 0.24979179254712208, accuracy: 0.9733333333333334\n",
      "iteration no 3365: Loss: 0.24978318347358497, accuracy: 0.9733333333333334\n",
      "iteration no 3366: Loss: 0.24977313787063488, accuracy: 0.9733333333333334\n",
      "iteration no 3367: Loss: 0.24976492348174018, accuracy: 0.9733333333333334\n",
      "iteration no 3368: Loss: 0.2497559316736575, accuracy: 0.9733333333333334\n",
      "iteration no 3369: Loss: 0.2497462899752465, accuracy: 0.9733333333333334\n",
      "iteration no 3370: Loss: 0.24973693054820023, accuracy: 0.9733333333333334\n",
      "iteration no 3371: Loss: 0.24972788912485888, accuracy: 0.9733333333333334\n",
      "iteration no 3372: Loss: 0.24971973360433175, accuracy: 0.9733333333333334\n",
      "iteration no 3373: Loss: 0.249710177029652, accuracy: 0.9733333333333334\n",
      "iteration no 3374: Loss: 0.24970113205493796, accuracy: 0.9733333333333334\n",
      "iteration no 3375: Loss: 0.24969199873612724, accuracy: 0.9733333333333334\n",
      "iteration no 3376: Loss: 0.24968364330664966, accuracy: 0.9733333333333334\n",
      "iteration no 3377: Loss: 0.24967404792476802, accuracy: 0.9733333333333334\n",
      "iteration no 3378: Loss: 0.2496643249232781, accuracy: 0.9733333333333334\n",
      "iteration no 3379: Loss: 0.2496559817162519, accuracy: 0.9733333333333334\n",
      "iteration no 3380: Loss: 0.2496470596466508, accuracy: 0.9733333333333334\n",
      "iteration no 3381: Loss: 0.249638115684518, accuracy: 0.9733333333333334\n",
      "iteration no 3382: Loss: 0.24962810412367764, accuracy: 0.9733333333333334\n",
      "iteration no 3383: Loss: 0.24961955294686505, accuracy: 0.9733333333333334\n",
      "iteration no 3384: Loss: 0.2496113994450453, accuracy: 0.9733333333333334\n",
      "iteration no 3385: Loss: 0.24960177808095796, accuracy: 0.9733333333333334\n",
      "iteration no 3386: Loss: 0.24959285823255223, accuracy: 0.9733333333333334\n",
      "iteration no 3387: Loss: 0.24958402356770626, accuracy: 0.9733333333333334\n",
      "iteration no 3388: Loss: 0.24957603155906333, accuracy: 0.9733333333333334\n",
      "iteration no 3389: Loss: 0.24956571389402438, accuracy: 0.9733333333333334\n",
      "iteration no 3390: Loss: 0.24955738244819076, accuracy: 0.9733333333333334\n",
      "iteration no 3391: Loss: 0.24954888870919922, accuracy: 0.9733333333333334\n",
      "iteration no 3392: Loss: 0.2495395523851418, accuracy: 0.9733333333333334\n",
      "iteration no 3393: Loss: 0.24953026835429987, accuracy: 0.9733333333333334\n",
      "iteration no 3394: Loss: 0.24952165322545428, accuracy: 0.9733333333333334\n",
      "iteration no 3395: Loss: 0.24951345620753032, accuracy: 0.9733333333333334\n",
      "iteration no 3396: Loss: 0.24950334613216757, accuracy: 0.9733333333333334\n",
      "iteration no 3397: Loss: 0.24949485420842482, accuracy: 0.9733333333333334\n",
      "iteration no 3398: Loss: 0.2494862106843991, accuracy: 0.9733333333333334\n",
      "iteration no 3399: Loss: 0.2494778335198036, accuracy: 0.9733333333333334\n",
      "iteration no 3400: Loss: 0.24946826008852327, accuracy: 0.9733333333333334\n",
      "iteration no 3401: Loss: 0.249459428173122, accuracy: 0.9733333333333334\n",
      "iteration no 3402: Loss: 0.2494519146511316, accuracy: 0.9733333333333334\n",
      "iteration no 3403: Loss: 0.24944282777024346, accuracy: 0.9733333333333334\n",
      "iteration no 3404: Loss: 0.24943433836258946, accuracy: 0.9733333333333334\n",
      "iteration no 3405: Loss: 0.24942557654051223, accuracy: 0.9733333333333334\n",
      "iteration no 3406: Loss: 0.2494179645734453, accuracy: 0.9733333333333334\n",
      "iteration no 3407: Loss: 0.24940834831864367, accuracy: 0.9733333333333334\n",
      "iteration no 3408: Loss: 0.24939946846470928, accuracy: 0.9733333333333334\n",
      "iteration no 3409: Loss: 0.24939237063324182, accuracy: 0.9733333333333334\n",
      "iteration no 3410: Loss: 0.2493828804648458, accuracy: 0.9733333333333334\n",
      "iteration no 3411: Loss: 0.24937434367758038, accuracy: 0.9733333333333334\n",
      "iteration no 3412: Loss: 0.24936535121344666, accuracy: 0.9733333333333334\n",
      "iteration no 3413: Loss: 0.2493583632307299, accuracy: 0.9733333333333334\n",
      "iteration no 3414: Loss: 0.24934870113371252, accuracy: 0.9733333333333334\n",
      "iteration no 3415: Loss: 0.24934008579573075, accuracy: 0.9733333333333334\n",
      "iteration no 3416: Loss: 0.24933194416906507, accuracy: 0.9733333333333334\n",
      "iteration no 3417: Loss: 0.24932387339149414, accuracy: 0.9733333333333334\n",
      "iteration no 3418: Loss: 0.2493150456776756, accuracy: 0.9733333333333334\n",
      "iteration no 3419: Loss: 0.24930569255414323, accuracy: 0.9733333333333334\n",
      "iteration no 3420: Loss: 0.24929850430119155, accuracy: 0.9733333333333334\n",
      "iteration no 3421: Loss: 0.24928963224159378, accuracy: 0.9733333333333334\n",
      "iteration no 3422: Loss: 0.24928172430643128, accuracy: 0.9733333333333334\n",
      "iteration no 3423: Loss: 0.24927245448886134, accuracy: 0.9733333333333334\n",
      "iteration no 3424: Loss: 0.24926512218351937, accuracy: 0.9733333333333334\n",
      "iteration no 3425: Loss: 0.24925667275940305, accuracy: 0.9733333333333334\n",
      "iteration no 3426: Loss: 0.24924730761431985, accuracy: 0.9733333333333334\n",
      "iteration no 3427: Loss: 0.24923933097255382, accuracy: 0.9733333333333334\n",
      "iteration no 3428: Loss: 0.249231137379064, accuracy: 0.9733333333333334\n",
      "iteration no 3429: Loss: 0.2492228629663431, accuracy: 0.9733333333333334\n",
      "iteration no 3430: Loss: 0.2492135589006576, accuracy: 0.9733333333333334\n",
      "iteration no 3431: Loss: 0.24920579219667033, accuracy: 0.9733333333333334\n",
      "iteration no 3432: Loss: 0.24919791509218542, accuracy: 0.9733333333333334\n",
      "iteration no 3433: Loss: 0.24918884960647383, accuracy: 0.9733333333333334\n",
      "iteration no 3434: Loss: 0.24918042993257086, accuracy: 0.9733333333333334\n",
      "iteration no 3435: Loss: 0.24917203096924506, accuracy: 0.9766666666666667\n",
      "iteration no 3436: Loss: 0.24916477296885714, accuracy: 0.9733333333333334\n",
      "iteration no 3437: Loss: 0.24915540996624086, accuracy: 0.9766666666666667\n",
      "iteration no 3438: Loss: 0.24914696289741728, accuracy: 0.9766666666666667\n",
      "iteration no 3439: Loss: 0.24913912838701438, accuracy: 0.9766666666666667\n",
      "iteration no 3440: Loss: 0.24913125762149832, accuracy: 0.9733333333333334\n",
      "iteration no 3441: Loss: 0.2491224132451289, accuracy: 0.9766666666666667\n",
      "iteration no 3442: Loss: 0.24911356792149558, accuracy: 0.9766666666666667\n",
      "iteration no 3443: Loss: 0.2491062674144668, accuracy: 0.9766666666666667\n",
      "iteration no 3444: Loss: 0.2490977584386349, accuracy: 0.9733333333333334\n",
      "iteration no 3445: Loss: 0.24908993766666615, accuracy: 0.9766666666666667\n",
      "iteration no 3446: Loss: 0.249081070626838, accuracy: 0.9733333333333334\n",
      "iteration no 3447: Loss: 0.24907403697215877, accuracy: 0.9766666666666667\n",
      "iteration no 3448: Loss: 0.24906589203226132, accuracy: 0.9733333333333334\n",
      "iteration no 3449: Loss: 0.24905700240270476, accuracy: 0.9766666666666667\n",
      "iteration no 3450: Loss: 0.24904874598573082, accuracy: 0.9766666666666667\n",
      "iteration no 3451: Loss: 0.24904101564203823, accuracy: 0.9766666666666667\n",
      "iteration no 3452: Loss: 0.24903361939152968, accuracy: 0.9766666666666667\n",
      "iteration no 3453: Loss: 0.24902446541752715, accuracy: 0.9766666666666667\n",
      "iteration no 3454: Loss: 0.24901636454222734, accuracy: 0.9766666666666667\n",
      "iteration no 3455: Loss: 0.24900903446236078, accuracy: 0.9766666666666667\n",
      "iteration no 3456: Loss: 0.24900106835187513, accuracy: 0.9766666666666667\n",
      "iteration no 3457: Loss: 0.24899279694122786, accuracy: 0.9766666666666667\n",
      "iteration no 3458: Loss: 0.24898374181922528, accuracy: 0.9766666666666667\n",
      "iteration no 3459: Loss: 0.24897716966061595, accuracy: 0.9766666666666667\n",
      "iteration no 3460: Loss: 0.24896864391555262, accuracy: 0.9766666666666667\n",
      "iteration no 3461: Loss: 0.24896071123684105, accuracy: 0.9766666666666667\n",
      "iteration no 3462: Loss: 0.24895198318850376, accuracy: 0.9766666666666667\n",
      "iteration no 3463: Loss: 0.24894466669123166, accuracy: 0.9766666666666667\n",
      "iteration no 3464: Loss: 0.24893691813713648, accuracy: 0.9766666666666667\n",
      "iteration no 3465: Loss: 0.24892842285255823, accuracy: 0.9766666666666667\n",
      "iteration no 3466: Loss: 0.24892033164287897, accuracy: 0.9766666666666667\n",
      "iteration no 3467: Loss: 0.24891214220894337, accuracy: 0.9766666666666667\n",
      "iteration no 3468: Loss: 0.2489051980184946, accuracy: 0.9766666666666667\n",
      "iteration no 3469: Loss: 0.2488965895910038, accuracy: 0.9766666666666667\n",
      "iteration no 3470: Loss: 0.24888856741923993, accuracy: 0.9766666666666667\n",
      "iteration no 3471: Loss: 0.24888032871957233, accuracy: 0.9766666666666667\n",
      "iteration no 3472: Loss: 0.24887282184873466, accuracy: 0.9766666666666667\n",
      "iteration no 3473: Loss: 0.2488649343267228, accuracy: 0.9766666666666667\n",
      "iteration no 3474: Loss: 0.2488562130946861, accuracy: 0.9766666666666667\n",
      "iteration no 3475: Loss: 0.24884916976123947, accuracy: 0.9766666666666667\n",
      "iteration no 3476: Loss: 0.24884075053251103, accuracy: 0.9766666666666667\n",
      "iteration no 3477: Loss: 0.24883291642358224, accuracy: 0.9766666666666667\n",
      "iteration no 3478: Loss: 0.24882420444045095, accuracy: 0.9766666666666667\n",
      "iteration no 3479: Loss: 0.2488165784943659, accuracy: 0.9766666666666667\n",
      "iteration no 3480: Loss: 0.248809075812245, accuracy: 0.9766666666666667\n",
      "iteration no 3481: Loss: 0.24880038532252272, accuracy: 0.9766666666666667\n",
      "iteration no 3482: Loss: 0.24879228621805222, accuracy: 0.9766666666666667\n",
      "iteration no 3483: Loss: 0.2487836225568889, accuracy: 0.9766666666666667\n",
      "iteration no 3484: Loss: 0.248776882189988, accuracy: 0.9766666666666667\n",
      "iteration no 3485: Loss: 0.24876817755382252, accuracy: 0.9766666666666667\n",
      "iteration no 3486: Loss: 0.248760488228251, accuracy: 0.9766666666666667\n",
      "iteration no 3487: Loss: 0.2487517152554896, accuracy: 0.9766666666666667\n",
      "iteration no 3488: Loss: 0.24874416704055752, accuracy: 0.9766666666666667\n",
      "iteration no 3489: Loss: 0.24873638184264796, accuracy: 0.9766666666666667\n",
      "iteration no 3490: Loss: 0.2487279054431011, accuracy: 0.9766666666666667\n",
      "iteration no 3491: Loss: 0.24872001873624586, accuracy: 0.9766666666666667\n",
      "iteration no 3492: Loss: 0.2487122255360164, accuracy: 0.9766666666666667\n",
      "iteration no 3493: Loss: 0.24870498748368558, accuracy: 0.9766666666666667\n",
      "iteration no 3494: Loss: 0.2486961212670612, accuracy: 0.9766666666666667\n",
      "iteration no 3495: Loss: 0.2486884138507745, accuracy: 0.9766666666666667\n",
      "iteration no 3496: Loss: 0.24868008165442024, accuracy: 0.9766666666666667\n",
      "iteration no 3497: Loss: 0.24867305784269272, accuracy: 0.9766666666666667\n",
      "iteration no 3498: Loss: 0.24866452997217403, accuracy: 0.9766666666666667\n",
      "iteration no 3499: Loss: 0.24865662810549097, accuracy: 0.9766666666666667\n",
      "iteration no 3500: Loss: 0.24864901434026224, accuracy: 0.9766666666666667\n",
      "iteration no 3501: Loss: 0.24864122304800845, accuracy: 0.9766666666666667\n",
      "iteration no 3502: Loss: 0.2486336301456997, accuracy: 0.9766666666666667\n",
      "iteration no 3503: Loss: 0.24862469684321467, accuracy: 0.9766666666666667\n",
      "iteration no 3504: Loss: 0.24861759073838097, accuracy: 0.9766666666666667\n",
      "iteration no 3505: Loss: 0.24860973776721532, accuracy: 0.9766666666666667\n",
      "iteration no 3506: Loss: 0.2486022961475599, accuracy: 0.9766666666666667\n",
      "iteration no 3507: Loss: 0.24859388533944712, accuracy: 0.9766666666666667\n",
      "iteration no 3508: Loss: 0.24858569303598413, accuracy: 0.9766666666666667\n",
      "iteration no 3509: Loss: 0.24857792765360226, accuracy: 0.9766666666666667\n",
      "iteration no 3510: Loss: 0.2485703910536928, accuracy: 0.9766666666666667\n",
      "iteration no 3511: Loss: 0.24856281044854406, accuracy: 0.9766666666666667\n",
      "iteration no 3512: Loss: 0.2485541860404839, accuracy: 0.9766666666666667\n",
      "iteration no 3513: Loss: 0.24854682311613538, accuracy: 0.9766666666666667\n",
      "iteration no 3514: Loss: 0.2485388586016411, accuracy: 0.9766666666666667\n",
      "iteration no 3515: Loss: 0.24853129515941574, accuracy: 0.9766666666666667\n",
      "iteration no 3516: Loss: 0.24852296298845142, accuracy: 0.9766666666666667\n",
      "iteration no 3517: Loss: 0.248515633353523, accuracy: 0.9766666666666667\n",
      "iteration no 3518: Loss: 0.2485077165718389, accuracy: 0.9766666666666667\n",
      "iteration no 3519: Loss: 0.2484999525631167, accuracy: 0.9766666666666667\n",
      "iteration no 3520: Loss: 0.24849198057400312, accuracy: 0.9766666666666667\n",
      "iteration no 3521: Loss: 0.24848407473862466, accuracy: 0.9766666666666667\n",
      "iteration no 3522: Loss: 0.24847701228361668, accuracy: 0.9766666666666667\n",
      "iteration no 3523: Loss: 0.24846834657564051, accuracy: 0.9766666666666667\n",
      "iteration no 3524: Loss: 0.24846069264618753, accuracy: 0.9766666666666667\n",
      "iteration no 3525: Loss: 0.24845222909595538, accuracy: 0.9766666666666667\n",
      "iteration no 3526: Loss: 0.24844528143374783, accuracy: 0.9766666666666667\n",
      "iteration no 3527: Loss: 0.24843734930047118, accuracy: 0.9766666666666667\n",
      "iteration no 3528: Loss: 0.24842941508801575, accuracy: 0.9766666666666667\n",
      "iteration no 3529: Loss: 0.24842162830233078, accuracy: 0.9766666666666667\n",
      "iteration no 3530: Loss: 0.24841344366746082, accuracy: 0.9766666666666667\n",
      "iteration no 3531: Loss: 0.24840656367631841, accuracy: 0.9766666666666667\n",
      "iteration no 3532: Loss: 0.24839799393385023, accuracy: 0.9766666666666667\n",
      "iteration no 3533: Loss: 0.248390587909142, accuracy: 0.9766666666666667\n",
      "iteration no 3534: Loss: 0.24838206937776083, accuracy: 0.9766666666666667\n",
      "iteration no 3535: Loss: 0.24837552201431334, accuracy: 0.9766666666666667\n",
      "iteration no 3536: Loss: 0.24836704245032631, accuracy: 0.9766666666666667\n",
      "iteration no 3537: Loss: 0.2483592548886473, accuracy: 0.9766666666666667\n",
      "iteration no 3538: Loss: 0.24835135879202924, accuracy: 0.9766666666666667\n",
      "iteration no 3539: Loss: 0.2483442135284354, accuracy: 0.9766666666666667\n",
      "iteration no 3540: Loss: 0.24833653841819783, accuracy: 0.9766666666666667\n",
      "iteration no 3541: Loss: 0.24832817092815088, accuracy: 0.9766666666666667\n",
      "iteration no 3542: Loss: 0.24832085473651946, accuracy: 0.9766666666666667\n",
      "iteration no 3543: Loss: 0.2483134968192342, accuracy: 0.9766666666666667\n",
      "iteration no 3544: Loss: 0.2483062246700964, accuracy: 0.9766666666666667\n",
      "iteration no 3545: Loss: 0.24829751596368532, accuracy: 0.9766666666666667\n",
      "iteration no 3546: Loss: 0.24829052011594505, accuracy: 0.9766666666666667\n",
      "iteration no 3547: Loss: 0.24828318482158834, accuracy: 0.9766666666666667\n",
      "iteration no 3548: Loss: 0.24827566847141497, accuracy: 0.9766666666666667\n",
      "iteration no 3549: Loss: 0.24826770937784226, accuracy: 0.9766666666666667\n",
      "iteration no 3550: Loss: 0.2482598564015839, accuracy: 0.9766666666666667\n",
      "iteration no 3551: Loss: 0.2482522773173954, accuracy: 0.9766666666666667\n",
      "iteration no 3552: Loss: 0.2482449927121697, accuracy: 0.9766666666666667\n",
      "iteration no 3553: Loss: 0.2482378626884064, accuracy: 0.9766666666666667\n",
      "iteration no 3554: Loss: 0.24822952185411018, accuracy: 0.9766666666666667\n",
      "iteration no 3555: Loss: 0.24822262645280424, accuracy: 0.9766666666666667\n",
      "iteration no 3556: Loss: 0.24821432287040848, accuracy: 0.9766666666666667\n",
      "iteration no 3557: Loss: 0.2482075389592442, accuracy: 0.9766666666666667\n",
      "iteration no 3558: Loss: 0.2481998757113797, accuracy: 0.9766666666666667\n",
      "iteration no 3559: Loss: 0.24819204402396255, accuracy: 0.9766666666666667\n",
      "iteration no 3560: Loss: 0.24818456473639638, accuracy: 0.9766666666666667\n",
      "iteration no 3561: Loss: 0.24817700817153227, accuracy: 0.9766666666666667\n",
      "iteration no 3562: Loss: 0.2481702600955083, accuracy: 0.9766666666666667\n",
      "iteration no 3563: Loss: 0.2481619854790788, accuracy: 0.9766666666666667\n",
      "iteration no 3564: Loss: 0.2481548602375485, accuracy: 0.9766666666666667\n",
      "iteration no 3565: Loss: 0.24814695573830384, accuracy: 0.9766666666666667\n",
      "iteration no 3566: Loss: 0.24814051683879235, accuracy: 0.9766666666666667\n",
      "iteration no 3567: Loss: 0.24813228464937132, accuracy: 0.9766666666666667\n",
      "iteration no 3568: Loss: 0.24812492778641732, accuracy: 0.9766666666666667\n",
      "iteration no 3569: Loss: 0.24811733409805783, accuracy: 0.9766666666666667\n",
      "iteration no 3570: Loss: 0.24811026244785095, accuracy: 0.9766666666666667\n",
      "iteration no 3571: Loss: 0.2481029221443436, accuracy: 0.9766666666666667\n",
      "iteration no 3572: Loss: 0.24809502821541438, accuracy: 0.9766666666666667\n",
      "iteration no 3573: Loss: 0.24808772203338295, accuracy: 0.9766666666666667\n",
      "iteration no 3574: Loss: 0.24808008611638238, accuracy: 0.9766666666666667\n",
      "iteration no 3575: Loss: 0.24807362166648206, accuracy: 0.9766666666666667\n",
      "iteration no 3576: Loss: 0.24806534464754731, accuracy: 0.9766666666666667\n",
      "iteration no 3577: Loss: 0.24805874181370088, accuracy: 0.9766666666666667\n",
      "iteration no 3578: Loss: 0.2480507546327413, accuracy: 0.9766666666666667\n",
      "iteration no 3579: Loss: 0.24804433863356712, accuracy: 0.9766666666666667\n",
      "iteration no 3580: Loss: 0.2480367092154444, accuracy: 0.9766666666666667\n",
      "iteration no 3581: Loss: 0.24802921239056613, accuracy: 0.9766666666666667\n",
      "iteration no 3582: Loss: 0.24802193344998585, accuracy: 0.9766666666666667\n",
      "iteration no 3583: Loss: 0.24801465748495277, accuracy: 0.9766666666666667\n",
      "iteration no 3584: Loss: 0.24800784270637838, accuracy: 0.9766666666666667\n",
      "iteration no 3585: Loss: 0.2479999173151362, accuracy: 0.9766666666666667\n",
      "iteration no 3586: Loss: 0.24799295155007905, accuracy: 0.9766666666666667\n",
      "iteration no 3587: Loss: 0.24798536610191735, accuracy: 0.9766666666666667\n",
      "iteration no 3588: Loss: 0.2479788442469148, accuracy: 0.9766666666666667\n",
      "iteration no 3589: Loss: 0.24797126599206548, accuracy: 0.9766666666666667\n",
      "iteration no 3590: Loss: 0.24796398865945918, accuracy: 0.9766666666666667\n",
      "iteration no 3591: Loss: 0.24795627897235964, accuracy: 0.9766666666666667\n",
      "iteration no 3592: Loss: 0.24794992416594158, accuracy: 0.9766666666666667\n",
      "iteration no 3593: Loss: 0.247942662749958, accuracy: 0.9766666666666667\n",
      "iteration no 3594: Loss: 0.24793481776132203, accuracy: 0.9766666666666667\n",
      "iteration no 3595: Loss: 0.2479282003619472, accuracy: 0.9766666666666667\n",
      "iteration no 3596: Loss: 0.2479206572930215, accuracy: 0.9766666666666667\n",
      "iteration no 3597: Loss: 0.24791418466793172, accuracy: 0.9766666666666667\n",
      "iteration no 3598: Loss: 0.2479063931226561, accuracy: 0.9766666666666667\n",
      "iteration no 3599: Loss: 0.2478994607310817, accuracy: 0.9766666666666667\n",
      "iteration no 3600: Loss: 0.2478918580156297, accuracy: 0.9766666666666667\n",
      "iteration no 3601: Loss: 0.24788552058222102, accuracy: 0.9766666666666667\n",
      "iteration no 3602: Loss: 0.24787801035267001, accuracy: 0.9766666666666667\n",
      "iteration no 3603: Loss: 0.24787061264407584, accuracy: 0.9766666666666667\n",
      "iteration no 3604: Loss: 0.24786330586059963, accuracy: 0.9766666666666667\n",
      "iteration no 3605: Loss: 0.24785656578076926, accuracy: 0.9766666666666667\n",
      "iteration no 3606: Loss: 0.24784970660008315, accuracy: 0.9766666666666667\n",
      "iteration no 3607: Loss: 0.2478422501892596, accuracy: 0.9766666666666667\n",
      "iteration no 3608: Loss: 0.24783535696856207, accuracy: 0.9766666666666667\n",
      "iteration no 3609: Loss: 0.2478273538922046, accuracy: 0.9766666666666667\n",
      "iteration no 3610: Loss: 0.24782111513431174, accuracy: 0.9766666666666667\n",
      "iteration no 3611: Loss: 0.24781368241910587, accuracy: 0.9766666666666667\n",
      "iteration no 3612: Loss: 0.24780715213351245, accuracy: 0.9766666666666667\n",
      "iteration no 3613: Loss: 0.24779930316293275, accuracy: 0.9766666666666667\n",
      "iteration no 3614: Loss: 0.2477922648868372, accuracy: 0.9766666666666667\n",
      "iteration no 3615: Loss: 0.24778557049134736, accuracy: 0.9766666666666667\n",
      "iteration no 3616: Loss: 0.24777844794252035, accuracy: 0.9766666666666667\n",
      "iteration no 3617: Loss: 0.2477713233754374, accuracy: 0.9766666666666667\n",
      "iteration no 3618: Loss: 0.2477638293990953, accuracy: 0.9766666666666667\n",
      "iteration no 3619: Loss: 0.24775680877950024, accuracy: 0.9766666666666667\n",
      "iteration no 3620: Loss: 0.24774967554815455, accuracy: 0.9766666666666667\n",
      "iteration no 3621: Loss: 0.24774342658966514, accuracy: 0.9766666666666667\n",
      "iteration no 3622: Loss: 0.2477354775167786, accuracy: 0.9766666666666667\n",
      "iteration no 3623: Loss: 0.24772862424355085, accuracy: 0.9766666666666667\n",
      "iteration no 3624: Loss: 0.24772087934907686, accuracy: 0.9766666666666667\n",
      "iteration no 3625: Loss: 0.24771501888409547, accuracy: 0.9766666666666667\n",
      "iteration no 3626: Loss: 0.24770749525383168, accuracy: 0.9766666666666667\n",
      "iteration no 3627: Loss: 0.2477004974278056, accuracy: 0.9766666666666667\n",
      "iteration no 3628: Loss: 0.24769296759122572, accuracy: 0.9766666666666667\n",
      "iteration no 3629: Loss: 0.24768654100552462, accuracy: 0.9766666666666667\n",
      "iteration no 3630: Loss: 0.24767967603306762, accuracy: 0.9766666666666667\n",
      "iteration no 3631: Loss: 0.24767221571274178, accuracy: 0.9766666666666667\n",
      "iteration no 3632: Loss: 0.24766530935976233, accuracy: 0.9766666666666667\n",
      "iteration no 3633: Loss: 0.24765766003310236, accuracy: 0.9766666666666667\n",
      "iteration no 3634: Loss: 0.24765134941204026, accuracy: 0.9766666666666667\n",
      "iteration no 3635: Loss: 0.24764405888935076, accuracy: 0.9766666666666667\n",
      "iteration no 3636: Loss: 0.2476373234864811, accuracy: 0.9766666666666667\n",
      "iteration no 3637: Loss: 0.2476297772083814, accuracy: 0.9766666666666667\n",
      "iteration no 3638: Loss: 0.24762312003687584, accuracy: 0.9766666666666667\n",
      "iteration no 3639: Loss: 0.24761630679949018, accuracy: 0.9766666666666667\n",
      "iteration no 3640: Loss: 0.2476098859934298, accuracy: 0.9766666666666667\n",
      "iteration no 3641: Loss: 0.24760224288578678, accuracy: 0.9766666666666667\n",
      "iteration no 3642: Loss: 0.2475952221772384, accuracy: 0.9766666666666667\n",
      "iteration no 3643: Loss: 0.24758806238602593, accuracy: 0.9766666666666667\n",
      "iteration no 3644: Loss: 0.24758108882960056, accuracy: 0.9766666666666667\n",
      "iteration no 3645: Loss: 0.2475747323761523, accuracy: 0.9766666666666667\n",
      "iteration no 3646: Loss: 0.24756714557059012, accuracy: 0.9766666666666667\n",
      "iteration no 3647: Loss: 0.24756033562930801, accuracy: 0.9766666666666667\n",
      "iteration no 3648: Loss: 0.24755286356637413, accuracy: 0.9766666666666667\n",
      "iteration no 3649: Loss: 0.2475469222839904, accuracy: 0.9766666666666667\n",
      "iteration no 3650: Loss: 0.24753948432668507, accuracy: 0.9766666666666667\n",
      "iteration no 3651: Loss: 0.24753328001917818, accuracy: 0.9766666666666667\n",
      "iteration no 3652: Loss: 0.2475253926983172, accuracy: 0.9766666666666667\n",
      "iteration no 3653: Loss: 0.24751885080162245, accuracy: 0.9766666666666667\n",
      "iteration no 3654: Loss: 0.24751178905620125, accuracy: 0.9766666666666667\n",
      "iteration no 3655: Loss: 0.24750528950819656, accuracy: 0.9766666666666667\n",
      "iteration no 3656: Loss: 0.24749788310368942, accuracy: 0.9766666666666667\n",
      "iteration no 3657: Loss: 0.2474910037564237, accuracy: 0.9766666666666667\n",
      "iteration no 3658: Loss: 0.2474837248442443, accuracy: 0.9766666666666667\n",
      "iteration no 3659: Loss: 0.24747762540290047, accuracy: 0.9766666666666667\n",
      "iteration no 3660: Loss: 0.2474707234718289, accuracy: 0.9766666666666667\n",
      "iteration no 3661: Loss: 0.24746339061957046, accuracy: 0.9766666666666667\n",
      "iteration no 3662: Loss: 0.24745693363177687, accuracy: 0.9766666666666667\n",
      "iteration no 3663: Loss: 0.24744965017020068, accuracy: 0.9766666666666667\n",
      "iteration no 3664: Loss: 0.24744308220231276, accuracy: 0.9766666666666667\n",
      "iteration no 3665: Loss: 0.2474362094505106, accuracy: 0.9766666666666667\n",
      "iteration no 3666: Loss: 0.24742966256501403, accuracy: 0.9766666666666667\n",
      "iteration no 3667: Loss: 0.2474218351495817, accuracy: 0.9766666666666667\n",
      "iteration no 3668: Loss: 0.2474157959447766, accuracy: 0.9766666666666667\n",
      "iteration no 3669: Loss: 0.24740848919602265, accuracy: 0.9766666666666667\n",
      "iteration no 3670: Loss: 0.24740237941282117, accuracy: 0.9766666666666667\n",
      "iteration no 3671: Loss: 0.2473951006339326, accuracy: 0.9766666666666667\n",
      "iteration no 3672: Loss: 0.24738822924247064, accuracy: 0.9766666666666667\n",
      "iteration no 3673: Loss: 0.24738080082889163, accuracy: 0.9766666666666667\n",
      "iteration no 3674: Loss: 0.2473744816227572, accuracy: 0.9766666666666667\n",
      "iteration no 3675: Loss: 0.24736807921922666, accuracy: 0.9766666666666667\n",
      "iteration no 3676: Loss: 0.24736081889687317, accuracy: 0.9766666666666667\n",
      "iteration no 3677: Loss: 0.24735427428814727, accuracy: 0.9766666666666667\n",
      "iteration no 3678: Loss: 0.2473468367191854, accuracy: 0.9766666666666667\n",
      "iteration no 3679: Loss: 0.24734054758980872, accuracy: 0.9766666666666667\n",
      "iteration no 3680: Loss: 0.2473338057386319, accuracy: 0.9766666666666667\n",
      "iteration no 3681: Loss: 0.2473271607845815, accuracy: 0.9766666666666667\n",
      "iteration no 3682: Loss: 0.24731969233183357, accuracy: 0.9766666666666667\n",
      "iteration no 3683: Loss: 0.24731317435621092, accuracy: 0.9766666666666667\n",
      "iteration no 3684: Loss: 0.24730558092288293, accuracy: 0.9766666666666667\n",
      "iteration no 3685: Loss: 0.24730033714493838, accuracy: 0.9766666666666667\n",
      "iteration no 3686: Loss: 0.24729325727327642, accuracy: 0.9766666666666667\n",
      "iteration no 3687: Loss: 0.2472863055143635, accuracy: 0.9766666666666667\n",
      "iteration no 3688: Loss: 0.2472790807304201, accuracy: 0.9766666666666667\n",
      "iteration no 3689: Loss: 0.24727254976818103, accuracy: 0.9766666666666667\n",
      "iteration no 3690: Loss: 0.24726591100144713, accuracy: 0.9766666666666667\n",
      "iteration no 3691: Loss: 0.24725938771203607, accuracy: 0.9766666666666667\n",
      "iteration no 3692: Loss: 0.24725313509349395, accuracy: 0.9766666666666667\n",
      "iteration no 3693: Loss: 0.24724589843644934, accuracy: 0.9766666666666667\n",
      "iteration no 3694: Loss: 0.24723944396505523, accuracy: 0.9766666666666667\n",
      "iteration no 3695: Loss: 0.24723329001854377, accuracy: 0.9766666666666667\n",
      "iteration no 3696: Loss: 0.24722702483722195, accuracy: 0.9766666666666667\n",
      "iteration no 3697: Loss: 0.2472201276442492, accuracy: 0.9766666666666667\n",
      "iteration no 3698: Loss: 0.24721406748560376, accuracy: 0.9766666666666667\n",
      "iteration no 3699: Loss: 0.24720684579122076, accuracy: 0.9766666666666667\n",
      "iteration no 3700: Loss: 0.2472010021365813, accuracy: 0.9766666666666667\n",
      "iteration no 3701: Loss: 0.24719440096271195, accuracy: 0.9766666666666667\n",
      "iteration no 3702: Loss: 0.24718826368244085, accuracy: 0.9766666666666667\n",
      "iteration no 3703: Loss: 0.24718144309088283, accuracy: 0.9766666666666667\n",
      "iteration no 3704: Loss: 0.24717495628574238, accuracy: 0.9766666666666667\n",
      "iteration no 3705: Loss: 0.24716903600524648, accuracy: 0.9766666666666667\n",
      "iteration no 3706: Loss: 0.24716261741473544, accuracy: 0.9766666666666667\n",
      "iteration no 3707: Loss: 0.24715626915616612, accuracy: 0.9766666666666667\n",
      "iteration no 3708: Loss: 0.24714932730325423, accuracy: 0.9766666666666667\n",
      "iteration no 3709: Loss: 0.24714311885958354, accuracy: 0.9766666666666667\n",
      "iteration no 3710: Loss: 0.24713665755763778, accuracy: 0.9766666666666667\n",
      "iteration no 3711: Loss: 0.2471307510991186, accuracy: 0.9766666666666667\n",
      "iteration no 3712: Loss: 0.2471239071580883, accuracy: 0.9766666666666667\n",
      "iteration no 3713: Loss: 0.24711741586019587, accuracy: 0.9766666666666667\n",
      "iteration no 3714: Loss: 0.2471110757891833, accuracy: 0.9766666666666667\n",
      "iteration no 3715: Loss: 0.2471053198819944, accuracy: 0.9766666666666667\n",
      "iteration no 3716: Loss: 0.24709838433970455, accuracy: 0.9766666666666667\n",
      "iteration no 3717: Loss: 0.24709250873591404, accuracy: 0.9766666666666667\n",
      "iteration no 3718: Loss: 0.24708511688211454, accuracy: 0.9766666666666667\n",
      "iteration no 3719: Loss: 0.2470799800610468, accuracy: 0.9766666666666667\n",
      "iteration no 3720: Loss: 0.24707311305964355, accuracy: 0.9766666666666667\n",
      "iteration no 3721: Loss: 0.24706727067293996, accuracy: 0.9766666666666667\n",
      "iteration no 3722: Loss: 0.24706020461261305, accuracy: 0.9766666666666667\n",
      "iteration no 3723: Loss: 0.24705379035154343, accuracy: 0.9766666666666667\n",
      "iteration no 3724: Loss: 0.24704779902106871, accuracy: 0.9766666666666667\n",
      "iteration no 3725: Loss: 0.24704153100616094, accuracy: 0.9766666666666667\n",
      "iteration no 3726: Loss: 0.24703531360516062, accuracy: 0.9766666666666667\n",
      "iteration no 3727: Loss: 0.24702876836781543, accuracy: 0.9766666666666667\n",
      "iteration no 3728: Loss: 0.24702219468525638, accuracy: 0.9766666666666667\n",
      "iteration no 3729: Loss: 0.2470164028285673, accuracy: 0.9766666666666667\n",
      "iteration no 3730: Loss: 0.24701054797190608, accuracy: 0.9766666666666667\n",
      "iteration no 3731: Loss: 0.2470035822505144, accuracy: 0.9766666666666667\n",
      "iteration no 3732: Loss: 0.2469977057436498, accuracy: 0.9766666666666667\n",
      "iteration no 3733: Loss: 0.24699092013942026, accuracy: 0.9766666666666667\n",
      "iteration no 3734: Loss: 0.2469852845846606, accuracy: 0.9766666666666667\n",
      "iteration no 3735: Loss: 0.24697883581557808, accuracy: 0.9766666666666667\n",
      "iteration no 3736: Loss: 0.2469731118200007, accuracy: 0.9766666666666667\n",
      "iteration no 3737: Loss: 0.24696607631536088, accuracy: 0.9766666666666667\n",
      "iteration no 3738: Loss: 0.24696058433001894, accuracy: 0.9766666666666667\n",
      "iteration no 3739: Loss: 0.24695420299787701, accuracy: 0.9766666666666667\n",
      "iteration no 3740: Loss: 0.24694877371223703, accuracy: 0.9766666666666667\n",
      "iteration no 3741: Loss: 0.2469418126896662, accuracy: 0.9766666666666667\n",
      "iteration no 3742: Loss: 0.2469358936165174, accuracy: 0.9766666666666667\n",
      "iteration no 3743: Loss: 0.24692908345365489, accuracy: 0.9766666666666667\n",
      "iteration no 3744: Loss: 0.24692366196890242, accuracy: 0.9766666666666667\n",
      "iteration no 3745: Loss: 0.2469172690784601, accuracy: 0.9766666666666667\n",
      "iteration no 3746: Loss: 0.246911245357666, accuracy: 0.9766666666666667\n",
      "iteration no 3747: Loss: 0.2469049575781166, accuracy: 0.9766666666666667\n",
      "iteration no 3748: Loss: 0.24689868618263094, accuracy: 0.9766666666666667\n",
      "iteration no 3749: Loss: 0.24689313053547104, accuracy: 0.9766666666666667\n",
      "iteration no 3750: Loss: 0.24688717439580704, accuracy: 0.9766666666666667\n",
      "iteration no 3751: Loss: 0.24688082539076364, accuracy: 0.9766666666666667\n",
      "iteration no 3752: Loss: 0.24687445615982462, accuracy: 0.9766666666666667\n",
      "iteration no 3753: Loss: 0.24686846703842424, accuracy: 0.9766666666666667\n",
      "iteration no 3754: Loss: 0.24686219084420435, accuracy: 0.9766666666666667\n",
      "iteration no 3755: Loss: 0.24685666363632774, accuracy: 0.9766666666666667\n",
      "iteration no 3756: Loss: 0.24685034581505377, accuracy: 0.9766666666666667\n",
      "iteration no 3757: Loss: 0.2468441981629959, accuracy: 0.9766666666666667\n",
      "iteration no 3758: Loss: 0.24683816186982654, accuracy: 0.9766666666666667\n",
      "iteration no 3759: Loss: 0.24683257647501325, accuracy: 0.9766666666666667\n",
      "iteration no 3760: Loss: 0.24682645020005872, accuracy: 0.9766666666666667\n",
      "iteration no 3761: Loss: 0.246821227341983, accuracy: 0.9766666666666667\n",
      "iteration no 3762: Loss: 0.246814336431009, accuracy: 0.9766666666666667\n",
      "iteration no 3763: Loss: 0.24680875331539442, accuracy: 0.9766666666666667\n",
      "iteration no 3764: Loss: 0.24680308792221162, accuracy: 0.9766666666666667\n",
      "iteration no 3765: Loss: 0.2467976914847798, accuracy: 0.9766666666666667\n",
      "iteration no 3766: Loss: 0.24679119963697624, accuracy: 0.9766666666666667\n",
      "iteration no 3767: Loss: 0.24678580494401386, accuracy: 0.9766666666666667\n",
      "iteration no 3768: Loss: 0.24677909241064433, accuracy: 0.9766666666666667\n",
      "iteration no 3769: Loss: 0.24677397833257586, accuracy: 0.9766666666666667\n",
      "iteration no 3770: Loss: 0.2467679317102492, accuracy: 0.9766666666666667\n",
      "iteration no 3771: Loss: 0.24676247644695448, accuracy: 0.9766666666666667\n",
      "iteration no 3772: Loss: 0.2467558776437694, accuracy: 0.9766666666666667\n",
      "iteration no 3773: Loss: 0.24675058859040644, accuracy: 0.9766666666666667\n",
      "iteration no 3774: Loss: 0.24674434184497562, accuracy: 0.9766666666666667\n",
      "iteration no 3775: Loss: 0.24673944373633408, accuracy: 0.9766666666666667\n",
      "iteration no 3776: Loss: 0.24673295636079162, accuracy: 0.9766666666666667\n",
      "iteration no 3777: Loss: 0.24672732668892056, accuracy: 0.9766666666666667\n",
      "iteration no 3778: Loss: 0.24672126207426187, accuracy: 0.9766666666666667\n",
      "iteration no 3779: Loss: 0.24671580780810287, accuracy: 0.9766666666666667\n",
      "iteration no 3780: Loss: 0.24671003476226006, accuracy: 0.9766666666666667\n",
      "iteration no 3781: Loss: 0.24670421472296555, accuracy: 0.9766666666666667\n",
      "iteration no 3782: Loss: 0.24669816983857623, accuracy: 0.9766666666666667\n",
      "iteration no 3783: Loss: 0.2466923680452784, accuracy: 0.9766666666666667\n",
      "iteration no 3784: Loss: 0.24668701040207233, accuracy: 0.9766666666666667\n",
      "iteration no 3785: Loss: 0.24668130840703684, accuracy: 0.9766666666666667\n",
      "iteration no 3786: Loss: 0.24667549578130799, accuracy: 0.9766666666666667\n",
      "iteration no 3787: Loss: 0.24666938727364984, accuracy: 0.9766666666666667\n",
      "iteration no 3788: Loss: 0.24666364171941282, accuracy: 0.9766666666666667\n",
      "iteration no 3789: Loss: 0.24665760732308023, accuracy: 0.9766666666666667\n",
      "iteration no 3790: Loss: 0.24665259197905592, accuracy: 0.9766666666666667\n",
      "iteration no 3791: Loss: 0.24664637228086905, accuracy: 0.9766666666666667\n",
      "iteration no 3792: Loss: 0.24664094383889013, accuracy: 0.9766666666666667\n",
      "iteration no 3793: Loss: 0.24663463459544338, accuracy: 0.9766666666666667\n",
      "iteration no 3794: Loss: 0.24662912868912495, accuracy: 0.9766666666666667\n",
      "iteration no 3795: Loss: 0.2466234766695914, accuracy: 0.9766666666666667\n",
      "iteration no 3796: Loss: 0.24661810480717744, accuracy: 0.9766666666666667\n",
      "iteration no 3797: Loss: 0.24661184970696493, accuracy: 0.9766666666666667\n",
      "iteration no 3798: Loss: 0.24660646774861983, accuracy: 0.9766666666666667\n",
      "iteration no 3799: Loss: 0.24660008061415845, accuracy: 0.9766666666666667\n",
      "iteration no 3800: Loss: 0.2465955752662332, accuracy: 0.9766666666666667\n",
      "iteration no 3801: Loss: 0.2465890393156021, accuracy: 0.9766666666666667\n",
      "iteration no 3802: Loss: 0.24658380154833076, accuracy: 0.9766666666666667\n",
      "iteration no 3803: Loss: 0.24657732440281283, accuracy: 0.9766666666666667\n",
      "iteration no 3804: Loss: 0.2465720617872971, accuracy: 0.9766666666666667\n",
      "iteration no 3805: Loss: 0.24656648485360774, accuracy: 0.9766666666666667\n",
      "iteration no 3806: Loss: 0.2465610105455226, accuracy: 0.9766666666666667\n",
      "iteration no 3807: Loss: 0.2465547022027349, accuracy: 0.9766666666666667\n",
      "iteration no 3808: Loss: 0.24654926269722963, accuracy: 0.9766666666666667\n",
      "iteration no 3809: Loss: 0.24654342167551468, accuracy: 0.9766666666666667\n",
      "iteration no 3810: Loss: 0.24653792891620213, accuracy: 0.9766666666666667\n",
      "iteration no 3811: Loss: 0.2465321222884269, accuracy: 0.9766666666666667\n",
      "iteration no 3812: Loss: 0.2465260686406512, accuracy: 0.9766666666666667\n",
      "iteration no 3813: Loss: 0.24652020012893816, accuracy: 0.9766666666666667\n",
      "iteration no 3814: Loss: 0.2465145724862742, accuracy: 0.9766666666666667\n",
      "iteration no 3815: Loss: 0.24650924853181141, accuracy: 0.9766666666666667\n",
      "iteration no 3816: Loss: 0.24650294180799262, accuracy: 0.9766666666666667\n",
      "iteration no 3817: Loss: 0.24649757378121973, accuracy: 0.9766666666666667\n",
      "iteration no 3818: Loss: 0.24649122793772, accuracy: 0.9766666666666667\n",
      "iteration no 3819: Loss: 0.246485983314854, accuracy: 0.9766666666666667\n",
      "iteration no 3820: Loss: 0.2464801633027352, accuracy: 0.9766666666666667\n",
      "iteration no 3821: Loss: 0.24647466929267475, accuracy: 0.98\n",
      "iteration no 3822: Loss: 0.24646838251587572, accuracy: 0.9766666666666667\n",
      "iteration no 3823: Loss: 0.24646317744690988, accuracy: 0.98\n",
      "iteration no 3824: Loss: 0.24645683682706254, accuracy: 0.9766666666666667\n",
      "iteration no 3825: Loss: 0.24645200178480803, accuracy: 0.9766666666666667\n",
      "iteration no 3826: Loss: 0.2464455495815191, accuracy: 0.9766666666666667\n",
      "iteration no 3827: Loss: 0.24644018555153624, accuracy: 0.9766666666666667\n",
      "iteration no 3828: Loss: 0.24643385846640145, accuracy: 0.9766666666666667\n",
      "iteration no 3829: Loss: 0.24642860174009018, accuracy: 0.9766666666666667\n",
      "iteration no 3830: Loss: 0.24642273161567085, accuracy: 0.98\n",
      "iteration no 3831: Loss: 0.2464172015220638, accuracy: 0.9766666666666667\n",
      "iteration no 3832: Loss: 0.24641118960354802, accuracy: 0.9766666666666667\n",
      "iteration no 3833: Loss: 0.24640525889509962, accuracy: 0.9766666666666667\n",
      "iteration no 3834: Loss: 0.2463997286600671, accuracy: 0.98\n",
      "iteration no 3835: Loss: 0.24639426239317633, accuracy: 0.98\n",
      "iteration no 3836: Loss: 0.2463883760922374, accuracy: 0.98\n",
      "iteration no 3837: Loss: 0.24638257722863727, accuracy: 0.9766666666666667\n",
      "iteration no 3838: Loss: 0.24637689942872948, accuracy: 0.98\n",
      "iteration no 3839: Loss: 0.24637079056855604, accuracy: 0.98\n",
      "iteration no 3840: Loss: 0.24636596925673504, accuracy: 0.98\n",
      "iteration no 3841: Loss: 0.24635960398419676, accuracy: 0.98\n",
      "iteration no 3842: Loss: 0.2463541443375132, accuracy: 0.98\n",
      "iteration no 3843: Loss: 0.2463479806384858, accuracy: 0.98\n",
      "iteration no 3844: Loss: 0.2463425876032512, accuracy: 0.98\n",
      "iteration no 3845: Loss: 0.24633681918723416, accuracy: 0.98\n",
      "iteration no 3846: Loss: 0.24633168697158517, accuracy: 0.98\n",
      "iteration no 3847: Loss: 0.24632518556081767, accuracy: 0.98\n",
      "iteration no 3848: Loss: 0.24632002484015364, accuracy: 0.98\n",
      "iteration no 3849: Loss: 0.2463136197480858, accuracy: 0.98\n",
      "iteration no 3850: Loss: 0.24630866470523466, accuracy: 0.98\n",
      "iteration no 3851: Loss: 0.24630265095140746, accuracy: 0.98\n",
      "iteration no 3852: Loss: 0.24629722761225736, accuracy: 0.98\n",
      "iteration no 3853: Loss: 0.24629102586375204, accuracy: 0.98\n",
      "iteration no 3854: Loss: 0.24628560478019002, accuracy: 0.98\n",
      "iteration no 3855: Loss: 0.2462798867164018, accuracy: 0.98\n",
      "iteration no 3856: Loss: 0.24627436929016017, accuracy: 0.98\n",
      "iteration no 3857: Loss: 0.24626869096375306, accuracy: 0.98\n",
      "iteration no 3858: Loss: 0.2462627297334708, accuracy: 0.98\n",
      "iteration no 3859: Loss: 0.24625701489422774, accuracy: 0.98\n",
      "iteration no 3860: Loss: 0.24625138504938515, accuracy: 0.98\n",
      "iteration no 3861: Loss: 0.24624620847528228, accuracy: 0.98\n",
      "iteration no 3862: Loss: 0.24624003663483152, accuracy: 0.98\n",
      "iteration no 3863: Loss: 0.24623473363236892, accuracy: 0.98\n",
      "iteration no 3864: Loss: 0.24622847295840067, accuracy: 0.98\n",
      "iteration no 3865: Loss: 0.24622309520500574, accuracy: 0.98\n",
      "iteration no 3866: Loss: 0.24621760872389875, accuracy: 0.98\n",
      "iteration no 3867: Loss: 0.24621228556086716, accuracy: 0.98\n",
      "iteration no 3868: Loss: 0.24620595116633642, accuracy: 0.98\n",
      "iteration no 3869: Loss: 0.2462009267975811, accuracy: 0.98\n",
      "iteration no 3870: Loss: 0.2461943785256109, accuracy: 0.98\n",
      "iteration no 3871: Loss: 0.24618979267498456, accuracy: 0.98\n",
      "iteration no 3872: Loss: 0.24618379689343678, accuracy: 0.98\n",
      "iteration no 3873: Loss: 0.24617826197631887, accuracy: 0.98\n",
      "iteration no 3874: Loss: 0.24617232218623852, accuracy: 0.98\n",
      "iteration no 3875: Loss: 0.24616676914952879, accuracy: 0.98\n",
      "iteration no 3876: Loss: 0.24616106612087463, accuracy: 0.98\n",
      "iteration no 3877: Loss: 0.24615586627285757, accuracy: 0.98\n",
      "iteration no 3878: Loss: 0.24615011052196578, accuracy: 0.98\n",
      "iteration no 3879: Loss: 0.24614422946225228, accuracy: 0.98\n",
      "iteration no 3880: Loss: 0.24613869648995715, accuracy: 0.98\n",
      "iteration no 3881: Loss: 0.24613286044443122, accuracy: 0.98\n",
      "iteration no 3882: Loss: 0.24612779289789896, accuracy: 0.98\n",
      "iteration no 3883: Loss: 0.2461219049271729, accuracy: 0.98\n",
      "iteration no 3884: Loss: 0.2461164724439102, accuracy: 0.98\n",
      "iteration no 3885: Loss: 0.24611024694853775, accuracy: 0.98\n",
      "iteration no 3886: Loss: 0.24610519373511275, accuracy: 0.98\n",
      "iteration no 3887: Loss: 0.2460993898681055, accuracy: 0.98\n",
      "iteration no 3888: Loss: 0.24609421949846177, accuracy: 0.98\n",
      "iteration no 3889: Loss: 0.2460880790657413, accuracy: 0.98\n",
      "iteration no 3890: Loss: 0.24608280240484967, accuracy: 0.98\n",
      "iteration no 3891: Loss: 0.2460765400366431, accuracy: 0.98\n",
      "iteration no 3892: Loss: 0.24607171552037882, accuracy: 0.98\n",
      "iteration no 3893: Loss: 0.24606582076072875, accuracy: 0.98\n",
      "iteration no 3894: Loss: 0.24606049895750726, accuracy: 0.98\n",
      "iteration no 3895: Loss: 0.2460544954582597, accuracy: 0.98\n",
      "iteration no 3896: Loss: 0.24604897309333273, accuracy: 0.98\n",
      "iteration no 3897: Loss: 0.2460432931113704, accuracy: 0.98\n",
      "iteration no 3898: Loss: 0.24603831560476747, accuracy: 0.98\n",
      "iteration no 3899: Loss: 0.24603257387797867, accuracy: 0.98\n",
      "iteration no 3900: Loss: 0.24602695732812743, accuracy: 0.98\n",
      "iteration no 3901: Loss: 0.24602151960031793, accuracy: 0.98\n",
      "iteration no 3902: Loss: 0.24601559215690594, accuracy: 0.98\n",
      "iteration no 3903: Loss: 0.24601088526396747, accuracy: 0.98\n",
      "iteration no 3904: Loss: 0.2460049969534993, accuracy: 0.98\n",
      "iteration no 3905: Loss: 0.24599956217393582, accuracy: 0.98\n",
      "iteration no 3906: Loss: 0.24599381947443244, accuracy: 0.98\n",
      "iteration no 3907: Loss: 0.24598853162560008, accuracy: 0.98\n",
      "iteration no 3908: Loss: 0.24598274968950815, accuracy: 0.98\n",
      "iteration no 3909: Loss: 0.24597816297184455, accuracy: 0.98\n",
      "iteration no 3910: Loss: 0.24597182891092262, accuracy: 0.98\n",
      "iteration no 3911: Loss: 0.24596680881165844, accuracy: 0.98\n",
      "iteration no 3912: Loss: 0.24596063232461035, accuracy: 0.98\n",
      "iteration no 3913: Loss: 0.2459557850102979, accuracy: 0.98\n",
      "iteration no 3914: Loss: 0.24595005531769742, accuracy: 0.98\n",
      "iteration no 3915: Loss: 0.2459451310977064, accuracy: 0.98\n",
      "iteration no 3916: Loss: 0.24593892855431432, accuracy: 0.98\n",
      "iteration no 3917: Loss: 0.2459338287837754, accuracy: 0.98\n",
      "iteration no 3918: Loss: 0.24592800681784657, accuracy: 0.98\n",
      "iteration no 3919: Loss: 0.2459231196197446, accuracy: 0.98\n",
      "iteration no 3920: Loss: 0.24591754537931343, accuracy: 0.98\n",
      "iteration no 3921: Loss: 0.24591195049182213, accuracy: 0.98\n",
      "iteration no 3922: Loss: 0.24590626066573035, accuracy: 0.98\n",
      "iteration no 3923: Loss: 0.24590077111143782, accuracy: 0.98\n",
      "iteration no 3924: Loss: 0.24589567308764315, accuracy: 0.98\n",
      "iteration no 3925: Loss: 0.2458900339873858, accuracy: 0.98\n",
      "iteration no 3926: Loss: 0.24588469995542195, accuracy: 0.98\n",
      "iteration no 3927: Loss: 0.24587879167012927, accuracy: 0.98\n",
      "iteration no 3928: Loss: 0.2458737940556679, accuracy: 0.98\n",
      "iteration no 3929: Loss: 0.24586801765591143, accuracy: 0.98\n",
      "iteration no 3930: Loss: 0.2458631800315671, accuracy: 0.98\n",
      "iteration no 3931: Loss: 0.2458572236419863, accuracy: 0.98\n",
      "iteration no 3932: Loss: 0.24585205167757662, accuracy: 0.98\n",
      "iteration no 3933: Loss: 0.24584598738723312, accuracy: 0.98\n",
      "iteration no 3934: Loss: 0.24584123027222957, accuracy: 0.98\n",
      "iteration no 3935: Loss: 0.2458353873574461, accuracy: 0.98\n",
      "iteration no 3936: Loss: 0.24583045404080442, accuracy: 0.98\n",
      "iteration no 3937: Loss: 0.24582429162051989, accuracy: 0.98\n",
      "iteration no 3938: Loss: 0.24581927930695854, accuracy: 0.98\n",
      "iteration no 3939: Loss: 0.24581347490089886, accuracy: 0.98\n",
      "iteration no 3940: Loss: 0.24580882383302805, accuracy: 0.98\n",
      "iteration no 3941: Loss: 0.2458027619509823, accuracy: 0.98\n",
      "iteration no 3942: Loss: 0.24579762211659278, accuracy: 0.98\n",
      "iteration no 3943: Loss: 0.2457918338028428, accuracy: 0.98\n",
      "iteration no 3944: Loss: 0.24578631378747284, accuracy: 0.98\n",
      "iteration no 3945: Loss: 0.24578141933500286, accuracy: 0.98\n",
      "iteration no 3946: Loss: 0.24577572404637127, accuracy: 0.98\n",
      "iteration no 3947: Loss: 0.24577020264551658, accuracy: 0.98\n",
      "iteration no 3948: Loss: 0.2457644726621055, accuracy: 0.98\n",
      "iteration no 3949: Loss: 0.24575902536932873, accuracy: 0.98\n",
      "iteration no 3950: Loss: 0.2457539332595643, accuracy: 0.98\n",
      "iteration no 3951: Loss: 0.2457485724579114, accuracy: 0.98\n",
      "iteration no 3952: Loss: 0.2457426769815993, accuracy: 0.98\n",
      "iteration no 3953: Loss: 0.2457375143846957, accuracy: 0.98\n",
      "iteration no 3954: Loss: 0.24573143950553544, accuracy: 0.98\n",
      "iteration no 3955: Loss: 0.24572675194930646, accuracy: 0.98\n",
      "iteration no 3956: Loss: 0.24572088986697083, accuracy: 0.98\n",
      "iteration no 3957: Loss: 0.24571569565303866, accuracy: 0.98\n",
      "iteration no 3958: Loss: 0.24570963729613582, accuracy: 0.98\n",
      "iteration no 3959: Loss: 0.24570466424129214, accuracy: 0.98\n",
      "iteration no 3960: Loss: 0.2456987293399342, accuracy: 0.98\n",
      "iteration no 3961: Loss: 0.24569419871868442, accuracy: 0.98\n",
      "iteration no 3962: Loss: 0.24568786600594877, accuracy: 0.98\n",
      "iteration no 3963: Loss: 0.24568282341951714, accuracy: 0.98\n",
      "iteration no 3964: Loss: 0.24567669550693835, accuracy: 0.98\n",
      "iteration no 3965: Loss: 0.24567196047044998, accuracy: 0.98\n",
      "iteration no 3966: Loss: 0.2456662188035047, accuracy: 0.98\n",
      "iteration no 3967: Loss: 0.2456610280296883, accuracy: 0.98\n",
      "iteration no 3968: Loss: 0.24565519461781993, accuracy: 0.98\n",
      "iteration no 3969: Loss: 0.24564969704720563, accuracy: 0.98\n",
      "iteration no 3970: Loss: 0.245644528207416, accuracy: 0.98\n",
      "iteration no 3971: Loss: 0.24563911858098364, accuracy: 0.98\n",
      "iteration no 3972: Loss: 0.24563351792545246, accuracy: 0.98\n",
      "iteration no 3973: Loss: 0.24562799043975336, accuracy: 0.98\n",
      "iteration no 3974: Loss: 0.24562236775830448, accuracy: 0.98\n",
      "iteration no 3975: Loss: 0.24561702088147666, accuracy: 0.98\n",
      "iteration no 3976: Loss: 0.24561200846205644, accuracy: 0.98\n",
      "iteration no 3977: Loss: 0.24560612923542063, accuracy: 0.98\n",
      "iteration no 3978: Loss: 0.24560093849203507, accuracy: 0.98\n",
      "iteration no 3979: Loss: 0.24559497535254826, accuracy: 0.98\n",
      "iteration no 3980: Loss: 0.2455899957806794, accuracy: 0.98\n",
      "iteration no 3981: Loss: 0.24558469551218787, accuracy: 0.98\n",
      "iteration no 3982: Loss: 0.245579622903157, accuracy: 0.98\n",
      "iteration no 3983: Loss: 0.24557364453891714, accuracy: 0.98\n",
      "iteration no 3984: Loss: 0.24556885554806324, accuracy: 0.98\n",
      "iteration no 3985: Loss: 0.24556299511993135, accuracy: 0.98\n",
      "iteration no 3986: Loss: 0.24555851254807434, accuracy: 0.98\n",
      "iteration no 3987: Loss: 0.24555267745133463, accuracy: 0.98\n",
      "iteration no 3988: Loss: 0.24554765799749423, accuracy: 0.98\n",
      "iteration no 3989: Loss: 0.2455418362433423, accuracy: 0.98\n",
      "iteration no 3990: Loss: 0.2455371032131427, accuracy: 0.98\n",
      "iteration no 3991: Loss: 0.2455314991079322, accuracy: 0.98\n",
      "iteration no 3992: Loss: 0.2455268296884643, accuracy: 0.98\n",
      "iteration no 3993: Loss: 0.2455207517826751, accuracy: 0.98\n",
      "iteration no 3994: Loss: 0.24551592543842635, accuracy: 0.98\n",
      "iteration no 3995: Loss: 0.24551003490971612, accuracy: 0.98\n",
      "iteration no 3996: Loss: 0.24550586825259807, accuracy: 0.98\n",
      "iteration no 3997: Loss: 0.2454999210049898, accuracy: 0.98\n",
      "iteration no 3998: Loss: 0.2454951164829281, accuracy: 0.98\n",
      "iteration no 3999: Loss: 0.24548914274732436, accuracy: 0.98\n",
      "iteration no 4000: Loss: 0.245484588240253, accuracy: 0.98\n",
      "iteration no 4001: Loss: 0.24547895064239245, accuracy: 0.98\n",
      "iteration no 4002: Loss: 0.24547434304944674, accuracy: 0.98\n",
      "iteration no 4003: Loss: 0.2454684477430002, accuracy: 0.98\n",
      "iteration no 4004: Loss: 0.24546340355750268, accuracy: 0.98\n",
      "iteration no 4005: Loss: 0.24545790173650517, accuracy: 0.98\n",
      "iteration no 4006: Loss: 0.24545338493930546, accuracy: 0.98\n",
      "iteration no 4007: Loss: 0.24544759762738166, accuracy: 0.98\n",
      "iteration no 4008: Loss: 0.24544273388698312, accuracy: 0.98\n",
      "iteration no 4009: Loss: 0.24543703036554165, accuracy: 0.98\n",
      "iteration no 4010: Loss: 0.24543189107744934, accuracy: 0.98\n",
      "iteration no 4011: Loss: 0.24542709245484015, accuracy: 0.98\n",
      "iteration no 4012: Loss: 0.24542174806412315, accuracy: 0.98\n",
      "iteration no 4013: Loss: 0.2454163912312616, accuracy: 0.98\n",
      "iteration no 4014: Loss: 0.24541109096741917, accuracy: 0.98\n",
      "iteration no 4015: Loss: 0.2454057447414485, accuracy: 0.98\n",
      "iteration no 4016: Loss: 0.24540086792718752, accuracy: 0.98\n",
      "iteration no 4017: Loss: 0.2453958232820634, accuracy: 0.98\n",
      "iteration no 4018: Loss: 0.24539023122757023, accuracy: 0.98\n",
      "iteration no 4019: Loss: 0.24538518088342903, accuracy: 0.98\n",
      "iteration no 4020: Loss: 0.24537961897066407, accuracy: 0.98\n",
      "iteration no 4021: Loss: 0.2453747665951143, accuracy: 0.98\n",
      "iteration no 4022: Loss: 0.2453696362556574, accuracy: 0.98\n",
      "iteration no 4023: Loss: 0.24536441352781097, accuracy: 0.98\n",
      "iteration no 4024: Loss: 0.2453588634856358, accuracy: 0.98\n",
      "iteration no 4025: Loss: 0.2453539012754775, accuracy: 0.98\n",
      "iteration no 4026: Loss: 0.24534842437987195, accuracy: 0.98\n",
      "iteration no 4027: Loss: 0.24534386034674388, accuracy: 0.98\n",
      "iteration no 4028: Loss: 0.2453381273844464, accuracy: 0.98\n",
      "iteration no 4029: Loss: 0.2453331372226353, accuracy: 0.98\n",
      "iteration no 4030: Loss: 0.2453275077543463, accuracy: 0.98\n",
      "iteration no 4031: Loss: 0.2453228832432165, accuracy: 0.98\n",
      "iteration no 4032: Loss: 0.24531733115187482, accuracy: 0.98\n",
      "iteration no 4033: Loss: 0.2453126972970501, accuracy: 0.98\n",
      "iteration no 4034: Loss: 0.24530674930492563, accuracy: 0.98\n",
      "iteration no 4035: Loss: 0.2453019170500221, accuracy: 0.98\n",
      "iteration no 4036: Loss: 0.2452963319728209, accuracy: 0.98\n",
      "iteration no 4037: Loss: 0.24529191447353504, accuracy: 0.98\n",
      "iteration no 4038: Loss: 0.24528611885130963, accuracy: 0.98\n",
      "iteration no 4039: Loss: 0.24528142652520274, accuracy: 0.98\n",
      "iteration no 4040: Loss: 0.24527532975529315, accuracy: 0.98\n",
      "iteration no 4041: Loss: 0.24527092591430763, accuracy: 0.98\n",
      "iteration no 4042: Loss: 0.24526539308657092, accuracy: 0.98\n",
      "iteration no 4043: Loss: 0.2452605366225603, accuracy: 0.98\n",
      "iteration no 4044: Loss: 0.24525470384026454, accuracy: 0.98\n",
      "iteration no 4045: Loss: 0.24524971730317613, accuracy: 0.98\n",
      "iteration no 4046: Loss: 0.24524418167526071, accuracy: 0.98\n",
      "iteration no 4047: Loss: 0.24523954418960475, accuracy: 0.98\n",
      "iteration no 4048: Loss: 0.24523385169050482, accuracy: 0.98\n",
      "iteration no 4049: Loss: 0.24522880409040154, accuracy: 0.98\n",
      "iteration no 4050: Loss: 0.24522323963986342, accuracy: 0.98\n",
      "iteration no 4051: Loss: 0.24521797973580595, accuracy: 0.98\n",
      "iteration no 4052: Loss: 0.24521336253493248, accuracy: 0.98\n",
      "iteration no 4053: Loss: 0.24520774577732218, accuracy: 0.98\n",
      "iteration no 4054: Loss: 0.24520257640154214, accuracy: 0.98\n",
      "iteration no 4055: Loss: 0.24519708195597378, accuracy: 0.98\n",
      "iteration no 4056: Loss: 0.24519182918324606, accuracy: 0.98\n",
      "iteration no 4057: Loss: 0.24518698201701317, accuracy: 0.98\n",
      "iteration no 4058: Loss: 0.24518188184316647, accuracy: 0.98\n",
      "iteration no 4059: Loss: 0.24517612988181858, accuracy: 0.98\n",
      "iteration no 4060: Loss: 0.24517108738194238, accuracy: 0.98\n",
      "iteration no 4061: Loss: 0.24516546479047843, accuracy: 0.98\n",
      "iteration no 4062: Loss: 0.24516101730811352, accuracy: 0.98\n",
      "iteration no 4063: Loss: 0.2451554189169317, accuracy: 0.98\n",
      "iteration no 4064: Loss: 0.24515048525672567, accuracy: 0.98\n",
      "iteration no 4065: Loss: 0.24514460886916684, accuracy: 0.98\n",
      "iteration no 4066: Loss: 0.24513994962700889, accuracy: 0.98\n",
      "iteration no 4067: Loss: 0.245134364632487, accuracy: 0.98\n",
      "iteration no 4068: Loss: 0.24512986038052548, accuracy: 0.98\n",
      "iteration no 4069: Loss: 0.24512389604068868, accuracy: 0.98\n",
      "iteration no 4070: Loss: 0.24511905082719865, accuracy: 0.98\n",
      "iteration no 4071: Loss: 0.24511334765786563, accuracy: 0.98\n",
      "iteration no 4072: Loss: 0.2451088179305227, accuracy: 0.98\n",
      "iteration no 4073: Loss: 0.24510315115805936, accuracy: 0.98\n",
      "iteration no 4074: Loss: 0.24509829972029595, accuracy: 0.98\n",
      "iteration no 4075: Loss: 0.24509268758280273, accuracy: 0.98\n",
      "iteration no 4076: Loss: 0.2450875873261389, accuracy: 0.98\n",
      "iteration no 4077: Loss: 0.24508228276400063, accuracy: 0.98\n",
      "iteration no 4078: Loss: 0.24507754383829294, accuracy: 0.98\n",
      "iteration no 4079: Loss: 0.24507195055589714, accuracy: 0.98\n",
      "iteration no 4080: Loss: 0.24506675033574477, accuracy: 0.98\n",
      "iteration no 4081: Loss: 0.2450614636950908, accuracy: 0.98\n",
      "iteration no 4082: Loss: 0.24505624775114312, accuracy: 0.98\n",
      "iteration no 4083: Loss: 0.24505140811130335, accuracy: 0.98\n",
      "iteration no 4084: Loss: 0.2450459786629922, accuracy: 0.98\n",
      "iteration no 4085: Loss: 0.24504061157501178, accuracy: 0.98\n",
      "iteration no 4086: Loss: 0.24503540735118606, accuracy: 0.98\n",
      "iteration no 4087: Loss: 0.24503027247053083, accuracy: 0.98\n",
      "iteration no 4088: Loss: 0.24502509862665053, accuracy: 0.98\n",
      "iteration no 4089: Loss: 0.24502008252711782, accuracy: 0.98\n",
      "iteration no 4090: Loss: 0.24501437774099405, accuracy: 0.98\n",
      "iteration no 4091: Loss: 0.24500954722435248, accuracy: 0.98\n",
      "iteration no 4092: Loss: 0.245003807422065, accuracy: 0.98\n",
      "iteration no 4093: Loss: 0.24499939772091833, accuracy: 0.98\n",
      "iteration no 4094: Loss: 0.24499371752372834, accuracy: 0.98\n",
      "iteration no 4095: Loss: 0.24498865361034836, accuracy: 0.98\n",
      "iteration no 4096: Loss: 0.24498311529020456, accuracy: 0.98\n",
      "iteration no 4097: Loss: 0.24497819802972481, accuracy: 0.98\n",
      "iteration no 4098: Loss: 0.24497286662325862, accuracy: 0.98\n",
      "iteration no 4099: Loss: 0.24496815905023087, accuracy: 0.98\n",
      "iteration no 4100: Loss: 0.24496218571939732, accuracy: 0.98\n",
      "iteration no 4101: Loss: 0.24495733980040418, accuracy: 0.98\n",
      "iteration no 4102: Loss: 0.24495164680437903, accuracy: 0.98\n",
      "iteration no 4103: Loss: 0.244947425011018, accuracy: 0.98\n",
      "iteration no 4104: Loss: 0.2449415245771664, accuracy: 0.98\n",
      "iteration no 4105: Loss: 0.24493681293825995, accuracy: 0.98\n",
      "iteration no 4106: Loss: 0.2449307820714935, accuracy: 0.98\n",
      "iteration no 4107: Loss: 0.24492613778153732, accuracy: 0.98\n",
      "iteration no 4108: Loss: 0.24492082217233307, accuracy: 0.98\n",
      "iteration no 4109: Loss: 0.24491599050481394, accuracy: 0.98\n",
      "iteration no 4110: Loss: 0.2449101713451976, accuracy: 0.98\n",
      "iteration no 4111: Loss: 0.24490540744825814, accuracy: 0.98\n",
      "iteration no 4112: Loss: 0.2448994481744056, accuracy: 0.98\n",
      "iteration no 4113: Loss: 0.24489554887364068, accuracy: 0.98\n",
      "iteration no 4114: Loss: 0.24488935186405142, accuracy: 0.98\n",
      "iteration no 4115: Loss: 0.24488477580168894, accuracy: 0.98\n",
      "iteration no 4116: Loss: 0.24487884537741184, accuracy: 0.98\n",
      "iteration no 4117: Loss: 0.24487407250039495, accuracy: 0.98\n",
      "iteration no 4118: Loss: 0.24486888422127595, accuracy: 0.98\n",
      "iteration no 4119: Loss: 0.24486386754766182, accuracy: 0.98\n",
      "iteration no 4120: Loss: 0.24485806559876927, accuracy: 0.98\n",
      "iteration no 4121: Loss: 0.2448532730455545, accuracy: 0.98\n",
      "iteration no 4122: Loss: 0.24484779818475227, accuracy: 0.98\n",
      "iteration no 4123: Loss: 0.24484305527234618, accuracy: 0.98\n",
      "iteration no 4124: Loss: 0.24483758791691568, accuracy: 0.98\n",
      "iteration no 4125: Loss: 0.24483229985642913, accuracy: 0.98\n",
      "iteration no 4126: Loss: 0.24482688332002756, accuracy: 0.98\n",
      "iteration no 4127: Loss: 0.24482204135718594, accuracy: 0.98\n",
      "iteration no 4128: Loss: 0.24481687705521174, accuracy: 0.98\n",
      "iteration no 4129: Loss: 0.24481159562435517, accuracy: 0.98\n",
      "iteration no 4130: Loss: 0.2448063201068114, accuracy: 0.98\n",
      "iteration no 4131: Loss: 0.24480082230945957, accuracy: 0.98\n",
      "iteration no 4132: Loss: 0.244796166383564, accuracy: 0.98\n",
      "iteration no 4133: Loss: 0.2447909492641601, accuracy: 0.98\n",
      "iteration no 4134: Loss: 0.24478554829609767, accuracy: 0.98\n",
      "iteration no 4135: Loss: 0.24478021530794097, accuracy: 0.98\n",
      "iteration no 4136: Loss: 0.24477491620735092, accuracy: 0.98\n",
      "iteration no 4137: Loss: 0.24477018596472333, accuracy: 0.98\n",
      "iteration no 4138: Loss: 0.24476487091667837, accuracy: 0.98\n",
      "iteration no 4139: Loss: 0.24475947166825018, accuracy: 0.98\n",
      "iteration no 4140: Loss: 0.24475434218917352, accuracy: 0.98\n",
      "iteration no 4141: Loss: 0.24474885133408758, accuracy: 0.98\n",
      "iteration no 4142: Loss: 0.24474434739780143, accuracy: 0.98\n",
      "iteration no 4143: Loss: 0.24473866741712552, accuracy: 0.98\n",
      "iteration no 4144: Loss: 0.2447334662934312, accuracy: 0.98\n",
      "iteration no 4145: Loss: 0.24472810154030705, accuracy: 0.98\n",
      "iteration no 4146: Loss: 0.24472325395926742, accuracy: 0.98\n",
      "iteration no 4147: Loss: 0.24471802519103103, accuracy: 0.98\n",
      "iteration no 4148: Loss: 0.24471292935154942, accuracy: 0.98\n",
      "iteration no 4149: Loss: 0.24470725167179286, accuracy: 0.98\n",
      "iteration no 4150: Loss: 0.24470222428214214, accuracy: 0.98\n",
      "iteration no 4151: Loss: 0.24469724525091352, accuracy: 0.98\n",
      "iteration no 4152: Loss: 0.2446920965421312, accuracy: 0.98\n",
      "iteration no 4153: Loss: 0.24468660305291384, accuracy: 0.98\n",
      "iteration no 4154: Loss: 0.24468140497918867, accuracy: 0.98\n",
      "iteration no 4155: Loss: 0.2446761352868076, accuracy: 0.98\n",
      "iteration no 4156: Loss: 0.24467142907707606, accuracy: 0.98\n",
      "iteration no 4157: Loss: 0.24466578736929667, accuracy: 0.98\n",
      "iteration no 4158: Loss: 0.24466069226736104, accuracy: 0.98\n",
      "iteration no 4159: Loss: 0.24465516048753666, accuracy: 0.98\n",
      "iteration no 4160: Loss: 0.24465038353152552, accuracy: 0.98\n",
      "iteration no 4161: Loss: 0.24464526524056857, accuracy: 0.98\n",
      "iteration no 4162: Loss: 0.24463984225560598, accuracy: 0.98\n",
      "iteration no 4163: Loss: 0.24463433915671684, accuracy: 0.98\n",
      "iteration no 4164: Loss: 0.24462943685933172, accuracy: 0.98\n",
      "iteration no 4165: Loss: 0.24462427692370964, accuracy: 0.98\n",
      "iteration no 4166: Loss: 0.24461912258060864, accuracy: 0.98\n",
      "iteration no 4167: Loss: 0.24461355839576804, accuracy: 0.98\n",
      "iteration no 4168: Loss: 0.24460822627677833, accuracy: 0.98\n",
      "iteration no 4169: Loss: 0.24460339013665688, accuracy: 0.98\n",
      "iteration no 4170: Loss: 0.24459817197102188, accuracy: 0.98\n",
      "iteration no 4171: Loss: 0.24459296855323706, accuracy: 0.98\n",
      "iteration no 4172: Loss: 0.24458750118198377, accuracy: 0.98\n",
      "iteration no 4173: Loss: 0.2445824484295099, accuracy: 0.98\n",
      "iteration no 4174: Loss: 0.24457750630528335, accuracy: 0.98\n",
      "iteration no 4175: Loss: 0.2445719711961047, accuracy: 0.98\n",
      "iteration no 4176: Loss: 0.24456653031538855, accuracy: 0.98\n",
      "iteration no 4177: Loss: 0.2445612986209666, accuracy: 0.98\n",
      "iteration no 4178: Loss: 0.2445563789376284, accuracy: 0.98\n",
      "iteration no 4179: Loss: 0.2445512698515724, accuracy: 0.98\n",
      "iteration no 4180: Loss: 0.24454579968490864, accuracy: 0.98\n",
      "iteration no 4181: Loss: 0.24454053885602045, accuracy: 0.98\n",
      "iteration no 4182: Loss: 0.2445353715124421, accuracy: 0.98\n",
      "iteration no 4183: Loss: 0.2445303728064297, accuracy: 0.98\n",
      "iteration no 4184: Loss: 0.24452478302186303, accuracy: 0.98\n",
      "iteration no 4185: Loss: 0.24451964254370187, accuracy: 0.98\n",
      "iteration no 4186: Loss: 0.2445140846240474, accuracy: 0.98\n",
      "iteration no 4187: Loss: 0.2445096280234447, accuracy: 0.98\n",
      "iteration no 4188: Loss: 0.24450385811737585, accuracy: 0.98\n",
      "iteration no 4189: Loss: 0.2444988862057878, accuracy: 0.98\n",
      "iteration no 4190: Loss: 0.244493192975752, accuracy: 0.98\n",
      "iteration no 4191: Loss: 0.24448849454561777, accuracy: 0.98\n",
      "iteration no 4192: Loss: 0.24448298300632487, accuracy: 0.98\n",
      "iteration no 4193: Loss: 0.24447789259078734, accuracy: 0.98\n",
      "iteration no 4194: Loss: 0.24447213805240897, accuracy: 0.98\n",
      "iteration no 4195: Loss: 0.24446748958812695, accuracy: 0.98\n",
      "iteration no 4196: Loss: 0.24446198910630978, accuracy: 0.98\n",
      "iteration no 4197: Loss: 0.24445713288410564, accuracy: 0.98\n",
      "iteration no 4198: Loss: 0.24445134681078193, accuracy: 0.98\n",
      "iteration no 4199: Loss: 0.24444640925091657, accuracy: 0.98\n",
      "iteration no 4200: Loss: 0.24444118772623463, accuracy: 0.98\n",
      "iteration no 4201: Loss: 0.24443616539044954, accuracy: 0.98\n",
      "iteration no 4202: Loss: 0.24443012862904112, accuracy: 0.98\n",
      "iteration no 4203: Loss: 0.24442535246956512, accuracy: 0.98\n",
      "iteration no 4204: Loss: 0.24441987820234942, accuracy: 0.98\n",
      "iteration no 4205: Loss: 0.24441537718274337, accuracy: 0.98\n",
      "iteration no 4206: Loss: 0.24440924682008902, accuracy: 0.98\n",
      "iteration no 4207: Loss: 0.24440430545940628, accuracy: 0.98\n",
      "iteration no 4208: Loss: 0.24439892984601225, accuracy: 0.98\n",
      "iteration no 4209: Loss: 0.24439414969447054, accuracy: 0.98\n",
      "iteration no 4210: Loss: 0.24438832979079342, accuracy: 0.98\n",
      "iteration no 4211: Loss: 0.2443832653601181, accuracy: 0.98\n",
      "iteration no 4212: Loss: 0.24437784403594143, accuracy: 0.98\n",
      "iteration no 4213: Loss: 0.2443732167516227, accuracy: 0.98\n",
      "iteration no 4214: Loss: 0.24436741743125617, accuracy: 0.98\n",
      "iteration no 4215: Loss: 0.24436210446067547, accuracy: 0.98\n",
      "iteration no 4216: Loss: 0.24435687381899063, accuracy: 0.98\n",
      "iteration no 4217: Loss: 0.24435176815844506, accuracy: 0.98\n",
      "iteration no 4218: Loss: 0.2443464320363411, accuracy: 0.98\n",
      "iteration no 4219: Loss: 0.24434103227017728, accuracy: 0.98\n",
      "iteration no 4220: Loss: 0.2443358627071726, accuracy: 0.98\n",
      "iteration no 4221: Loss: 0.24433081547290503, accuracy: 0.98\n",
      "iteration no 4222: Loss: 0.24432559572662083, accuracy: 0.98\n",
      "iteration no 4223: Loss: 0.2443196580735436, accuracy: 0.98\n",
      "iteration no 4224: Loss: 0.24431478611721824, accuracy: 0.98\n",
      "iteration no 4225: Loss: 0.24430942733033972, accuracy: 0.98\n",
      "iteration no 4226: Loss: 0.24430470646947317, accuracy: 0.98\n",
      "iteration no 4227: Loss: 0.24429852694298435, accuracy: 0.98\n",
      "iteration no 4228: Loss: 0.24429383985719605, accuracy: 0.98\n",
      "iteration no 4229: Loss: 0.24428837436351641, accuracy: 0.98\n",
      "iteration no 4230: Loss: 0.24428341604594606, accuracy: 0.98\n",
      "iteration no 4231: Loss: 0.24427758151582457, accuracy: 0.98\n",
      "iteration no 4232: Loss: 0.2442724007017278, accuracy: 0.98\n",
      "iteration no 4233: Loss: 0.2442674835105129, accuracy: 0.98\n",
      "iteration no 4234: Loss: 0.2442622293169501, accuracy: 0.98\n",
      "iteration no 4235: Loss: 0.2442566129540537, accuracy: 0.98\n",
      "iteration no 4236: Loss: 0.24425123708356564, accuracy: 0.98\n",
      "iteration no 4237: Loss: 0.2442462453363995, accuracy: 0.98\n",
      "iteration no 4238: Loss: 0.24424087221871074, accuracy: 0.98\n",
      "iteration no 4239: Loss: 0.2442356361786655, accuracy: 0.98\n",
      "iteration no 4240: Loss: 0.24422981949728317, accuracy: 0.98\n",
      "iteration no 4241: Loss: 0.24422524803157328, accuracy: 0.98\n",
      "iteration no 4242: Loss: 0.2442194623429289, accuracy: 0.98\n",
      "iteration no 4243: Loss: 0.24421452669835414, accuracy: 0.98\n",
      "iteration no 4244: Loss: 0.24420850678873068, accuracy: 0.98\n",
      "iteration no 4245: Loss: 0.2442040440785969, accuracy: 0.98\n",
      "iteration no 4246: Loss: 0.24419848892451554, accuracy: 0.98\n",
      "iteration no 4247: Loss: 0.24419329742531382, accuracy: 0.98\n",
      "iteration no 4248: Loss: 0.24418747417996434, accuracy: 0.98\n",
      "iteration no 4249: Loss: 0.24418264910048076, accuracy: 0.98\n",
      "iteration no 4250: Loss: 0.2441771573379665, accuracy: 0.98\n",
      "iteration no 4251: Loss: 0.2441719731181801, accuracy: 0.98\n",
      "iteration no 4252: Loss: 0.24416637529418148, accuracy: 0.98\n",
      "iteration no 4253: Loss: 0.2441612386494585, accuracy: 0.98\n",
      "iteration no 4254: Loss: 0.24415614817619036, accuracy: 0.98\n",
      "iteration no 4255: Loss: 0.24414929135377783, accuracy: 0.98\n",
      "iteration no 4256: Loss: 0.2441433295064766, accuracy: 0.98\n",
      "iteration no 4257: Loss: 0.24413678094595304, accuracy: 0.98\n",
      "iteration no 4258: Loss: 0.24413141399548843, accuracy: 0.98\n",
      "iteration no 4259: Loss: 0.24412426607599713, accuracy: 0.98\n",
      "iteration no 4260: Loss: 0.24411846960105596, accuracy: 0.98\n",
      "iteration no 4261: Loss: 0.24411210714452458, accuracy: 0.98\n",
      "iteration no 4262: Loss: 0.24410637878139618, accuracy: 0.98\n",
      "iteration no 4263: Loss: 0.24409995295420753, accuracy: 0.98\n",
      "iteration no 4264: Loss: 0.24409381625424997, accuracy: 0.98\n",
      "iteration no 4265: Loss: 0.24408828075117378, accuracy: 0.98\n",
      "iteration no 4266: Loss: 0.24408281569002874, accuracy: 0.98\n",
      "iteration no 4267: Loss: 0.24407706234328977, accuracy: 0.98\n",
      "iteration no 4268: Loss: 0.24407108727880691, accuracy: 0.98\n",
      "iteration no 4269: Loss: 0.24406605429559672, accuracy: 0.98\n",
      "iteration no 4270: Loss: 0.2440600988835786, accuracy: 0.98\n",
      "iteration no 4271: Loss: 0.2440549163771494, accuracy: 0.98\n",
      "iteration no 4272: Loss: 0.2440483375992901, accuracy: 0.98\n",
      "iteration no 4273: Loss: 0.24404373817705666, accuracy: 0.98\n",
      "iteration no 4274: Loss: 0.24403785108416914, accuracy: 0.98\n",
      "iteration no 4275: Loss: 0.24403228317596143, accuracy: 0.98\n",
      "iteration no 4276: Loss: 0.24402633773260585, accuracy: 0.98\n",
      "iteration no 4277: Loss: 0.24402090047190317, accuracy: 0.98\n",
      "iteration no 4278: Loss: 0.24401555888737136, accuracy: 0.98\n",
      "iteration no 4279: Loss: 0.24400970490111762, accuracy: 0.98\n",
      "iteration no 4280: Loss: 0.24400411168520247, accuracy: 0.98\n",
      "iteration no 4281: Loss: 0.24399844895869088, accuracy: 0.98\n",
      "iteration no 4282: Loss: 0.24399338174883833, accuracy: 0.98\n",
      "iteration no 4283: Loss: 0.24398705207451393, accuracy: 0.98\n",
      "iteration no 4284: Loss: 0.24398192841884353, accuracy: 0.98\n",
      "iteration no 4285: Loss: 0.2439762192844131, accuracy: 0.98\n",
      "iteration no 4286: Loss: 0.24397079598180813, accuracy: 0.98\n",
      "iteration no 4287: Loss: 0.24396498805377992, accuracy: 0.98\n",
      "iteration no 4288: Loss: 0.24395921783063773, accuracy: 0.98\n",
      "iteration no 4289: Loss: 0.24395422137812123, accuracy: 0.98\n",
      "iteration no 4290: Loss: 0.2439480392873175, accuracy: 0.98\n",
      "iteration no 4291: Loss: 0.24394290530809395, accuracy: 0.98\n",
      "iteration no 4292: Loss: 0.24393654879350718, accuracy: 0.98\n",
      "iteration no 4293: Loss: 0.2439321244650661, accuracy: 0.98\n",
      "iteration no 4294: Loss: 0.24392575493567678, accuracy: 0.98\n",
      "iteration no 4295: Loss: 0.2439201683365276, accuracy: 0.98\n",
      "iteration no 4296: Loss: 0.24391477275124945, accuracy: 0.98\n",
      "iteration no 4297: Loss: 0.24390918155850677, accuracy: 0.98\n",
      "iteration no 4298: Loss: 0.24390370671263073, accuracy: 0.98\n",
      "iteration no 4299: Loss: 0.24389742360382596, accuracy: 0.98\n",
      "iteration no 4300: Loss: 0.2438927923179984, accuracy: 0.98\n",
      "iteration no 4301: Loss: 0.24388663122897697, accuracy: 0.98\n",
      "iteration no 4302: Loss: 0.24388146174899167, accuracy: 0.98\n",
      "iteration no 4303: Loss: 0.24387520467711354, accuracy: 0.98\n",
      "iteration no 4304: Loss: 0.24387032310349172, accuracy: 0.98\n",
      "iteration no 4305: Loss: 0.24386476119823008, accuracy: 0.98\n",
      "iteration no 4306: Loss: 0.24385871153005612, accuracy: 0.98\n",
      "iteration no 4307: Loss: 0.24385354480264865, accuracy: 0.98\n",
      "iteration no 4308: Loss: 0.2438478965946812, accuracy: 0.9833333333333333\n",
      "iteration no 4309: Loss: 0.24384300748133853, accuracy: 0.98\n",
      "iteration no 4310: Loss: 0.24383665944473493, accuracy: 0.9833333333333333\n",
      "iteration no 4311: Loss: 0.2438314788334407, accuracy: 0.98\n",
      "iteration no 4312: Loss: 0.24382638585906719, accuracy: 0.9833333333333333\n",
      "iteration no 4313: Loss: 0.24382037633935658, accuracy: 0.98\n",
      "iteration no 4314: Loss: 0.24381499458219485, accuracy: 0.9833333333333333\n",
      "iteration no 4315: Loss: 0.2438092482659659, accuracy: 0.9833333333333333\n",
      "iteration no 4316: Loss: 0.24380463228615826, accuracy: 0.9833333333333333\n",
      "iteration no 4317: Loss: 0.24379822692554312, accuracy: 0.9833333333333333\n",
      "iteration no 4318: Loss: 0.2437929438524917, accuracy: 0.9833333333333333\n",
      "iteration no 4319: Loss: 0.24378770102419667, accuracy: 0.9833333333333333\n",
      "iteration no 4320: Loss: 0.24378205248012497, accuracy: 0.9833333333333333\n",
      "iteration no 4321: Loss: 0.24377657697154764, accuracy: 0.9833333333333333\n",
      "iteration no 4322: Loss: 0.24377047623528536, accuracy: 0.9833333333333333\n",
      "iteration no 4323: Loss: 0.24376488493005927, accuracy: 0.9833333333333333\n",
      "iteration no 4324: Loss: 0.24375717308586922, accuracy: 0.9833333333333333\n",
      "iteration no 4325: Loss: 0.24375181982761565, accuracy: 0.9833333333333333\n",
      "iteration no 4326: Loss: 0.2437459126689726, accuracy: 0.9833333333333333\n",
      "iteration no 4327: Loss: 0.2437404353196398, accuracy: 0.9833333333333333\n",
      "iteration no 4328: Loss: 0.24373469755586413, accuracy: 0.9833333333333333\n",
      "iteration no 4329: Loss: 0.24372844865623194, accuracy: 0.9833333333333333\n",
      "iteration no 4330: Loss: 0.24372363012665083, accuracy: 0.9833333333333333\n",
      "iteration no 4331: Loss: 0.24371733090700862, accuracy: 0.9833333333333333\n",
      "iteration no 4332: Loss: 0.24371203886558324, accuracy: 0.9833333333333333\n",
      "iteration no 4333: Loss: 0.24370586280743917, accuracy: 0.9833333333333333\n",
      "iteration no 4334: Loss: 0.24370061626700604, accuracy: 0.9833333333333333\n",
      "iteration no 4335: Loss: 0.24369497287462372, accuracy: 0.9833333333333333\n",
      "iteration no 4336: Loss: 0.24368881049929192, accuracy: 0.9833333333333333\n",
      "iteration no 4337: Loss: 0.2436836218090469, accuracy: 0.9833333333333333\n",
      "iteration no 4338: Loss: 0.24367771217890086, accuracy: 0.9833333333333333\n",
      "iteration no 4339: Loss: 0.24367246409241677, accuracy: 0.9833333333333333\n",
      "iteration no 4340: Loss: 0.24366628234200044, accuracy: 0.9833333333333333\n",
      "iteration no 4341: Loss: 0.24366070850773236, accuracy: 0.9833333333333333\n",
      "iteration no 4342: Loss: 0.24365573199986393, accuracy: 0.9833333333333333\n",
      "iteration no 4343: Loss: 0.24364912450354026, accuracy: 0.9833333333333333\n",
      "iteration no 4344: Loss: 0.24364393650935912, accuracy: 0.9833333333333333\n",
      "iteration no 4345: Loss: 0.24363837905813338, accuracy: 0.9833333333333333\n",
      "iteration no 4346: Loss: 0.24363302671220666, accuracy: 0.9833333333333333\n",
      "iteration no 4347: Loss: 0.24362684813760874, accuracy: 0.9833333333333333\n",
      "iteration no 4348: Loss: 0.24362133062807068, accuracy: 0.9833333333333333\n",
      "iteration no 4349: Loss: 0.24361619972520152, accuracy: 0.9833333333333333\n",
      "iteration no 4350: Loss: 0.24360994523753637, accuracy: 0.9833333333333333\n",
      "iteration no 4351: Loss: 0.24360477623374455, accuracy: 0.9833333333333333\n",
      "iteration no 4352: Loss: 0.24359894073473667, accuracy: 0.9833333333333333\n",
      "iteration no 4353: Loss: 0.24359356924079345, accuracy: 0.9833333333333333\n",
      "iteration no 4354: Loss: 0.2435878734718617, accuracy: 0.9833333333333333\n",
      "iteration no 4355: Loss: 0.24358171540065487, accuracy: 0.9833333333333333\n",
      "iteration no 4356: Loss: 0.24357706068301976, accuracy: 0.9833333333333333\n",
      "iteration no 4357: Loss: 0.24357110399868986, accuracy: 0.9833333333333333\n",
      "iteration no 4358: Loss: 0.24356600312930338, accuracy: 0.9833333333333333\n",
      "iteration no 4359: Loss: 0.24356079035730166, accuracy: 0.9833333333333333\n",
      "iteration no 4360: Loss: 0.24355522834101373, accuracy: 0.9833333333333333\n",
      "iteration no 4361: Loss: 0.24354981419562494, accuracy: 0.9833333333333333\n",
      "iteration no 4362: Loss: 0.24354456392559992, accuracy: 0.9833333333333333\n",
      "iteration no 4363: Loss: 0.24353961563958382, accuracy: 0.9833333333333333\n",
      "iteration no 4364: Loss: 0.24353391642474098, accuracy: 0.9833333333333333\n",
      "iteration no 4365: Loss: 0.24352829725902952, accuracy: 0.9833333333333333\n",
      "iteration no 4366: Loss: 0.24352351506865927, accuracy: 0.9833333333333333\n",
      "iteration no 4367: Loss: 0.2435179580293208, accuracy: 0.9833333333333333\n",
      "iteration no 4368: Loss: 0.2435127563183898, accuracy: 0.9833333333333333\n",
      "iteration no 4369: Loss: 0.24350716573271408, accuracy: 0.9833333333333333\n",
      "iteration no 4370: Loss: 0.2435022918454603, accuracy: 0.9833333333333333\n",
      "iteration no 4371: Loss: 0.24349679639629002, accuracy: 0.9833333333333333\n",
      "iteration no 4372: Loss: 0.2434909386595893, accuracy: 0.9833333333333333\n",
      "iteration no 4373: Loss: 0.24348634716790485, accuracy: 0.9833333333333333\n",
      "iteration no 4374: Loss: 0.24348092752677314, accuracy: 0.9833333333333333\n",
      "iteration no 4375: Loss: 0.24347516634840172, accuracy: 0.9833333333333333\n",
      "iteration no 4376: Loss: 0.24347052564738697, accuracy: 0.9833333333333333\n",
      "iteration no 4377: Loss: 0.2434643654502502, accuracy: 0.9833333333333333\n",
      "iteration no 4378: Loss: 0.2434599016228431, accuracy: 0.9833333333333333\n",
      "iteration no 4379: Loss: 0.2434539370409231, accuracy: 0.9833333333333333\n",
      "iteration no 4380: Loss: 0.24344883536157408, accuracy: 0.9833333333333333\n",
      "iteration no 4381: Loss: 0.24344378595875366, accuracy: 0.9833333333333333\n",
      "iteration no 4382: Loss: 0.24343764430179743, accuracy: 0.9833333333333333\n",
      "iteration no 4383: Loss: 0.2434336778342797, accuracy: 0.9833333333333333\n",
      "iteration no 4384: Loss: 0.2434272535123992, accuracy: 0.9833333333333333\n",
      "iteration no 4385: Loss: 0.24342245613700297, accuracy: 0.9833333333333333\n",
      "iteration no 4386: Loss: 0.24341733314093567, accuracy: 0.9833333333333333\n",
      "iteration no 4387: Loss: 0.24341156691504057, accuracy: 0.9833333333333333\n",
      "iteration no 4388: Loss: 0.24340665758420246, accuracy: 0.9833333333333333\n",
      "iteration no 4389: Loss: 0.24340131072002175, accuracy: 0.9833333333333333\n",
      "iteration no 4390: Loss: 0.24339686627038748, accuracy: 0.9833333333333333\n",
      "iteration no 4391: Loss: 0.24339106480953068, accuracy: 0.9833333333333333\n",
      "iteration no 4392: Loss: 0.24338647570076438, accuracy: 0.9833333333333333\n",
      "iteration no 4393: Loss: 0.24338186100013529, accuracy: 0.9833333333333333\n",
      "iteration no 4394: Loss: 0.24337636047376693, accuracy: 0.9833333333333333\n",
      "iteration no 4395: Loss: 0.2433718281370687, accuracy: 0.9833333333333333\n",
      "iteration no 4396: Loss: 0.24336716356163207, accuracy: 0.9833333333333333\n",
      "iteration no 4397: Loss: 0.24336182731440315, accuracy: 0.9833333333333333\n",
      "iteration no 4398: Loss: 0.24335719346399265, accuracy: 0.9833333333333333\n",
      "iteration no 4399: Loss: 0.24335223043966606, accuracy: 0.9833333333333333\n",
      "iteration no 4400: Loss: 0.24334783076524327, accuracy: 0.9833333333333333\n",
      "iteration no 4401: Loss: 0.24334258476472498, accuracy: 0.9833333333333333\n",
      "iteration no 4402: Loss: 0.24333771907271776, accuracy: 0.9833333333333333\n",
      "iteration no 4403: Loss: 0.24333345411033547, accuracy: 0.9833333333333333\n",
      "iteration no 4404: Loss: 0.2433280444540622, accuracy: 0.9833333333333333\n",
      "iteration no 4405: Loss: 0.24332334456157473, accuracy: 0.9833333333333333\n",
      "iteration no 4406: Loss: 0.24331854849619752, accuracy: 0.9833333333333333\n",
      "iteration no 4407: Loss: 0.24331373662138175, accuracy: 0.9833333333333333\n",
      "iteration no 4408: Loss: 0.24330884142623338, accuracy: 0.9833333333333333\n",
      "iteration no 4409: Loss: 0.24330354098033258, accuracy: 0.9833333333333333\n",
      "iteration no 4410: Loss: 0.24329937178508118, accuracy: 0.9833333333333333\n",
      "iteration no 4411: Loss: 0.24329421653846883, accuracy: 0.9833333333333333\n",
      "iteration no 4412: Loss: 0.24328896212215514, accuracy: 0.9833333333333333\n",
      "iteration no 4413: Loss: 0.2432847230166845, accuracy: 0.9833333333333333\n",
      "iteration no 4414: Loss: 0.24327958923889817, accuracy: 0.9833333333333333\n",
      "iteration no 4415: Loss: 0.24327500494885143, accuracy: 0.9833333333333333\n",
      "iteration no 4416: Loss: 0.2432698221779352, accuracy: 0.9833333333333333\n",
      "iteration no 4417: Loss: 0.2432652693825864, accuracy: 0.9833333333333333\n",
      "iteration no 4418: Loss: 0.24326073414829052, accuracy: 0.9833333333333333\n",
      "iteration no 4419: Loss: 0.24325513692482992, accuracy: 0.9833333333333333\n",
      "iteration no 4420: Loss: 0.24325065235039156, accuracy: 0.9833333333333333\n",
      "iteration no 4421: Loss: 0.24324605083173367, accuracy: 0.9833333333333333\n",
      "iteration no 4422: Loss: 0.2432407111690078, accuracy: 0.9833333333333333\n",
      "iteration no 4423: Loss: 0.24323607096722555, accuracy: 0.9833333333333333\n",
      "iteration no 4424: Loss: 0.24323127996414623, accuracy: 0.9833333333333333\n",
      "iteration no 4425: Loss: 0.243226496089742, accuracy: 0.9833333333333333\n",
      "iteration no 4426: Loss: 0.2432214516657959, accuracy: 0.9833333333333333\n",
      "iteration no 4427: Loss: 0.2432162643302981, accuracy: 0.9833333333333333\n",
      "iteration no 4428: Loss: 0.24321252141247957, accuracy: 0.9833333333333333\n",
      "iteration no 4429: Loss: 0.24320709935174006, accuracy: 0.9833333333333333\n",
      "iteration no 4430: Loss: 0.24320215349955204, accuracy: 0.9833333333333333\n",
      "iteration no 4431: Loss: 0.2431976191356281, accuracy: 0.9833333333333333\n",
      "iteration no 4432: Loss: 0.2431925325283143, accuracy: 0.9833333333333333\n",
      "iteration no 4433: Loss: 0.24318792201068878, accuracy: 0.9833333333333333\n",
      "iteration no 4434: Loss: 0.24318262721943723, accuracy: 0.9833333333333333\n",
      "iteration no 4435: Loss: 0.2431782306594776, accuracy: 0.9833333333333333\n",
      "iteration no 4436: Loss: 0.24317342048780538, accuracy: 0.9833333333333333\n",
      "iteration no 4437: Loss: 0.2431678721770949, accuracy: 0.9833333333333333\n",
      "iteration no 4438: Loss: 0.2431637537016433, accuracy: 0.9833333333333333\n",
      "iteration no 4439: Loss: 0.24315893625172996, accuracy: 0.9833333333333333\n",
      "iteration no 4440: Loss: 0.24315389056267595, accuracy: 0.9833333333333333\n",
      "iteration no 4441: Loss: 0.24314916581704166, accuracy: 0.9833333333333333\n",
      "iteration no 4442: Loss: 0.24314437553853546, accuracy: 0.9833333333333333\n",
      "iteration no 4443: Loss: 0.24313973688324253, accuracy: 0.9833333333333333\n",
      "iteration no 4444: Loss: 0.2431345729883274, accuracy: 0.9833333333333333\n",
      "iteration no 4445: Loss: 0.24312948484864533, accuracy: 0.9833333333333333\n",
      "iteration no 4446: Loss: 0.24312540481439265, accuracy: 0.9833333333333333\n",
      "iteration no 4447: Loss: 0.24311989371469686, accuracy: 0.9833333333333333\n",
      "iteration no 4448: Loss: 0.24311497904656795, accuracy: 0.9833333333333333\n",
      "iteration no 4449: Loss: 0.2431106132670346, accuracy: 0.9833333333333333\n",
      "iteration no 4450: Loss: 0.2431052328623225, accuracy: 0.9833333333333333\n",
      "iteration no 4451: Loss: 0.243101072023247, accuracy: 0.9833333333333333\n",
      "iteration no 4452: Loss: 0.24309579544979654, accuracy: 0.9833333333333333\n",
      "iteration no 4453: Loss: 0.2430916144496201, accuracy: 0.9833333333333333\n",
      "iteration no 4454: Loss: 0.24308648771939484, accuracy: 0.9833333333333333\n",
      "iteration no 4455: Loss: 0.24308109599698757, accuracy: 0.9833333333333333\n",
      "iteration no 4456: Loss: 0.24307705637031612, accuracy: 0.9833333333333333\n",
      "iteration no 4457: Loss: 0.24307189123792386, accuracy: 0.9833333333333333\n",
      "iteration no 4458: Loss: 0.2430667437437235, accuracy: 0.9833333333333333\n",
      "iteration no 4459: Loss: 0.24306206227664837, accuracy: 0.9833333333333333\n",
      "iteration no 4460: Loss: 0.24305742611067113, accuracy: 0.9833333333333333\n",
      "iteration no 4461: Loss: 0.2430524927987624, accuracy: 0.9833333333333333\n",
      "iteration no 4462: Loss: 0.24304737156056855, accuracy: 0.9833333333333333\n",
      "iteration no 4463: Loss: 0.2430434627857161, accuracy: 0.9833333333333333\n",
      "iteration no 4464: Loss: 0.24303873121827013, accuracy: 0.9833333333333333\n",
      "iteration no 4465: Loss: 0.243033386340814, accuracy: 0.9833333333333333\n",
      "iteration no 4466: Loss: 0.24302883734593422, accuracy: 0.9833333333333333\n",
      "iteration no 4467: Loss: 0.24302462579030196, accuracy: 0.9833333333333333\n",
      "iteration no 4468: Loss: 0.24301900560781328, accuracy: 0.9833333333333333\n",
      "iteration no 4469: Loss: 0.24301539988054693, accuracy: 0.9833333333333333\n",
      "iteration no 4470: Loss: 0.24301061092288267, accuracy: 0.9833333333333333\n",
      "iteration no 4471: Loss: 0.24300556853242175, accuracy: 0.9833333333333333\n",
      "iteration no 4472: Loss: 0.24300150555761305, accuracy: 0.9833333333333333\n",
      "iteration no 4473: Loss: 0.24299674128512716, accuracy: 0.9833333333333333\n",
      "iteration no 4474: Loss: 0.24299228107920384, accuracy: 0.9833333333333333\n",
      "iteration no 4475: Loss: 0.2429877271040819, accuracy: 0.9833333333333333\n",
      "iteration no 4476: Loss: 0.24298318543157088, accuracy: 0.9833333333333333\n",
      "iteration no 4477: Loss: 0.24297861940038223, accuracy: 0.9833333333333333\n",
      "iteration no 4478: Loss: 0.24297419476759569, accuracy: 0.9833333333333333\n",
      "iteration no 4479: Loss: 0.2429696554634321, accuracy: 0.9833333333333333\n",
      "iteration no 4480: Loss: 0.2429649962931693, accuracy: 0.9833333333333333\n",
      "iteration no 4481: Loss: 0.24296022995462002, accuracy: 0.9833333333333333\n",
      "iteration no 4482: Loss: 0.2429565177885148, accuracy: 0.9833333333333333\n",
      "iteration no 4483: Loss: 0.24295129218564837, accuracy: 0.9833333333333333\n",
      "iteration no 4484: Loss: 0.24294699393687835, accuracy: 0.9833333333333333\n",
      "iteration no 4485: Loss: 0.2429429098711762, accuracy: 0.9833333333333333\n",
      "iteration no 4486: Loss: 0.24293748511157687, accuracy: 0.9833333333333333\n",
      "iteration no 4487: Loss: 0.24293382575107408, accuracy: 0.9833333333333333\n",
      "iteration no 4488: Loss: 0.2429292156388962, accuracy: 0.9833333333333333\n",
      "iteration no 4489: Loss: 0.2429238405919342, accuracy: 0.9833333333333333\n",
      "iteration no 4490: Loss: 0.24292065351059844, accuracy: 0.9833333333333333\n",
      "iteration no 4491: Loss: 0.2429152939403129, accuracy: 0.9833333333333333\n",
      "iteration no 4492: Loss: 0.24291083001000707, accuracy: 0.9833333333333333\n",
      "iteration no 4493: Loss: 0.24290689320543968, accuracy: 0.9833333333333333\n",
      "iteration no 4494: Loss: 0.2429021115049162, accuracy: 0.9833333333333333\n",
      "iteration no 4495: Loss: 0.2428974895177714, accuracy: 0.9833333333333333\n",
      "iteration no 4496: Loss: 0.2428937277584951, accuracy: 0.9833333333333333\n",
      "iteration no 4497: Loss: 0.24288838943203994, accuracy: 0.9833333333333333\n",
      "iteration no 4498: Loss: 0.2428847920184744, accuracy: 0.9833333333333333\n",
      "iteration no 4499: Loss: 0.24287998561219454, accuracy: 0.9833333333333333\n",
      "iteration no 4500: Loss: 0.24287556326855592, accuracy: 0.9833333333333333\n",
      "iteration no 4501: Loss: 0.24287132359676664, accuracy: 0.9833333333333333\n",
      "iteration no 4502: Loss: 0.2428666919850404, accuracy: 0.9833333333333333\n",
      "iteration no 4503: Loss: 0.24286259143835603, accuracy: 0.9833333333333333\n",
      "iteration no 4504: Loss: 0.2428580829346137, accuracy: 0.9833333333333333\n",
      "iteration no 4505: Loss: 0.24285408281174956, accuracy: 0.9833333333333333\n",
      "iteration no 4506: Loss: 0.2428487312971284, accuracy: 0.9833333333333333\n",
      "iteration no 4507: Loss: 0.2428453913311871, accuracy: 0.9833333333333333\n",
      "iteration no 4508: Loss: 0.24284069087462168, accuracy: 0.9833333333333333\n",
      "iteration no 4509: Loss: 0.2428356357465968, accuracy: 0.9833333333333333\n",
      "iteration no 4510: Loss: 0.242832577568383, accuracy: 0.9833333333333333\n",
      "iteration no 4511: Loss: 0.24282739840254838, accuracy: 0.9833333333333333\n",
      "iteration no 4512: Loss: 0.2428231116325566, accuracy: 0.9833333333333333\n",
      "iteration no 4513: Loss: 0.24281913116849277, accuracy: 0.9833333333333333\n",
      "iteration no 4514: Loss: 0.24281453328068936, accuracy: 0.9833333333333333\n",
      "iteration no 4515: Loss: 0.24280962880123758, accuracy: 0.9833333333333333\n",
      "iteration no 4516: Loss: 0.24280610235348388, accuracy: 0.9833333333333333\n",
      "iteration no 4517: Loss: 0.24280112031957368, accuracy: 0.9833333333333333\n",
      "iteration no 4518: Loss: 0.24279747941707225, accuracy: 0.9833333333333333\n",
      "iteration no 4519: Loss: 0.24279284844419285, accuracy: 0.9833333333333333\n",
      "iteration no 4520: Loss: 0.24278779735703643, accuracy: 0.9833333333333333\n",
      "iteration no 4521: Loss: 0.24278481530254986, accuracy: 0.9833333333333333\n",
      "iteration no 4522: Loss: 0.2427796530162633, accuracy: 0.9833333333333333\n",
      "iteration no 4523: Loss: 0.24277546209105766, accuracy: 0.9833333333333333\n",
      "iteration no 4524: Loss: 0.24277130110924522, accuracy: 0.9833333333333333\n",
      "iteration no 4525: Loss: 0.2427669980006975, accuracy: 0.9833333333333333\n",
      "iteration no 4526: Loss: 0.24276217971855235, accuracy: 0.9833333333333333\n",
      "iteration no 4527: Loss: 0.2427584216297822, accuracy: 0.9833333333333333\n",
      "iteration no 4528: Loss: 0.2427533345254136, accuracy: 0.9833333333333333\n",
      "iteration no 4529: Loss: 0.24274990294250962, accuracy: 0.9833333333333333\n",
      "iteration no 4530: Loss: 0.24274524128813524, accuracy: 0.9833333333333333\n",
      "iteration no 4531: Loss: 0.2427409258114257, accuracy: 0.9833333333333333\n",
      "iteration no 4532: Loss: 0.2427366940391626, accuracy: 0.9833333333333333\n",
      "iteration no 4533: Loss: 0.24273249176372216, accuracy: 0.9833333333333333\n",
      "iteration no 4534: Loss: 0.24272766415099478, accuracy: 0.9833333333333333\n",
      "iteration no 4535: Loss: 0.2427240341000548, accuracy: 0.9833333333333333\n",
      "iteration no 4536: Loss: 0.24271914276634118, accuracy: 0.9833333333333333\n",
      "iteration no 4537: Loss: 0.24271505087091966, accuracy: 0.9833333333333333\n",
      "iteration no 4538: Loss: 0.24271069563659, accuracy: 0.9833333333333333\n",
      "iteration no 4539: Loss: 0.2427061142701306, accuracy: 0.9833333333333333\n",
      "iteration no 4540: Loss: 0.2427017208334819, accuracy: 0.9833333333333333\n",
      "iteration no 4541: Loss: 0.24269763194573304, accuracy: 0.9833333333333333\n",
      "iteration no 4542: Loss: 0.24269309775306114, accuracy: 0.9833333333333333\n",
      "iteration no 4543: Loss: 0.24268921828651724, accuracy: 0.9833333333333333\n",
      "iteration no 4544: Loss: 0.24268445879802902, accuracy: 0.9833333333333333\n",
      "iteration no 4545: Loss: 0.2426804760030491, accuracy: 0.9833333333333333\n",
      "iteration no 4546: Loss: 0.24267613933930493, accuracy: 0.9833333333333333\n",
      "iteration no 4547: Loss: 0.24267146063090395, accuracy: 0.9833333333333333\n",
      "iteration no 4548: Loss: 0.24266776644801552, accuracy: 0.9833333333333333\n",
      "iteration no 4549: Loss: 0.2426629948501891, accuracy: 0.9833333333333333\n",
      "iteration no 4550: Loss: 0.24265875988434263, accuracy: 0.9833333333333333\n",
      "iteration no 4551: Loss: 0.2426546082367974, accuracy: 0.9833333333333333\n",
      "iteration no 4552: Loss: 0.24264996371368486, accuracy: 0.9833333333333333\n",
      "iteration no 4553: Loss: 0.24264605527261235, accuracy: 0.9833333333333333\n",
      "iteration no 4554: Loss: 0.2426414402402371, accuracy: 0.9833333333333333\n",
      "iteration no 4555: Loss: 0.24263711719619163, accuracy: 0.9833333333333333\n",
      "iteration no 4556: Loss: 0.24263309221000348, accuracy: 0.9833333333333333\n",
      "iteration no 4557: Loss: 0.24262858274557952, accuracy: 0.9833333333333333\n",
      "iteration no 4558: Loss: 0.24262429803880187, accuracy: 0.9833333333333333\n",
      "iteration no 4559: Loss: 0.24262026094514866, accuracy: 0.9833333333333333\n",
      "iteration no 4560: Loss: 0.24261544762841783, accuracy: 0.9833333333333333\n",
      "iteration no 4561: Loss: 0.2426117387804359, accuracy: 0.9833333333333333\n",
      "iteration no 4562: Loss: 0.24260701488960285, accuracy: 0.9833333333333333\n",
      "iteration no 4563: Loss: 0.2426027325188433, accuracy: 0.9833333333333333\n",
      "iteration no 4564: Loss: 0.24259869079038904, accuracy: 0.9833333333333333\n",
      "iteration no 4565: Loss: 0.24259408735205654, accuracy: 0.9833333333333333\n",
      "iteration no 4566: Loss: 0.24259004200992612, accuracy: 0.9833333333333333\n",
      "iteration no 4567: Loss: 0.24258566966964598, accuracy: 0.9833333333333333\n",
      "iteration no 4568: Loss: 0.24258119022296185, accuracy: 0.9833333333333333\n",
      "iteration no 4569: Loss: 0.24257731315220143, accuracy: 0.9833333333333333\n",
      "iteration no 4570: Loss: 0.24257296652440802, accuracy: 0.9833333333333333\n",
      "iteration no 4571: Loss: 0.24256829600239144, accuracy: 0.9833333333333333\n",
      "iteration no 4572: Loss: 0.24256466742326369, accuracy: 0.9833333333333333\n",
      "iteration no 4573: Loss: 0.24255955403041396, accuracy: 0.9833333333333333\n",
      "iteration no 4574: Loss: 0.24255626859257262, accuracy: 0.9833333333333333\n",
      "iteration no 4575: Loss: 0.24255120523682772, accuracy: 0.9833333333333333\n",
      "iteration no 4576: Loss: 0.24254737575097302, accuracy: 0.9833333333333333\n",
      "iteration no 4577: Loss: 0.24254287444532865, accuracy: 0.9833333333333333\n",
      "iteration no 4578: Loss: 0.24253888108993893, accuracy: 0.9833333333333333\n",
      "iteration no 4579: Loss: 0.2425341420066005, accuracy: 0.9833333333333333\n",
      "iteration no 4580: Loss: 0.24253023246309524, accuracy: 0.9833333333333333\n",
      "iteration no 4581: Loss: 0.2425255959616226, accuracy: 0.9833333333333333\n",
      "iteration no 4582: Loss: 0.24252194195991345, accuracy: 0.9833333333333333\n",
      "iteration no 4583: Loss: 0.24251744170886091, accuracy: 0.9833333333333333\n",
      "iteration no 4584: Loss: 0.24251286388655763, accuracy: 0.9833333333333333\n",
      "iteration no 4585: Loss: 0.24250930110497393, accuracy: 0.9833333333333333\n",
      "iteration no 4586: Loss: 0.2425042771823443, accuracy: 0.9833333333333333\n",
      "iteration no 4587: Loss: 0.24250070138524757, accuracy: 0.9833333333333333\n",
      "iteration no 4588: Loss: 0.24249608120809885, accuracy: 0.9833333333333333\n",
      "iteration no 4589: Loss: 0.24249201083860378, accuracy: 0.9833333333333333\n",
      "iteration no 4590: Loss: 0.24248762129130363, accuracy: 0.9833333333333333\n",
      "iteration no 4591: Loss: 0.24248373731004683, accuracy: 0.9833333333333333\n",
      "iteration no 4592: Loss: 0.24247894206727852, accuracy: 0.9833333333333333\n",
      "iteration no 4593: Loss: 0.24247506597812785, accuracy: 0.9833333333333333\n",
      "iteration no 4594: Loss: 0.24247053111535033, accuracy: 0.9833333333333333\n",
      "iteration no 4595: Loss: 0.24246668215953876, accuracy: 0.9833333333333333\n",
      "iteration no 4596: Loss: 0.24246230553834947, accuracy: 0.9833333333333333\n",
      "iteration no 4597: Loss: 0.24245757863093914, accuracy: 0.9833333333333333\n",
      "iteration no 4598: Loss: 0.24245436553973312, accuracy: 0.9833333333333333\n",
      "iteration no 4599: Loss: 0.24244900904521208, accuracy: 0.9833333333333333\n",
      "iteration no 4600: Loss: 0.24244575005721603, accuracy: 0.9833333333333333\n",
      "iteration no 4601: Loss: 0.24244088438243339, accuracy: 0.9833333333333333\n",
      "iteration no 4602: Loss: 0.242436995342656, accuracy: 0.9833333333333333\n",
      "iteration no 4603: Loss: 0.24243253393272907, accuracy: 0.9833333333333333\n",
      "iteration no 4604: Loss: 0.24242835502710391, accuracy: 0.9833333333333333\n",
      "iteration no 4605: Loss: 0.24242414757106429, accuracy: 0.9833333333333333\n",
      "iteration no 4606: Loss: 0.24241997682392982, accuracy: 0.9833333333333333\n",
      "iteration no 4607: Loss: 0.24241570956600772, accuracy: 0.9833333333333333\n",
      "iteration no 4608: Loss: 0.24241130291941604, accuracy: 0.9833333333333333\n",
      "iteration no 4609: Loss: 0.24240763905258794, accuracy: 0.9833333333333333\n",
      "iteration no 4610: Loss: 0.2424025297239903, accuracy: 0.9833333333333333\n",
      "iteration no 4611: Loss: 0.24239948175934006, accuracy: 0.9833333333333333\n",
      "iteration no 4612: Loss: 0.2423940937649235, accuracy: 0.9833333333333333\n",
      "iteration no 4613: Loss: 0.2423903846458938, accuracy: 0.9833333333333333\n",
      "iteration no 4614: Loss: 0.24238639203358622, accuracy: 0.9833333333333333\n",
      "iteration no 4615: Loss: 0.24238143099264847, accuracy: 0.9833333333333333\n",
      "iteration no 4616: Loss: 0.24237831630250123, accuracy: 0.9833333333333333\n",
      "iteration no 4617: Loss: 0.24237290389039012, accuracy: 0.9833333333333333\n",
      "iteration no 4618: Loss: 0.24236948607886072, accuracy: 0.9833333333333333\n",
      "iteration no 4619: Loss: 0.24236456491280306, accuracy: 0.9833333333333333\n",
      "iteration no 4620: Loss: 0.24236020439865402, accuracy: 0.9833333333333333\n",
      "iteration no 4621: Loss: 0.24235646634057745, accuracy: 0.9833333333333333\n",
      "iteration no 4622: Loss: 0.2423517434653427, accuracy: 0.9833333333333333\n",
      "iteration no 4623: Loss: 0.24234760292243998, accuracy: 0.9833333333333333\n",
      "iteration no 4624: Loss: 0.24234309419376712, accuracy: 0.9833333333333333\n",
      "iteration no 4625: Loss: 0.24233940055886044, accuracy: 0.9833333333333333\n",
      "iteration no 4626: Loss: 0.24233409429215275, accuracy: 0.9833333333333333\n",
      "iteration no 4627: Loss: 0.242330546898272, accuracy: 0.9833333333333333\n",
      "iteration no 4628: Loss: 0.2423261543486954, accuracy: 0.9833333333333333\n",
      "iteration no 4629: Loss: 0.2423217773281664, accuracy: 0.9833333333333333\n",
      "iteration no 4630: Loss: 0.24231793380967453, accuracy: 0.9833333333333333\n",
      "iteration no 4631: Loss: 0.24231270689171616, accuracy: 0.9833333333333333\n",
      "iteration no 4632: Loss: 0.24230936536052405, accuracy: 0.9833333333333333\n",
      "iteration no 4633: Loss: 0.24230361764165625, accuracy: 0.9833333333333333\n",
      "iteration no 4634: Loss: 0.24229986408471488, accuracy: 0.9833333333333333\n",
      "iteration no 4635: Loss: 0.24229585483043392, accuracy: 0.9833333333333333\n",
      "iteration no 4636: Loss: 0.24229053098671066, accuracy: 0.9833333333333333\n",
      "iteration no 4637: Loss: 0.2422871615958998, accuracy: 0.9833333333333333\n",
      "iteration no 4638: Loss: 0.24228145722760946, accuracy: 0.9833333333333333\n",
      "iteration no 4639: Loss: 0.24227775228412995, accuracy: 0.9833333333333333\n",
      "iteration no 4640: Loss: 0.24227320303866148, accuracy: 0.9833333333333333\n",
      "iteration no 4641: Loss: 0.2422688978194813, accuracy: 0.9833333333333333\n",
      "iteration no 4642: Loss: 0.24226418145677775, accuracy: 0.9833333333333333\n",
      "iteration no 4643: Loss: 0.2422601618071976, accuracy: 0.9833333333333333\n",
      "iteration no 4644: Loss: 0.24225473553093024, accuracy: 0.9833333333333333\n",
      "iteration no 4645: Loss: 0.2422514085991819, accuracy: 0.9833333333333333\n",
      "iteration no 4646: Loss: 0.24224532847143365, accuracy: 0.9833333333333333\n",
      "iteration no 4647: Loss: 0.24224233857776223, accuracy: 0.9833333333333333\n",
      "iteration no 4648: Loss: 0.24223675132663064, accuracy: 0.9833333333333333\n",
      "iteration no 4649: Loss: 0.24223273220835811, accuracy: 0.9833333333333333\n",
      "iteration no 4650: Loss: 0.24222815867420763, accuracy: 0.9833333333333333\n",
      "iteration no 4651: Loss: 0.24222301010674196, accuracy: 0.9833333333333333\n",
      "iteration no 4652: Loss: 0.24221935778191178, accuracy: 0.9833333333333333\n",
      "iteration no 4653: Loss: 0.24221385356151387, accuracy: 0.9833333333333333\n",
      "iteration no 4654: Loss: 0.24221016513415317, accuracy: 0.9833333333333333\n",
      "iteration no 4655: Loss: 0.24220474853440654, accuracy: 0.9833333333333333\n",
      "iteration no 4656: Loss: 0.24220096091592466, accuracy: 0.9833333333333333\n",
      "iteration no 4657: Loss: 0.24219603231126668, accuracy: 0.9833333333333333\n",
      "iteration no 4658: Loss: 0.242191818897134, accuracy: 0.9833333333333333\n",
      "iteration no 4659: Loss: 0.24218705712677369, accuracy: 0.9833333333333333\n",
      "iteration no 4660: Loss: 0.2421823773249515, accuracy: 0.9833333333333333\n",
      "iteration no 4661: Loss: 0.2421778686304793, accuracy: 0.9833333333333333\n",
      "iteration no 4662: Loss: 0.24217326726410277, accuracy: 0.9833333333333333\n",
      "iteration no 4663: Loss: 0.242169222169854, accuracy: 0.9833333333333333\n",
      "iteration no 4664: Loss: 0.2421635579658002, accuracy: 0.9833333333333333\n",
      "iteration no 4665: Loss: 0.24216034330394376, accuracy: 0.9833333333333333\n",
      "iteration no 4666: Loss: 0.24215452810699656, accuracy: 0.9833333333333333\n",
      "iteration no 4667: Loss: 0.2421510329741908, accuracy: 0.9833333333333333\n",
      "iteration no 4668: Loss: 0.24214585234228014, accuracy: 0.9833333333333333\n",
      "iteration no 4669: Loss: 0.2421413198677185, accuracy: 0.9833333333333333\n",
      "iteration no 4670: Loss: 0.24213748927385761, accuracy: 0.9833333333333333\n",
      "iteration no 4671: Loss: 0.2421315349835756, accuracy: 0.9833333333333333\n",
      "iteration no 4672: Loss: 0.24212865072660833, accuracy: 0.9833333333333333\n",
      "iteration no 4673: Loss: 0.2421227374388305, accuracy: 0.9833333333333333\n",
      "iteration no 4674: Loss: 0.24211913890015552, accuracy: 0.9833333333333333\n",
      "iteration no 4675: Loss: 0.24211424838421294, accuracy: 0.9833333333333333\n",
      "iteration no 4676: Loss: 0.2421095661686563, accuracy: 0.9833333333333333\n",
      "iteration no 4677: Loss: 0.24210546101995944, accuracy: 0.9833333333333333\n",
      "iteration no 4678: Loss: 0.24210018905073166, accuracy: 0.9833333333333333\n",
      "iteration no 4679: Loss: 0.24209650591203025, accuracy: 0.9833333333333333\n",
      "iteration no 4680: Loss: 0.24209092894332684, accuracy: 0.9833333333333333\n",
      "iteration no 4681: Loss: 0.2420868371126862, accuracy: 0.9833333333333333\n",
      "iteration no 4682: Loss: 0.24208266010089233, accuracy: 0.9833333333333333\n",
      "iteration no 4683: Loss: 0.24207793394010202, accuracy: 0.9833333333333333\n",
      "iteration no 4684: Loss: 0.2420730410396482, accuracy: 0.9833333333333333\n",
      "iteration no 4685: Loss: 0.24206954697326688, accuracy: 0.9833333333333333\n",
      "iteration no 4686: Loss: 0.24206354442750666, accuracy: 0.9833333333333333\n",
      "iteration no 4687: Loss: 0.24206035000245774, accuracy: 0.9833333333333333\n",
      "iteration no 4688: Loss: 0.2420550369989489, accuracy: 0.9833333333333333\n",
      "iteration no 4689: Loss: 0.2420506720442408, accuracy: 0.9833333333333333\n",
      "iteration no 4690: Loss: 0.24204636561789705, accuracy: 0.9833333333333333\n",
      "iteration no 4691: Loss: 0.24204113925044363, accuracy: 0.9833333333333333\n",
      "iteration no 4692: Loss: 0.24203763608944961, accuracy: 0.9833333333333333\n",
      "iteration no 4693: Loss: 0.24203183600908268, accuracy: 0.9833333333333333\n",
      "iteration no 4694: Loss: 0.24202878701227643, accuracy: 0.9833333333333333\n",
      "iteration no 4695: Loss: 0.24202329882234735, accuracy: 0.9833333333333333\n",
      "iteration no 4696: Loss: 0.24201884715353816, accuracy: 0.9833333333333333\n",
      "iteration no 4697: Loss: 0.24201487379441936, accuracy: 0.9833333333333333\n",
      "iteration no 4698: Loss: 0.2420090907576208, accuracy: 0.9833333333333333\n",
      "iteration no 4699: Loss: 0.2420060825734034, accuracy: 0.9833333333333333\n",
      "iteration no 4700: Loss: 0.24200016896002063, accuracy: 0.9833333333333333\n",
      "iteration no 4701: Loss: 0.24199689495835386, accuracy: 0.9833333333333333\n",
      "iteration no 4702: Loss: 0.24199163440855964, accuracy: 0.9833333333333333\n",
      "iteration no 4703: Loss: 0.24198693754158035, accuracy: 0.9833333333333333\n",
      "iteration no 4704: Loss: 0.24198338126933627, accuracy: 0.9833333333333333\n",
      "iteration no 4705: Loss: 0.24197739401742716, accuracy: 0.9833333333333333\n",
      "iteration no 4706: Loss: 0.24197446639131842, accuracy: 0.9833333333333333\n",
      "iteration no 4707: Loss: 0.24196875734886752, accuracy: 0.9833333333333333\n",
      "iteration no 4708: Loss: 0.24196476986927323, accuracy: 0.9833333333333333\n",
      "iteration no 4709: Loss: 0.24196055818465662, accuracy: 0.9833333333333333\n",
      "iteration no 4710: Loss: 0.2419552121799079, accuracy: 0.9833333333333333\n",
      "iteration no 4711: Loss: 0.24195180646999181, accuracy: 0.9833333333333333\n",
      "iteration no 4712: Loss: 0.24194590814008549, accuracy: 0.9833333333333333\n",
      "iteration no 4713: Loss: 0.24194232455079195, accuracy: 0.9833333333333333\n",
      "iteration no 4714: Loss: 0.2419372971768189, accuracy: 0.9833333333333333\n",
      "iteration no 4715: Loss: 0.24193360656281654, accuracy: 0.9833333333333333\n",
      "iteration no 4716: Loss: 0.24192839013984158, accuracy: 0.9833333333333333\n",
      "iteration no 4717: Loss: 0.24192431318595203, accuracy: 0.9833333333333333\n",
      "iteration no 4718: Loss: 0.24191968547831713, accuracy: 0.9833333333333333\n",
      "iteration no 4719: Loss: 0.24191488229867592, accuracy: 0.9833333333333333\n",
      "iteration no 4720: Loss: 0.24191102983563123, accuracy: 0.9833333333333333\n",
      "iteration no 4721: Loss: 0.24190582442096567, accuracy: 0.9833333333333333\n",
      "iteration no 4722: Loss: 0.24190173314607588, accuracy: 0.9833333333333333\n",
      "iteration no 4723: Loss: 0.2418973273878411, accuracy: 0.9833333333333333\n",
      "iteration no 4724: Loss: 0.24189238184728312, accuracy: 0.9833333333333333\n",
      "iteration no 4725: Loss: 0.2418884683455314, accuracy: 0.9833333333333333\n",
      "iteration no 4726: Loss: 0.24188314847922987, accuracy: 0.9833333333333333\n",
      "iteration no 4727: Loss: 0.24187900849304195, accuracy: 0.9833333333333333\n",
      "iteration no 4728: Loss: 0.2418744438893655, accuracy: 0.9833333333333333\n",
      "iteration no 4729: Loss: 0.2418703855975029, accuracy: 0.9833333333333333\n",
      "iteration no 4730: Loss: 0.24186545227666195, accuracy: 0.9833333333333333\n",
      "iteration no 4731: Loss: 0.2418615911061613, accuracy: 0.9833333333333333\n",
      "iteration no 4732: Loss: 0.2418569656407341, accuracy: 0.9833333333333333\n",
      "iteration no 4733: Loss: 0.24185241892972342, accuracy: 0.9833333333333333\n",
      "iteration no 4734: Loss: 0.24184853476347123, accuracy: 0.9833333333333333\n",
      "iteration no 4735: Loss: 0.2418438747863172, accuracy: 0.9833333333333333\n",
      "iteration no 4736: Loss: 0.2418397611219945, accuracy: 0.9833333333333333\n",
      "iteration no 4737: Loss: 0.24183543253611017, accuracy: 0.9833333333333333\n",
      "iteration no 4738: Loss: 0.24183071847040555, accuracy: 0.9833333333333333\n",
      "iteration no 4739: Loss: 0.24182699905728783, accuracy: 0.9833333333333333\n",
      "iteration no 4740: Loss: 0.24182214876099034, accuracy: 0.9833333333333333\n",
      "iteration no 4741: Loss: 0.2418180111655358, accuracy: 0.9833333333333333\n",
      "iteration no 4742: Loss: 0.24181395864557442, accuracy: 0.9833333333333333\n",
      "iteration no 4743: Loss: 0.24180895968599325, accuracy: 0.9833333333333333\n",
      "iteration no 4744: Loss: 0.24180543100309843, accuracy: 0.9833333333333333\n",
      "iteration no 4745: Loss: 0.2418003966362952, accuracy: 0.9833333333333333\n",
      "iteration no 4746: Loss: 0.24179631499555854, accuracy: 0.9833333333333333\n",
      "iteration no 4747: Loss: 0.24179243048452576, accuracy: 0.9833333333333333\n",
      "iteration no 4748: Loss: 0.24178721914969759, accuracy: 0.9833333333333333\n",
      "iteration no 4749: Loss: 0.24178341335093967, accuracy: 0.9833333333333333\n",
      "iteration no 4750: Loss: 0.2417789644596835, accuracy: 0.9833333333333333\n",
      "iteration no 4751: Loss: 0.24177499651717352, accuracy: 0.9833333333333333\n",
      "iteration no 4752: Loss: 0.24177017027526276, accuracy: 0.9833333333333333\n",
      "iteration no 4753: Loss: 0.2417663990291089, accuracy: 0.9833333333333333\n",
      "iteration no 4754: Loss: 0.24176205930527545, accuracy: 0.9833333333333333\n",
      "iteration no 4755: Loss: 0.2417571928577194, accuracy: 0.9833333333333333\n",
      "iteration no 4756: Loss: 0.2417533386983825, accuracy: 0.9833333333333333\n",
      "iteration no 4757: Loss: 0.24174859435787158, accuracy: 0.9833333333333333\n",
      "iteration no 4758: Loss: 0.2417447761028314, accuracy: 0.9833333333333333\n",
      "iteration no 4759: Loss: 0.24174037122476968, accuracy: 0.9833333333333333\n",
      "iteration no 4760: Loss: 0.2417355043184063, accuracy: 0.9833333333333333\n",
      "iteration no 4761: Loss: 0.2417323342508218, accuracy: 0.9833333333333333\n",
      "iteration no 4762: Loss: 0.24172741341596246, accuracy: 0.9833333333333333\n",
      "iteration no 4763: Loss: 0.24172348624183337, accuracy: 0.9833333333333333\n",
      "iteration no 4764: Loss: 0.24171917597979864, accuracy: 0.9833333333333333\n",
      "iteration no 4765: Loss: 0.2417152390921595, accuracy: 0.9833333333333333\n",
      "iteration no 4766: Loss: 0.24171076372180544, accuracy: 0.9833333333333333\n",
      "iteration no 4767: Loss: 0.24170677396375878, accuracy: 0.9833333333333333\n",
      "iteration no 4768: Loss: 0.24170311568592565, accuracy: 0.9833333333333333\n",
      "iteration no 4769: Loss: 0.24169784368377156, accuracy: 0.9833333333333333\n",
      "iteration no 4770: Loss: 0.24169433559056153, accuracy: 0.9833333333333333\n",
      "iteration no 4771: Loss: 0.24169063014369963, accuracy: 0.9833333333333333\n",
      "iteration no 4772: Loss: 0.2416856766222691, accuracy: 0.9833333333333333\n",
      "iteration no 4773: Loss: 0.24168160910417902, accuracy: 0.9833333333333333\n",
      "iteration no 4774: Loss: 0.241678247410481, accuracy: 0.9833333333333333\n",
      "iteration no 4775: Loss: 0.24167373568606615, accuracy: 0.9833333333333333\n",
      "iteration no 4776: Loss: 0.2416692447466677, accuracy: 0.9833333333333333\n",
      "iteration no 4777: Loss: 0.24166622924983566, accuracy: 0.9833333333333333\n",
      "iteration no 4778: Loss: 0.24166182328566882, accuracy: 0.9833333333333333\n",
      "iteration no 4779: Loss: 0.24165674432128723, accuracy: 0.9833333333333333\n",
      "iteration no 4780: Loss: 0.24165373288107367, accuracy: 0.9833333333333333\n",
      "iteration no 4781: Loss: 0.24164956297999862, accuracy: 0.9833333333333333\n",
      "iteration no 4782: Loss: 0.24164511736468608, accuracy: 0.9833333333333333\n",
      "iteration no 4783: Loss: 0.24164128933852108, accuracy: 0.9833333333333333\n",
      "iteration no 4784: Loss: 0.24163780156508757, accuracy: 0.9833333333333333\n",
      "iteration no 4785: Loss: 0.24163298499276586, accuracy: 0.9833333333333333\n",
      "iteration no 4786: Loss: 0.24162909177892603, accuracy: 0.9833333333333333\n",
      "iteration no 4787: Loss: 0.2416255866960559, accuracy: 0.9833333333333333\n",
      "iteration no 4788: Loss: 0.24162068981899634, accuracy: 0.9833333333333333\n",
      "iteration no 4789: Loss: 0.24161718153294764, accuracy: 0.9833333333333333\n",
      "iteration no 4790: Loss: 0.24161357899094582, accuracy: 0.9833333333333333\n",
      "iteration no 4791: Loss: 0.2416086780046607, accuracy: 0.9833333333333333\n",
      "iteration no 4792: Loss: 0.24160461239474987, accuracy: 0.9833333333333333\n",
      "iteration no 4793: Loss: 0.2416014581307892, accuracy: 0.9833333333333333\n",
      "iteration no 4794: Loss: 0.24159711017034835, accuracy: 0.9833333333333333\n",
      "iteration no 4795: Loss: 0.24159259733115218, accuracy: 0.9833333333333333\n",
      "iteration no 4796: Loss: 0.24158967183987376, accuracy: 0.9833333333333333\n",
      "iteration no 4797: Loss: 0.24158520758891205, accuracy: 0.9833333333333333\n",
      "iteration no 4798: Loss: 0.24158041900875432, accuracy: 0.9833333333333333\n",
      "iteration no 4799: Loss: 0.24157744901006234, accuracy: 0.9833333333333333\n",
      "iteration no 4800: Loss: 0.2415733960938632, accuracy: 0.9833333333333333\n",
      "iteration no 4801: Loss: 0.24156908343840533, accuracy: 0.9833333333333333\n",
      "iteration no 4802: Loss: 0.24156517879514, accuracy: 0.9833333333333333\n",
      "iteration no 4803: Loss: 0.24156173824239122, accuracy: 0.9833333333333333\n",
      "iteration no 4804: Loss: 0.24155722411689262, accuracy: 0.9833333333333333\n",
      "iteration no 4805: Loss: 0.24155325484984091, accuracy: 0.9833333333333333\n",
      "iteration no 4806: Loss: 0.24154937960737233, accuracy: 0.9833333333333333\n",
      "iteration no 4807: Loss: 0.24154527326848907, accuracy: 0.9833333333333333\n",
      "iteration no 4808: Loss: 0.24154161746768196, accuracy: 0.9833333333333333\n",
      "iteration no 4809: Loss: 0.24153761760473663, accuracy: 0.9833333333333333\n",
      "iteration no 4810: Loss: 0.2415339783436651, accuracy: 0.9833333333333333\n",
      "iteration no 4811: Loss: 0.24152945684365734, accuracy: 0.9833333333333333\n",
      "iteration no 4812: Loss: 0.24152606631428142, accuracy: 0.9833333333333333\n",
      "iteration no 4813: Loss: 0.24152151085796608, accuracy: 0.9833333333333333\n",
      "iteration no 4814: Loss: 0.24151776996579083, accuracy: 0.9833333333333333\n",
      "iteration no 4815: Loss: 0.24151471028501448, accuracy: 0.9833333333333333\n",
      "iteration no 4816: Loss: 0.2415098895683762, accuracy: 0.9833333333333333\n",
      "iteration no 4817: Loss: 0.2415061498286511, accuracy: 0.9833333333333333\n",
      "iteration no 4818: Loss: 0.24150286194373355, accuracy: 0.9833333333333333\n",
      "iteration no 4819: Loss: 0.24149843558944242, accuracy: 0.9833333333333333\n",
      "iteration no 4820: Loss: 0.24149398814992046, accuracy: 0.9833333333333333\n",
      "iteration no 4821: Loss: 0.24149092677183726, accuracy: 0.9833333333333333\n",
      "iteration no 4822: Loss: 0.2414874449377601, accuracy: 0.9833333333333333\n",
      "iteration no 4823: Loss: 0.2414823347172199, accuracy: 0.9833333333333333\n",
      "iteration no 4824: Loss: 0.24147936116707996, accuracy: 0.9833333333333333\n",
      "iteration no 4825: Loss: 0.24147530700920203, accuracy: 0.9833333333333333\n",
      "iteration no 4826: Loss: 0.2414709248336035, accuracy: 0.9833333333333333\n",
      "iteration no 4827: Loss: 0.24146704034086858, accuracy: 0.9833333333333333\n",
      "iteration no 4828: Loss: 0.24146377812759665, accuracy: 0.9833333333333333\n",
      "iteration no 4829: Loss: 0.24145974677731594, accuracy: 0.9833333333333333\n",
      "iteration no 4830: Loss: 0.24145512268839942, accuracy: 0.9833333333333333\n",
      "iteration no 4831: Loss: 0.24145247016078572, accuracy: 0.9833333333333333\n",
      "iteration no 4832: Loss: 0.24144808369214082, accuracy: 0.9833333333333333\n",
      "iteration no 4833: Loss: 0.24144346391442684, accuracy: 0.9833333333333333\n",
      "iteration no 4834: Loss: 0.24144049601209086, accuracy: 0.9833333333333333\n",
      "iteration no 4835: Loss: 0.24143621365252638, accuracy: 0.9833333333333333\n",
      "iteration no 4836: Loss: 0.24143231584975683, accuracy: 0.9833333333333333\n",
      "iteration no 4837: Loss: 0.24142903931430393, accuracy: 0.9833333333333333\n",
      "iteration no 4838: Loss: 0.24142506533379982, accuracy: 0.9833333333333333\n",
      "iteration no 4839: Loss: 0.24142091645434866, accuracy: 0.9833333333333333\n",
      "iteration no 4840: Loss: 0.24141672412128207, accuracy: 0.9833333333333333\n",
      "iteration no 4841: Loss: 0.24141335179613332, accuracy: 0.9833333333333333\n",
      "iteration no 4842: Loss: 0.2414089157967207, accuracy: 0.9833333333333333\n",
      "iteration no 4843: Loss: 0.24140597629836072, accuracy: 0.9833333333333333\n",
      "iteration no 4844: Loss: 0.24140208496608068, accuracy: 0.9833333333333333\n",
      "iteration no 4845: Loss: 0.24139742043418333, accuracy: 0.9833333333333333\n",
      "iteration no 4846: Loss: 0.2413944378621767, accuracy: 0.9833333333333333\n",
      "iteration no 4847: Loss: 0.24138982495351935, accuracy: 0.9833333333333333\n",
      "iteration no 4848: Loss: 0.24138710649214035, accuracy: 0.9833333333333333\n",
      "iteration no 4849: Loss: 0.24138207497705488, accuracy: 0.9833333333333333\n",
      "iteration no 4850: Loss: 0.24137891768881808, accuracy: 0.9833333333333333\n",
      "iteration no 4851: Loss: 0.2413748598050159, accuracy: 0.9833333333333333\n",
      "iteration no 4852: Loss: 0.2413705172139596, accuracy: 0.9833333333333333\n",
      "iteration no 4853: Loss: 0.24136812431110616, accuracy: 0.9833333333333333\n",
      "iteration no 4854: Loss: 0.24136321589287446, accuracy: 0.9833333333333333\n",
      "iteration no 4855: Loss: 0.24135963321142764, accuracy: 0.9833333333333333\n",
      "iteration no 4856: Loss: 0.2413563344469165, accuracy: 0.9833333333333333\n",
      "iteration no 4857: Loss: 0.24135202284982785, accuracy: 0.9833333333333333\n",
      "iteration no 4858: Loss: 0.24134808487224624, accuracy: 0.9833333333333333\n",
      "iteration no 4859: Loss: 0.2413443513207698, accuracy: 0.9833333333333333\n",
      "iteration no 4860: Loss: 0.24134082037233123, accuracy: 0.9833333333333333\n",
      "iteration no 4861: Loss: 0.24133679252325962, accuracy: 0.9833333333333333\n",
      "iteration no 4862: Loss: 0.24133329547737592, accuracy: 0.9833333333333333\n",
      "iteration no 4863: Loss: 0.2413297533187812, accuracy: 0.9833333333333333\n",
      "iteration no 4864: Loss: 0.24132544838029737, accuracy: 0.9833333333333333\n",
      "iteration no 4865: Loss: 0.24132221140430904, accuracy: 0.9833333333333333\n",
      "iteration no 4866: Loss: 0.24131818200257688, accuracy: 0.9833333333333333\n",
      "iteration no 4867: Loss: 0.24131467129745265, accuracy: 0.9833333333333333\n",
      "iteration no 4868: Loss: 0.2413102735471383, accuracy: 0.9833333333333333\n",
      "iteration no 4869: Loss: 0.24130675964921267, accuracy: 0.9833333333333333\n",
      "iteration no 4870: Loss: 0.24130374183183567, accuracy: 0.9833333333333333\n",
      "iteration no 4871: Loss: 0.24129857608871844, accuracy: 0.9833333333333333\n",
      "iteration no 4872: Loss: 0.24129614127027918, accuracy: 0.9833333333333333\n",
      "iteration no 4873: Loss: 0.2412912784142039, accuracy: 0.9833333333333333\n",
      "iteration no 4874: Loss: 0.24128801614467588, accuracy: 0.9833333333333333\n",
      "iteration no 4875: Loss: 0.2412847327087939, accuracy: 0.9833333333333333\n",
      "iteration no 4876: Loss: 0.2412812179932885, accuracy: 0.9833333333333333\n",
      "iteration no 4877: Loss: 0.24127678901296545, accuracy: 0.9833333333333333\n",
      "iteration no 4878: Loss: 0.24127384420747208, accuracy: 0.9833333333333333\n",
      "iteration no 4879: Loss: 0.24126957451929734, accuracy: 0.9833333333333333\n",
      "iteration no 4880: Loss: 0.24126677670010827, accuracy: 0.9833333333333333\n",
      "iteration no 4881: Loss: 0.2412622739974347, accuracy: 0.9833333333333333\n",
      "iteration no 4882: Loss: 0.24125893650575042, accuracy: 0.9833333333333333\n",
      "iteration no 4883: Loss: 0.24125531673902068, accuracy: 0.9833333333333333\n",
      "iteration no 4884: Loss: 0.24125072583641255, accuracy: 0.9833333333333333\n",
      "iteration no 4885: Loss: 0.24124865366664516, accuracy: 0.9833333333333333\n",
      "iteration no 4886: Loss: 0.24124386097181194, accuracy: 0.9833333333333333\n",
      "iteration no 4887: Loss: 0.24124057152798817, accuracy: 0.9833333333333333\n",
      "iteration no 4888: Loss: 0.2412367504481806, accuracy: 0.9833333333333333\n",
      "iteration no 4889: Loss: 0.24123422942072675, accuracy: 0.9833333333333333\n",
      "iteration no 4890: Loss: 0.24122957080007157, accuracy: 0.9833333333333333\n",
      "iteration no 4891: Loss: 0.24122611471695446, accuracy: 0.9833333333333333\n",
      "iteration no 4892: Loss: 0.24122279516181605, accuracy: 0.9833333333333333\n",
      "iteration no 4893: Loss: 0.2412186322679079, accuracy: 0.9833333333333333\n",
      "iteration no 4894: Loss: 0.2412156562720873, accuracy: 0.9833333333333333\n",
      "iteration no 4895: Loss: 0.24121240011107864, accuracy: 0.9833333333333333\n",
      "iteration no 4896: Loss: 0.24120767212998614, accuracy: 0.9833333333333333\n",
      "iteration no 4897: Loss: 0.24120482939200338, accuracy: 0.9833333333333333\n",
      "iteration no 4898: Loss: 0.24120106667253405, accuracy: 0.9833333333333333\n",
      "iteration no 4899: Loss: 0.241197813708432, accuracy: 0.9833333333333333\n",
      "iteration no 4900: Loss: 0.2411930464459307, accuracy: 0.9833333333333333\n",
      "iteration no 4901: Loss: 0.24119132141902905, accuracy: 0.9833333333333333\n",
      "iteration no 4902: Loss: 0.24118715391894957, accuracy: 0.9833333333333333\n",
      "iteration no 4903: Loss: 0.24118242818343782, accuracy: 0.9833333333333333\n",
      "iteration no 4904: Loss: 0.24118041351107666, accuracy: 0.9833333333333333\n",
      "iteration no 4905: Loss: 0.24117562568166073, accuracy: 0.9833333333333333\n",
      "iteration no 4906: Loss: 0.2411730683167736, accuracy: 0.9833333333333333\n",
      "iteration no 4907: Loss: 0.2411692272941786, accuracy: 0.9833333333333333\n",
      "iteration no 4908: Loss: 0.24116571934627817, accuracy: 0.9833333333333333\n",
      "iteration no 4909: Loss: 0.24116172856181783, accuracy: 0.9833333333333333\n",
      "iteration no 4910: Loss: 0.24115828949338636, accuracy: 0.9833333333333333\n",
      "iteration no 4911: Loss: 0.24115544937563116, accuracy: 0.9833333333333333\n",
      "iteration no 4912: Loss: 0.24115173445828492, accuracy: 0.9833333333333333\n",
      "iteration no 4913: Loss: 0.24114711704128156, accuracy: 0.9833333333333333\n",
      "iteration no 4914: Loss: 0.24114516106499406, accuracy: 0.9833333333333333\n",
      "iteration no 4915: Loss: 0.24114085721174403, accuracy: 0.9833333333333333\n",
      "iteration no 4916: Loss: 0.24113647066097665, accuracy: 0.9833333333333333\n",
      "iteration no 4917: Loss: 0.2411345662304272, accuracy: 0.9833333333333333\n",
      "iteration no 4918: Loss: 0.24112969988220162, accuracy: 0.9833333333333333\n",
      "iteration no 4919: Loss: 0.24112691629273234, accuracy: 0.9833333333333333\n",
      "iteration no 4920: Loss: 0.24112345422324744, accuracy: 0.9833333333333333\n",
      "iteration no 4921: Loss: 0.2411198861333797, accuracy: 0.9833333333333333\n",
      "iteration no 4922: Loss: 0.2411157705234041, accuracy: 0.9833333333333333\n",
      "iteration no 4923: Loss: 0.2411126741201256, accuracy: 0.9833333333333333\n",
      "iteration no 4924: Loss: 0.2411094040580948, accuracy: 0.9833333333333333\n",
      "iteration no 4925: Loss: 0.24110583240886016, accuracy: 0.9833333333333333\n",
      "iteration no 4926: Loss: 0.24110150781327555, accuracy: 0.9833333333333333\n",
      "iteration no 4927: Loss: 0.24109913093935642, accuracy: 0.9833333333333333\n",
      "iteration no 4928: Loss: 0.2410945365683123, accuracy: 0.9833333333333333\n",
      "iteration no 4929: Loss: 0.24109151756430885, accuracy: 0.9833333333333333\n",
      "iteration no 4930: Loss: 0.2410888289531084, accuracy: 0.9833333333333333\n",
      "iteration no 4931: Loss: 0.24108410801801283, accuracy: 0.9833333333333333\n",
      "iteration no 4932: Loss: 0.24108097504302506, accuracy: 0.9833333333333333\n",
      "iteration no 4933: Loss: 0.24107804805898794, accuracy: 0.9833333333333333\n",
      "iteration no 4934: Loss: 0.24107385322402664, accuracy: 0.9833333333333333\n",
      "iteration no 4935: Loss: 0.24107046273716037, accuracy: 0.9833333333333333\n",
      "iteration no 4936: Loss: 0.2410672105955471, accuracy: 0.9833333333333333\n",
      "iteration no 4937: Loss: 0.24106376640454036, accuracy: 0.9833333333333333\n",
      "iteration no 4938: Loss: 0.24105983984803525, accuracy: 0.9833333333333333\n",
      "iteration no 4939: Loss: 0.24105660608001594, accuracy: 0.9833333333333333\n",
      "iteration no 4940: Loss: 0.24105377234379732, accuracy: 0.9833333333333333\n",
      "iteration no 4941: Loss: 0.24104928360081468, accuracy: 0.9833333333333333\n",
      "iteration no 4942: Loss: 0.24104654168498926, accuracy: 0.9833333333333333\n",
      "iteration no 4943: Loss: 0.24104271095779764, accuracy: 0.9833333333333333\n",
      "iteration no 4944: Loss: 0.24103974380832616, accuracy: 0.9833333333333333\n",
      "iteration no 4945: Loss: 0.2410352692227214, accuracy: 0.9833333333333333\n",
      "iteration no 4946: Loss: 0.24103333544365108, accuracy: 0.9833333333333333\n",
      "iteration no 4947: Loss: 0.24102942973760366, accuracy: 0.9833333333333333\n",
      "iteration no 4948: Loss: 0.2410248598363, accuracy: 0.9833333333333333\n",
      "iteration no 4949: Loss: 0.24102320597402582, accuracy: 0.9833333333333333\n",
      "iteration no 4950: Loss: 0.24101896379720977, accuracy: 0.9833333333333333\n",
      "iteration no 4951: Loss: 0.24101503587195344, accuracy: 0.9833333333333333\n",
      "iteration no 4952: Loss: 0.24101231794128652, accuracy: 0.9833333333333333\n",
      "iteration no 4953: Loss: 0.24100895679514384, accuracy: 0.9833333333333333\n",
      "iteration no 4954: Loss: 0.24100474910574696, accuracy: 0.9833333333333333\n",
      "iteration no 4955: Loss: 0.2410021526600503, accuracy: 0.9833333333333333\n",
      "iteration no 4956: Loss: 0.24099837773414692, accuracy: 0.9833333333333333\n",
      "iteration no 4957: Loss: 0.240995142409138, accuracy: 0.9833333333333333\n",
      "iteration no 4958: Loss: 0.24099112646334017, accuracy: 0.9833333333333333\n",
      "iteration no 4959: Loss: 0.24098893391815224, accuracy: 0.9833333333333333\n",
      "iteration no 4960: Loss: 0.2409844983624863, accuracy: 0.9833333333333333\n",
      "iteration no 4961: Loss: 0.24098130683486757, accuracy: 0.9833333333333333\n",
      "iteration no 4962: Loss: 0.24097878809106238, accuracy: 0.9833333333333333\n",
      "iteration no 4963: Loss: 0.24097456116132954, accuracy: 0.9833333333333333\n",
      "iteration no 4964: Loss: 0.24097123095682027, accuracy: 0.9833333333333333\n",
      "iteration no 4965: Loss: 0.24096822616369806, accuracy: 0.9833333333333333\n",
      "iteration no 4966: Loss: 0.24096483819091952, accuracy: 0.9833333333333333\n",
      "iteration no 4967: Loss: 0.24096077765270207, accuracy: 0.9833333333333333\n",
      "iteration no 4968: Loss: 0.2409579806009286, accuracy: 0.9833333333333333\n",
      "iteration no 4969: Loss: 0.24095469637473188, accuracy: 0.9833333333333333\n",
      "iteration no 4970: Loss: 0.24095101968916627, accuracy: 0.9833333333333333\n",
      "iteration no 4971: Loss: 0.24094767771043013, accuracy: 0.9833333333333333\n",
      "iteration no 4972: Loss: 0.24094519557754673, accuracy: 0.9833333333333333\n",
      "iteration no 4973: Loss: 0.24094066436309147, accuracy: 0.9833333333333333\n",
      "iteration no 4974: Loss: 0.24093776667099837, accuracy: 0.9833333333333333\n",
      "iteration no 4975: Loss: 0.24093496656862462, accuracy: 0.9833333333333333\n",
      "iteration no 4976: Loss: 0.24093092350701512, accuracy: 0.9833333333333333\n",
      "iteration no 4977: Loss: 0.24092755560903673, accuracy: 0.9833333333333333\n",
      "iteration no 4978: Loss: 0.24092451403136547, accuracy: 0.9833333333333333\n",
      "iteration no 4979: Loss: 0.24092157518643303, accuracy: 0.9833333333333333\n",
      "iteration no 4980: Loss: 0.24091711467019533, accuracy: 0.9833333333333333\n",
      "iteration no 4981: Loss: 0.24091490526694134, accuracy: 0.9833333333333333\n",
      "iteration no 4982: Loss: 0.24091118344985712, accuracy: 0.9833333333333333\n",
      "iteration no 4983: Loss: 0.2409076656350644, accuracy: 0.9833333333333333\n",
      "iteration no 4984: Loss: 0.24090448951652896, accuracy: 0.9833333333333333\n",
      "iteration no 4985: Loss: 0.24090145565824678, accuracy: 0.9833333333333333\n",
      "iteration no 4986: Loss: 0.24089777133062923, accuracy: 0.9833333333333333\n",
      "iteration no 4987: Loss: 0.24089518304359855, accuracy: 0.9833333333333333\n",
      "iteration no 4988: Loss: 0.24089123080722735, accuracy: 0.9833333333333333\n",
      "iteration no 4989: Loss: 0.24088825860848417, accuracy: 0.9833333333333333\n",
      "iteration no 4990: Loss: 0.2408842263938781, accuracy: 0.9833333333333333\n",
      "iteration no 4991: Loss: 0.24088166403881578, accuracy: 0.9833333333333333\n",
      "iteration no 4992: Loss: 0.2408783704093367, accuracy: 0.9833333333333333\n",
      "iteration no 4993: Loss: 0.24087410740769605, accuracy: 0.9833333333333333\n",
      "iteration no 4994: Loss: 0.24087229787759967, accuracy: 0.9833333333333333\n",
      "iteration no 4995: Loss: 0.24086805958733856, accuracy: 0.9833333333333333\n",
      "iteration no 4996: Loss: 0.24086479137262834, accuracy: 0.9833333333333333\n",
      "iteration no 4997: Loss: 0.24086186439290563, accuracy: 0.9833333333333333\n",
      "iteration no 4998: Loss: 0.24085871037894413, accuracy: 0.9833333333333333\n",
      "iteration no 4999: Loss: 0.24085499103743496, accuracy: 0.9833333333333333\n",
      "iteration no 5000: Loss: 0.24085191453977528, accuracy: 0.9833333333333333\n",
      "iteration no 5001: Loss: 0.24084876886665613, accuracy: 0.9833333333333333\n",
      "iteration no 5002: Loss: 0.24084587265606516, accuracy: 0.9833333333333333\n",
      "iteration no 5003: Loss: 0.24084159168948518, accuracy: 0.9833333333333333\n",
      "iteration no 5004: Loss: 0.24083949438342467, accuracy: 0.9833333333333333\n",
      "iteration no 5005: Loss: 0.24083540114002971, accuracy: 0.9833333333333333\n",
      "iteration no 5006: Loss: 0.24083204110859963, accuracy: 0.9833333333333333\n",
      "iteration no 5007: Loss: 0.24082966980063183, accuracy: 0.9833333333333333\n",
      "iteration no 5008: Loss: 0.24082559748789417, accuracy: 0.9833333333333333\n",
      "iteration no 5009: Loss: 0.24082230213007358, accuracy: 0.9833333333333333\n",
      "iteration no 5010: Loss: 0.24081996786087911, accuracy: 0.9833333333333333\n",
      "iteration no 5011: Loss: 0.24081607016276396, accuracy: 0.9866666666666667\n",
      "iteration no 5012: Loss: 0.24081265188274742, accuracy: 0.9833333333333333\n",
      "iteration no 5013: Loss: 0.24080942857231252, accuracy: 0.9833333333333333\n",
      "iteration no 5014: Loss: 0.24080635681020512, accuracy: 0.9866666666666667\n",
      "iteration no 5015: Loss: 0.2408032629426739, accuracy: 0.9833333333333333\n",
      "iteration no 5016: Loss: 0.24079937696158502, accuracy: 0.9833333333333333\n",
      "iteration no 5017: Loss: 0.24079695594781259, accuracy: 0.9833333333333333\n",
      "iteration no 5018: Loss: 0.24079267456367764, accuracy: 0.9833333333333333\n",
      "iteration no 5019: Loss: 0.24078978182250022, accuracy: 0.9833333333333333\n",
      "iteration no 5020: Loss: 0.24078692675091815, accuracy: 0.9833333333333333\n",
      "iteration no 5021: Loss: 0.2407832129082489, accuracy: 0.9833333333333333\n",
      "iteration no 5022: Loss: 0.24077978194391386, accuracy: 0.9833333333333333\n",
      "iteration no 5023: Loss: 0.24077727687470585, accuracy: 0.9833333333333333\n",
      "iteration no 5024: Loss: 0.24077297604775105, accuracy: 0.9866666666666667\n",
      "iteration no 5025: Loss: 0.2407704455864813, accuracy: 0.9833333333333333\n",
      "iteration no 5026: Loss: 0.24076718374447842, accuracy: 0.9866666666666667\n",
      "iteration no 5027: Loss: 0.24076394532268047, accuracy: 0.9866666666666667\n",
      "iteration no 5028: Loss: 0.24076000316110785, accuracy: 0.9833333333333333\n",
      "iteration no 5029: Loss: 0.2407573970859904, accuracy: 0.9866666666666667\n",
      "iteration no 5030: Loss: 0.2407542860213987, accuracy: 0.9833333333333333\n",
      "iteration no 5031: Loss: 0.24075015578889208, accuracy: 0.9866666666666667\n",
      "iteration no 5032: Loss: 0.2407479504822173, accuracy: 0.9866666666666667\n",
      "iteration no 5033: Loss: 0.24074466578204196, accuracy: 0.9833333333333333\n",
      "iteration no 5034: Loss: 0.2407406718224154, accuracy: 0.9866666666666667\n",
      "iteration no 5035: Loss: 0.24073790638749232, accuracy: 0.9866666666666667\n",
      "iteration no 5036: Loss: 0.24073504278600144, accuracy: 0.9833333333333333\n",
      "iteration no 5037: Loss: 0.2407306353246037, accuracy: 0.9866666666666667\n",
      "iteration no 5038: Loss: 0.240728667446018, accuracy: 0.9833333333333333\n",
      "iteration no 5039: Loss: 0.24072470001394694, accuracy: 0.9866666666666667\n",
      "iteration no 5040: Loss: 0.24072140019092322, accuracy: 0.9866666666666667\n",
      "iteration no 5041: Loss: 0.24071844004106113, accuracy: 0.9833333333333333\n",
      "iteration no 5042: Loss: 0.2407155612500788, accuracy: 0.9866666666666667\n",
      "iteration no 5043: Loss: 0.240711644470182, accuracy: 0.9866666666666667\n",
      "iteration no 5044: Loss: 0.24070891499465613, accuracy: 0.9833333333333333\n",
      "iteration no 5045: Loss: 0.24070532551660012, accuracy: 0.9866666666666667\n",
      "iteration no 5046: Loss: 0.2407026895338265, accuracy: 0.9833333333333333\n",
      "iteration no 5047: Loss: 0.2406985296397145, accuracy: 0.9866666666666667\n",
      "iteration no 5048: Loss: 0.2406965721892338, accuracy: 0.9866666666666667\n",
      "iteration no 5049: Loss: 0.24069234662946914, accuracy: 0.9866666666666667\n",
      "iteration no 5050: Loss: 0.24068912690755645, accuracy: 0.9866666666666667\n",
      "iteration no 5051: Loss: 0.2406870707641096, accuracy: 0.9866666666666667\n",
      "iteration no 5052: Loss: 0.24068296590227584, accuracy: 0.9866666666666667\n",
      "iteration no 5053: Loss: 0.24067953171459222, accuracy: 0.9866666666666667\n",
      "iteration no 5054: Loss: 0.2406780762942064, accuracy: 0.9866666666666667\n",
      "iteration no 5055: Loss: 0.24067384974991682, accuracy: 0.9866666666666667\n",
      "iteration no 5056: Loss: 0.24067052074400605, accuracy: 0.9866666666666667\n",
      "iteration no 5057: Loss: 0.24066841726365323, accuracy: 0.9866666666666667\n",
      "iteration no 5058: Loss: 0.2406647765759059, accuracy: 0.9866666666666667\n",
      "iteration no 5059: Loss: 0.2406617488532624, accuracy: 0.9866666666666667\n",
      "iteration no 5060: Loss: 0.24065925928426, accuracy: 0.9866666666666667\n",
      "iteration no 5061: Loss: 0.24065579566464695, accuracy: 0.9866666666666667\n",
      "iteration no 5062: Loss: 0.24065277749623668, accuracy: 0.9866666666666667\n",
      "iteration no 5063: Loss: 0.24065016729957323, accuracy: 0.9866666666666667\n",
      "iteration no 5064: Loss: 0.24064681919818953, accuracy: 0.9866666666666667\n",
      "iteration no 5065: Loss: 0.24064354263489945, accuracy: 0.9866666666666667\n",
      "iteration no 5066: Loss: 0.24064054673486812, accuracy: 0.9866666666666667\n",
      "iteration no 5067: Loss: 0.24063813042764262, accuracy: 0.9866666666666667\n",
      "iteration no 5068: Loss: 0.24063377605590905, accuracy: 0.9833333333333333\n",
      "iteration no 5069: Loss: 0.24063193076718953, accuracy: 0.9866666666666667\n",
      "iteration no 5070: Loss: 0.24062827606693418, accuracy: 0.9866666666666667\n",
      "iteration no 5071: Loss: 0.24062481520377793, accuracy: 0.9866666666666667\n",
      "iteration no 5072: Loss: 0.24062256306678864, accuracy: 0.9866666666666667\n",
      "iteration no 5073: Loss: 0.24062005201717362, accuracy: 0.9866666666666667\n",
      "iteration no 5074: Loss: 0.24061533803731197, accuracy: 0.9833333333333333\n",
      "iteration no 5075: Loss: 0.2406135476403631, accuracy: 0.9866666666666667\n",
      "iteration no 5076: Loss: 0.24061010412554612, accuracy: 0.9833333333333333\n",
      "iteration no 5077: Loss: 0.240606888490431, accuracy: 0.9833333333333333\n",
      "iteration no 5078: Loss: 0.24060407790887786, accuracy: 0.9833333333333333\n",
      "iteration no 5079: Loss: 0.24060141162803667, accuracy: 0.9833333333333333\n",
      "iteration no 5080: Loss: 0.24059768738517295, accuracy: 0.9833333333333333\n",
      "iteration no 5081: Loss: 0.24059508821812453, accuracy: 0.9833333333333333\n",
      "iteration no 5082: Loss: 0.24059216589645888, accuracy: 0.9866666666666667\n",
      "iteration no 5083: Loss: 0.24058949805098373, accuracy: 0.9833333333333333\n",
      "iteration no 5084: Loss: 0.2405859391291958, accuracy: 0.9833333333333333\n",
      "iteration no 5085: Loss: 0.24058318280241978, accuracy: 0.9833333333333333\n",
      "iteration no 5086: Loss: 0.24058017977660395, accuracy: 0.9833333333333333\n",
      "iteration no 5087: Loss: 0.24057705029755605, accuracy: 0.9833333333333333\n",
      "iteration no 5088: Loss: 0.24057430225289683, accuracy: 0.9866666666666667\n",
      "iteration no 5089: Loss: 0.24057142694732572, accuracy: 0.9833333333333333\n",
      "iteration no 5090: Loss: 0.24056779654553162, accuracy: 0.9833333333333333\n",
      "iteration no 5091: Loss: 0.24056573950864532, accuracy: 0.9866666666666667\n",
      "iteration no 5092: Loss: 0.24056247987472756, accuracy: 0.9833333333333333\n",
      "iteration no 5093: Loss: 0.24055872952339108, accuracy: 0.9833333333333333\n",
      "iteration no 5094: Loss: 0.24055716054292398, accuracy: 0.9866666666666667\n",
      "iteration no 5095: Loss: 0.2405534485809816, accuracy: 0.9833333333333333\n",
      "iteration no 5096: Loss: 0.2405502017962446, accuracy: 0.9866666666666667\n",
      "iteration no 5097: Loss: 0.24054808857724394, accuracy: 0.9833333333333333\n",
      "iteration no 5098: Loss: 0.24054490508654341, accuracy: 0.9833333333333333\n",
      "iteration no 5099: Loss: 0.24054147597519349, accuracy: 0.9866666666666667\n",
      "iteration no 5100: Loss: 0.24053911950351078, accuracy: 0.9833333333333333\n",
      "iteration no 5101: Loss: 0.24053588093538575, accuracy: 0.9866666666666667\n",
      "iteration no 5102: Loss: 0.2405333754222076, accuracy: 0.9833333333333333\n",
      "iteration no 5103: Loss: 0.24052981841727727, accuracy: 0.9833333333333333\n",
      "iteration no 5104: Loss: 0.24052783749284914, accuracy: 0.9866666666666667\n",
      "iteration no 5105: Loss: 0.24052411417203018, accuracy: 0.9833333333333333\n",
      "iteration no 5106: Loss: 0.2405214501129409, accuracy: 0.9833333333333333\n",
      "iteration no 5107: Loss: 0.24051878458541698, accuracy: 0.9866666666666667\n",
      "iteration no 5108: Loss: 0.24051614417614722, accuracy: 0.9833333333333333\n",
      "iteration no 5109: Loss: 0.2405123424464129, accuracy: 0.9833333333333333\n",
      "iteration no 5110: Loss: 0.24051042013099283, accuracy: 0.9866666666666667\n",
      "iteration no 5111: Loss: 0.24050666704477658, accuracy: 0.9866666666666667\n",
      "iteration no 5112: Loss: 0.2405040495395963, accuracy: 0.9833333333333333\n",
      "iteration no 5113: Loss: 0.24050175329352022, accuracy: 0.9866666666666667\n",
      "iteration no 5114: Loss: 0.24049872364279923, accuracy: 0.9833333333333333\n",
      "iteration no 5115: Loss: 0.24049492912190823, accuracy: 0.9833333333333333\n",
      "iteration no 5116: Loss: 0.2404932414801737, accuracy: 0.9866666666666667\n",
      "iteration no 5117: Loss: 0.24048971482620876, accuracy: 0.9866666666666667\n",
      "iteration no 5118: Loss: 0.2404863128538175, accuracy: 0.9833333333333333\n",
      "iteration no 5119: Loss: 0.24048467742465635, accuracy: 0.9866666666666667\n",
      "iteration no 5120: Loss: 0.24048141047207733, accuracy: 0.9833333333333333\n",
      "iteration no 5121: Loss: 0.24047766692380765, accuracy: 0.9866666666666667\n",
      "iteration no 5122: Loss: 0.2404759887213004, accuracy: 0.9866666666666667\n",
      "iteration no 5123: Loss: 0.24047249398014997, accuracy: 0.9866666666666667\n",
      "iteration no 5124: Loss: 0.24046953406647092, accuracy: 0.9833333333333333\n",
      "iteration no 5125: Loss: 0.24046754103926507, accuracy: 0.9866666666666667\n",
      "iteration no 5126: Loss: 0.24046368509315913, accuracy: 0.9866666666666667\n",
      "iteration no 5127: Loss: 0.24046127060867584, accuracy: 0.9866666666666667\n",
      "iteration no 5128: Loss: 0.240459243388596, accuracy: 0.9833333333333333\n",
      "iteration no 5129: Loss: 0.24045569267920508, accuracy: 0.9866666666666667\n",
      "iteration no 5130: Loss: 0.24045210871411093, accuracy: 0.9866666666666667\n",
      "iteration no 5131: Loss: 0.24045057657183286, accuracy: 0.9866666666666667\n",
      "iteration no 5132: Loss: 0.24044688592443222, accuracy: 0.9866666666666667\n",
      "iteration no 5133: Loss: 0.24044418907355003, accuracy: 0.9866666666666667\n",
      "iteration no 5134: Loss: 0.24044151876984643, accuracy: 0.9866666666666667\n",
      "iteration no 5135: Loss: 0.24043883693837736, accuracy: 0.9866666666666667\n",
      "iteration no 5136: Loss: 0.2404356547089593, accuracy: 0.9866666666666667\n",
      "iteration no 5137: Loss: 0.24043316452249075, accuracy: 0.9866666666666667\n",
      "iteration no 5138: Loss: 0.24043007420794568, accuracy: 0.9866666666666667\n",
      "iteration no 5139: Loss: 0.24042755243824676, accuracy: 0.9866666666666667\n",
      "iteration no 5140: Loss: 0.24042425574353907, accuracy: 0.9866666666666667\n",
      "iteration no 5141: Loss: 0.2404224585691987, accuracy: 0.9866666666666667\n",
      "iteration no 5142: Loss: 0.24041890236190988, accuracy: 0.9866666666666667\n",
      "iteration no 5143: Loss: 0.24041645960528388, accuracy: 0.9866666666666667\n",
      "iteration no 5144: Loss: 0.24041326419511322, accuracy: 0.9866666666666667\n",
      "iteration no 5145: Loss: 0.2404109107245878, accuracy: 0.9866666666666667\n",
      "iteration no 5146: Loss: 0.2404073008082428, accuracy: 0.9866666666666667\n",
      "iteration no 5147: Loss: 0.24040559982795787, accuracy: 0.9866666666666667\n",
      "iteration no 5148: Loss: 0.24040207047708811, accuracy: 0.9866666666666667\n",
      "iteration no 5149: Loss: 0.2403991148176751, accuracy: 0.9866666666666667\n",
      "iteration no 5150: Loss: 0.24039715192508526, accuracy: 0.9866666666666667\n",
      "iteration no 5151: Loss: 0.24039441508006557, accuracy: 0.9866666666666667\n",
      "iteration no 5152: Loss: 0.2403906247403135, accuracy: 0.9866666666666667\n",
      "iteration no 5153: Loss: 0.24038861780648185, accuracy: 0.9866666666666667\n",
      "iteration no 5154: Loss: 0.24038549749527005, accuracy: 0.9866666666666667\n",
      "iteration no 5155: Loss: 0.24038242637939822, accuracy: 0.9866666666666667\n",
      "iteration no 5156: Loss: 0.24038040135052846, accuracy: 0.9866666666666667\n",
      "iteration no 5157: Loss: 0.24037727594763275, accuracy: 0.9866666666666667\n",
      "iteration no 5158: Loss: 0.24037371107322603, accuracy: 0.9866666666666667\n",
      "iteration no 5159: Loss: 0.24037243704495648, accuracy: 0.9866666666666667\n",
      "iteration no 5160: Loss: 0.24036910920417395, accuracy: 0.9866666666666667\n",
      "iteration no 5161: Loss: 0.2403657254454491, accuracy: 0.9866666666666667\n",
      "iteration no 5162: Loss: 0.2403637165677749, accuracy: 0.9866666666666667\n",
      "iteration no 5163: Loss: 0.24036089871272748, accuracy: 0.9866666666666667\n",
      "iteration no 5164: Loss: 0.24035720982464934, accuracy: 0.9866666666666667\n",
      "iteration no 5165: Loss: 0.24035550236083558, accuracy: 0.9866666666666667\n",
      "iteration no 5166: Loss: 0.24035247684870742, accuracy: 0.9866666666666667\n",
      "iteration no 5167: Loss: 0.2403490177105257, accuracy: 0.9866666666666667\n",
      "iteration no 5168: Loss: 0.24034746879432006, accuracy: 0.9866666666666667\n",
      "iteration no 5169: Loss: 0.24034444469384136, accuracy: 0.9866666666666667\n",
      "iteration no 5170: Loss: 0.2403410402331525, accuracy: 0.9866666666666667\n",
      "iteration no 5171: Loss: 0.24033892696988568, accuracy: 0.9866666666666667\n",
      "iteration no 5172: Loss: 0.24033598384601018, accuracy: 0.9866666666666667\n",
      "iteration no 5173: Loss: 0.24033269466232549, accuracy: 0.9866666666666667\n",
      "iteration no 5174: Loss: 0.24033107072910756, accuracy: 0.9866666666666667\n",
      "iteration no 5175: Loss: 0.24032779030836204, accuracy: 0.9866666666666667\n",
      "iteration no 5176: Loss: 0.24032435674131836, accuracy: 0.9866666666666667\n",
      "iteration no 5177: Loss: 0.24032292100461788, accuracy: 0.9866666666666667\n",
      "iteration no 5178: Loss: 0.24031967237407778, accuracy: 0.9866666666666667\n",
      "iteration no 5179: Loss: 0.24031628937417981, accuracy: 0.9866666666666667\n",
      "iteration no 5180: Loss: 0.24031533177998132, accuracy: 0.9866666666666667\n",
      "iteration no 5181: Loss: 0.2403120280162579, accuracy: 0.9866666666666667\n",
      "iteration no 5182: Loss: 0.24030857777963158, accuracy: 0.9866666666666667\n",
      "iteration no 5183: Loss: 0.24030637827990023, accuracy: 0.9866666666666667\n",
      "iteration no 5184: Loss: 0.24030387817514204, accuracy: 0.9866666666666667\n",
      "iteration no 5185: Loss: 0.24030025648991116, accuracy: 0.9866666666666667\n",
      "iteration no 5186: Loss: 0.24029878924626047, accuracy: 0.9866666666666667\n",
      "iteration no 5187: Loss: 0.24029559568453895, accuracy: 0.9866666666666667\n",
      "iteration no 5188: Loss: 0.24029241572030202, accuracy: 0.9866666666666667\n",
      "iteration no 5189: Loss: 0.2402906412401504, accuracy: 0.9866666666666667\n",
      "iteration no 5190: Loss: 0.2402877105375857, accuracy: 0.9866666666666667\n",
      "iteration no 5191: Loss: 0.24028435165226092, accuracy: 0.9866666666666667\n",
      "iteration no 5192: Loss: 0.2402830976613114, accuracy: 0.9866666666666667\n",
      "iteration no 5193: Loss: 0.24027981171262952, accuracy: 0.9866666666666667\n",
      "iteration no 5194: Loss: 0.24027680641683963, accuracy: 0.9866666666666667\n",
      "iteration no 5195: Loss: 0.24027447895166365, accuracy: 0.9866666666666667\n",
      "iteration no 5196: Loss: 0.2402720676067292, accuracy: 0.9866666666666667\n",
      "iteration no 5197: Loss: 0.24026846091507567, accuracy: 0.9866666666666667\n",
      "iteration no 5198: Loss: 0.2402671585354197, accuracy: 0.9866666666666667\n",
      "iteration no 5199: Loss: 0.24026378766353704, accuracy: 0.9866666666666667\n",
      "iteration no 5200: Loss: 0.24026088460180944, accuracy: 0.9866666666666667\n",
      "iteration no 5201: Loss: 0.24025882639840762, accuracy: 0.9866666666666667\n",
      "iteration no 5202: Loss: 0.24025626509763715, accuracy: 0.9866666666666667\n",
      "iteration no 5203: Loss: 0.2402526633171167, accuracy: 0.9866666666666667\n",
      "iteration no 5204: Loss: 0.2402514473972963, accuracy: 0.9866666666666667\n",
      "iteration no 5205: Loss: 0.24024835767417468, accuracy: 0.9866666666666667\n",
      "iteration no 5206: Loss: 0.24024533987223534, accuracy: 0.9866666666666667\n",
      "iteration no 5207: Loss: 0.24024281425547456, accuracy: 0.9866666666666667\n",
      "iteration no 5208: Loss: 0.24024057943641858, accuracy: 0.9866666666666667\n",
      "iteration no 5209: Loss: 0.24023716883686103, accuracy: 0.9866666666666667\n",
      "iteration no 5210: Loss: 0.24023557568013648, accuracy: 0.9866666666666667\n",
      "iteration no 5211: Loss: 0.24023233091645246, accuracy: 0.9866666666666667\n",
      "iteration no 5212: Loss: 0.2402295994928899, accuracy: 0.9866666666666667\n",
      "iteration no 5213: Loss: 0.2402275018512563, accuracy: 0.9866666666666667\n",
      "iteration no 5214: Loss: 0.24022485686305234, accuracy: 0.9866666666666667\n",
      "iteration no 5215: Loss: 0.24022132864927132, accuracy: 0.9866666666666667\n",
      "iteration no 5216: Loss: 0.24022030229328728, accuracy: 0.9866666666666667\n",
      "iteration no 5217: Loss: 0.24021715377166353, accuracy: 0.9866666666666667\n",
      "iteration no 5218: Loss: 0.24021415864567788, accuracy: 0.9866666666666667\n",
      "iteration no 5219: Loss: 0.24021158629026412, accuracy: 0.9866666666666667\n",
      "iteration no 5220: Loss: 0.24020970831822827, accuracy: 0.9866666666666667\n",
      "iteration no 5221: Loss: 0.24020609510674235, accuracy: 0.9866666666666667\n",
      "iteration no 5222: Loss: 0.2402044405502374, accuracy: 0.9866666666666667\n",
      "iteration no 5223: Loss: 0.24020146291172273, accuracy: 0.9866666666666667\n",
      "iteration no 5224: Loss: 0.24019875293099113, accuracy: 0.9866666666666667\n",
      "iteration no 5225: Loss: 0.24019635683043572, accuracy: 0.9866666666666667\n",
      "iteration no 5226: Loss: 0.24019416828102708, accuracy: 0.9866666666666667\n",
      "iteration no 5227: Loss: 0.24019061595877775, accuracy: 0.9866666666666667\n",
      "iteration no 5228: Loss: 0.2401894407608025, accuracy: 0.9866666666666667\n",
      "iteration no 5229: Loss: 0.24018598839265232, accuracy: 0.9866666666666667\n",
      "iteration no 5230: Loss: 0.24018343538169773, accuracy: 0.9866666666666667\n",
      "iteration no 5231: Loss: 0.2401812738706206, accuracy: 0.9866666666666667\n",
      "iteration no 5232: Loss: 0.2401791660768441, accuracy: 0.9866666666666667\n",
      "iteration no 5233: Loss: 0.24017551542981966, accuracy: 0.9866666666666667\n",
      "iteration no 5234: Loss: 0.24017394220379756, accuracy: 0.9866666666666667\n",
      "iteration no 5235: Loss: 0.2401710097144847, accuracy: 0.9866666666666667\n",
      "iteration no 5236: Loss: 0.24016889144784798, accuracy: 0.9866666666666667\n",
      "iteration no 5237: Loss: 0.24016592308968476, accuracy: 0.9866666666666667\n",
      "iteration no 5238: Loss: 0.24016364985413402, accuracy: 0.9866666666666667\n",
      "iteration no 5239: Loss: 0.240160599393684, accuracy: 0.9866666666666667\n",
      "iteration no 5240: Loss: 0.24015897881495535, accuracy: 0.9866666666666667\n",
      "iteration no 5241: Loss: 0.24015562277585778, accuracy: 0.9866666666666667\n",
      "iteration no 5242: Loss: 0.24015369095419253, accuracy: 0.9866666666666667\n",
      "iteration no 5243: Loss: 0.2401510940055294, accuracy: 0.9866666666666667\n",
      "iteration no 5244: Loss: 0.24014882409375585, accuracy: 0.9866666666666667\n",
      "iteration no 5245: Loss: 0.2401455851012465, accuracy: 0.9866666666666667\n",
      "iteration no 5246: Loss: 0.24014389238382886, accuracy: 0.9866666666666667\n",
      "iteration no 5247: Loss: 0.24014103400460243, accuracy: 0.9866666666666667\n",
      "iteration no 5248: Loss: 0.24013849698158868, accuracy: 0.9866666666666667\n",
      "iteration no 5249: Loss: 0.24013624588167834, accuracy: 0.9866666666666667\n",
      "iteration no 5250: Loss: 0.24013396452141356, accuracy: 0.9866666666666667\n",
      "iteration no 5251: Loss: 0.24013122592414246, accuracy: 0.9866666666666667\n",
      "iteration no 5252: Loss: 0.240129138222716, accuracy: 0.9866666666666667\n",
      "iteration no 5253: Loss: 0.24012587529140306, accuracy: 0.9866666666666667\n",
      "iteration no 5254: Loss: 0.2401239466055825, accuracy: 0.9866666666666667\n",
      "iteration no 5255: Loss: 0.24012173689169586, accuracy: 0.9866666666666667\n",
      "iteration no 5256: Loss: 0.24011877802670173, accuracy: 0.9866666666666667\n",
      "iteration no 5257: Loss: 0.24011621607052663, accuracy: 0.9866666666666667\n",
      "iteration no 5258: Loss: 0.24011446825295618, accuracy: 0.9866666666666667\n",
      "iteration no 5259: Loss: 0.2401112492039273, accuracy: 0.9866666666666667\n",
      "iteration no 5260: Loss: 0.24010909725783083, accuracy: 0.9866666666666667\n",
      "iteration no 5261: Loss: 0.24010697491915897, accuracy: 0.9866666666666667\n",
      "iteration no 5262: Loss: 0.24010437584735528, accuracy: 0.9866666666666667\n",
      "iteration no 5263: Loss: 0.24010138491492977, accuracy: 0.9866666666666667\n",
      "iteration no 5264: Loss: 0.2400996880703618, accuracy: 0.9866666666666667\n",
      "iteration no 5265: Loss: 0.2400970094758074, accuracy: 0.9866666666666667\n",
      "iteration no 5266: Loss: 0.2400944923393329, accuracy: 0.9866666666666667\n",
      "iteration no 5267: Loss: 0.24009225287179214, accuracy: 0.9866666666666667\n",
      "iteration no 5268: Loss: 0.24008981800351997, accuracy: 0.9866666666666667\n",
      "iteration no 5269: Loss: 0.24008747141480324, accuracy: 0.9866666666666667\n",
      "iteration no 5270: Loss: 0.24008532015209227, accuracy: 0.9866666666666667\n",
      "iteration no 5271: Loss: 0.24008211923651374, accuracy: 0.9866666666666667\n",
      "iteration no 5272: Loss: 0.24007995923991332, accuracy: 0.9866666666666667\n",
      "iteration no 5273: Loss: 0.24007813702353997, accuracy: 0.9866666666666667\n",
      "iteration no 5274: Loss: 0.2400749722447754, accuracy: 0.9866666666666667\n",
      "iteration no 5275: Loss: 0.24007263309510918, accuracy: 0.9866666666666667\n",
      "iteration no 5276: Loss: 0.24007134680954142, accuracy: 0.9866666666666667\n",
      "iteration no 5277: Loss: 0.24006781144339281, accuracy: 0.9866666666666667\n",
      "iteration no 5278: Loss: 0.24006520185167887, accuracy: 0.9866666666666667\n",
      "iteration no 5279: Loss: 0.2400641127305112, accuracy: 0.9866666666666667\n",
      "iteration no 5280: Loss: 0.24006074769918775, accuracy: 0.9866666666666667\n",
      "iteration no 5281: Loss: 0.2400579443565988, accuracy: 0.9866666666666667\n",
      "iteration no 5282: Loss: 0.24005677419777016, accuracy: 0.9866666666666667\n",
      "iteration no 5283: Loss: 0.24005363011241687, accuracy: 0.9866666666666667\n",
      "iteration no 5284: Loss: 0.24005086114773716, accuracy: 0.9866666666666667\n",
      "iteration no 5285: Loss: 0.24004972261048385, accuracy: 0.9866666666666667\n",
      "iteration no 5286: Loss: 0.24004647800350692, accuracy: 0.9866666666666667\n",
      "iteration no 5287: Loss: 0.24004372958291476, accuracy: 0.9866666666666667\n",
      "iteration no 5288: Loss: 0.24004253415113413, accuracy: 0.9866666666666667\n",
      "iteration no 5289: Loss: 0.24003925556625438, accuracy: 0.9866666666666667\n",
      "iteration no 5290: Loss: 0.24003680168677652, accuracy: 0.9866666666666667\n",
      "iteration no 5291: Loss: 0.2400354434094895, accuracy: 0.9866666666666667\n",
      "iteration no 5292: Loss: 0.24003219293505196, accuracy: 0.9866666666666667\n",
      "iteration no 5293: Loss: 0.24002969851910777, accuracy: 0.9866666666666667\n",
      "iteration no 5294: Loss: 0.24002826029501667, accuracy: 0.9866666666666667\n",
      "iteration no 5295: Loss: 0.24002526264159496, accuracy: 0.9866666666666667\n",
      "iteration no 5296: Loss: 0.24002260325259472, accuracy: 0.9866666666666667\n",
      "iteration no 5297: Loss: 0.24002139425594013, accuracy: 0.9866666666666667\n",
      "iteration no 5298: Loss: 0.2400182965497491, accuracy: 0.9866666666666667\n",
      "iteration no 5299: Loss: 0.2400155360237355, accuracy: 0.9866666666666667\n",
      "iteration no 5300: Loss: 0.24001429858809048, accuracy: 0.9866666666666667\n",
      "iteration no 5301: Loss: 0.24001130095247403, accuracy: 0.9866666666666667\n",
      "iteration no 5302: Loss: 0.2400085814271699, accuracy: 0.9866666666666667\n",
      "iteration no 5303: Loss: 0.2400073780933954, accuracy: 0.9866666666666667\n",
      "iteration no 5304: Loss: 0.2400042898423254, accuracy: 0.9866666666666667\n",
      "iteration no 5305: Loss: 0.24000168761001756, accuracy: 0.9866666666666667\n",
      "iteration no 5306: Loss: 0.24000057693746063, accuracy: 0.9866666666666667\n",
      "iteration no 5307: Loss: 0.23999738889886782, accuracy: 0.9866666666666667\n",
      "iteration no 5308: Loss: 0.23999504880461373, accuracy: 0.9866666666666667\n",
      "iteration no 5309: Loss: 0.23999298868188654, accuracy: 0.9866666666666667\n",
      "iteration no 5310: Loss: 0.239990401499588, accuracy: 0.9866666666666667\n",
      "iteration no 5311: Loss: 0.23998823343903475, accuracy: 0.9866666666666667\n",
      "iteration no 5312: Loss: 0.23998590046653662, accuracy: 0.9866666666666667\n",
      "iteration no 5313: Loss: 0.2399835394118029, accuracy: 0.9866666666666667\n",
      "iteration no 5314: Loss: 0.23998129263983875, accuracy: 0.9866666666666667\n",
      "iteration no 5315: Loss: 0.2399789990413747, accuracy: 0.9866666666666667\n",
      "iteration no 5316: Loss: 0.23997691768694185, accuracy: 0.9866666666666667\n",
      "iteration no 5317: Loss: 0.23997478276996292, accuracy: 0.9866666666666667\n",
      "iteration no 5318: Loss: 0.23997225801726957, accuracy: 0.9866666666666667\n",
      "iteration no 5319: Loss: 0.2399693066611893, accuracy: 0.9866666666666667\n",
      "iteration no 5320: Loss: 0.239968032560598, accuracy: 0.9866666666666667\n",
      "iteration no 5321: Loss: 0.23996583243458613, accuracy: 0.9866666666666667\n",
      "iteration no 5322: Loss: 0.23996273814246188, accuracy: 0.9866666666666667\n",
      "iteration no 5323: Loss: 0.23996042955736832, accuracy: 0.9866666666666667\n",
      "iteration no 5324: Loss: 0.23995887250224857, accuracy: 0.9866666666666667\n",
      "iteration no 5325: Loss: 0.2399561463752091, accuracy: 0.9866666666666667\n",
      "iteration no 5326: Loss: 0.239953916235073, accuracy: 0.9866666666666667\n",
      "iteration no 5327: Loss: 0.23995146342541865, accuracy: 0.9866666666666667\n",
      "iteration no 5328: Loss: 0.23994918880658314, accuracy: 0.9866666666666667\n",
      "iteration no 5329: Loss: 0.23994707878517654, accuracy: 0.9866666666666667\n",
      "iteration no 5330: Loss: 0.23994454013714916, accuracy: 0.9866666666666667\n",
      "iteration no 5331: Loss: 0.2399421994310337, accuracy: 0.9866666666666667\n",
      "iteration no 5332: Loss: 0.23994074323276116, accuracy: 0.9866666666666667\n",
      "iteration no 5333: Loss: 0.23993810944750887, accuracy: 0.9866666666666667\n",
      "iteration no 5334: Loss: 0.23993477161379298, accuracy: 0.9866666666666667\n",
      "iteration no 5335: Loss: 0.23993369635462714, accuracy: 0.9866666666666667\n",
      "iteration no 5336: Loss: 0.23993128751773196, accuracy: 0.9866666666666667\n",
      "iteration no 5337: Loss: 0.2399279728750554, accuracy: 0.9866666666666667\n",
      "iteration no 5338: Loss: 0.23992684978567907, accuracy: 0.9866666666666667\n",
      "iteration no 5339: Loss: 0.23992453278490017, accuracy: 0.9866666666666667\n",
      "iteration no 5340: Loss: 0.23992130658568395, accuracy: 0.9866666666666667\n",
      "iteration no 5341: Loss: 0.23991989147173975, accuracy: 0.9866666666666667\n",
      "iteration no 5342: Loss: 0.23991768798153176, accuracy: 0.9866666666666667\n",
      "iteration no 5343: Loss: 0.2399144304691603, accuracy: 0.9866666666666667\n",
      "iteration no 5344: Loss: 0.23991326064141294, accuracy: 0.9866666666666667\n",
      "iteration no 5345: Loss: 0.2399108505995618, accuracy: 0.9866666666666667\n",
      "iteration no 5346: Loss: 0.23990796820138666, accuracy: 0.9866666666666667\n",
      "iteration no 5347: Loss: 0.2399062973716468, accuracy: 0.9866666666666667\n",
      "iteration no 5348: Loss: 0.2399040305708383, accuracy: 0.9866666666666667\n",
      "iteration no 5349: Loss: 0.23990120377190532, accuracy: 0.9866666666666667\n",
      "iteration no 5350: Loss: 0.23989959281036471, accuracy: 0.9866666666666667\n",
      "iteration no 5351: Loss: 0.2398971423216735, accuracy: 0.9866666666666667\n",
      "iteration no 5352: Loss: 0.23989514523020872, accuracy: 0.9866666666666667\n",
      "iteration no 5353: Loss: 0.2398930583995386, accuracy: 0.9866666666666667\n",
      "iteration no 5354: Loss: 0.23989006523101586, accuracy: 0.9866666666666667\n",
      "iteration no 5355: Loss: 0.23988793862105873, accuracy: 0.9866666666666667\n",
      "iteration no 5356: Loss: 0.2398869115035599, accuracy: 0.9866666666666667\n",
      "iteration no 5357: Loss: 0.2398833128419193, accuracy: 0.9866666666666667\n",
      "iteration no 5358: Loss: 0.23988134936350372, accuracy: 0.9866666666666667\n",
      "iteration no 5359: Loss: 0.2398796817311235, accuracy: 0.9866666666666667\n",
      "iteration no 5360: Loss: 0.23987708817862996, accuracy: 0.9866666666666667\n",
      "iteration no 5361: Loss: 0.23987436991159863, accuracy: 0.9866666666666667\n",
      "iteration no 5362: Loss: 0.2398729696845952, accuracy: 0.9866666666666667\n",
      "iteration no 5363: Loss: 0.2398699998240122, accuracy: 0.9866666666666667\n",
      "iteration no 5364: Loss: 0.23986846800350692, accuracy: 0.9866666666666667\n",
      "iteration no 5365: Loss: 0.23986561643029242, accuracy: 0.9866666666666667\n",
      "iteration no 5366: Loss: 0.239863964063619, accuracy: 0.9866666666666667\n",
      "iteration no 5367: Loss: 0.239861517167773, accuracy: 0.9866666666666667\n",
      "iteration no 5368: Loss: 0.23985921010064465, accuracy: 0.9866666666666667\n",
      "iteration no 5369: Loss: 0.23985673142467623, accuracy: 0.9866666666666667\n",
      "iteration no 5370: Loss: 0.23985599371824842, accuracy: 0.9866666666666667\n",
      "iteration no 5371: Loss: 0.23985244758179852, accuracy: 0.9866666666666667\n",
      "iteration no 5372: Loss: 0.2398500377377999, accuracy: 0.9866666666666667\n",
      "iteration no 5373: Loss: 0.23984865325294175, accuracy: 0.9866666666666667\n",
      "iteration no 5374: Loss: 0.23984661978811633, accuracy: 0.9866666666666667\n",
      "iteration no 5375: Loss: 0.23984358506122994, accuracy: 0.9866666666666667\n",
      "iteration no 5376: Loss: 0.23984218980892164, accuracy: 0.9866666666666667\n",
      "iteration no 5377: Loss: 0.23983924952170557, accuracy: 0.9866666666666667\n",
      "iteration no 5378: Loss: 0.2398374784616955, accuracy: 0.9866666666666667\n",
      "iteration no 5379: Loss: 0.23983531320592677, accuracy: 0.9866666666666667\n",
      "iteration no 5380: Loss: 0.2398325751151352, accuracy: 0.9866666666666667\n",
      "iteration no 5381: Loss: 0.23983106989485753, accuracy: 0.9866666666666667\n",
      "iteration no 5382: Loss: 0.23982875492153471, accuracy: 0.9866666666666667\n",
      "iteration no 5383: Loss: 0.2398256634349314, accuracy: 0.9866666666666667\n",
      "iteration no 5384: Loss: 0.23982421610292676, accuracy: 0.9866666666666667\n",
      "iteration no 5385: Loss: 0.2398217873209334, accuracy: 0.9866666666666667\n",
      "iteration no 5386: Loss: 0.23981831166016537, accuracy: 0.9866666666666667\n",
      "iteration no 5387: Loss: 0.23981764841559247, accuracy: 0.9866666666666667\n",
      "iteration no 5388: Loss: 0.2398148748549404, accuracy: 0.9866666666666667\n",
      "iteration no 5389: Loss: 0.23981226120375104, accuracy: 0.9866666666666667\n",
      "iteration no 5390: Loss: 0.23981010444419346, accuracy: 0.9866666666666667\n",
      "iteration no 5391: Loss: 0.23980795218755818, accuracy: 0.9866666666666667\n",
      "iteration no 5392: Loss: 0.23980527821054054, accuracy: 0.9866666666666667\n",
      "iteration no 5393: Loss: 0.2398037317653714, accuracy: 0.9866666666666667\n",
      "iteration no 5394: Loss: 0.23980060630007566, accuracy: 0.9866666666666667\n",
      "iteration no 5395: Loss: 0.23979854484323787, accuracy: 0.9866666666666667\n",
      "iteration no 5396: Loss: 0.23979688713591013, accuracy: 0.9866666666666667\n",
      "iteration no 5397: Loss: 0.23979342865206182, accuracy: 0.9866666666666667\n",
      "iteration no 5398: Loss: 0.23979198277685104, accuracy: 0.9866666666666667\n",
      "iteration no 5399: Loss: 0.2397903087535589, accuracy: 0.9866666666666667\n",
      "iteration no 5400: Loss: 0.2397868974641547, accuracy: 0.9866666666666667\n",
      "iteration no 5401: Loss: 0.2397851926841864, accuracy: 0.9866666666666667\n",
      "iteration no 5402: Loss: 0.23978270437589705, accuracy: 0.9866666666666667\n",
      "iteration no 5403: Loss: 0.23978048103647753, accuracy: 0.9866666666666667\n",
      "iteration no 5404: Loss: 0.23977839236713527, accuracy: 0.9866666666666667\n",
      "iteration no 5405: Loss: 0.23977588245275774, accuracy: 0.9866666666666667\n",
      "iteration no 5406: Loss: 0.239773726382425, accuracy: 0.9866666666666667\n",
      "iteration no 5407: Loss: 0.23977175809209914, accuracy: 0.9866666666666667\n",
      "iteration no 5408: Loss: 0.23976966648212095, accuracy: 0.9866666666666667\n",
      "iteration no 5409: Loss: 0.23976632362382178, accuracy: 0.9866666666666667\n",
      "iteration no 5410: Loss: 0.23976498153461373, accuracy: 0.9866666666666667\n",
      "iteration no 5411: Loss: 0.23976264898588479, accuracy: 0.9866666666666667\n",
      "iteration no 5412: Loss: 0.23975966252485567, accuracy: 0.9866666666666667\n",
      "iteration no 5413: Loss: 0.23975821246314794, accuracy: 0.9866666666666667\n",
      "iteration no 5414: Loss: 0.23975591196734714, accuracy: 0.9866666666666667\n",
      "iteration no 5415: Loss: 0.23975331109317277, accuracy: 0.9866666666666667\n",
      "iteration no 5416: Loss: 0.23975204462046357, accuracy: 0.9866666666666667\n",
      "iteration no 5417: Loss: 0.2397485522654737, accuracy: 0.9866666666666667\n",
      "iteration no 5418: Loss: 0.2397465661980428, accuracy: 0.9866666666666667\n",
      "iteration no 5419: Loss: 0.23974520807241162, accuracy: 0.9866666666666667\n",
      "iteration no 5420: Loss: 0.2397418672085702, accuracy: 0.9866666666666667\n",
      "iteration no 5421: Loss: 0.2397398463113635, accuracy: 0.9866666666666667\n",
      "iteration no 5422: Loss: 0.2397383054331978, accuracy: 0.9866666666666667\n",
      "iteration no 5423: Loss: 0.2397356543781171, accuracy: 0.9866666666666667\n",
      "iteration no 5424: Loss: 0.2397336965311004, accuracy: 0.9866666666666667\n",
      "iteration no 5425: Loss: 0.2397311574459886, accuracy: 0.9866666666666667\n",
      "iteration no 5426: Loss: 0.23972865091212292, accuracy: 0.9866666666666667\n",
      "iteration no 5427: Loss: 0.23972722253664866, accuracy: 0.9866666666666667\n",
      "iteration no 5428: Loss: 0.23972455780062202, accuracy: 0.9866666666666667\n",
      "iteration no 5429: Loss: 0.23972194697989402, accuracy: 0.9866666666666667\n",
      "iteration no 5430: Loss: 0.23972045889367383, accuracy: 0.9866666666666667\n",
      "iteration no 5431: Loss: 0.23971806135111265, accuracy: 0.9866666666666667\n",
      "iteration no 5432: Loss: 0.23971559032674805, accuracy: 0.9866666666666667\n",
      "iteration no 5433: Loss: 0.2397131924203879, accuracy: 0.9866666666666667\n",
      "iteration no 5434: Loss: 0.23971185198238618, accuracy: 0.9866666666666667\n",
      "iteration no 5435: Loss: 0.23970888602306414, accuracy: 0.9866666666666667\n",
      "iteration no 5436: Loss: 0.23970655207787447, accuracy: 0.9866666666666667\n",
      "iteration no 5437: Loss: 0.23970509006123422, accuracy: 0.9866666666666667\n",
      "iteration no 5438: Loss: 0.2397022593086685, accuracy: 0.9866666666666667\n",
      "iteration no 5439: Loss: 0.23970032830940713, accuracy: 0.9866666666666667\n",
      "iteration no 5440: Loss: 0.23969807088421385, accuracy: 0.9866666666666667\n",
      "iteration no 5441: Loss: 0.23969575847953456, accuracy: 0.9866666666666667\n",
      "iteration no 5442: Loss: 0.23969436695568172, accuracy: 0.9866666666666667\n",
      "iteration no 5443: Loss: 0.2396919342725402, accuracy: 0.9866666666666667\n",
      "iteration no 5444: Loss: 0.23968865474882817, accuracy: 0.9866666666666667\n",
      "iteration no 5445: Loss: 0.23968767490297577, accuracy: 0.9866666666666667\n",
      "iteration no 5446: Loss: 0.2396854258395918, accuracy: 0.9866666666666667\n",
      "iteration no 5447: Loss: 0.23968243530491465, accuracy: 0.9866666666666667\n",
      "iteration no 5448: Loss: 0.23968109257825831, accuracy: 0.9866666666666667\n",
      "iteration no 5449: Loss: 0.23967852292970163, accuracy: 0.9866666666666667\n",
      "iteration no 5450: Loss: 0.23967617762133142, accuracy: 0.9866666666666667\n",
      "iteration no 5451: Loss: 0.23967484615713336, accuracy: 0.9866666666666667\n",
      "iteration no 5452: Loss: 0.23967170039147306, accuracy: 0.9866666666666667\n",
      "iteration no 5453: Loss: 0.23966987115063637, accuracy: 0.9866666666666667\n",
      "iteration no 5454: Loss: 0.2396688408380211, accuracy: 0.9866666666666667\n",
      "iteration no 5455: Loss: 0.23966520703158128, accuracy: 0.9866666666666667\n",
      "iteration no 5456: Loss: 0.23966355392897903, accuracy: 0.9866666666666667\n",
      "iteration no 5457: Loss: 0.23966165347541973, accuracy: 0.9866666666666667\n",
      "iteration no 5458: Loss: 0.23965894062826465, accuracy: 0.9866666666666667\n",
      "iteration no 5459: Loss: 0.23965747400152088, accuracy: 0.9866666666666667\n",
      "iteration no 5460: Loss: 0.2396549419762199, accuracy: 0.9866666666666667\n",
      "iteration no 5461: Loss: 0.2396523688785899, accuracy: 0.9866666666666667\n",
      "iteration no 5462: Loss: 0.23965148779747347, accuracy: 0.9866666666666667\n",
      "iteration no 5463: Loss: 0.239648618166735, accuracy: 0.9866666666666667\n",
      "iteration no 5464: Loss: 0.23964571944813512, accuracy: 0.9866666666666667\n",
      "iteration no 5465: Loss: 0.23964529289645675, accuracy: 0.9866666666666667\n",
      "iteration no 5466: Loss: 0.23964246661191752, accuracy: 0.9866666666666667\n",
      "iteration no 5467: Loss: 0.2396399889507319, accuracy: 0.9866666666666667\n",
      "iteration no 5468: Loss: 0.23963847308824557, accuracy: 0.9866666666666667\n",
      "iteration no 5469: Loss: 0.2396356323290451, accuracy: 0.9866666666666667\n",
      "iteration no 5470: Loss: 0.2396338531489809, accuracy: 0.9866666666666667\n",
      "iteration no 5471: Loss: 0.2396324494550715, accuracy: 0.9866666666666667\n",
      "iteration no 5472: Loss: 0.2396290282951264, accuracy: 0.9866666666666667\n",
      "iteration no 5473: Loss: 0.2396276833439402, accuracy: 0.9866666666666667\n",
      "iteration no 5474: Loss: 0.23962611687562035, accuracy: 0.9866666666666667\n",
      "iteration no 5475: Loss: 0.23962304145012653, accuracy: 0.9866666666666667\n",
      "iteration no 5476: Loss: 0.23962115609587664, accuracy: 0.9866666666666667\n",
      "iteration no 5477: Loss: 0.23961962937197184, accuracy: 0.9866666666666667\n",
      "iteration no 5478: Loss: 0.23961704062049097, accuracy: 0.9866666666666667\n",
      "iteration no 5479: Loss: 0.2396150591902106, accuracy: 0.9866666666666667\n",
      "iteration no 5480: Loss: 0.23961296635548193, accuracy: 0.9866666666666667\n",
      "iteration no 5481: Loss: 0.2396109175348379, accuracy: 0.9866666666666667\n",
      "iteration no 5482: Loss: 0.23960938171872442, accuracy: 0.9866666666666667\n",
      "iteration no 5483: Loss: 0.23960680079728633, accuracy: 0.9866666666666667\n",
      "iteration no 5484: Loss: 0.23960449253710625, accuracy: 0.9866666666666667\n",
      "iteration no 5485: Loss: 0.23960247819118463, accuracy: 0.9866666666666667\n",
      "iteration no 5486: Loss: 0.23960083087355444, accuracy: 0.9866666666666667\n",
      "iteration no 5487: Loss: 0.23959840473618002, accuracy: 0.9866666666666667\n",
      "iteration no 5488: Loss: 0.23959630361554704, accuracy: 0.9866666666666667\n",
      "iteration no 5489: Loss: 0.23959423235439972, accuracy: 0.9866666666666667\n",
      "iteration no 5490: Loss: 0.23959246041211452, accuracy: 0.9866666666666667\n",
      "iteration no 5491: Loss: 0.23959026597082628, accuracy: 0.9866666666666667\n",
      "iteration no 5492: Loss: 0.23958813997170003, accuracy: 0.9866666666666667\n",
      "iteration no 5493: Loss: 0.23958571239901666, accuracy: 0.9866666666666667\n",
      "iteration no 5494: Loss: 0.23958447884477854, accuracy: 0.9866666666666667\n",
      "iteration no 5495: Loss: 0.2395822104963113, accuracy: 0.9866666666666667\n",
      "iteration no 5496: Loss: 0.23957942334589333, accuracy: 0.9866666666666667\n",
      "iteration no 5497: Loss: 0.23957821472663454, accuracy: 0.9866666666666667\n",
      "iteration no 5498: Loss: 0.2395763175801014, accuracy: 0.9866666666666667\n",
      "iteration no 5499: Loss: 0.23957345807061184, accuracy: 0.9866666666666667\n",
      "iteration no 5500: Loss: 0.239572073817987, accuracy: 0.9866666666666667\n",
      "iteration no 5501: Loss: 0.23956970342480796, accuracy: 0.9866666666666667\n",
      "iteration no 5502: Loss: 0.23956761590866302, accuracy: 0.9866666666666667\n",
      "iteration no 5503: Loss: 0.2395661741627014, accuracy: 0.9866666666666667\n",
      "iteration no 5504: Loss: 0.2395634745124069, accuracy: 0.9866666666666667\n",
      "iteration no 5505: Loss: 0.239561272967436, accuracy: 0.9866666666666667\n",
      "iteration no 5506: Loss: 0.23956025646418236, accuracy: 0.9866666666666667\n",
      "iteration no 5507: Loss: 0.23955746583053333, accuracy: 0.9866666666666667\n",
      "iteration no 5508: Loss: 0.23955520429594332, accuracy: 0.9866666666666667\n",
      "iteration no 5509: Loss: 0.23955389939552246, accuracy: 0.9866666666666667\n",
      "iteration no 5510: Loss: 0.23955153770849058, accuracy: 0.9866666666666667\n",
      "iteration no 5511: Loss: 0.23954947336988708, accuracy: 0.9866666666666667\n",
      "iteration no 5512: Loss: 0.2395477407988787, accuracy: 0.9866666666666667\n",
      "iteration no 5513: Loss: 0.2395453438635958, accuracy: 0.9866666666666667\n",
      "iteration no 5514: Loss: 0.23954396562253533, accuracy: 0.9866666666666667\n",
      "iteration no 5515: Loss: 0.23954180429462427, accuracy: 0.9866666666666667\n",
      "iteration no 5516: Loss: 0.23953928698266133, accuracy: 0.9866666666666667\n",
      "iteration no 5517: Loss: 0.23953795628749958, accuracy: 0.9866666666666667\n",
      "iteration no 5518: Loss: 0.23953593937785045, accuracy: 0.9866666666666667\n",
      "iteration no 5519: Loss: 0.23953342611897832, accuracy: 0.9866666666666667\n",
      "iteration no 5520: Loss: 0.2395322690796471, accuracy: 0.9866666666666667\n",
      "iteration no 5521: Loss: 0.2395296553158592, accuracy: 0.9866666666666667\n",
      "iteration no 5522: Loss: 0.239527590267996, accuracy: 0.9866666666666667\n",
      "iteration no 5523: Loss: 0.2395266371949606, accuracy: 0.9866666666666667\n",
      "iteration no 5524: Loss: 0.23952379976596516, accuracy: 0.9866666666666667\n",
      "iteration no 5525: Loss: 0.23952158807703064, accuracy: 0.9866666666666667\n",
      "iteration no 5526: Loss: 0.23952090203669404, accuracy: 0.9866666666666667\n",
      "iteration no 5527: Loss: 0.2395181413091243, accuracy: 0.9866666666666667\n",
      "iteration no 5528: Loss: 0.23951600285610092, accuracy: 0.9866666666666667\n",
      "iteration no 5529: Loss: 0.23951467704646895, accuracy: 0.9866666666666667\n",
      "iteration no 5530: Loss: 0.23951241506248475, accuracy: 0.9866666666666667\n",
      "iteration no 5531: Loss: 0.2395109430287395, accuracy: 0.9866666666666667\n",
      "iteration no 5532: Loss: 0.239509011325943, accuracy: 0.9866666666666667\n",
      "iteration no 5533: Loss: 0.23950650942576668, accuracy: 0.9866666666666667\n",
      "iteration no 5534: Loss: 0.23950475282936304, accuracy: 0.9866666666666667\n",
      "iteration no 5535: Loss: 0.239503040511356, accuracy: 0.9866666666666667\n",
      "iteration no 5536: Loss: 0.23950109516962267, accuracy: 0.9866666666666667\n",
      "iteration no 5537: Loss: 0.23949909833045507, accuracy: 0.9866666666666667\n",
      "iteration no 5538: Loss: 0.23949701083660524, accuracy: 0.9866666666666667\n",
      "iteration no 5539: Loss: 0.23949514570499508, accuracy: 0.9866666666666667\n",
      "iteration no 5540: Loss: 0.2394935484245473, accuracy: 0.9866666666666667\n",
      "iteration no 5541: Loss: 0.2394912277730687, accuracy: 0.9866666666666667\n",
      "iteration no 5542: Loss: 0.23948940247597855, accuracy: 0.9866666666666667\n",
      "iteration no 5543: Loss: 0.23948748779576506, accuracy: 0.9866666666666667\n",
      "iteration no 5544: Loss: 0.23948584528368172, accuracy: 0.9866666666666667\n",
      "iteration no 5545: Loss: 0.2394835397166541, accuracy: 0.9866666666666667\n",
      "iteration no 5546: Loss: 0.23948171920738665, accuracy: 0.9866666666666667\n",
      "iteration no 5547: Loss: 0.2394797183130974, accuracy: 0.9866666666666667\n",
      "iteration no 5548: Loss: 0.23947796481561223, accuracy: 0.9866666666666667\n",
      "iteration no 5549: Loss: 0.23947617683197903, accuracy: 0.9866666666666667\n",
      "iteration no 5550: Loss: 0.23947422765870896, accuracy: 0.9866666666666667\n",
      "iteration no 5551: Loss: 0.23947168093744603, accuracy: 0.9866666666666667\n",
      "iteration no 5552: Loss: 0.23947033009221835, accuracy: 0.9866666666666667\n",
      "iteration no 5553: Loss: 0.23946888504859776, accuracy: 0.9866666666666667\n",
      "iteration no 5554: Loss: 0.23946611261241038, accuracy: 0.9866666666666667\n",
      "iteration no 5555: Loss: 0.23946474083893887, accuracy: 0.9866666666666667\n",
      "iteration no 5556: Loss: 0.23946278329257972, accuracy: 0.9866666666666667\n",
      "iteration no 5557: Loss: 0.23946069309284176, accuracy: 0.9866666666666667\n",
      "iteration no 5558: Loss: 0.2394591665064485, accuracy: 0.9866666666666667\n",
      "iteration no 5559: Loss: 0.23945689614946386, accuracy: 0.9866666666666667\n",
      "iteration no 5560: Loss: 0.23945453563530433, accuracy: 0.9866666666666667\n",
      "iteration no 5561: Loss: 0.23945351886207086, accuracy: 0.9866666666666667\n",
      "iteration no 5562: Loss: 0.23945128595956117, accuracy: 0.9866666666666667\n",
      "iteration no 5563: Loss: 0.23944893978895593, accuracy: 0.9866666666666667\n",
      "iteration no 5564: Loss: 0.23944744109905702, accuracy: 0.9866666666666667\n",
      "iteration no 5565: Loss: 0.23944509850382212, accuracy: 0.9866666666666667\n",
      "iteration no 5566: Loss: 0.23944350264866443, accuracy: 0.9866666666666667\n",
      "iteration no 5567: Loss: 0.2394419531930984, accuracy: 0.9866666666666667\n",
      "iteration no 5568: Loss: 0.23943910292728932, accuracy: 0.9866666666666667\n",
      "iteration no 5569: Loss: 0.23943746242369573, accuracy: 0.9866666666666667\n",
      "iteration no 5570: Loss: 0.23943620043667557, accuracy: 0.9866666666666667\n",
      "iteration no 5571: Loss: 0.23943349695175115, accuracy: 0.9866666666666667\n",
      "iteration no 5572: Loss: 0.23943220410733468, accuracy: 0.9866666666666667\n",
      "iteration no 5573: Loss: 0.23942995761981364, accuracy: 0.9866666666666667\n",
      "iteration no 5574: Loss: 0.23942741104456294, accuracy: 0.9866666666666667\n",
      "iteration no 5575: Loss: 0.23942680756097998, accuracy: 0.9866666666666667\n",
      "iteration no 5576: Loss: 0.2394243193352743, accuracy: 0.9866666666666667\n",
      "iteration no 5577: Loss: 0.23942177310880852, accuracy: 0.9866666666666667\n",
      "iteration no 5578: Loss: 0.23942080050491196, accuracy: 0.9866666666666667\n",
      "iteration no 5579: Loss: 0.23941859137969995, accuracy: 0.9866666666666667\n",
      "iteration no 5580: Loss: 0.23941632564757775, accuracy: 0.9866666666666667\n",
      "iteration no 5581: Loss: 0.23941528229052028, accuracy: 0.9866666666666667\n",
      "iteration no 5582: Loss: 0.23941250146397597, accuracy: 0.9866666666666667\n",
      "iteration no 5583: Loss: 0.23941064743829651, accuracy: 0.9866666666666667\n",
      "iteration no 5584: Loss: 0.23940958856690026, accuracy: 0.9866666666666667\n",
      "iteration no 5585: Loss: 0.23940688010239797, accuracy: 0.9866666666666667\n",
      "iteration no 5586: Loss: 0.23940523273446418, accuracy: 0.9866666666666667\n",
      "iteration no 5587: Loss: 0.23940347083888494, accuracy: 0.9866666666666667\n",
      "iteration no 5588: Loss: 0.23940109434709378, accuracy: 0.9866666666666667\n",
      "iteration no 5589: Loss: 0.23939999515597093, accuracy: 0.9866666666666667\n",
      "iteration no 5590: Loss: 0.23939790394409943, accuracy: 0.9866666666666667\n",
      "iteration no 5591: Loss: 0.23939512672702049, accuracy: 0.9866666666666667\n",
      "iteration no 5592: Loss: 0.23939423440448515, accuracy: 0.9866666666666667\n",
      "iteration no 5593: Loss: 0.2393921656992185, accuracy: 0.9866666666666667\n",
      "iteration no 5594: Loss: 0.23938974787514977, accuracy: 0.9866666666666667\n",
      "iteration no 5595: Loss: 0.23938891924542044, accuracy: 0.9866666666666667\n",
      "iteration no 5596: Loss: 0.23938626810198366, accuracy: 0.9866666666666667\n",
      "iteration no 5597: Loss: 0.23938403115091378, accuracy: 0.9866666666666667\n",
      "iteration no 5598: Loss: 0.2393833763675683, accuracy: 0.9866666666666667\n",
      "iteration no 5599: Loss: 0.23938070166565953, accuracy: 0.9866666666666667\n",
      "iteration no 5600: Loss: 0.2393786964431176, accuracy: 0.9866666666666667\n",
      "iteration no 5601: Loss: 0.23937731782516142, accuracy: 0.9866666666666667\n",
      "iteration no 5602: Loss: 0.2393748744780489, accuracy: 0.9866666666666667\n",
      "iteration no 5603: Loss: 0.23937361593513978, accuracy: 0.9866666666666667\n",
      "iteration no 5604: Loss: 0.23937191573600874, accuracy: 0.9866666666666667\n",
      "iteration no 5605: Loss: 0.23936920134805628, accuracy: 0.9866666666666667\n",
      "iteration no 5606: Loss: 0.23936798171059712, accuracy: 0.9866666666666667\n",
      "iteration no 5607: Loss: 0.23936615683755733, accuracy: 0.9866666666666667\n",
      "iteration no 5608: Loss: 0.23936336631308133, accuracy: 0.9866666666666667\n",
      "iteration no 5609: Loss: 0.2393619214870562, accuracy: 0.9866666666666667\n",
      "iteration no 5610: Loss: 0.239359159506363, accuracy: 0.9866666666666667\n",
      "iteration no 5611: Loss: 0.2393562052176785, accuracy: 0.9866666666666667\n",
      "iteration no 5612: Loss: 0.23935547536379104, accuracy: 0.9866666666666667\n",
      "iteration no 5613: Loss: 0.23935278922314707, accuracy: 0.9866666666666667\n",
      "iteration no 5614: Loss: 0.23935010416158856, accuracy: 0.9866666666666667\n",
      "iteration no 5615: Loss: 0.2393486206134075, accuracy: 0.9866666666666667\n",
      "iteration no 5616: Loss: 0.23934558257828698, accuracy: 0.9866666666666667\n",
      "iteration no 5617: Loss: 0.23934380840299202, accuracy: 0.9866666666666667\n",
      "iteration no 5618: Loss: 0.23934235457130487, accuracy: 0.9866666666666667\n",
      "iteration no 5619: Loss: 0.23933914113593202, accuracy: 0.9866666666666667\n",
      "iteration no 5620: Loss: 0.23933725477265136, accuracy: 0.9866666666666667\n",
      "iteration no 5621: Loss: 0.2393353024827809, accuracy: 0.9866666666666667\n",
      "iteration no 5622: Loss: 0.23933262766001345, accuracy: 0.9866666666666667\n",
      "iteration no 5623: Loss: 0.23933140983535484, accuracy: 0.9866666666666667\n",
      "iteration no 5624: Loss: 0.23932892990413485, accuracy: 0.9866666666666667\n",
      "iteration no 5625: Loss: 0.23932589863170706, accuracy: 0.9866666666666667\n",
      "iteration no 5626: Loss: 0.23932483414942496, accuracy: 0.9866666666666667\n",
      "iteration no 5627: Loss: 0.2393224576274225, accuracy: 0.9866666666666667\n",
      "iteration no 5628: Loss: 0.23931991201735342, accuracy: 0.9866666666666667\n",
      "iteration no 5629: Loss: 0.23931866117044606, accuracy: 0.9866666666666667\n",
      "iteration no 5630: Loss: 0.23931557008910118, accuracy: 0.9866666666666667\n",
      "iteration no 5631: Loss: 0.2393135606571501, accuracy: 0.9866666666666667\n",
      "iteration no 5632: Loss: 0.23931248462043886, accuracy: 0.9866666666666667\n",
      "iteration no 5633: Loss: 0.2393092629828082, accuracy: 0.9866666666666667\n",
      "iteration no 5634: Loss: 0.23930757571625055, accuracy: 0.9866666666666667\n",
      "iteration no 5635: Loss: 0.23930562824772514, accuracy: 0.9866666666666667\n",
      "iteration no 5636: Loss: 0.2393026739826453, accuracy: 0.9866666666666667\n",
      "iteration no 5637: Loss: 0.2393017075192998, accuracy: 0.9866666666666667\n",
      "iteration no 5638: Loss: 0.23929934857768126, accuracy: 0.9866666666666667\n",
      "iteration no 5639: Loss: 0.23929640603384658, accuracy: 0.9866666666666667\n",
      "iteration no 5640: Loss: 0.2392953388155823, accuracy: 0.9866666666666667\n",
      "iteration no 5641: Loss: 0.23929281762351143, accuracy: 0.9866666666666667\n",
      "iteration no 5642: Loss: 0.23929053604756906, accuracy: 0.9866666666666667\n",
      "iteration no 5643: Loss: 0.2392891956932017, accuracy: 0.9866666666666667\n",
      "iteration no 5644: Loss: 0.2392863843956158, accuracy: 0.9866666666666667\n",
      "iteration no 5645: Loss: 0.23928418795878842, accuracy: 0.9866666666666667\n",
      "iteration no 5646: Loss: 0.23928284104428116, accuracy: 0.9866666666666667\n",
      "iteration no 5647: Loss: 0.23928011982458708, accuracy: 0.9866666666666667\n",
      "iteration no 5648: Loss: 0.23927827120313444, accuracy: 0.9866666666666667\n",
      "iteration no 5649: Loss: 0.23927646964493, accuracy: 0.9866666666666667\n",
      "iteration no 5650: Loss: 0.23927358407297705, accuracy: 0.9866666666666667\n",
      "iteration no 5651: Loss: 0.23927222707235973, accuracy: 0.9866666666666667\n",
      "iteration no 5652: Loss: 0.2392704585058359, accuracy: 0.9866666666666667\n",
      "iteration no 5653: Loss: 0.23926740511221023, accuracy: 0.9866666666666667\n",
      "iteration no 5654: Loss: 0.23926616487797286, accuracy: 0.9866666666666667\n",
      "iteration no 5655: Loss: 0.23926381149438344, accuracy: 0.9866666666666667\n",
      "iteration no 5656: Loss: 0.23926117133282454, accuracy: 0.9866666666666667\n",
      "iteration no 5657: Loss: 0.23926061284587555, accuracy: 0.9866666666666667\n",
      "iteration no 5658: Loss: 0.23925776020729783, accuracy: 0.9866666666666667\n",
      "iteration no 5659: Loss: 0.23925516557641724, accuracy: 0.9866666666666667\n",
      "iteration no 5660: Loss: 0.23925402449856714, accuracy: 0.9866666666666667\n",
      "iteration no 5661: Loss: 0.2392512528739294, accuracy: 0.9866666666666667\n",
      "iteration no 5662: Loss: 0.23924954946734928, accuracy: 0.9866666666666667\n",
      "iteration no 5663: Loss: 0.23924806615872324, accuracy: 0.9866666666666667\n",
      "iteration no 5664: Loss: 0.23924506373190657, accuracy: 0.9866666666666667\n",
      "iteration no 5665: Loss: 0.23924330300533797, accuracy: 0.9866666666666667\n",
      "iteration no 5666: Loss: 0.23924172953494605, accuracy: 0.9866666666666667\n",
      "iteration no 5667: Loss: 0.23923912023694865, accuracy: 0.9866666666666667\n",
      "iteration no 5668: Loss: 0.2392376584763092, accuracy: 0.9866666666666667\n",
      "iteration no 5669: Loss: 0.2392356138797585, accuracy: 0.9866666666666667\n",
      "iteration no 5670: Loss: 0.23923258929493973, accuracy: 0.9866666666666667\n",
      "iteration no 5671: Loss: 0.2392316060543758, accuracy: 0.9866666666666667\n",
      "iteration no 5672: Loss: 0.2392296846547709, accuracy: 0.9866666666666667\n",
      "iteration no 5673: Loss: 0.23922672909863374, accuracy: 0.9866666666666667\n",
      "iteration no 5674: Loss: 0.23922601573837776, accuracy: 0.9866666666666667\n",
      "iteration no 5675: Loss: 0.23922315008168454, accuracy: 0.9866666666666667\n",
      "iteration no 5676: Loss: 0.2392205629503799, accuracy: 0.9866666666666667\n",
      "iteration no 5677: Loss: 0.2392200485638602, accuracy: 0.9866666666666667\n",
      "iteration no 5678: Loss: 0.23921729415851878, accuracy: 0.9866666666666667\n",
      "iteration no 5679: Loss: 0.23921502729125357, accuracy: 0.9866666666666667\n",
      "iteration no 5680: Loss: 0.23921382597859803, accuracy: 0.9866666666666667\n",
      "iteration no 5681: Loss: 0.23921088299065546, accuracy: 0.9866666666666667\n",
      "iteration no 5682: Loss: 0.23920926436315237, accuracy: 0.9866666666666667\n",
      "iteration no 5683: Loss: 0.23920788759835496, accuracy: 0.9866666666666667\n",
      "iteration no 5684: Loss: 0.23920502497542434, accuracy: 0.9866666666666667\n",
      "iteration no 5685: Loss: 0.23920349094791807, accuracy: 0.9866666666666667\n",
      "iteration no 5686: Loss: 0.2392016045315647, accuracy: 0.9866666666666667\n",
      "iteration no 5687: Loss: 0.23919886889520292, accuracy: 0.9866666666666667\n",
      "iteration no 5688: Loss: 0.23919797174050145, accuracy: 0.9866666666666667\n",
      "iteration no 5689: Loss: 0.23919564777102184, accuracy: 0.9866666666666667\n",
      "iteration no 5690: Loss: 0.23919313299689704, accuracy: 0.9866666666666667\n",
      "iteration no 5691: Loss: 0.23919173680988715, accuracy: 0.9866666666666667\n",
      "iteration no 5692: Loss: 0.23918935894818788, accuracy: 0.9866666666666667\n",
      "iteration no 5693: Loss: 0.2391873819465544, accuracy: 0.9866666666666667\n",
      "iteration no 5694: Loss: 0.23918611583005514, accuracy: 0.9866666666666667\n",
      "iteration no 5695: Loss: 0.2391835203248895, accuracy: 0.9866666666666667\n",
      "iteration no 5696: Loss: 0.23918140371718433, accuracy: 0.9866666666666667\n",
      "iteration no 5697: Loss: 0.2391799643267951, accuracy: 0.9866666666666667\n",
      "iteration no 5698: Loss: 0.2391775482829709, accuracy: 0.9866666666666667\n",
      "iteration no 5699: Loss: 0.23917578390047656, accuracy: 0.9866666666666667\n",
      "iteration no 5700: Loss: 0.23917418829630915, accuracy: 0.9866666666666667\n",
      "iteration no 5701: Loss: 0.23917160561290363, accuracy: 0.9866666666666667\n",
      "iteration no 5702: Loss: 0.2391697139684837, accuracy: 0.9866666666666667\n",
      "iteration no 5703: Loss: 0.23916801185589048, accuracy: 0.9866666666666667\n",
      "iteration no 5704: Loss: 0.23916608163884834, accuracy: 0.9866666666666667\n",
      "iteration no 5705: Loss: 0.23916412923837127, accuracy: 0.9866666666666667\n",
      "iteration no 5706: Loss: 0.2391621578413191, accuracy: 0.9866666666666667\n",
      "iteration no 5707: Loss: 0.23915992544689774, accuracy: 0.9866666666666667\n",
      "iteration no 5708: Loss: 0.23915810967564854, accuracy: 0.9866666666666667\n",
      "iteration no 5709: Loss: 0.23915632650899482, accuracy: 0.9866666666666667\n",
      "iteration no 5710: Loss: 0.23915449249294196, accuracy: 0.9866666666666667\n",
      "iteration no 5711: Loss: 0.23915249710944292, accuracy: 0.9866666666666667\n",
      "iteration no 5712: Loss: 0.23915049976558841, accuracy: 0.9866666666666667\n",
      "iteration no 5713: Loss: 0.23914818443322466, accuracy: 0.9866666666666667\n",
      "iteration no 5714: Loss: 0.2391464862495934, accuracy: 0.9866666666666667\n",
      "iteration no 5715: Loss: 0.2391450717642603, accuracy: 0.9866666666666667\n",
      "iteration no 5716: Loss: 0.23914246798729982, accuracy: 0.9866666666666667\n",
      "iteration no 5717: Loss: 0.23914092073703463, accuracy: 0.9866666666666667\n",
      "iteration no 5718: Loss: 0.23913902521688385, accuracy: 0.9866666666666667\n",
      "iteration no 5719: Loss: 0.23913634315641838, accuracy: 0.9866666666666667\n",
      "iteration no 5720: Loss: 0.23913539533840597, accuracy: 0.9866666666666667\n",
      "iteration no 5721: Loss: 0.23913343386048225, accuracy: 0.9866666666666667\n",
      "iteration no 5722: Loss: 0.23913094566771143, accuracy: 0.9866666666666667\n",
      "iteration no 5723: Loss: 0.23912952519310174, accuracy: 0.9866666666666667\n",
      "iteration no 5724: Loss: 0.23912735041363786, accuracy: 0.9866666666666667\n",
      "iteration no 5725: Loss: 0.23912497918970732, accuracy: 0.9866666666666667\n",
      "iteration no 5726: Loss: 0.23912393740429683, accuracy: 0.9866666666666667\n",
      "iteration no 5727: Loss: 0.23912170064193006, accuracy: 0.9866666666666667\n",
      "iteration no 5728: Loss: 0.23911958079251497, accuracy: 0.9866666666666667\n",
      "iteration no 5729: Loss: 0.23911811630308746, accuracy: 0.9866666666666667\n",
      "iteration no 5730: Loss: 0.23911561808266224, accuracy: 0.9866666666666667\n",
      "iteration no 5731: Loss: 0.2391137848819041, accuracy: 0.9866666666666667\n",
      "iteration no 5732: Loss: 0.2391126209579621, accuracy: 0.9866666666666667\n",
      "iteration no 5733: Loss: 0.23911002610244797, accuracy: 0.9866666666666667\n",
      "iteration no 5734: Loss: 0.23910835885082937, accuracy: 0.9866666666666667\n",
      "iteration no 5735: Loss: 0.2391066993382, accuracy: 0.9866666666666667\n",
      "iteration no 5736: Loss: 0.23910402126911406, accuracy: 0.9866666666666667\n",
      "iteration no 5737: Loss: 0.239102775784092, accuracy: 0.9866666666666667\n",
      "iteration no 5738: Loss: 0.23910103324895532, accuracy: 0.9866666666666667\n",
      "iteration no 5739: Loss: 0.23909844740429997, accuracy: 0.9866666666666667\n",
      "iteration no 5740: Loss: 0.23909738477978487, accuracy: 0.9866666666666667\n",
      "iteration no 5741: Loss: 0.23909508414465394, accuracy: 0.9866666666666667\n",
      "iteration no 5742: Loss: 0.2390928037560044, accuracy: 0.9866666666666667\n",
      "iteration no 5743: Loss: 0.23909164464760557, accuracy: 0.9866666666666667\n",
      "iteration no 5744: Loss: 0.23908949699930154, accuracy: 0.9866666666666667\n",
      "iteration no 5745: Loss: 0.23908735110730883, accuracy: 0.9866666666666667\n",
      "iteration no 5746: Loss: 0.23908597441717414, accuracy: 0.9866666666666667\n",
      "iteration no 5747: Loss: 0.2390834585926629, accuracy: 0.9866666666666667\n",
      "iteration no 5748: Loss: 0.23908162255101745, accuracy: 0.9866666666666667\n",
      "iteration no 5749: Loss: 0.23908041569988664, accuracy: 0.9866666666666667\n",
      "iteration no 5750: Loss: 0.23907795193072376, accuracy: 0.9866666666666667\n",
      "iteration no 5751: Loss: 0.239076377218355, accuracy: 0.9866666666666667\n",
      "iteration no 5752: Loss: 0.23907468687000888, accuracy: 0.9866666666666667\n",
      "iteration no 5753: Loss: 0.2390719217824699, accuracy: 0.9866666666666667\n",
      "iteration no 5754: Loss: 0.23907077794185727, accuracy: 0.9866666666666667\n",
      "iteration no 5755: Loss: 0.23906911292763047, accuracy: 0.9866666666666667\n",
      "iteration no 5756: Loss: 0.2390664864709393, accuracy: 0.9866666666666667\n",
      "iteration no 5757: Loss: 0.23906568427681602, accuracy: 0.9866666666666667\n",
      "iteration no 5758: Loss: 0.23906326635893682, accuracy: 0.9866666666666667\n",
      "iteration no 5759: Loss: 0.2390607077778074, accuracy: 0.9866666666666667\n",
      "iteration no 5760: Loss: 0.23905980186101633, accuracy: 0.9866666666666667\n",
      "iteration no 5761: Loss: 0.23905751749892257, accuracy: 0.9866666666666667\n",
      "iteration no 5762: Loss: 0.23905554384761638, accuracy: 0.9866666666666667\n",
      "iteration no 5763: Loss: 0.23905451086515955, accuracy: 0.9866666666666667\n",
      "iteration no 5764: Loss: 0.2390518294342559, accuracy: 0.9866666666666667\n",
      "iteration no 5765: Loss: 0.2390498880359857, accuracy: 0.9866666666666667\n",
      "iteration no 5766: Loss: 0.23904857851373087, accuracy: 0.9866666666666667\n",
      "iteration no 5767: Loss: 0.23904603196089724, accuracy: 0.9866666666666667\n",
      "iteration no 5768: Loss: 0.23904485958618688, accuracy: 0.9866666666666667\n",
      "iteration no 5769: Loss: 0.23904313663188478, accuracy: 0.9866666666666667\n",
      "iteration no 5770: Loss: 0.23904044252326906, accuracy: 0.9866666666666667\n",
      "iteration no 5771: Loss: 0.23903938452143197, accuracy: 0.9866666666666667\n",
      "iteration no 5772: Loss: 0.23903713725173817, accuracy: 0.9866666666666667\n",
      "iteration no 5773: Loss: 0.23903469229684854, accuracy: 0.9866666666666667\n",
      "iteration no 5774: Loss: 0.23903422345729292, accuracy: 0.9866666666666667\n",
      "iteration no 5775: Loss: 0.23903161356401054, accuracy: 0.9866666666666667\n",
      "iteration no 5776: Loss: 0.23902948992448264, accuracy: 0.9866666666666667\n",
      "iteration no 5777: Loss: 0.2390283377587105, accuracy: 0.9866666666666667\n",
      "iteration no 5778: Loss: 0.23902564675290677, accuracy: 0.9866666666666667\n",
      "iteration no 5779: Loss: 0.2390239124174381, accuracy: 0.9866666666666667\n",
      "iteration no 5780: Loss: 0.23902292281262502, accuracy: 0.9866666666666667\n",
      "iteration no 5781: Loss: 0.23902013686676196, accuracy: 0.9866666666666667\n",
      "iteration no 5782: Loss: 0.23901872568057086, accuracy: 0.9866666666666667\n",
      "iteration no 5783: Loss: 0.23901700566332257, accuracy: 0.9866666666666667\n",
      "iteration no 5784: Loss: 0.23901428866371227, accuracy: 0.9866666666666667\n",
      "iteration no 5785: Loss: 0.2390132352483934, accuracy: 0.9866666666666667\n",
      "iteration no 5786: Loss: 0.23901156402115187, accuracy: 0.9866666666666667\n",
      "iteration no 5787: Loss: 0.23900900738844327, accuracy: 0.9866666666666667\n",
      "iteration no 5788: Loss: 0.23900794741816056, accuracy: 0.9866666666666667\n",
      "iteration no 5789: Loss: 0.2390059101892586, accuracy: 0.9866666666666667\n",
      "iteration no 5790: Loss: 0.2390032491767492, accuracy: 0.9866666666666667\n",
      "iteration no 5791: Loss: 0.23900235797881847, accuracy: 0.9866666666666667\n",
      "iteration no 5792: Loss: 0.23900037462627194, accuracy: 0.9866666666666667\n",
      "iteration no 5793: Loss: 0.23899795196512838, accuracy: 0.9866666666666667\n",
      "iteration no 5794: Loss: 0.23899711460804274, accuracy: 0.9866666666666667\n",
      "iteration no 5795: Loss: 0.2389949313772515, accuracy: 0.9866666666666667\n",
      "iteration no 5796: Loss: 0.238992155996549, accuracy: 0.9866666666666667\n",
      "iteration no 5797: Loss: 0.23899136002787505, accuracy: 0.9866666666666667\n",
      "iteration no 5798: Loss: 0.23898943838878656, accuracy: 0.9866666666666667\n",
      "iteration no 5799: Loss: 0.2389869682571516, accuracy: 0.9866666666666667\n",
      "iteration no 5800: Loss: 0.2389861309223351, accuracy: 0.9866666666666667\n",
      "iteration no 5801: Loss: 0.23898417836238778, accuracy: 0.9866666666666667\n",
      "iteration no 5802: Loss: 0.238981381306397, accuracy: 0.9866666666666667\n",
      "iteration no 5803: Loss: 0.23898047256495386, accuracy: 0.9866666666666667\n",
      "iteration no 5804: Loss: 0.23897859942546687, accuracy: 0.9866666666666667\n",
      "iteration no 5805: Loss: 0.23897596396943754, accuracy: 0.9866666666666667\n",
      "iteration no 5806: Loss: 0.2389752751232212, accuracy: 0.9866666666666667\n",
      "iteration no 5807: Loss: 0.2389734241304069, accuracy: 0.9866666666666667\n",
      "iteration no 5808: Loss: 0.23897072305645123, accuracy: 0.9866666666666667\n",
      "iteration no 5809: Loss: 0.23896955502591655, accuracy: 0.9866666666666667\n",
      "iteration no 5810: Loss: 0.23896778412754066, accuracy: 0.9866666666666667\n",
      "iteration no 5811: Loss: 0.2389652241201437, accuracy: 0.9866666666666667\n",
      "iteration no 5812: Loss: 0.23896421252308006, accuracy: 0.9866666666666667\n",
      "iteration no 5813: Loss: 0.23896270503399972, accuracy: 0.9866666666666667\n",
      "iteration no 5814: Loss: 0.2389601013081426, accuracy: 0.9866666666666667\n",
      "iteration no 5815: Loss: 0.23895876226142052, accuracy: 0.9866666666666667\n",
      "iteration no 5816: Loss: 0.2389573093618831, accuracy: 0.9866666666666667\n",
      "iteration no 5817: Loss: 0.23895460648346536, accuracy: 0.9866666666666667\n",
      "iteration no 5818: Loss: 0.2389531343345792, accuracy: 0.9866666666666667\n",
      "iteration no 5819: Loss: 0.2389522342802305, accuracy: 0.9866666666666667\n",
      "iteration no 5820: Loss: 0.23894946720542748, accuracy: 0.9866666666666667\n",
      "iteration no 5821: Loss: 0.23894796150548087, accuracy: 0.9866666666666667\n",
      "iteration no 5822: Loss: 0.2389466917339304, accuracy: 0.9866666666666667\n",
      "iteration no 5823: Loss: 0.23894401019522335, accuracy: 0.9866666666666667\n",
      "iteration no 5824: Loss: 0.2389424874834701, accuracy: 0.9866666666666667\n",
      "iteration no 5825: Loss: 0.23894139494158967, accuracy: 0.9866666666666667\n",
      "iteration no 5826: Loss: 0.23893901123975958, accuracy: 0.9866666666666667\n",
      "iteration no 5827: Loss: 0.23893733576573717, accuracy: 0.9866666666666667\n",
      "iteration no 5828: Loss: 0.23893606136701204, accuracy: 0.9866666666666667\n",
      "iteration no 5829: Loss: 0.23893359098919242, accuracy: 0.9866666666666667\n",
      "iteration no 5830: Loss: 0.23893187720739695, accuracy: 0.9866666666666667\n",
      "iteration no 5831: Loss: 0.2389307336944036, accuracy: 0.9866666666666667\n",
      "iteration no 5832: Loss: 0.2389284156423877, accuracy: 0.9866666666666667\n",
      "iteration no 5833: Loss: 0.23892677659442776, accuracy: 0.9866666666666667\n",
      "iteration no 5834: Loss: 0.2389256499846944, accuracy: 0.9866666666666667\n",
      "iteration no 5835: Loss: 0.2389231643993498, accuracy: 0.9866666666666667\n",
      "iteration no 5836: Loss: 0.2389213013534695, accuracy: 0.9866666666666667\n",
      "iteration no 5837: Loss: 0.23892028817254332, accuracy: 0.9866666666666667\n",
      "iteration no 5838: Loss: 0.2389179795928856, accuracy: 0.9866666666666667\n",
      "iteration no 5839: Loss: 0.23891616140667854, accuracy: 0.9866666666666667\n",
      "iteration no 5840: Loss: 0.23891510554643436, accuracy: 0.9866666666666667\n",
      "iteration no 5841: Loss: 0.23891304119080942, accuracy: 0.9866666666666667\n",
      "iteration no 5842: Loss: 0.238911128367096, accuracy: 0.9866666666666667\n",
      "iteration no 5843: Loss: 0.2389094370876238, accuracy: 0.9866666666666667\n",
      "iteration no 5844: Loss: 0.23890775171489417, accuracy: 0.9866666666666667\n",
      "iteration no 5845: Loss: 0.23890598116497685, accuracy: 0.9866666666666667\n",
      "iteration no 5846: Loss: 0.23890404000639154, accuracy: 0.9866666666666667\n",
      "iteration no 5847: Loss: 0.23890285328610567, accuracy: 0.9866666666666667\n",
      "iteration no 5848: Loss: 0.23890105369626027, accuracy: 0.9866666666666667\n",
      "iteration no 5849: Loss: 0.23889878869883174, accuracy: 0.9866666666666667\n",
      "iteration no 5850: Loss: 0.23889758527208696, accuracy: 0.9866666666666667\n",
      "iteration no 5851: Loss: 0.23889581715728603, accuracy: 0.9866666666666667\n",
      "iteration no 5852: Loss: 0.23889352605039404, accuracy: 0.9866666666666667\n",
      "iteration no 5853: Loss: 0.2388922276352628, accuracy: 0.9866666666666667\n",
      "iteration no 5854: Loss: 0.23889085511548871, accuracy: 0.9866666666666667\n",
      "iteration no 5855: Loss: 0.23888867325366975, accuracy: 0.9866666666666667\n",
      "iteration no 5856: Loss: 0.23888715438339503, accuracy: 0.9866666666666667\n",
      "iteration no 5857: Loss: 0.2388857446771806, accuracy: 0.9866666666666667\n",
      "iteration no 5858: Loss: 0.23888353892490582, accuracy: 0.9866666666666667\n",
      "iteration no 5859: Loss: 0.23888162423733245, accuracy: 0.9866666666666667\n",
      "iteration no 5860: Loss: 0.2388805804142573, accuracy: 0.9866666666666667\n",
      "iteration no 5861: Loss: 0.23887844458637947, accuracy: 0.9866666666666667\n",
      "iteration no 5862: Loss: 0.23887660851649445, accuracy: 0.9866666666666667\n",
      "iteration no 5863: Loss: 0.23887568615967686, accuracy: 0.9866666666666667\n",
      "iteration no 5864: Loss: 0.238873522440613, accuracy: 0.9866666666666667\n",
      "iteration no 5865: Loss: 0.23887138135979347, accuracy: 0.9866666666666667\n",
      "iteration no 5866: Loss: 0.238870535433151, accuracy: 0.9866666666666667\n",
      "iteration no 5867: Loss: 0.2388684116350815, accuracy: 0.9866666666666667\n",
      "iteration no 5868: Loss: 0.23886601688753997, accuracy: 0.9866666666666667\n",
      "iteration no 5869: Loss: 0.2388654774756852, accuracy: 0.9866666666666667\n",
      "iteration no 5870: Loss: 0.23886356726804056, accuracy: 0.9866666666666667\n",
      "iteration no 5871: Loss: 0.2388611852483088, accuracy: 0.9866666666666667\n",
      "iteration no 5872: Loss: 0.23886032989178757, accuracy: 0.9866666666666667\n",
      "iteration no 5873: Loss: 0.2388584249296733, accuracy: 0.9866666666666667\n",
      "iteration no 5874: Loss: 0.23885612184911353, accuracy: 0.9866666666666667\n",
      "iteration no 5875: Loss: 0.23885491031464456, accuracy: 0.9866666666666667\n",
      "iteration no 5876: Loss: 0.23885331633182738, accuracy: 0.9866666666666667\n",
      "iteration no 5877: Loss: 0.23885126292854567, accuracy: 0.9866666666666667\n",
      "iteration no 5878: Loss: 0.23884987659380477, accuracy: 0.9866666666666667\n",
      "iteration no 5879: Loss: 0.23884856274143523, accuracy: 0.9866666666666667\n",
      "iteration no 5880: Loss: 0.23884640231684773, accuracy: 0.9866666666666667\n",
      "iteration no 5881: Loss: 0.2388445364999454, accuracy: 0.9866666666666667\n",
      "iteration no 5882: Loss: 0.23884355979151645, accuracy: 0.9866666666666667\n",
      "iteration no 5883: Loss: 0.23884128459945303, accuracy: 0.9866666666666667\n",
      "iteration no 5884: Loss: 0.23883934022380116, accuracy: 0.9866666666666667\n",
      "iteration no 5885: Loss: 0.2388387423314272, accuracy: 0.9866666666666667\n",
      "iteration no 5886: Loss: 0.23883649627640524, accuracy: 0.9866666666666667\n",
      "iteration no 5887: Loss: 0.23883441856328358, accuracy: 0.9866666666666667\n",
      "iteration no 5888: Loss: 0.23883384708473948, accuracy: 0.9866666666666667\n",
      "iteration no 5889: Loss: 0.2388314944162579, accuracy: 0.9866666666666667\n",
      "iteration no 5890: Loss: 0.23882942744539104, accuracy: 0.9866666666666667\n",
      "iteration no 5891: Loss: 0.23882863456286268, accuracy: 0.9866666666666667\n",
      "iteration no 5892: Loss: 0.23882645002609781, accuracy: 0.9866666666666667\n",
      "iteration no 5893: Loss: 0.23882435080149184, accuracy: 0.9866666666666667\n",
      "iteration no 5894: Loss: 0.23882383977817712, accuracy: 0.9866666666666667\n",
      "iteration no 5895: Loss: 0.2388218062456465, accuracy: 0.9866666666666667\n",
      "iteration no 5896: Loss: 0.23881947136888848, accuracy: 0.9866666666666667\n",
      "iteration no 5897: Loss: 0.2388187827962612, accuracy: 0.9866666666666667\n",
      "iteration no 5898: Loss: 0.23881682330472204, accuracy: 0.9866666666666667\n",
      "iteration no 5899: Loss: 0.23881444285757397, accuracy: 0.9866666666666667\n",
      "iteration no 5900: Loss: 0.23881366592179942, accuracy: 0.9866666666666667\n",
      "iteration no 5901: Loss: 0.2388118385978777, accuracy: 0.9866666666666667\n",
      "iteration no 5902: Loss: 0.23880956499285388, accuracy: 0.9866666666666667\n",
      "iteration no 5903: Loss: 0.23880899328453326, accuracy: 0.9866666666666667\n",
      "iteration no 5904: Loss: 0.2388072345829646, accuracy: 0.9866666666666667\n",
      "iteration no 5905: Loss: 0.23880482981085271, accuracy: 0.9866666666666667\n",
      "iteration no 5906: Loss: 0.23880383886653178, accuracy: 0.9866666666666667\n",
      "iteration no 5907: Loss: 0.23880219548441625, accuracy: 0.9866666666666667\n",
      "iteration no 5908: Loss: 0.23879992990329052, accuracy: 0.9866666666666667\n",
      "iteration no 5909: Loss: 0.23879884764466425, accuracy: 0.9866666666666667\n",
      "iteration no 5910: Loss: 0.23879716723056635, accuracy: 0.9866666666666667\n",
      "iteration no 5911: Loss: 0.23879522760231497, accuracy: 0.9866666666666667\n",
      "iteration no 5912: Loss: 0.23879418434303418, accuracy: 0.9866666666666667\n",
      "iteration no 5913: Loss: 0.23879249970097666, accuracy: 0.9866666666666667\n",
      "iteration no 5914: Loss: 0.23879037599031505, accuracy: 0.9866666666666667\n",
      "iteration no 5915: Loss: 0.23878912274390912, accuracy: 0.9866666666666667\n",
      "iteration no 5916: Loss: 0.2387874909462005, accuracy: 0.9866666666666667\n",
      "iteration no 5917: Loss: 0.23878544405520252, accuracy: 0.9866666666666667\n",
      "iteration no 5918: Loss: 0.23878429525255337, accuracy: 0.9866666666666667\n",
      "iteration no 5919: Loss: 0.23878250119167796, accuracy: 0.9866666666666667\n",
      "iteration no 5920: Loss: 0.2387806459367975, accuracy: 0.9866666666666667\n",
      "iteration no 5921: Loss: 0.2387797039074564, accuracy: 0.9866666666666667\n",
      "iteration no 5922: Loss: 0.2387777757418978, accuracy: 0.9866666666666667\n",
      "iteration no 5923: Loss: 0.23877587837015002, accuracy: 0.9866666666666667\n",
      "iteration no 5924: Loss: 0.23877487061413966, accuracy: 0.9866666666666667\n",
      "iteration no 5925: Loss: 0.23877282646267584, accuracy: 0.9866666666666667\n",
      "iteration no 5926: Loss: 0.238770869730997, accuracy: 0.9866666666666667\n",
      "iteration no 5927: Loss: 0.23877000241169719, accuracy: 0.9866666666666667\n",
      "iteration no 5928: Loss: 0.23876793348476258, accuracy: 0.9866666666666667\n",
      "iteration no 5929: Loss: 0.23876593632069315, accuracy: 0.9866666666666667\n",
      "iteration no 5930: Loss: 0.23876536915645682, accuracy: 0.9866666666666667\n",
      "iteration no 5931: Loss: 0.23876345410732064, accuracy: 0.9866666666666667\n",
      "iteration no 5932: Loss: 0.23876126315667467, accuracy: 0.9866666666666667\n",
      "iteration no 5933: Loss: 0.23876057047085225, accuracy: 0.9866666666666667\n",
      "iteration no 5934: Loss: 0.23875863280473703, accuracy: 0.9866666666666667\n",
      "iteration no 5935: Loss: 0.23875636220623153, accuracy: 0.9866666666666667\n",
      "iteration no 5936: Loss: 0.2387556584724039, accuracy: 0.9866666666666667\n",
      "iteration no 5937: Loss: 0.23875379691258886, accuracy: 0.9866666666666667\n",
      "iteration no 5938: Loss: 0.23875150377612772, accuracy: 0.9866666666666667\n",
      "iteration no 5939: Loss: 0.2387507614420089, accuracy: 0.9866666666666667\n",
      "iteration no 5940: Loss: 0.23874911054552372, accuracy: 0.9866666666666667\n",
      "iteration no 5941: Loss: 0.23874698227831337, accuracy: 0.9866666666666667\n",
      "iteration no 5942: Loss: 0.23874614968713148, accuracy: 0.9866666666666667\n",
      "iteration no 5943: Loss: 0.23874441736689667, accuracy: 0.9866666666666667\n",
      "iteration no 5944: Loss: 0.23874223289439547, accuracy: 0.9866666666666667\n",
      "iteration no 5945: Loss: 0.23874114404312985, accuracy: 0.9866666666666667\n",
      "iteration no 5946: Loss: 0.23873958334995155, accuracy: 0.9866666666666667\n",
      "iteration no 5947: Loss: 0.23873759895515495, accuracy: 0.9866666666666667\n",
      "iteration no 5948: Loss: 0.2387363925227285, accuracy: 0.9866666666666667\n",
      "iteration no 5949: Loss: 0.23873473476405305, accuracy: 0.9866666666666667\n",
      "iteration no 5950: Loss: 0.23873286973611296, accuracy: 0.9866666666666667\n",
      "iteration no 5951: Loss: 0.2387319114644991, accuracy: 0.9866666666666667\n",
      "iteration no 5952: Loss: 0.23873010832173905, accuracy: 0.9866666666666667\n",
      "iteration no 5953: Loss: 0.23872836993046917, accuracy: 0.9866666666666667\n",
      "iteration no 5954: Loss: 0.23872728836671234, accuracy: 0.9866666666666667\n",
      "iteration no 5955: Loss: 0.2387250630213836, accuracy: 0.9866666666666667\n",
      "iteration no 5956: Loss: 0.23872365502806392, accuracy: 0.9866666666666667\n",
      "iteration no 5957: Loss: 0.23872252634365992, accuracy: 0.9866666666666667\n",
      "iteration no 5958: Loss: 0.23872025728356694, accuracy: 0.9866666666666667\n",
      "iteration no 5959: Loss: 0.23871880177370597, accuracy: 0.9866666666666667\n",
      "iteration no 5960: Loss: 0.23871781012804827, accuracy: 0.9866666666666667\n",
      "iteration no 5961: Loss: 0.23871567856852738, accuracy: 0.9866666666666667\n",
      "iteration no 5962: Loss: 0.2387142504116846, accuracy: 0.9866666666666667\n",
      "iteration no 5963: Loss: 0.23871336559041823, accuracy: 0.9866666666666667\n",
      "iteration no 5964: Loss: 0.23871111644131515, accuracy: 0.9866666666666667\n",
      "iteration no 5965: Loss: 0.23870952208776475, accuracy: 0.9866666666666667\n",
      "iteration no 5966: Loss: 0.23870857565121795, accuracy: 0.9866666666666667\n",
      "iteration no 5967: Loss: 0.23870648082607848, accuracy: 0.9866666666666667\n",
      "iteration no 5968: Loss: 0.23870504776512647, accuracy: 0.9866666666666667\n",
      "iteration no 5969: Loss: 0.23870358461824265, accuracy: 0.9866666666666667\n",
      "iteration no 5970: Loss: 0.23870185857781484, accuracy: 0.9866666666666667\n",
      "iteration no 5971: Loss: 0.23870049695955126, accuracy: 0.9866666666666667\n",
      "iteration no 5972: Loss: 0.23869877577067733, accuracy: 0.9866666666666667\n",
      "iteration no 5973: Loss: 0.23869742992205983, accuracy: 0.9866666666666667\n",
      "iteration no 5974: Loss: 0.23869620472221698, accuracy: 0.9866666666666667\n",
      "iteration no 5975: Loss: 0.23869421890545878, accuracy: 0.9866666666666667\n",
      "iteration no 5976: Loss: 0.23869264092260087, accuracy: 0.9866666666666667\n",
      "iteration no 5977: Loss: 0.2386916296323754, accuracy: 0.9866666666666667\n",
      "iteration no 5978: Loss: 0.23868965567266856, accuracy: 0.9866666666666667\n",
      "iteration no 5979: Loss: 0.23868780015133229, accuracy: 0.9866666666666667\n",
      "iteration no 5980: Loss: 0.2386872193992343, accuracy: 0.9866666666666667\n",
      "iteration no 5981: Loss: 0.23868500241684362, accuracy: 0.9866666666666667\n",
      "iteration no 5982: Loss: 0.23868301071592862, accuracy: 0.9866666666666667\n",
      "iteration no 5983: Loss: 0.23868264557337954, accuracy: 0.9866666666666667\n",
      "iteration no 5984: Loss: 0.23868045645939223, accuracy: 0.9866666666666667\n",
      "iteration no 5985: Loss: 0.2386787745517309, accuracy: 0.9866666666666667\n",
      "iteration no 5986: Loss: 0.23867797777615848, accuracy: 0.9866666666666667\n",
      "iteration no 5987: Loss: 0.23867593664180967, accuracy: 0.9866666666666667\n",
      "iteration no 5988: Loss: 0.23867446971347728, accuracy: 0.9866666666666667\n",
      "iteration no 5989: Loss: 0.2386730060286999, accuracy: 0.9866666666666667\n",
      "iteration no 5990: Loss: 0.23867143099271595, accuracy: 0.9866666666666667\n",
      "iteration no 5991: Loss: 0.238670244423547, accuracy: 0.9866666666666667\n",
      "iteration no 5992: Loss: 0.23866836163235028, accuracy: 0.9866666666666667\n",
      "iteration no 5993: Loss: 0.23866658425828496, accuracy: 0.9866666666666667\n",
      "iteration no 5994: Loss: 0.238665711591863, accuracy: 0.9866666666666667\n",
      "iteration no 5995: Loss: 0.23866367634308516, accuracy: 0.9866666666666667\n",
      "iteration no 5996: Loss: 0.2386618517347365, accuracy: 0.9866666666666667\n",
      "iteration no 5997: Loss: 0.23866153134165663, accuracy: 0.9866666666666667\n",
      "iteration no 5998: Loss: 0.23865933246666843, accuracy: 0.9866666666666667\n",
      "iteration no 5999: Loss: 0.23865731489158723, accuracy: 0.9866666666666667\n",
      "iteration no 6000: Loss: 0.23865687074309236, accuracy: 0.9866666666666667\n",
      "iteration no 6001: Loss: 0.2386547851605908, accuracy: 0.9866666666666667\n",
      "iteration no 6002: Loss: 0.23865318708609362, accuracy: 0.9866666666666667\n",
      "iteration no 6003: Loss: 0.23865192679068492, accuracy: 0.9866666666666667\n",
      "iteration no 6004: Loss: 0.23865021739704162, accuracy: 0.9866666666666667\n",
      "iteration no 6005: Loss: 0.23864874428875754, accuracy: 0.9866666666666667\n",
      "iteration no 6006: Loss: 0.2386472379820127, accuracy: 0.9866666666666667\n",
      "iteration no 6007: Loss: 0.23864539509341798, accuracy: 0.9866666666666667\n",
      "iteration no 6008: Loss: 0.23864444323351164, accuracy: 0.9866666666666667\n",
      "iteration no 6009: Loss: 0.23864281521717295, accuracy: 0.9866666666666667\n",
      "iteration no 6010: Loss: 0.23864067682315676, accuracy: 0.9866666666666667\n",
      "iteration no 6011: Loss: 0.23864031721555456, accuracy: 0.9866666666666667\n",
      "iteration no 6012: Loss: 0.238638249492882, accuracy: 0.9866666666666667\n",
      "iteration no 6013: Loss: 0.23863605933348347, accuracy: 0.9866666666666667\n",
      "iteration no 6014: Loss: 0.23863582671671996, accuracy: 0.9866666666666667\n",
      "iteration no 6015: Loss: 0.23863374599917112, accuracy: 0.9866666666666667\n",
      "iteration no 6016: Loss: 0.23863209333313337, accuracy: 0.9866666666666667\n",
      "iteration no 6017: Loss: 0.2386309535825802, accuracy: 0.9866666666666667\n",
      "iteration no 6018: Loss: 0.2386291212001886, accuracy: 0.9866666666666667\n",
      "iteration no 6019: Loss: 0.23862782636254817, accuracy: 0.9866666666666667\n",
      "iteration no 6020: Loss: 0.2386262918303692, accuracy: 0.9866666666666667\n",
      "iteration no 6021: Loss: 0.23862422254581342, accuracy: 0.9866666666666667\n",
      "iteration no 6022: Loss: 0.2386237172071188, accuracy: 0.9866666666666667\n",
      "iteration no 6023: Loss: 0.23862195146363574, accuracy: 0.9866666666666667\n",
      "iteration no 6024: Loss: 0.23861979090121127, accuracy: 0.9866666666666667\n",
      "iteration no 6025: Loss: 0.2386196356546806, accuracy: 0.9866666666666667\n",
      "iteration no 6026: Loss: 0.23861739203801152, accuracy: 0.9866666666666667\n",
      "iteration no 6027: Loss: 0.23861547719635054, accuracy: 0.9866666666666667\n",
      "iteration no 6028: Loss: 0.23861467078253962, accuracy: 0.9866666666666667\n",
      "iteration no 6029: Loss: 0.2386128116539926, accuracy: 0.9866666666666667\n",
      "iteration no 6030: Loss: 0.23861125515695297, accuracy: 0.9866666666666667\n",
      "iteration no 6031: Loss: 0.23860994404182315, accuracy: 0.9866666666666667\n",
      "iteration no 6032: Loss: 0.2386082090288784, accuracy: 0.9866666666666667\n",
      "iteration no 6033: Loss: 0.2386071433052896, accuracy: 0.9866666666666667\n",
      "iteration no 6034: Loss: 0.23860538463026393, accuracy: 0.9866666666666667\n",
      "iteration no 6035: Loss: 0.23860337087409278, accuracy: 0.9866666666666667\n",
      "iteration no 6036: Loss: 0.2386030656897197, accuracy: 0.9866666666666667\n",
      "iteration no 6037: Loss: 0.2386012138711544, accuracy: 0.9866666666666667\n",
      "iteration no 6038: Loss: 0.2385991650732749, accuracy: 0.9866666666666667\n",
      "iteration no 6039: Loss: 0.2385983333048262, accuracy: 0.9866666666666667\n",
      "iteration no 6040: Loss: 0.2385966886545925, accuracy: 0.9866666666666667\n",
      "iteration no 6041: Loss: 0.23859497524656137, accuracy: 0.9866666666666667\n",
      "iteration no 6042: Loss: 0.23859352929425182, accuracy: 0.9866666666666667\n",
      "iteration no 6043: Loss: 0.23859204265123818, accuracy: 0.9866666666666667\n",
      "iteration no 6044: Loss: 0.23859092574509866, accuracy: 0.9866666666666667\n",
      "iteration no 6045: Loss: 0.23858907659112993, accuracy: 0.9866666666666667\n",
      "iteration no 6046: Loss: 0.23858716377331576, accuracy: 0.9866666666666667\n",
      "iteration no 6047: Loss: 0.2385868789157378, accuracy: 0.9866666666666667\n",
      "iteration no 6048: Loss: 0.23858477176488072, accuracy: 0.9866666666666667\n",
      "iteration no 6049: Loss: 0.2385829560927485, accuracy: 0.9866666666666667\n",
      "iteration no 6050: Loss: 0.2385822228706337, accuracy: 0.9866666666666667\n",
      "iteration no 6051: Loss: 0.2385804441863707, accuracy: 0.9866666666666667\n",
      "iteration no 6052: Loss: 0.23857901525933883, accuracy: 0.9866666666666667\n",
      "iteration no 6053: Loss: 0.23857754526776984, accuracy: 0.9866666666666667\n",
      "iteration no 6054: Loss: 0.23857568222060804, accuracy: 0.9866666666666667\n",
      "iteration no 6055: Loss: 0.23857498265550947, accuracy: 0.9866666666666667\n",
      "iteration no 6056: Loss: 0.23857304271408467, accuracy: 0.9866666666666667\n",
      "iteration no 6057: Loss: 0.23857101522353233, accuracy: 0.9866666666666667\n",
      "iteration no 6058: Loss: 0.23857098147096856, accuracy: 0.9866666666666667\n",
      "iteration no 6059: Loss: 0.23856859486947143, accuracy: 0.9866666666666667\n",
      "iteration no 6060: Loss: 0.23856703991945077, accuracy: 0.9866666666666667\n",
      "iteration no 6061: Loss: 0.23856603715325342, accuracy: 0.9866666666666667\n",
      "iteration no 6062: Loss: 0.23856401964220914, accuracy: 0.9866666666666667\n",
      "iteration no 6063: Loss: 0.2385630421871316, accuracy: 0.9866666666666667\n",
      "iteration no 6064: Loss: 0.2385613898359611, accuracy: 0.9866666666666667\n",
      "iteration no 6065: Loss: 0.23855932998215323, accuracy: 0.9866666666666667\n",
      "iteration no 6066: Loss: 0.2385590195811668, accuracy: 0.9866666666666667\n",
      "iteration no 6067: Loss: 0.23855686630202144, accuracy: 0.9866666666666667\n",
      "iteration no 6068: Loss: 0.23855520328223495, accuracy: 0.9866666666666667\n",
      "iteration no 6069: Loss: 0.2385546621295525, accuracy: 0.9866666666666667\n",
      "iteration no 6070: Loss: 0.23855242915498606, accuracy: 0.9866666666666667\n",
      "iteration no 6071: Loss: 0.23855127006971802, accuracy: 0.9866666666666667\n",
      "iteration no 6072: Loss: 0.23854953937988016, accuracy: 0.9866666666666667\n",
      "iteration no 6073: Loss: 0.23854791024708932, accuracy: 0.9866666666666667\n",
      "iteration no 6074: Loss: 0.2385471209300268, accuracy: 0.9866666666666667\n",
      "iteration no 6075: Loss: 0.23854498750962017, accuracy: 0.9866666666666667\n",
      "iteration no 6076: Loss: 0.23854349928047675, accuracy: 0.9866666666666667\n",
      "iteration no 6077: Loss: 0.2385425857086404, accuracy: 0.9866666666666667\n",
      "iteration no 6078: Loss: 0.23854047690665525, accuracy: 0.9866666666666667\n",
      "iteration no 6079: Loss: 0.2385393812831404, accuracy: 0.9866666666666667\n",
      "iteration no 6080: Loss: 0.23853802581161535, accuracy: 0.9866666666666667\n",
      "iteration no 6081: Loss: 0.23853631077684706, accuracy: 0.9866666666666667\n",
      "iteration no 6082: Loss: 0.23853551753825278, accuracy: 0.9866666666666667\n",
      "iteration no 6083: Loss: 0.23853317720467682, accuracy: 0.9866666666666667\n",
      "iteration no 6084: Loss: 0.23853219910801315, accuracy: 0.9866666666666667\n",
      "iteration no 6085: Loss: 0.23853112312233915, accuracy: 0.9866666666666667\n",
      "iteration no 6086: Loss: 0.23852882173938023, accuracy: 0.9866666666666667\n",
      "iteration no 6087: Loss: 0.23852784746088895, accuracy: 0.9866666666666667\n",
      "iteration no 6088: Loss: 0.23852651032315852, accuracy: 0.9866666666666667\n",
      "iteration no 6089: Loss: 0.2385246024934622, accuracy: 0.9866666666666667\n",
      "iteration no 6090: Loss: 0.23852400911075078, accuracy: 0.9866666666666667\n",
      "iteration no 6091: Loss: 0.23852166346771658, accuracy: 0.9866666666666667\n",
      "iteration no 6092: Loss: 0.23852067293692913, accuracy: 0.9866666666666667\n",
      "iteration no 6093: Loss: 0.23851938834520087, accuracy: 0.9866666666666667\n",
      "iteration no 6094: Loss: 0.23851718726974672, accuracy: 0.9866666666666667\n",
      "iteration no 6095: Loss: 0.23851618795531693, accuracy: 0.9866666666666667\n",
      "iteration no 6096: Loss: 0.23851492528575974, accuracy: 0.9866666666666667\n",
      "iteration no 6097: Loss: 0.23851300718439294, accuracy: 0.9866666666666667\n",
      "iteration no 6098: Loss: 0.23851206377448864, accuracy: 0.9866666666666667\n",
      "iteration no 6099: Loss: 0.23851008058479545, accuracy: 0.9866666666666667\n",
      "iteration no 6100: Loss: 0.23850925708752738, accuracy: 0.9866666666666667\n",
      "iteration no 6101: Loss: 0.23850778985799986, accuracy: 0.9866666666666667\n",
      "iteration no 6102: Loss: 0.23850588094089464, accuracy: 0.9866666666666667\n",
      "iteration no 6103: Loss: 0.2385051681710872, accuracy: 0.9866666666666667\n",
      "iteration no 6104: Loss: 0.23850340915467264, accuracy: 0.9866666666666667\n",
      "iteration no 6105: Loss: 0.238501828720716, accuracy: 0.9866666666666667\n",
      "iteration no 6106: Loss: 0.23850079519528156, accuracy: 0.9866666666666667\n",
      "iteration no 6107: Loss: 0.2384987469848065, accuracy: 0.9866666666666667\n",
      "iteration no 6108: Loss: 0.23849802399700387, accuracy: 0.9866666666666667\n",
      "iteration no 6109: Loss: 0.23849635264410365, accuracy: 0.9866666666666667\n",
      "iteration no 6110: Loss: 0.23849438415465946, accuracy: 0.9866666666666667\n",
      "iteration no 6111: Loss: 0.23849407383951488, accuracy: 0.9866666666666667\n",
      "iteration no 6112: Loss: 0.23849209335142232, accuracy: 0.9866666666666667\n",
      "iteration no 6113: Loss: 0.238490486372602, accuracy: 0.9866666666666667\n",
      "iteration no 6114: Loss: 0.23848934196178617, accuracy: 0.9866666666666667\n",
      "iteration no 6115: Loss: 0.2384875022570838, accuracy: 0.9866666666666667\n",
      "iteration no 6116: Loss: 0.23848675200213837, accuracy: 0.9866666666666667\n",
      "iteration no 6117: Loss: 0.23848496246033646, accuracy: 0.9866666666666667\n",
      "iteration no 6118: Loss: 0.23848312541436373, accuracy: 0.9866666666666667\n",
      "iteration no 6119: Loss: 0.23848257490802527, accuracy: 0.9866666666666667\n",
      "iteration no 6120: Loss: 0.23848075109169134, accuracy: 0.9866666666666667\n",
      "iteration no 6121: Loss: 0.23847960243163624, accuracy: 0.9866666666666667\n",
      "iteration no 6122: Loss: 0.2384779094034527, accuracy: 0.9866666666666667\n",
      "iteration no 6123: Loss: 0.23847623434587842, accuracy: 0.9866666666666667\n",
      "iteration no 6124: Loss: 0.23847594577619471, accuracy: 0.9866666666666667\n",
      "iteration no 6125: Loss: 0.23847368489082396, accuracy: 0.9866666666666667\n",
      "iteration no 6126: Loss: 0.2384722460384377, accuracy: 0.9866666666666667\n",
      "iteration no 6127: Loss: 0.23847140979946385, accuracy: 0.9866666666666667\n",
      "iteration no 6128: Loss: 0.23846931866791943, accuracy: 0.9866666666666667\n",
      "iteration no 6129: Loss: 0.23846847243617592, accuracy: 0.9866666666666667\n",
      "iteration no 6130: Loss: 0.23846677703617794, accuracy: 0.9866666666666667\n",
      "iteration no 6131: Loss: 0.23846527782147325, accuracy: 0.9866666666666667\n",
      "iteration no 6132: Loss: 0.23846456292050128, accuracy: 0.9866666666666667\n",
      "iteration no 6133: Loss: 0.23846236485758388, accuracy: 0.9866666666666667\n",
      "iteration no 6134: Loss: 0.23846123475786266, accuracy: 0.9866666666666667\n",
      "iteration no 6135: Loss: 0.23845988138855992, accuracy: 0.9866666666666667\n",
      "iteration no 6136: Loss: 0.23845827221694205, accuracy: 0.9866666666666667\n",
      "iteration no 6137: Loss: 0.2384574246089305, accuracy: 0.9866666666666667\n",
      "iteration no 6138: Loss: 0.23845529278523225, accuracy: 0.9866666666666667\n",
      "iteration no 6139: Loss: 0.23845430831291226, accuracy: 0.9866666666666667\n",
      "iteration no 6140: Loss: 0.23845306964411717, accuracy: 0.9866666666666667\n",
      "iteration no 6141: Loss: 0.23845108132924564, accuracy: 0.9866666666666667\n",
      "iteration no 6142: Loss: 0.23845052047333593, accuracy: 0.9866666666666667\n",
      "iteration no 6143: Loss: 0.2384485331646961, accuracy: 0.9866666666666667\n",
      "iteration no 6144: Loss: 0.23844752701334387, accuracy: 0.9866666666666667\n",
      "iteration no 6145: Loss: 0.23844628997666578, accuracy: 0.9866666666666667\n",
      "iteration no 6146: Loss: 0.2384439726863679, accuracy: 0.9866666666666667\n",
      "iteration no 6147: Loss: 0.2384433856242393, accuracy: 0.9866666666666667\n",
      "iteration no 6148: Loss: 0.23844190477312488, accuracy: 0.9866666666666667\n",
      "iteration no 6149: Loss: 0.23844028750667767, accuracy: 0.9866666666666667\n",
      "iteration no 6150: Loss: 0.2384395241899835, accuracy: 0.9866666666666667\n",
      "iteration no 6151: Loss: 0.2384372891350066, accuracy: 0.9866666666666667\n",
      "iteration no 6152: Loss: 0.238436894665578, accuracy: 0.9866666666666667\n",
      "iteration no 6153: Loss: 0.23843509036320953, accuracy: 0.9866666666666667\n",
      "iteration no 6154: Loss: 0.2384333083065776, accuracy: 0.9866666666666667\n",
      "iteration no 6155: Loss: 0.23843235739253152, accuracy: 0.9866666666666667\n",
      "iteration no 6156: Loss: 0.23843068348755447, accuracy: 0.9866666666666667\n",
      "iteration no 6157: Loss: 0.23843007783608686, accuracy: 0.9866666666666667\n",
      "iteration no 6158: Loss: 0.23842828873496977, accuracy: 0.9866666666666667\n",
      "iteration no 6159: Loss: 0.2384265560022828, accuracy: 0.9866666666666667\n",
      "iteration no 6160: Loss: 0.23842644781690686, accuracy: 0.9866666666666667\n",
      "iteration no 6161: Loss: 0.23842408874420185, accuracy: 0.9866666666666667\n",
      "iteration no 6162: Loss: 0.23842309085567603, accuracy: 0.9866666666666667\n",
      "iteration no 6163: Loss: 0.23842151960160946, accuracy: 0.9866666666666667\n",
      "iteration no 6164: Loss: 0.2384197503375387, accuracy: 0.9866666666666667\n",
      "iteration no 6165: Loss: 0.23841930707881992, accuracy: 0.9866666666666667\n",
      "iteration no 6166: Loss: 0.23841718064498846, accuracy: 0.9866666666666667\n",
      "iteration no 6167: Loss: 0.2384160277960972, accuracy: 0.9866666666666667\n",
      "iteration no 6168: Loss: 0.2384148533387621, accuracy: 0.9866666666666667\n",
      "iteration no 6169: Loss: 0.23841331594344076, accuracy: 0.9866666666666667\n",
      "iteration no 6170: Loss: 0.238412406773092, accuracy: 0.9866666666666667\n",
      "iteration no 6171: Loss: 0.2384103090992979, accuracy: 0.9866666666666667\n",
      "iteration no 6172: Loss: 0.23840918640765574, accuracy: 0.9866666666666667\n",
      "iteration no 6173: Loss: 0.23840838274426637, accuracy: 0.9866666666666667\n",
      "iteration no 6174: Loss: 0.23840659900609074, accuracy: 0.9866666666666667\n",
      "iteration no 6175: Loss: 0.23840596471120973, accuracy: 0.9866666666666667\n",
      "iteration no 6176: Loss: 0.23840378347335, accuracy: 0.9866666666666667\n",
      "iteration no 6177: Loss: 0.23840298733307658, accuracy: 0.9866666666666667\n",
      "iteration no 6178: Loss: 0.23840186645002193, accuracy: 0.9866666666666667\n",
      "iteration no 6179: Loss: 0.23839986259864487, accuracy: 0.9866666666666667\n",
      "iteration no 6180: Loss: 0.2383989676605032, accuracy: 0.9866666666666667\n",
      "iteration no 6181: Loss: 0.23839722661649823, accuracy: 0.9866666666666667\n",
      "iteration no 6182: Loss: 0.23839640399640477, accuracy: 0.9866666666666667\n",
      "iteration no 6183: Loss: 0.23839475269022045, accuracy: 0.9866666666666667\n",
      "iteration no 6184: Loss: 0.2383930649723775, accuracy: 0.9866666666666667\n",
      "iteration no 6185: Loss: 0.23839220592302712, accuracy: 0.9866666666666667\n",
      "iteration no 6186: Loss: 0.23839066018194105, accuracy: 0.9866666666666667\n",
      "iteration no 6187: Loss: 0.2383898206781466, accuracy: 0.9866666666666667\n",
      "iteration no 6188: Loss: 0.23838845024174132, accuracy: 0.9866666666666667\n",
      "iteration no 6189: Loss: 0.2383865379091436, accuracy: 0.9866666666666667\n",
      "iteration no 6190: Loss: 0.2383862213829046, accuracy: 0.9866666666666667\n",
      "iteration no 6191: Loss: 0.23838416054431477, accuracy: 0.9866666666666667\n",
      "iteration no 6192: Loss: 0.23838310195845858, accuracy: 0.9866666666666667\n",
      "iteration no 6193: Loss: 0.23838152968858334, accuracy: 0.9866666666666667\n",
      "iteration no 6194: Loss: 0.2383800403218162, accuracy: 0.9866666666666667\n",
      "iteration no 6195: Loss: 0.23837939055046753, accuracy: 0.9866666666666667\n",
      "iteration no 6196: Loss: 0.2383774746046137, accuracy: 0.9866666666666667\n",
      "iteration no 6197: Loss: 0.23837668381909202, accuracy: 0.9866666666666667\n",
      "iteration no 6198: Loss: 0.23837497278811406, accuracy: 0.9866666666666667\n",
      "iteration no 6199: Loss: 0.23837361972611543, accuracy: 0.9866666666666667\n",
      "iteration no 6200: Loss: 0.2383730995854847, accuracy: 0.9866666666666667\n",
      "iteration no 6201: Loss: 0.23837103759057462, accuracy: 0.9866666666666667\n",
      "iteration no 6202: Loss: 0.23837002258330142, accuracy: 0.9866666666666667\n",
      "iteration no 6203: Loss: 0.23836904518835508, accuracy: 0.9866666666666667\n",
      "iteration no 6204: Loss: 0.23836730793708666, accuracy: 0.9866666666666667\n",
      "iteration no 6205: Loss: 0.23836673519181284, accuracy: 0.9866666666666667\n",
      "iteration no 6206: Loss: 0.23836457543394945, accuracy: 0.9866666666666667\n",
      "iteration no 6207: Loss: 0.23836374310241054, accuracy: 0.9866666666666667\n",
      "iteration no 6208: Loss: 0.23836234426432196, accuracy: 0.9866666666666667\n",
      "iteration no 6209: Loss: 0.2383606251801368, accuracy: 0.9866666666666667\n",
      "iteration no 6210: Loss: 0.23835989224356705, accuracy: 0.9866666666666667\n",
      "iteration no 6211: Loss: 0.23835778988061496, accuracy: 0.9866666666666667\n",
      "iteration no 6212: Loss: 0.23835757439626115, accuracy: 0.9866666666666667\n",
      "iteration no 6213: Loss: 0.238356243460192, accuracy: 0.9866666666666667\n",
      "iteration no 6214: Loss: 0.23835434093986962, accuracy: 0.9866666666666667\n",
      "iteration no 6215: Loss: 0.23835366194133922, accuracy: 0.9866666666666667\n",
      "iteration no 6216: Loss: 0.2383521262549967, accuracy: 0.9866666666666667\n",
      "iteration no 6217: Loss: 0.23835105867253198, accuracy: 0.9866666666666667\n",
      "iteration no 6218: Loss: 0.2383493949693604, accuracy: 0.9866666666666667\n",
      "iteration no 6219: Loss: 0.23834775283193751, accuracy: 0.9866666666666667\n",
      "iteration no 6220: Loss: 0.23834720865344022, accuracy: 0.9866666666666667\n",
      "iteration no 6221: Loss: 0.2383453071427448, accuracy: 0.9866666666666667\n",
      "iteration no 6222: Loss: 0.23834436557787153, accuracy: 0.9866666666666667\n",
      "iteration no 6223: Loss: 0.23834317053299203, accuracy: 0.9866666666666667\n",
      "iteration no 6224: Loss: 0.23834173310062057, accuracy: 0.9866666666666667\n",
      "iteration no 6225: Loss: 0.23834100762438193, accuracy: 0.9866666666666667\n",
      "iteration no 6226: Loss: 0.23833908644360002, accuracy: 0.9866666666666667\n",
      "iteration no 6227: Loss: 0.23833809459867833, accuracy: 0.9866666666666667\n",
      "iteration no 6228: Loss: 0.23833634944208615, accuracy: 0.9866666666666667\n",
      "iteration no 6229: Loss: 0.2383353635643926, accuracy: 0.9866666666666667\n",
      "iteration no 6230: Loss: 0.23833445894682043, accuracy: 0.9866666666666667\n",
      "iteration no 6231: Loss: 0.2383323379641066, accuracy: 0.9866666666666667\n",
      "iteration no 6232: Loss: 0.23833149867360168, accuracy: 0.9866666666666667\n",
      "iteration no 6233: Loss: 0.23833025392280693, accuracy: 0.9866666666666667\n",
      "iteration no 6234: Loss: 0.23832901400107254, accuracy: 0.9866666666666667\n",
      "iteration no 6235: Loss: 0.2383280808424284, accuracy: 0.9866666666666667\n",
      "iteration no 6236: Loss: 0.23832617581033685, accuracy: 0.9866666666666667\n",
      "iteration no 6237: Loss: 0.23832536446167613, accuracy: 0.9866666666666667\n",
      "iteration no 6238: Loss: 0.2383239688721532, accuracy: 0.9866666666666667\n",
      "iteration no 6239: Loss: 0.23832228254127885, accuracy: 0.9866666666666667\n",
      "iteration no 6240: Loss: 0.23832151500656948, accuracy: 0.9866666666666667\n",
      "iteration no 6241: Loss: 0.2383194414349308, accuracy: 0.9866666666666667\n",
      "iteration no 6242: Loss: 0.23831894885908333, accuracy: 0.9866666666666667\n",
      "iteration no 6243: Loss: 0.23831762118871416, accuracy: 0.9866666666666667\n",
      "iteration no 6244: Loss: 0.2383161261036683, accuracy: 0.9866666666666667\n",
      "iteration no 6245: Loss: 0.23831514859474537, accuracy: 0.9866666666666667\n",
      "iteration no 6246: Loss: 0.2383135665956843, accuracy: 0.9866666666666667\n",
      "iteration no 6247: Loss: 0.23831272558682917, accuracy: 0.9866666666666667\n",
      "iteration no 6248: Loss: 0.2383109697763559, accuracy: 0.9866666666666667\n",
      "iteration no 6249: Loss: 0.2383094591642394, accuracy: 0.9866666666666667\n",
      "iteration no 6250: Loss: 0.2383090094427746, accuracy: 0.9866666666666667\n",
      "iteration no 6251: Loss: 0.2383070497026239, accuracy: 0.9866666666666667\n",
      "iteration no 6252: Loss: 0.23830606096024926, accuracy: 0.9866666666666667\n",
      "iteration no 6253: Loss: 0.2383048242774108, accuracy: 0.9866666666666667\n",
      "iteration no 6254: Loss: 0.23830350054257807, accuracy: 0.9866666666666667\n",
      "iteration no 6255: Loss: 0.23830281230615571, accuracy: 0.9866666666666667\n",
      "iteration no 6256: Loss: 0.23830069251722097, accuracy: 0.9866666666666667\n",
      "iteration no 6257: Loss: 0.23830003396423044, accuracy: 0.9866666666666667\n",
      "iteration no 6258: Loss: 0.23829827044319318, accuracy: 0.9866666666666667\n",
      "iteration no 6259: Loss: 0.2382970674030488, accuracy: 0.9866666666666667\n",
      "iteration no 6260: Loss: 0.2382962770754855, accuracy: 0.9866666666666667\n",
      "iteration no 6261: Loss: 0.23829415495840744, accuracy: 0.9866666666666667\n",
      "iteration no 6262: Loss: 0.23829372191414866, accuracy: 0.9866666666666667\n",
      "iteration no 6263: Loss: 0.2382924189540378, accuracy: 0.9866666666666667\n",
      "iteration no 6264: Loss: 0.23829115012506286, accuracy: 0.9866666666666667\n",
      "iteration no 6265: Loss: 0.23829014533694293, accuracy: 0.9866666666666667\n",
      "iteration no 6266: Loss: 0.2382880593232575, accuracy: 0.9866666666666667\n",
      "iteration no 6267: Loss: 0.23828756915998478, accuracy: 0.9866666666666667\n",
      "iteration no 6268: Loss: 0.23828620216590257, accuracy: 0.9866666666666667\n",
      "iteration no 6269: Loss: 0.23828458506216124, accuracy: 0.9866666666666667\n",
      "iteration no 6270: Loss: 0.23828345426263267, accuracy: 0.9866666666666667\n",
      "iteration no 6271: Loss: 0.2382820850427232, accuracy: 0.9866666666666667\n",
      "iteration no 6272: Loss: 0.23828178827760207, accuracy: 0.9866666666666667\n",
      "iteration no 6273: Loss: 0.23827997670408904, accuracy: 0.9866666666666667\n",
      "iteration no 6274: Loss: 0.2382786568486805, accuracy: 0.9866666666666667\n",
      "iteration no 6275: Loss: 0.23827776932175737, accuracy: 0.9866666666666667\n",
      "iteration no 6276: Loss: 0.2382756880357063, accuracy: 0.9866666666666667\n",
      "iteration no 6277: Loss: 0.23827548091308492, accuracy: 0.9866666666666667\n",
      "iteration no 6278: Loss: 0.23827351780994716, accuracy: 0.9866666666666667\n",
      "iteration no 6279: Loss: 0.23827241364706664, accuracy: 0.9866666666666667\n",
      "iteration no 6280: Loss: 0.23827153476338264, accuracy: 0.9866666666666667\n",
      "iteration no 6281: Loss: 0.23826989353667424, accuracy: 0.9866666666666667\n",
      "iteration no 6282: Loss: 0.23826956291114712, accuracy: 0.9866666666666667\n",
      "iteration no 6283: Loss: 0.23826736125545395, accuracy: 0.9866666666666667\n",
      "iteration no 6284: Loss: 0.23826622560553892, accuracy: 0.9866666666666667\n",
      "iteration no 6285: Loss: 0.23826535273927724, accuracy: 0.9866666666666667\n",
      "iteration no 6286: Loss: 0.23826354240516967, accuracy: 0.9866666666666667\n",
      "iteration no 6287: Loss: 0.2382630859810337, accuracy: 0.9866666666666667\n",
      "iteration no 6288: Loss: 0.23826149922130468, accuracy: 0.9866666666666667\n",
      "iteration no 6289: Loss: 0.23826045463697007, accuracy: 0.9866666666666667\n",
      "iteration no 6290: Loss: 0.23825938561556703, accuracy: 0.9866666666666667\n",
      "iteration no 6291: Loss: 0.23825755160950252, accuracy: 0.9866666666666667\n",
      "iteration no 6292: Loss: 0.23825691167942764, accuracy: 0.9866666666666667\n",
      "iteration no 6293: Loss: 0.2382551026618468, accuracy: 0.9866666666666667\n",
      "iteration no 6294: Loss: 0.2382543154026095, accuracy: 0.9866666666666667\n",
      "iteration no 6295: Loss: 0.2382529782649936, accuracy: 0.9866666666666667\n",
      "iteration no 6296: Loss: 0.23825196729049491, accuracy: 0.9866666666666667\n",
      "iteration no 6297: Loss: 0.2382509913738115, accuracy: 0.9866666666666667\n",
      "iteration no 6298: Loss: 0.23824894314250508, accuracy: 0.9866666666666667\n",
      "iteration no 6299: Loss: 0.23824863398932417, accuracy: 0.9866666666666667\n",
      "iteration no 6300: Loss: 0.23824698352625623, accuracy: 0.9866666666666667\n",
      "iteration no 6301: Loss: 0.23824548331347384, accuracy: 0.9866666666666667\n",
      "iteration no 6302: Loss: 0.2382445079758964, accuracy: 0.9866666666666667\n",
      "iteration no 6303: Loss: 0.23824312091109873, accuracy: 0.9866666666666667\n",
      "iteration no 6304: Loss: 0.23824288026834653, accuracy: 0.9866666666666667\n",
      "iteration no 6305: Loss: 0.23824097232556912, accuracy: 0.9866666666666667\n",
      "iteration no 6306: Loss: 0.23823943326176378, accuracy: 0.9866666666666667\n",
      "iteration no 6307: Loss: 0.23823871167615207, accuracy: 0.9866666666666667\n",
      "iteration no 6308: Loss: 0.23823700898436884, accuracy: 0.9866666666666667\n",
      "iteration no 6309: Loss: 0.23823640903020138, accuracy: 0.9866666666666667\n",
      "iteration no 6310: Loss: 0.2382344842679377, accuracy: 0.9866666666666667\n",
      "iteration no 6311: Loss: 0.2382338293799985, accuracy: 0.9866666666666667\n",
      "iteration no 6312: Loss: 0.23823303475883062, accuracy: 0.9866666666666667\n",
      "iteration no 6313: Loss: 0.2382311748070683, accuracy: 0.9866666666666667\n",
      "iteration no 6314: Loss: 0.23823055639220753, accuracy: 0.9866666666666667\n",
      "iteration no 6315: Loss: 0.2382286361813267, accuracy: 0.9866666666666667\n",
      "iteration no 6316: Loss: 0.23822777779737264, accuracy: 0.9866666666666667\n",
      "iteration no 6317: Loss: 0.23822655976242796, accuracy: 0.9866666666666667\n",
      "iteration no 6318: Loss: 0.23822529571624296, accuracy: 0.9866666666666667\n",
      "iteration no 6319: Loss: 0.2382248678737715, accuracy: 0.9866666666666667\n",
      "iteration no 6320: Loss: 0.2382226351772247, accuracy: 0.9866666666666667\n",
      "iteration no 6321: Loss: 0.23822213506467604, accuracy: 0.9866666666666667\n",
      "iteration no 6322: Loss: 0.2382207337795283, accuracy: 0.9866666666666667\n",
      "iteration no 6323: Loss: 0.23821912111455892, accuracy: 0.9866666666666667\n",
      "iteration no 6324: Loss: 0.23821812905292486, accuracy: 0.9866666666666667\n",
      "iteration no 6325: Loss: 0.2382166517167163, accuracy: 0.9866666666666667\n",
      "iteration no 6326: Loss: 0.23821683624469991, accuracy: 0.9866666666666667\n",
      "iteration no 6327: Loss: 0.2382149123127375, accuracy: 0.9866666666666667\n",
      "iteration no 6328: Loss: 0.23821344360856644, accuracy: 0.9866666666666667\n",
      "iteration no 6329: Loss: 0.2382122696800295, accuracy: 0.9866666666666667\n",
      "iteration no 6330: Loss: 0.2382106710415688, accuracy: 0.9866666666666667\n",
      "iteration no 6331: Loss: 0.2382102125504027, accuracy: 0.9866666666666667\n",
      "iteration no 6332: Loss: 0.23820855909401217, accuracy: 0.9866666666666667\n",
      "iteration no 6333: Loss: 0.23820800951633675, accuracy: 0.9866666666666667\n",
      "iteration no 6334: Loss: 0.23820662122084535, accuracy: 0.9866666666666667\n",
      "iteration no 6335: Loss: 0.23820514210817278, accuracy: 0.9866666666666667\n",
      "iteration no 6336: Loss: 0.23820435902553083, accuracy: 0.9866666666666667\n",
      "iteration no 6337: Loss: 0.2382023857029574, accuracy: 0.9866666666666667\n",
      "iteration no 6338: Loss: 0.23820141946577728, accuracy: 0.9866666666666667\n",
      "iteration no 6339: Loss: 0.2382006096402402, accuracy: 0.9866666666666667\n",
      "iteration no 6340: Loss: 0.23819956721529434, accuracy: 0.9866666666666667\n",
      "iteration no 6341: Loss: 0.2381986323980662, accuracy: 0.9866666666666667\n",
      "iteration no 6342: Loss: 0.23819659839679144, accuracy: 0.9866666666666667\n",
      "iteration no 6343: Loss: 0.2381958133440074, accuracy: 0.9866666666666667\n",
      "iteration no 6344: Loss: 0.23819441749293008, accuracy: 0.9866666666666667\n",
      "iteration no 6345: Loss: 0.2381932944499542, accuracy: 0.9866666666666667\n",
      "iteration no 6346: Loss: 0.23819256823615814, accuracy: 0.9866666666666667\n",
      "iteration no 6347: Loss: 0.23819073629506832, accuracy: 0.9866666666666667\n",
      "iteration no 6348: Loss: 0.23819041251928624, accuracy: 0.9866666666666667\n",
      "iteration no 6349: Loss: 0.238188742837182, accuracy: 0.9866666666666667\n",
      "iteration no 6350: Loss: 0.23818764392871922, accuracy: 0.9866666666666667\n",
      "iteration no 6351: Loss: 0.23818619572009264, accuracy: 0.9866666666666667\n",
      "iteration no 6352: Loss: 0.23818446313142796, accuracy: 0.9866666666666667\n",
      "iteration no 6353: Loss: 0.23818491909583422, accuracy: 0.9866666666666667\n",
      "iteration no 6354: Loss: 0.23818290240898551, accuracy: 0.9866666666666667\n",
      "iteration no 6355: Loss: 0.2381820418332345, accuracy: 0.9866666666666667\n",
      "iteration no 6356: Loss: 0.23818040294346898, accuracy: 0.9866666666666667\n",
      "iteration no 6357: Loss: 0.23817925440381194, accuracy: 0.9866666666666667\n",
      "iteration no 6358: Loss: 0.2381783448201485, accuracy: 0.9866666666666667\n",
      "iteration no 6359: Loss: 0.23817680211486286, accuracy: 0.9866666666666667\n",
      "iteration no 6360: Loss: 0.238176539935732, accuracy: 0.9866666666666667\n",
      "iteration no 6361: Loss: 0.2381746676422964, accuracy: 0.9866666666666667\n",
      "iteration no 6362: Loss: 0.23817370798420176, accuracy: 0.9866666666666667\n",
      "iteration no 6363: Loss: 0.23817250955886948, accuracy: 0.9866666666666667\n",
      "iteration no 6364: Loss: 0.23817081062836926, accuracy: 0.9866666666666667\n",
      "iteration no 6365: Loss: 0.23817061322339833, accuracy: 0.9866666666666667\n",
      "iteration no 6366: Loss: 0.2381689642131255, accuracy: 0.9866666666666667\n",
      "iteration no 6367: Loss: 0.23816827058080858, accuracy: 0.9866666666666667\n",
      "iteration no 6368: Loss: 0.2381667712572984, accuracy: 0.9866666666666667\n",
      "iteration no 6369: Loss: 0.23816532970403215, accuracy: 0.9866666666666667\n",
      "iteration no 6370: Loss: 0.23816433725279745, accuracy: 0.9866666666666667\n",
      "iteration no 6371: Loss: 0.23816235577354672, accuracy: 0.9866666666666667\n",
      "iteration no 6372: Loss: 0.23816291320968908, accuracy: 0.9866666666666667\n",
      "iteration no 6373: Loss: 0.23816115845306868, accuracy: 0.9866666666666667\n",
      "iteration no 6374: Loss: 0.2381598541973281, accuracy: 0.9866666666666667\n",
      "iteration no 6375: Loss: 0.23815850464900967, accuracy: 0.9866666666666667\n",
      "iteration no 6376: Loss: 0.2381568870195951, accuracy: 0.9866666666666667\n",
      "iteration no 6377: Loss: 0.2381565992580335, accuracy: 0.9866666666666667\n",
      "iteration no 6378: Loss: 0.23815478001152474, accuracy: 0.9866666666666667\n",
      "iteration no 6379: Loss: 0.23815432143962129, accuracy: 0.9866666666666667\n",
      "iteration no 6380: Loss: 0.23815279796595107, accuracy: 0.9866666666666667\n",
      "iteration no 6381: Loss: 0.23815165434306876, accuracy: 0.9866666666666667\n",
      "iteration no 6382: Loss: 0.23815071971456908, accuracy: 0.9866666666666667\n",
      "iteration no 6383: Loss: 0.2381486624989938, accuracy: 0.9866666666666667\n",
      "iteration no 6384: Loss: 0.2381484018624641, accuracy: 0.9866666666666667\n",
      "iteration no 6385: Loss: 0.23814718662662684, accuracy: 0.9866666666666667\n",
      "iteration no 6386: Loss: 0.23814634936505463, accuracy: 0.9866666666666667\n",
      "iteration no 6387: Loss: 0.23814516061592647, accuracy: 0.9866666666666667\n",
      "iteration no 6388: Loss: 0.23814329088739894, accuracy: 0.9866666666666667\n",
      "iteration no 6389: Loss: 0.2381426961856699, accuracy: 0.9866666666666667\n",
      "iteration no 6390: Loss: 0.23814067818901047, accuracy: 0.9866666666666667\n",
      "iteration no 6391: Loss: 0.23814098575000633, accuracy: 0.9866666666666667\n",
      "iteration no 6392: Loss: 0.23813920786567486, accuracy: 0.9866666666666667\n",
      "iteration no 6393: Loss: 0.2381377658401832, accuracy: 0.9866666666666667\n",
      "iteration no 6394: Loss: 0.23813686412118887, accuracy: 0.9866666666666667\n",
      "iteration no 6395: Loss: 0.2381350245602321, accuracy: 0.9866666666666667\n",
      "iteration no 6396: Loss: 0.23813495600138646, accuracy: 0.9866666666666667\n",
      "iteration no 6397: Loss: 0.23813360785329107, accuracy: 0.9866666666666667\n",
      "iteration no 6398: Loss: 0.23813245885460785, accuracy: 0.9866666666666667\n",
      "iteration no 6399: Loss: 0.23813107229174868, accuracy: 0.9866666666666667\n",
      "iteration no 6400: Loss: 0.23812963932581138, accuracy: 0.9866666666666667\n",
      "iteration no 6401: Loss: 0.23812895493606598, accuracy: 0.9866666666666667\n",
      "iteration no 6402: Loss: 0.2381272779424027, accuracy: 0.9866666666666667\n",
      "iteration no 6403: Loss: 0.2381272334329967, accuracy: 0.9866666666666667\n",
      "iteration no 6404: Loss: 0.23812548708989772, accuracy: 0.9866666666666667\n",
      "iteration no 6405: Loss: 0.23812445017990572, accuracy: 0.9866666666666667\n",
      "iteration no 6406: Loss: 0.23812341837180398, accuracy: 0.9866666666666667\n",
      "iteration no 6407: Loss: 0.23812147288931695, accuracy: 0.9866666666666667\n",
      "iteration no 6408: Loss: 0.23812128698751187, accuracy: 0.9866666666666667\n",
      "iteration no 6409: Loss: 0.23811974136899697, accuracy: 0.9866666666666667\n",
      "iteration no 6410: Loss: 0.23811942370632286, accuracy: 0.9866666666666667\n",
      "iteration no 6411: Loss: 0.23811759514495534, accuracy: 0.9866666666666667\n",
      "iteration no 6412: Loss: 0.23811623973097706, accuracy: 0.9866666666666667\n",
      "iteration no 6413: Loss: 0.2381150816428937, accuracy: 0.9866666666666667\n",
      "iteration no 6414: Loss: 0.23811392683016244, accuracy: 0.9866666666666667\n",
      "iteration no 6415: Loss: 0.2381139967598278, accuracy: 0.9866666666666667\n",
      "iteration no 6416: Loss: 0.23811180752110953, accuracy: 0.9866666666666667\n",
      "iteration no 6417: Loss: 0.2381109549568801, accuracy: 0.9866666666666667\n",
      "iteration no 6418: Loss: 0.2381099973283365, accuracy: 0.9866666666666667\n",
      "iteration no 6419: Loss: 0.23810794629442808, accuracy: 0.9866666666666667\n",
      "iteration no 6420: Loss: 0.23810839599285785, accuracy: 0.9866666666666667\n",
      "iteration no 6421: Loss: 0.23810638353559943, accuracy: 0.9866666666666667\n",
      "iteration no 6422: Loss: 0.23810568334833454, accuracy: 0.9866666666666667\n",
      "iteration no 6423: Loss: 0.23810412235664088, accuracy: 0.9866666666666667\n",
      "iteration no 6424: Loss: 0.23810227132073533, accuracy: 0.9866666666666667\n",
      "iteration no 6425: Loss: 0.23810271299823366, accuracy: 0.9866666666666667\n",
      "iteration no 6426: Loss: 0.23810131101137083, accuracy: 0.9866666666666667\n",
      "iteration no 6427: Loss: 0.23810019094485652, accuracy: 0.9866666666666667\n",
      "iteration no 6428: Loss: 0.23809888513826666, accuracy: 0.9866666666666667\n",
      "iteration no 6429: Loss: 0.23809679230352374, accuracy: 0.9866666666666667\n",
      "iteration no 6430: Loss: 0.23809648534959732, accuracy: 0.9866666666666667\n",
      "iteration no 6431: Loss: 0.23809612864869975, accuracy: 0.9866666666666667\n",
      "iteration no 6432: Loss: 0.23809419759678246, accuracy: 0.9866666666666667\n",
      "iteration no 6433: Loss: 0.23809417440801275, accuracy: 0.9866666666666667\n",
      "iteration no 6434: Loss: 0.2380919110430656, accuracy: 0.9866666666666667\n",
      "iteration no 6435: Loss: 0.23809092814650923, accuracy: 0.9866666666666667\n",
      "iteration no 6436: Loss: 0.2380903414989811, accuracy: 0.9866666666666667\n",
      "iteration no 6437: Loss: 0.2380888464882683, accuracy: 0.9866666666666667\n",
      "iteration no 6438: Loss: 0.23808798171262036, accuracy: 0.9866666666666667\n",
      "iteration no 6439: Loss: 0.23808699348789994, accuracy: 0.9866666666666667\n",
      "iteration no 6440: Loss: 0.23808512806071758, accuracy: 0.9866666666666667\n",
      "iteration no 6441: Loss: 0.23808480976259555, accuracy: 0.9866666666666667\n",
      "iteration no 6442: Loss: 0.23808404650578377, accuracy: 0.9866666666666667\n",
      "iteration no 6443: Loss: 0.23808238807181453, accuracy: 0.9866666666666667\n",
      "iteration no 6444: Loss: 0.2380819877516473, accuracy: 0.9866666666666667\n",
      "iteration no 6445: Loss: 0.2380799023285472, accuracy: 0.9866666666666667\n",
      "iteration no 6446: Loss: 0.23807895570811788, accuracy: 0.9866666666666667\n",
      "iteration no 6447: Loss: 0.23807865456434987, accuracy: 0.9866666666666667\n",
      "iteration no 6448: Loss: 0.23807734264752847, accuracy: 0.9866666666666667\n",
      "iteration no 6449: Loss: 0.2380761260169329, accuracy: 0.9866666666666667\n",
      "iteration no 6450: Loss: 0.23807510219805172, accuracy: 0.9866666666666667\n",
      "iteration no 6451: Loss: 0.23807342296242123, accuracy: 0.9866666666666667\n",
      "iteration no 6452: Loss: 0.2380729238430775, accuracy: 0.9866666666666667\n",
      "iteration no 6453: Loss: 0.23807242531591918, accuracy: 0.9866666666666667\n",
      "iteration no 6454: Loss: 0.2380706633250187, accuracy: 0.9866666666666667\n",
      "iteration no 6455: Loss: 0.23806999216856495, accuracy: 0.9866666666666667\n",
      "iteration no 6456: Loss: 0.2380686726749885, accuracy: 0.9866666666666667\n",
      "iteration no 6457: Loss: 0.23806730833028328, accuracy: 0.9866666666666667\n",
      "iteration no 6458: Loss: 0.23806730059335113, accuracy: 0.9866666666666667\n",
      "iteration no 6459: Loss: 0.23806556689511277, accuracy: 0.9866666666666667\n",
      "iteration no 6460: Loss: 0.2380643968947881, accuracy: 0.9866666666666667\n",
      "iteration no 6461: Loss: 0.23806366602663565, accuracy: 0.9866666666666667\n",
      "iteration no 6462: Loss: 0.23806191375580182, accuracy: 0.9866666666666667\n",
      "iteration no 6463: Loss: 0.23806164229059695, accuracy: 0.9866666666666667\n",
      "iteration no 6464: Loss: 0.23806091262781898, accuracy: 0.9866666666666667\n",
      "iteration no 6465: Loss: 0.23805906945943117, accuracy: 0.9866666666666667\n",
      "iteration no 6466: Loss: 0.2380583695234692, accuracy: 0.9866666666666667\n",
      "iteration no 6467: Loss: 0.2380569884402885, accuracy: 0.9866666666666667\n",
      "iteration no 6468: Loss: 0.2380557482177247, accuracy: 0.9866666666666667\n",
      "iteration no 6469: Loss: 0.2380562806103682, accuracy: 0.9866666666666667\n",
      "iteration no 6470: Loss: 0.23805413958546448, accuracy: 0.9866666666666667\n",
      "iteration no 6471: Loss: 0.23805288791178622, accuracy: 0.9866666666666667\n",
      "iteration no 6472: Loss: 0.23805212280866495, accuracy: 0.9866666666666667\n",
      "iteration no 6473: Loss: 0.23805026937540252, accuracy: 0.9866666666666667\n",
      "iteration no 6474: Loss: 0.23805052107008917, accuracy: 0.9866666666666667\n",
      "iteration no 6475: Loss: 0.23804947550242456, accuracy: 0.9866666666666667\n",
      "iteration no 6476: Loss: 0.23804755288050572, accuracy: 0.9866666666666667\n",
      "iteration no 6477: Loss: 0.2380469455065476, accuracy: 0.9866666666666667\n",
      "iteration no 6478: Loss: 0.23804573565505655, accuracy: 0.9866666666666667\n",
      "iteration no 6479: Loss: 0.23804465184492918, accuracy: 0.9866666666666667\n",
      "iteration no 6480: Loss: 0.2380446902467403, accuracy: 0.9866666666666667\n",
      "iteration no 6481: Loss: 0.238042740466798, accuracy: 0.9866666666666667\n",
      "iteration no 6482: Loss: 0.23804149849318407, accuracy: 0.9866666666666667\n",
      "iteration no 6483: Loss: 0.2380408235685369, accuracy: 0.9866666666666667\n",
      "iteration no 6484: Loss: 0.23803965903978952, accuracy: 0.9866666666666667\n",
      "iteration no 6485: Loss: 0.23803905757218258, accuracy: 0.9866666666666667\n",
      "iteration no 6486: Loss: 0.23803830097934242, accuracy: 0.9866666666666667\n",
      "iteration no 6487: Loss: 0.2380364473006818, accuracy: 0.9866666666666667\n",
      "iteration no 6488: Loss: 0.2380356325413867, accuracy: 0.9866666666666667\n",
      "iteration no 6489: Loss: 0.23803479561249077, accuracy: 0.9866666666666667\n",
      "iteration no 6490: Loss: 0.23803363047332138, accuracy: 0.9866666666666667\n",
      "iteration no 6491: Loss: 0.23803369083500103, accuracy: 0.9866666666666667\n",
      "iteration no 6492: Loss: 0.23803171434325837, accuracy: 0.9866666666666667\n",
      "iteration no 6493: Loss: 0.23803039475392473, accuracy: 0.9866666666666667\n",
      "iteration no 6494: Loss: 0.23802994095627783, accuracy: 0.9866666666666667\n",
      "iteration no 6495: Loss: 0.23802877747180998, accuracy: 0.9866666666666667\n",
      "iteration no 6496: Loss: 0.23802778378836675, accuracy: 0.9866666666666667\n",
      "iteration no 6497: Loss: 0.23802679904594212, accuracy: 0.9866666666666667\n",
      "iteration no 6498: Loss: 0.2380252335245007, accuracy: 0.9866666666666667\n",
      "iteration no 6499: Loss: 0.23802516127810422, accuracy: 0.9866666666666667\n",
      "iteration no 6500: Loss: 0.23802418606567288, accuracy: 0.9866666666666667\n",
      "iteration no 6501: Loss: 0.2380224510787422, accuracy: 0.9866666666666667\n",
      "iteration no 6502: Loss: 0.2380216746855583, accuracy: 0.9866666666666667\n",
      "iteration no 6503: Loss: 0.23802057294306259, accuracy: 0.9866666666666667\n",
      "iteration no 6504: Loss: 0.23801951869541363, accuracy: 0.9866666666666667\n",
      "iteration no 6505: Loss: 0.23801964156611055, accuracy: 0.9866666666666667\n",
      "iteration no 6506: Loss: 0.23801768503079906, accuracy: 0.9866666666666667\n",
      "iteration no 6507: Loss: 0.23801650772385324, accuracy: 0.9866666666666667\n",
      "iteration no 6508: Loss: 0.23801574657900898, accuracy: 0.9866666666666667\n",
      "iteration no 6509: Loss: 0.2380145473975754, accuracy: 0.9866666666666667\n",
      "iteration no 6510: Loss: 0.23801440370664972, accuracy: 0.9866666666666667\n",
      "iteration no 6511: Loss: 0.2380130528128203, accuracy: 0.9866666666666667\n",
      "iteration no 6512: Loss: 0.23801123309729713, accuracy: 0.9866666666666667\n",
      "iteration no 6513: Loss: 0.23801081891729353, accuracy: 0.9866666666666667\n",
      "iteration no 6514: Loss: 0.23800998167115664, accuracy: 0.9866666666666667\n",
      "iteration no 6515: Loss: 0.23800886389599613, accuracy: 0.9866666666666667\n",
      "iteration no 6516: Loss: 0.23800832868540056, accuracy: 0.9866666666666667\n",
      "iteration no 6517: Loss: 0.23800691825835746, accuracy: 0.9866666666666667\n",
      "iteration no 6518: Loss: 0.2380052293386642, accuracy: 0.9866666666666667\n",
      "iteration no 6519: Loss: 0.2380051879606545, accuracy: 0.9866666666666667\n",
      "iteration no 6520: Loss: 0.23800444420339523, accuracy: 0.9866666666666667\n",
      "iteration no 6521: Loss: 0.23800291857007513, accuracy: 0.9866666666666667\n",
      "iteration no 6522: Loss: 0.23800243661331263, accuracy: 0.9866666666666667\n",
      "iteration no 6523: Loss: 0.238000531822678, accuracy: 0.9866666666666667\n",
      "iteration no 6524: Loss: 0.23800025423775273, accuracy: 0.9866666666666667\n",
      "iteration no 6525: Loss: 0.23799944413301616, accuracy: 0.9866666666666667\n",
      "iteration no 6526: Loss: 0.237997757372355, accuracy: 0.9866666666666667\n",
      "iteration no 6527: Loss: 0.23799732219938102, accuracy: 0.9866666666666667\n",
      "iteration no 6528: Loss: 0.2379960573273078, accuracy: 0.9866666666666667\n",
      "iteration no 6529: Loss: 0.23799529020402926, accuracy: 0.9866666666666667\n",
      "iteration no 6530: Loss: 0.23799477988608925, accuracy: 0.9866666666666667\n",
      "iteration no 6531: Loss: 0.23799342909766436, accuracy: 0.9866666666666667\n",
      "iteration no 6532: Loss: 0.23799178857572972, accuracy: 0.9866666666666667\n",
      "iteration no 6533: Loss: 0.2379914625310425, accuracy: 0.9866666666666667\n",
      "iteration no 6534: Loss: 0.2379907708206262, accuracy: 0.9866666666666667\n",
      "iteration no 6535: Loss: 0.23798995147278595, accuracy: 0.9866666666666667\n",
      "iteration no 6536: Loss: 0.23798869649659843, accuracy: 0.9866666666666667\n",
      "iteration no 6537: Loss: 0.23798701557626228, accuracy: 0.9866666666666667\n",
      "iteration no 6538: Loss: 0.23798645198553894, accuracy: 0.9866666666666667\n",
      "iteration no 6539: Loss: 0.23798628344169534, accuracy: 0.9866666666666667\n",
      "iteration no 6540: Loss: 0.23798459374065173, accuracy: 0.9866666666666667\n",
      "iteration no 6541: Loss: 0.23798405714754894, accuracy: 0.9866666666666667\n",
      "iteration no 6542: Loss: 0.2379824698906991, accuracy: 0.9866666666666667\n",
      "iteration no 6543: Loss: 0.23798153726423998, accuracy: 0.9866666666666667\n",
      "iteration no 6544: Loss: 0.2379815172210933, accuracy: 0.9866666666666667\n",
      "iteration no 6545: Loss: 0.2379800385387007, accuracy: 0.9866666666666667\n",
      "iteration no 6546: Loss: 0.23797854732680623, accuracy: 0.9866666666666667\n",
      "iteration no 6547: Loss: 0.23797802007061283, accuracy: 0.9866666666666667\n",
      "iteration no 6548: Loss: 0.2379774766796916, accuracy: 0.9866666666666667\n",
      "iteration no 6549: Loss: 0.2379762123452303, accuracy: 0.9866666666666667\n",
      "iteration no 6550: Loss: 0.23797569132546365, accuracy: 0.9866666666666667\n",
      "iteration no 6551: Loss: 0.2379740409303917, accuracy: 0.9866666666666667\n",
      "iteration no 6552: Loss: 0.23797270425521289, accuracy: 0.9866666666666667\n",
      "iteration no 6553: Loss: 0.2379733881145744, accuracy: 0.9866666666666667\n",
      "iteration no 6554: Loss: 0.23797179089995785, accuracy: 0.9866666666666667\n",
      "iteration no 6555: Loss: 0.23797031025101245, accuracy: 0.9866666666666667\n",
      "iteration no 6556: Loss: 0.23796958114302108, accuracy: 0.9866666666666667\n",
      "iteration no 6557: Loss: 0.2379683602438048, accuracy: 0.9866666666666667\n",
      "iteration no 6558: Loss: 0.23796828726210428, accuracy: 0.9866666666666667\n",
      "iteration no 6559: Loss: 0.23796715267769003, accuracy: 0.9866666666666667\n",
      "iteration no 6560: Loss: 0.2379654478160258, accuracy: 0.9866666666666667\n",
      "iteration no 6561: Loss: 0.23796480463719993, accuracy: 0.9866666666666667\n",
      "iteration no 6562: Loss: 0.23796443881042384, accuracy: 0.9866666666666667\n",
      "iteration no 6563: Loss: 0.2379628809285158, accuracy: 0.9866666666666667\n",
      "iteration no 6564: Loss: 0.2379623038982107, accuracy: 0.9866666666666667\n",
      "iteration no 6565: Loss: 0.23796132863144037, accuracy: 0.9866666666666667\n",
      "iteration no 6566: Loss: 0.2379597960178077, accuracy: 0.9866666666666667\n",
      "iteration no 6567: Loss: 0.23796016996709346, accuracy: 0.9866666666666667\n",
      "iteration no 6568: Loss: 0.23795865387815712, accuracy: 0.9866666666666667\n",
      "iteration no 6569: Loss: 0.23795716243043488, accuracy: 0.9866666666666667\n",
      "iteration no 6570: Loss: 0.23795656417247688, accuracy: 0.9866666666666667\n",
      "iteration no 6571: Loss: 0.2379557112885921, accuracy: 0.9866666666666667\n",
      "iteration no 6572: Loss: 0.2379556040390291, accuracy: 0.9866666666666667\n",
      "iteration no 6573: Loss: 0.23795402050952602, accuracy: 0.9866666666666667\n",
      "iteration no 6574: Loss: 0.2379524496367914, accuracy: 0.9866666666666667\n",
      "iteration no 6575: Loss: 0.23795188334108805, accuracy: 0.9866666666666667\n",
      "iteration no 6576: Loss: 0.2379514663928734, accuracy: 0.9866666666666667\n",
      "iteration no 6577: Loss: 0.23794999157844965, accuracy: 0.9866666666666667\n",
      "iteration no 6578: Loss: 0.23794974770536123, accuracy: 0.9866666666666667\n",
      "iteration no 6579: Loss: 0.23794833865985196, accuracy: 0.9866666666666667\n",
      "iteration no 6580: Loss: 0.23794701704382437, accuracy: 0.9866666666666667\n",
      "iteration no 6581: Loss: 0.23794747517904472, accuracy: 0.9866666666666667\n",
      "iteration no 6582: Loss: 0.2379454343106262, accuracy: 0.9866666666666667\n",
      "iteration no 6583: Loss: 0.237944555586781, accuracy: 0.9866666666666667\n",
      "iteration no 6584: Loss: 0.2379437113774539, accuracy: 0.9866666666666667\n",
      "iteration no 6585: Loss: 0.23794284118734416, accuracy: 0.9866666666666667\n",
      "iteration no 6586: Loss: 0.23794286186284871, accuracy: 0.9866666666666667\n",
      "iteration no 6587: Loss: 0.2379410178374581, accuracy: 0.9866666666666667\n",
      "iteration no 6588: Loss: 0.2379399339703426, accuracy: 0.9866666666666667\n",
      "iteration no 6589: Loss: 0.23793926104623025, accuracy: 0.99\n",
      "iteration no 6590: Loss: 0.23793835193545132, accuracy: 0.9866666666666667\n",
      "iteration no 6591: Loss: 0.237937688086373, accuracy: 0.9866666666666667\n",
      "iteration no 6592: Loss: 0.23793678015971226, accuracy: 0.9866666666666667\n",
      "iteration no 6593: Loss: 0.23793513262101074, accuracy: 0.9866666666666667\n",
      "iteration no 6594: Loss: 0.237935226728534, accuracy: 0.9866666666666667\n",
      "iteration no 6595: Loss: 0.23793419021285833, accuracy: 0.9866666666666667\n",
      "iteration no 6596: Loss: 0.23793261136940086, accuracy: 0.9866666666666667\n",
      "iteration no 6597: Loss: 0.23793205939713047, accuracy: 0.9866666666666667\n",
      "iteration no 6598: Loss: 0.23793111058822902, accuracy: 0.9866666666666667\n",
      "iteration no 6599: Loss: 0.23793025851181376, accuracy: 0.9866666666666667\n",
      "iteration no 6600: Loss: 0.23792972344414878, accuracy: 0.9866666666666667\n",
      "iteration no 6601: Loss: 0.23792859431918237, accuracy: 0.99\n",
      "iteration no 6602: Loss: 0.23792688998376396, accuracy: 0.9866666666666667\n",
      "iteration no 6603: Loss: 0.23792726344114348, accuracy: 0.99\n",
      "iteration no 6604: Loss: 0.23792575899745944, accuracy: 0.9866666666666667\n",
      "iteration no 6605: Loss: 0.2379252250027542, accuracy: 0.9866666666666667\n",
      "iteration no 6606: Loss: 0.23792392773492255, accuracy: 0.9866666666666667\n",
      "iteration no 6607: Loss: 0.23792271218554634, accuracy: 0.9866666666666667\n",
      "iteration no 6608: Loss: 0.23792284200855618, accuracy: 0.9866666666666667\n",
      "iteration no 6609: Loss: 0.23792142626644086, accuracy: 0.9866666666666667\n",
      "iteration no 6610: Loss: 0.23791993744904794, accuracy: 0.9866666666666667\n",
      "iteration no 6611: Loss: 0.23791956052383123, accuracy: 0.9866666666666667\n",
      "iteration no 6612: Loss: 0.23791900252045872, accuracy: 0.99\n",
      "iteration no 6613: Loss: 0.2379176934062336, accuracy: 0.9866666666666667\n",
      "iteration no 6614: Loss: 0.23791742269067617, accuracy: 0.99\n",
      "iteration no 6615: Loss: 0.23791551885545875, accuracy: 0.9866666666666667\n",
      "iteration no 6616: Loss: 0.23791528234463513, accuracy: 0.9866666666666667\n",
      "iteration no 6617: Loss: 0.23791459629843195, accuracy: 0.9866666666666667\n",
      "iteration no 6618: Loss: 0.23791317382616567, accuracy: 0.9866666666666667\n",
      "iteration no 6619: Loss: 0.23791278040238956, accuracy: 0.9866666666666667\n",
      "iteration no 6620: Loss: 0.23791131443277858, accuracy: 0.99\n",
      "iteration no 6621: Loss: 0.23791067511857483, accuracy: 0.9866666666666667\n",
      "iteration no 6622: Loss: 0.23791051178037848, accuracy: 0.9866666666666667\n",
      "iteration no 6623: Loss: 0.23790895296190095, accuracy: 0.9866666666666667\n",
      "iteration no 6624: Loss: 0.23790736738537904, accuracy: 0.99\n",
      "iteration no 6625: Loss: 0.2379078859952053, accuracy: 0.99\n",
      "iteration no 6626: Loss: 0.2379063888608288, accuracy: 0.9866666666666667\n",
      "iteration no 6627: Loss: 0.23790573712413327, accuracy: 0.9866666666666667\n",
      "iteration no 6628: Loss: 0.23790450957883177, accuracy: 0.99\n",
      "iteration no 6629: Loss: 0.2379031738357822, accuracy: 0.9866666666666667\n",
      "iteration no 6630: Loss: 0.23790350550518213, accuracy: 0.9866666666666667\n",
      "iteration no 6631: Loss: 0.23790212371244607, accuracy: 0.9866666666666667\n",
      "iteration no 6632: Loss: 0.23790079093220712, accuracy: 0.9866666666666667\n",
      "iteration no 6633: Loss: 0.2379002222006983, accuracy: 0.99\n",
      "iteration no 6634: Loss: 0.23789975951025283, accuracy: 0.9866666666666667\n",
      "iteration no 6635: Loss: 0.23789856066242349, accuracy: 0.9866666666666667\n",
      "iteration no 6636: Loss: 0.2378978479663087, accuracy: 0.99\n",
      "iteration no 6637: Loss: 0.2378963022959546, accuracy: 0.9866666666666667\n",
      "iteration no 6638: Loss: 0.2378962383772837, accuracy: 0.99\n",
      "iteration no 6639: Loss: 0.23789547031156705, accuracy: 0.9866666666666667\n",
      "iteration no 6640: Loss: 0.23789409121294788, accuracy: 0.9866666666666667\n",
      "iteration no 6641: Loss: 0.23789342330783372, accuracy: 0.9866666666666667\n",
      "iteration no 6642: Loss: 0.23789243008116795, accuracy: 0.9866666666666667\n",
      "iteration no 6643: Loss: 0.2378915682412131, accuracy: 0.9866666666666667\n",
      "iteration no 6644: Loss: 0.23789132226115953, accuracy: 0.9866666666666667\n",
      "iteration no 6645: Loss: 0.23788949730310172, accuracy: 0.99\n",
      "iteration no 6646: Loss: 0.23788892450050947, accuracy: 0.9866666666666667\n",
      "iteration no 6647: Loss: 0.23788887795358948, accuracy: 0.9866666666666667\n",
      "iteration no 6648: Loss: 0.23788715094784396, accuracy: 0.9866666666666667\n",
      "iteration no 6649: Loss: 0.23788672973043468, accuracy: 0.9866666666666667\n",
      "iteration no 6650: Loss: 0.23788531995819961, accuracy: 0.9866666666666667\n",
      "iteration no 6651: Loss: 0.23788482007737122, accuracy: 0.99\n",
      "iteration no 6652: Loss: 0.23788464114505015, accuracy: 0.99\n",
      "iteration no 6653: Loss: 0.23788285656758068, accuracy: 0.99\n",
      "iteration no 6654: Loss: 0.23788196792984842, accuracy: 0.99\n",
      "iteration no 6655: Loss: 0.23788185606018736, accuracy: 0.99\n",
      "iteration no 6656: Loss: 0.23788076542575132, accuracy: 0.9866666666666667\n",
      "iteration no 6657: Loss: 0.23787970936009767, accuracy: 0.99\n",
      "iteration no 6658: Loss: 0.23787875697624256, accuracy: 0.9866666666666667\n",
      "iteration no 6659: Loss: 0.23787761096578014, accuracy: 0.99\n",
      "iteration no 6660: Loss: 0.23787785388208754, accuracy: 0.9866666666666667\n",
      "iteration no 6661: Loss: 0.23787655566288357, accuracy: 0.99\n",
      "iteration no 6662: Loss: 0.23787509321884845, accuracy: 0.9866666666666667\n",
      "iteration no 6663: Loss: 0.23787480978399644, accuracy: 0.99\n",
      "iteration no 6664: Loss: 0.2378742603153176, accuracy: 0.9866666666666667\n",
      "iteration no 6665: Loss: 0.2378730285632088, accuracy: 0.99\n",
      "iteration no 6666: Loss: 0.2378721362389818, accuracy: 0.9866666666666667\n",
      "iteration no 6667: Loss: 0.2378706131664034, accuracy: 0.99\n",
      "iteration no 6668: Loss: 0.23787118223464512, accuracy: 0.99\n",
      "iteration no 6669: Loss: 0.23787008245328878, accuracy: 0.99\n",
      "iteration no 6670: Loss: 0.23786840403521142, accuracy: 0.99\n",
      "iteration no 6671: Loss: 0.23786798922033373, accuracy: 0.9866666666666667\n",
      "iteration no 6672: Loss: 0.23786738385643097, accuracy: 0.99\n",
      "iteration no 6673: Loss: 0.23786658503564706, accuracy: 0.9866666666666667\n",
      "iteration no 6674: Loss: 0.23786604343806222, accuracy: 0.99\n",
      "iteration no 6675: Loss: 0.23786407215063168, accuracy: 0.99\n",
      "iteration no 6676: Loss: 0.2378640302335875, accuracy: 0.99\n",
      "iteration no 6677: Loss: 0.2378637940989932, accuracy: 0.99\n",
      "iteration no 6678: Loss: 0.23786200337318855, accuracy: 0.99\n",
      "iteration no 6679: Loss: 0.23786143899622447, accuracy: 0.9866666666666667\n",
      "iteration no 6680: Loss: 0.23786048507942453, accuracy: 0.99\n",
      "iteration no 6681: Loss: 0.23786002815479196, accuracy: 0.99\n",
      "iteration no 6682: Loss: 0.23785949217390026, accuracy: 0.99\n",
      "iteration no 6683: Loss: 0.2378577444840095, accuracy: 0.99\n",
      "iteration no 6684: Loss: 0.23785700282715266, accuracy: 0.99\n",
      "iteration no 6685: Loss: 0.23785733069777637, accuracy: 0.99\n",
      "iteration no 6686: Loss: 0.2378556732094622, accuracy: 0.99\n",
      "iteration no 6687: Loss: 0.23785497674593087, accuracy: 0.99\n",
      "iteration no 6688: Loss: 0.237853762401042, accuracy: 0.9866666666666667\n",
      "iteration no 6689: Loss: 0.2378532291861155, accuracy: 0.99\n",
      "iteration no 6690: Loss: 0.23785313668991104, accuracy: 0.99\n",
      "iteration no 6691: Loss: 0.23785137607246382, accuracy: 0.99\n",
      "iteration no 6692: Loss: 0.2378506582399033, accuracy: 0.99\n",
      "iteration no 6693: Loss: 0.237850395294339, accuracy: 0.99\n",
      "iteration no 6694: Loss: 0.23784928813679104, accuracy: 0.99\n",
      "iteration no 6695: Loss: 0.23784873545373558, accuracy: 0.9866666666666667\n",
      "iteration no 6696: Loss: 0.23784748667475575, accuracy: 0.99\n",
      "iteration no 6697: Loss: 0.23784628931948765, accuracy: 0.99\n",
      "iteration no 6698: Loss: 0.2378467328902189, accuracy: 0.99\n",
      "iteration no 6699: Loss: 0.23784512607563646, accuracy: 0.99\n",
      "iteration no 6700: Loss: 0.23784430929355171, accuracy: 0.99\n",
      "iteration no 6701: Loss: 0.23784367240097143, accuracy: 0.99\n",
      "iteration no 6702: Loss: 0.23784299426936756, accuracy: 0.9866666666666667\n",
      "iteration no 6703: Loss: 0.23784243816290795, accuracy: 0.99\n",
      "iteration no 6704: Loss: 0.2378412343924659, accuracy: 0.99\n",
      "iteration no 6705: Loss: 0.2378396356650559, accuracy: 0.99\n",
      "iteration no 6706: Loss: 0.23784035890547778, accuracy: 0.99\n",
      "iteration no 6707: Loss: 0.23783880192894719, accuracy: 0.99\n",
      "iteration no 6708: Loss: 0.2378380057998289, accuracy: 0.99\n",
      "iteration no 6709: Loss: 0.23783713815826507, accuracy: 0.9866666666666667\n",
      "iteration no 6710: Loss: 0.23783663673687483, accuracy: 0.99\n",
      "iteration no 6711: Loss: 0.23783617707503613, accuracy: 0.99\n",
      "iteration no 6712: Loss: 0.23783479420027315, accuracy: 0.99\n",
      "iteration no 6713: Loss: 0.23783342739593105, accuracy: 0.99\n",
      "iteration no 6714: Loss: 0.23783372230359345, accuracy: 0.99\n",
      "iteration no 6715: Loss: 0.23783243108330587, accuracy: 0.99\n",
      "iteration no 6716: Loss: 0.23783188922371395, accuracy: 0.9866666666666667\n",
      "iteration no 6717: Loss: 0.237831099097141, accuracy: 0.99\n",
      "iteration no 6718: Loss: 0.23782985668899653, accuracy: 0.99\n",
      "iteration no 6719: Loss: 0.23783003504404038, accuracy: 0.99\n",
      "iteration no 6720: Loss: 0.23782844523449007, accuracy: 0.99\n",
      "iteration no 6721: Loss: 0.23782720142132852, accuracy: 0.99\n",
      "iteration no 6722: Loss: 0.23782728113242707, accuracy: 0.99\n",
      "iteration no 6723: Loss: 0.237826316252355, accuracy: 0.9866666666666667\n",
      "iteration no 6724: Loss: 0.2378256812312909, accuracy: 0.99\n",
      "iteration no 6725: Loss: 0.23782469985793267, accuracy: 0.99\n",
      "iteration no 6726: Loss: 0.23782334120403406, accuracy: 0.99\n",
      "iteration no 6727: Loss: 0.23782383674290708, accuracy: 0.99\n",
      "iteration no 6728: Loss: 0.23782228448502363, accuracy: 0.99\n",
      "iteration no 6729: Loss: 0.23782106084693583, accuracy: 0.99\n",
      "iteration no 6730: Loss: 0.23782060233232027, accuracy: 0.9866666666666667\n",
      "iteration no 6731: Loss: 0.23782015737430395, accuracy: 0.99\n",
      "iteration no 6732: Loss: 0.23781961619865843, accuracy: 0.99\n",
      "iteration no 6733: Loss: 0.23781825665947995, accuracy: 0.99\n",
      "iteration no 6734: Loss: 0.237817016252168, accuracy: 0.99\n",
      "iteration no 6735: Loss: 0.2378177731182026, accuracy: 0.99\n",
      "iteration no 6736: Loss: 0.23781592841127913, accuracy: 0.99\n",
      "iteration no 6737: Loss: 0.23781511912412243, accuracy: 0.9866666666666667\n",
      "iteration no 6738: Loss: 0.237814276509563, accuracy: 0.99\n",
      "iteration no 6739: Loss: 0.23781384859884291, accuracy: 0.99\n",
      "iteration no 6740: Loss: 0.23781356332389747, accuracy: 0.99\n",
      "iteration no 6741: Loss: 0.2378120258414516, accuracy: 0.99\n",
      "iteration no 6742: Loss: 0.23781073820976498, accuracy: 0.99\n",
      "iteration no 6743: Loss: 0.23781130450792903, accuracy: 0.99\n",
      "iteration no 6744: Loss: 0.23780967650402257, accuracy: 0.99\n",
      "iteration no 6745: Loss: 0.2378088946131487, accuracy: 0.99\n",
      "iteration no 6746: Loss: 0.23780777947554174, accuracy: 0.99\n",
      "iteration no 6747: Loss: 0.23780742027609136, accuracy: 0.99\n",
      "iteration no 6748: Loss: 0.23780706155538556, accuracy: 0.99\n",
      "iteration no 6749: Loss: 0.2378056487269561, accuracy: 0.99\n",
      "iteration no 6750: Loss: 0.23780462672595898, accuracy: 0.99\n",
      "iteration no 6751: Loss: 0.2378047415211444, accuracy: 0.99\n",
      "iteration no 6752: Loss: 0.23780328306292176, accuracy: 0.99\n",
      "iteration no 6753: Loss: 0.23780292850032364, accuracy: 0.99\n",
      "iteration no 6754: Loss: 0.23780146724335077, accuracy: 0.99\n",
      "iteration no 6755: Loss: 0.2378012085076538, accuracy: 0.99\n",
      "iteration no 6756: Loss: 0.23780073924830242, accuracy: 0.99\n",
      "iteration no 6757: Loss: 0.23779921289031997, accuracy: 0.99\n",
      "iteration no 6758: Loss: 0.23779873853292527, accuracy: 0.99\n",
      "iteration no 6759: Loss: 0.23779836200341292, accuracy: 0.99\n",
      "iteration no 6760: Loss: 0.23779713843711542, accuracy: 0.99\n",
      "iteration no 6761: Loss: 0.23779690130002792, accuracy: 0.99\n",
      "iteration no 6762: Loss: 0.23779519559255763, accuracy: 0.99\n",
      "iteration no 6763: Loss: 0.2377950657246582, accuracy: 0.99\n",
      "iteration no 6764: Loss: 0.23779448055668817, accuracy: 0.99\n",
      "iteration no 6765: Loss: 0.2377927702707568, accuracy: 0.99\n",
      "iteration no 6766: Loss: 0.23779276358670562, accuracy: 0.99\n",
      "iteration no 6767: Loss: 0.2377920336067883, accuracy: 0.99\n",
      "iteration no 6768: Loss: 0.2377913913735215, accuracy: 0.9866666666666667\n",
      "iteration no 6769: Loss: 0.2377906709338522, accuracy: 0.99\n",
      "iteration no 6770: Loss: 0.2377888771629636, accuracy: 0.99\n",
      "iteration no 6771: Loss: 0.23778918363897855, accuracy: 0.99\n",
      "iteration no 6772: Loss: 0.23778826419906313, accuracy: 0.99\n",
      "iteration no 6773: Loss: 0.23778685840752478, accuracy: 0.99\n",
      "iteration no 6774: Loss: 0.23778635310352908, accuracy: 0.99\n",
      "iteration no 6775: Loss: 0.2377857854979869, accuracy: 0.9866666666666667\n",
      "iteration no 6776: Loss: 0.23778560545563848, accuracy: 0.99\n",
      "iteration no 6777: Loss: 0.2377843512535648, accuracy: 0.99\n",
      "iteration no 6778: Loss: 0.23778264799445492, accuracy: 0.99\n",
      "iteration no 6779: Loss: 0.2377831704094692, accuracy: 0.99\n",
      "iteration no 6780: Loss: 0.2377820770500776, accuracy: 0.99\n",
      "iteration no 6781: Loss: 0.2377810295022219, accuracy: 0.99\n",
      "iteration no 6782: Loss: 0.2377800159796846, accuracy: 0.99\n",
      "iteration no 6783: Loss: 0.23777951502209982, accuracy: 0.99\n",
      "iteration no 6784: Loss: 0.23777979017497147, accuracy: 0.99\n",
      "iteration no 6785: Loss: 0.23777800111583872, accuracy: 0.99\n",
      "iteration no 6786: Loss: 0.23777697804132886, accuracy: 0.99\n",
      "iteration no 6787: Loss: 0.2377769862899057, accuracy: 0.99\n",
      "iteration no 6788: Loss: 0.23777570799394215, accuracy: 0.99\n",
      "iteration no 6789: Loss: 0.23777535402255703, accuracy: 0.99\n",
      "iteration no 6790: Loss: 0.2377738645575654, accuracy: 0.99\n",
      "iteration no 6791: Loss: 0.23777371062262764, accuracy: 0.99\n",
      "iteration no 6792: Loss: 0.23777342153873063, accuracy: 0.99\n",
      "iteration no 6793: Loss: 0.23777178618434797, accuracy: 0.99\n",
      "iteration no 6794: Loss: 0.23777120457209966, accuracy: 0.99\n",
      "iteration no 6795: Loss: 0.23777100507250487, accuracy: 0.99\n",
      "iteration no 6796: Loss: 0.23776986922034732, accuracy: 0.9866666666666667\n",
      "iteration no 6797: Loss: 0.23776934045232703, accuracy: 0.99\n",
      "iteration no 6798: Loss: 0.2377676639793535, accuracy: 0.99\n",
      "iteration no 6799: Loss: 0.23776831661622105, accuracy: 0.99\n",
      "iteration no 6800: Loss: 0.23776694853509608, accuracy: 0.99\n",
      "iteration no 6801: Loss: 0.23776561450693756, accuracy: 0.99\n",
      "iteration no 6802: Loss: 0.23776516109943635, accuracy: 0.99\n",
      "iteration no 6803: Loss: 0.23776474208647783, accuracy: 0.99\n",
      "iteration no 6804: Loss: 0.23776405484598687, accuracy: 0.99\n",
      "iteration no 6805: Loss: 0.2377628929869502, accuracy: 0.99\n",
      "iteration no 6806: Loss: 0.23776135665688364, accuracy: 0.99\n",
      "iteration no 6807: Loss: 0.23776231527076075, accuracy: 0.99\n",
      "iteration no 6808: Loss: 0.23776034546095376, accuracy: 0.99\n",
      "iteration no 6809: Loss: 0.23775933032277652, accuracy: 0.99\n",
      "iteration no 6810: Loss: 0.23775892984044167, accuracy: 0.99\n",
      "iteration no 6811: Loss: 0.23775840355788247, accuracy: 0.9866666666666667\n",
      "iteration no 6812: Loss: 0.23775808107919397, accuracy: 0.99\n",
      "iteration no 6813: Loss: 0.23775620631979932, accuracy: 0.99\n",
      "iteration no 6814: Loss: 0.23775583152017793, accuracy: 0.99\n",
      "iteration no 6815: Loss: 0.23775548952416062, accuracy: 0.99\n",
      "iteration no 6816: Loss: 0.2377538652115081, accuracy: 0.99\n",
      "iteration no 6817: Loss: 0.23775332482411937, accuracy: 0.99\n",
      "iteration no 6818: Loss: 0.23775272813106113, accuracy: 0.99\n",
      "iteration no 6819: Loss: 0.23775236589221072, accuracy: 0.99\n",
      "iteration no 6820: Loss: 0.23775131617172873, accuracy: 0.99\n",
      "iteration no 6821: Loss: 0.23774976036198572, accuracy: 0.99\n",
      "iteration no 6822: Loss: 0.23775024378724796, accuracy: 0.99\n",
      "iteration no 6823: Loss: 0.23774905130176804, accuracy: 0.99\n",
      "iteration no 6824: Loss: 0.23774770150697538, accuracy: 0.99\n",
      "iteration no 6825: Loss: 0.2377469480843704, accuracy: 0.99\n",
      "iteration no 6826: Loss: 0.237746648647418, accuracy: 0.99\n",
      "iteration no 6827: Loss: 0.2377463015781333, accuracy: 0.99\n",
      "iteration no 6828: Loss: 0.23774491166245457, accuracy: 0.99\n",
      "iteration no 6829: Loss: 0.23774364168688633, accuracy: 0.99\n",
      "iteration no 6830: Loss: 0.2377443590213226, accuracy: 0.99\n",
      "iteration no 6831: Loss: 0.2377426263114203, accuracy: 0.99\n",
      "iteration no 6832: Loss: 0.23774180980664728, accuracy: 0.99\n",
      "iteration no 6833: Loss: 0.237740946825981, accuracy: 0.99\n",
      "iteration no 6834: Loss: 0.23774055715476158, accuracy: 0.99\n",
      "iteration no 6835: Loss: 0.23774009164116033, accuracy: 0.99\n",
      "iteration no 6836: Loss: 0.23773834422518547, accuracy: 0.99\n",
      "iteration no 6837: Loss: 0.23773847571358758, accuracy: 0.99\n",
      "iteration no 6838: Loss: 0.2377378946008454, accuracy: 0.99\n",
      "iteration no 6839: Loss: 0.2377361668334475, accuracy: 0.99\n",
      "iteration no 6840: Loss: 0.2377359153969958, accuracy: 0.99\n",
      "iteration no 6841: Loss: 0.23773525686660363, accuracy: 0.99\n",
      "iteration no 6842: Loss: 0.23773468527173533, accuracy: 0.99\n",
      "iteration no 6843: Loss: 0.23773359565663604, accuracy: 0.99\n",
      "iteration no 6844: Loss: 0.23773193500227485, accuracy: 0.99\n",
      "iteration no 6845: Loss: 0.237733120515273, accuracy: 0.99\n",
      "iteration no 6846: Loss: 0.237731491119947, accuracy: 0.99\n",
      "iteration no 6847: Loss: 0.2377304893628098, accuracy: 0.99\n",
      "iteration no 6848: Loss: 0.23772957770043485, accuracy: 0.99\n",
      "iteration no 6849: Loss: 0.23772930524710806, accuracy: 0.99\n",
      "iteration no 6850: Loss: 0.23772890315496903, accuracy: 0.99\n",
      "iteration no 6851: Loss: 0.23772710224352242, accuracy: 0.99\n",
      "iteration no 6852: Loss: 0.23772669467725652, accuracy: 0.99\n",
      "iteration no 6853: Loss: 0.23772659282120456, accuracy: 0.99\n",
      "iteration no 6854: Loss: 0.23772488763218902, accuracy: 0.99\n",
      "iteration no 6855: Loss: 0.23772471210766055, accuracy: 0.99\n",
      "iteration no 6856: Loss: 0.2377240718312514, accuracy: 0.99\n",
      "iteration no 6857: Loss: 0.23772357215664566, accuracy: 0.99\n",
      "iteration no 6858: Loss: 0.23772240940515468, accuracy: 0.99\n",
      "iteration no 6859: Loss: 0.23772073808491911, accuracy: 0.99\n",
      "iteration no 6860: Loss: 0.2377219124215755, accuracy: 0.99\n",
      "iteration no 6861: Loss: 0.23772017023049336, accuracy: 0.99\n",
      "iteration no 6862: Loss: 0.23771903681375894, accuracy: 0.99\n",
      "iteration no 6863: Loss: 0.23771852011654945, accuracy: 0.99\n",
      "iteration no 6864: Loss: 0.2377182648473322, accuracy: 0.99\n",
      "iteration no 6865: Loss: 0.2377178394535837, accuracy: 0.99\n",
      "iteration no 6866: Loss: 0.23771600722659958, accuracy: 0.99\n",
      "iteration no 6867: Loss: 0.23771576281849274, accuracy: 0.99\n",
      "iteration no 6868: Loss: 0.23771553542957885, accuracy: 0.99\n",
      "iteration no 6869: Loss: 0.23771373658257786, accuracy: 0.99\n",
      "iteration no 6870: Loss: 0.23771347141250032, accuracy: 0.99\n",
      "iteration no 6871: Loss: 0.2377131531808354, accuracy: 0.99\n",
      "iteration no 6872: Loss: 0.23771229212698816, accuracy: 0.99\n",
      "iteration no 6873: Loss: 0.23771145247324282, accuracy: 0.99\n",
      "iteration no 6874: Loss: 0.2377098731223196, accuracy: 0.99\n",
      "iteration no 6875: Loss: 0.23771091273467476, accuracy: 0.99\n",
      "iteration no 6876: Loss: 0.237709156875145, accuracy: 0.99\n",
      "iteration no 6877: Loss: 0.23770801530130564, accuracy: 0.99\n",
      "iteration no 6878: Loss: 0.23770803086319578, accuracy: 0.99\n",
      "iteration no 6879: Loss: 0.23770707675513483, accuracy: 0.99\n",
      "iteration no 6880: Loss: 0.23770652155932873, accuracy: 0.99\n",
      "iteration no 6881: Loss: 0.23770496547733388, accuracy: 0.99\n",
      "iteration no 6882: Loss: 0.23770512241162944, accuracy: 0.99\n",
      "iteration no 6883: Loss: 0.2377046463259474, accuracy: 0.99\n",
      "iteration no 6884: Loss: 0.23770282755517413, accuracy: 0.99\n",
      "iteration no 6885: Loss: 0.23770279483219103, accuracy: 0.99\n",
      "iteration no 6886: Loss: 0.23770232894983018, accuracy: 0.99\n",
      "iteration no 6887: Loss: 0.2377012012378601, accuracy: 0.99\n",
      "iteration no 6888: Loss: 0.23770025827494853, accuracy: 0.99\n",
      "iteration no 6889: Loss: 0.23769937359901275, accuracy: 0.99\n",
      "iteration no 6890: Loss: 0.23769973816869594, accuracy: 0.99\n",
      "iteration no 6891: Loss: 0.23769822842268434, accuracy: 0.99\n",
      "iteration no 6892: Loss: 0.23769711697529455, accuracy: 0.99\n",
      "iteration no 6893: Loss: 0.2376976705754247, accuracy: 0.99\n",
      "iteration no 6894: Loss: 0.23769608410888576, accuracy: 0.99\n",
      "iteration no 6895: Loss: 0.23769567665596428, accuracy: 0.99\n",
      "iteration no 6896: Loss: 0.23769425782151216, accuracy: 0.99\n",
      "iteration no 6897: Loss: 0.2376943675097525, accuracy: 0.99\n",
      "iteration no 6898: Loss: 0.2376935002820012, accuracy: 0.99\n",
      "iteration no 6899: Loss: 0.2376918931901793, accuracy: 0.99\n",
      "iteration no 6900: Loss: 0.23769269193733064, accuracy: 0.99\n",
      "iteration no 6901: Loss: 0.23769153908585394, accuracy: 0.99\n",
      "iteration no 6902: Loss: 0.23769046277602185, accuracy: 0.99\n",
      "iteration no 6903: Loss: 0.23768957014264624, accuracy: 0.99\n",
      "iteration no 6904: Loss: 0.23768939748156784, accuracy: 0.99\n",
      "iteration no 6905: Loss: 0.23768900195585652, accuracy: 0.99\n",
      "iteration no 6906: Loss: 0.23768750966657032, accuracy: 0.99\n",
      "iteration no 6907: Loss: 0.2376871079095635, accuracy: 0.99\n",
      "iteration no 6908: Loss: 0.23768705527147588, accuracy: 0.99\n",
      "iteration no 6909: Loss: 0.23768549922155957, accuracy: 0.99\n",
      "iteration no 6910: Loss: 0.2376851582284178, accuracy: 0.99\n",
      "iteration no 6911: Loss: 0.2376846831289326, accuracy: 0.99\n",
      "iteration no 6912: Loss: 0.2376843029892332, accuracy: 0.99\n",
      "iteration no 6913: Loss: 0.23768323098591398, accuracy: 0.99\n",
      "iteration no 6914: Loss: 0.23768216588560426, accuracy: 0.99\n",
      "iteration no 6915: Loss: 0.23768265021244445, accuracy: 0.99\n",
      "iteration no 6916: Loss: 0.23768101128370842, accuracy: 0.99\n",
      "iteration no 6917: Loss: 0.23768022093368457, accuracy: 0.99\n",
      "iteration no 6918: Loss: 0.23768015822115077, accuracy: 0.99\n",
      "iteration no 6919: Loss: 0.23767940653390482, accuracy: 0.99\n",
      "iteration no 6920: Loss: 0.2376789252139417, accuracy: 0.99\n",
      "iteration no 6921: Loss: 0.23767722879912861, accuracy: 0.99\n",
      "iteration no 6922: Loss: 0.23767821583106832, accuracy: 0.99\n",
      "iteration no 6923: Loss: 0.23767674331641314, accuracy: 0.99\n",
      "iteration no 6924: Loss: 0.23767558092865565, accuracy: 0.99\n",
      "iteration no 6925: Loss: 0.23767553497670935, accuracy: 0.99\n",
      "iteration no 6926: Loss: 0.2376745964841336, accuracy: 0.99\n",
      "iteration no 6927: Loss: 0.23767444886624212, accuracy: 0.99\n",
      "iteration no 6928: Loss: 0.2376726838055794, accuracy: 0.99\n",
      "iteration no 6929: Loss: 0.23767337066913574, accuracy: 0.99\n",
      "iteration no 6930: Loss: 0.23767225428823113, accuracy: 0.99\n",
      "iteration no 6931: Loss: 0.23767095254459047, accuracy: 0.99\n",
      "iteration no 6932: Loss: 0.2376712624939088, accuracy: 0.99\n",
      "iteration no 6933: Loss: 0.23767031655228116, accuracy: 0.99\n",
      "iteration no 6934: Loss: 0.23766976172565518, accuracy: 0.99\n",
      "iteration no 6935: Loss: 0.23766826317609746, accuracy: 0.99\n",
      "iteration no 6936: Loss: 0.23766856632347372, accuracy: 0.99\n",
      "iteration no 6937: Loss: 0.23766800515975958, accuracy: 0.99\n",
      "iteration no 6938: Loss: 0.23766601318597622, accuracy: 0.99\n",
      "iteration no 6939: Loss: 0.23766662755173096, accuracy: 0.99\n",
      "iteration no 6940: Loss: 0.23766565348307786, accuracy: 0.99\n",
      "iteration no 6941: Loss: 0.2376650487537614, accuracy: 0.99\n",
      "iteration no 6942: Loss: 0.23766390329771075, accuracy: 0.99\n",
      "iteration no 6943: Loss: 0.23766399416992773, accuracy: 0.99\n",
      "iteration no 6944: Loss: 0.2376634992476156, accuracy: 0.99\n",
      "iteration no 6945: Loss: 0.23766182397071595, accuracy: 0.99\n",
      "iteration no 6946: Loss: 0.2376617102704118, accuracy: 0.99\n",
      "iteration no 6947: Loss: 0.23766164197846534, accuracy: 0.99\n",
      "iteration no 6948: Loss: 0.2376602705342471, accuracy: 0.99\n",
      "iteration no 6949: Loss: 0.23765951420998865, accuracy: 0.99\n",
      "iteration no 6950: Loss: 0.23765902580982118, accuracy: 0.99\n",
      "iteration no 6951: Loss: 0.23765911219884533, accuracy: 0.99\n",
      "iteration no 6952: Loss: 0.23765754793673233, accuracy: 0.99\n",
      "iteration no 6953: Loss: 0.23765729376620784, accuracy: 0.99\n",
      "iteration no 6954: Loss: 0.2376573591339008, accuracy: 0.99\n",
      "iteration no 6955: Loss: 0.2376557028654462, accuracy: 0.99\n",
      "iteration no 6956: Loss: 0.23765529739234123, accuracy: 0.99\n",
      "iteration no 6957: Loss: 0.23765474295065603, accuracy: 0.99\n",
      "iteration no 6958: Loss: 0.23765457502298487, accuracy: 0.99\n",
      "iteration no 6959: Loss: 0.23765310365955944, accuracy: 0.99\n",
      "iteration no 6960: Loss: 0.23765245417273173, accuracy: 0.99\n",
      "iteration no 6961: Loss: 0.23765293215356947, accuracy: 0.99\n",
      "iteration no 6962: Loss: 0.23765114975075424, accuracy: 0.99\n",
      "iteration no 6963: Loss: 0.23765075241336234, accuracy: 0.99\n",
      "iteration no 6964: Loss: 0.23765045283458452, accuracy: 0.99\n",
      "iteration no 6965: Loss: 0.23764997034837465, accuracy: 0.99\n",
      "iteration no 6966: Loss: 0.23764888143420485, accuracy: 0.99\n",
      "iteration no 6967: Loss: 0.2376479592173436, accuracy: 0.99\n",
      "iteration no 6968: Loss: 0.23764870184008446, accuracy: 0.99\n",
      "iteration no 6969: Loss: 0.2376467805734793, accuracy: 0.99\n",
      "iteration no 6970: Loss: 0.23764622427114268, accuracy: 0.99\n",
      "iteration no 6971: Loss: 0.2376461251610291, accuracy: 0.99\n",
      "iteration no 6972: Loss: 0.2376453779130817, accuracy: 0.99\n",
      "iteration no 6973: Loss: 0.23764450225577444, accuracy: 0.99\n",
      "iteration no 6974: Loss: 0.237643345425392, accuracy: 0.99\n",
      "iteration no 6975: Loss: 0.23764410393140944, accuracy: 0.99\n",
      "iteration no 6976: Loss: 0.2376425259083783, accuracy: 0.99\n",
      "iteration no 6977: Loss: 0.23764195086159717, accuracy: 0.99\n",
      "iteration no 6978: Loss: 0.2376418955847117, accuracy: 0.99\n",
      "iteration no 6979: Loss: 0.23764105671880476, accuracy: 0.99\n",
      "iteration no 6980: Loss: 0.23764032251782635, accuracy: 0.99\n",
      "iteration no 6981: Loss: 0.23763918288783684, accuracy: 0.99\n",
      "iteration no 6982: Loss: 0.23763977272653924, accuracy: 0.99\n",
      "iteration no 6983: Loss: 0.23763822893780245, accuracy: 0.99\n",
      "iteration no 6984: Loss: 0.23763711686629807, accuracy: 0.99\n",
      "iteration no 6985: Loss: 0.23763762202079927, accuracy: 0.99\n",
      "iteration no 6986: Loss: 0.23763645295909364, accuracy: 0.99\n",
      "iteration no 6987: Loss: 0.23763615006468647, accuracy: 0.99\n",
      "iteration no 6988: Loss: 0.23763478843881855, accuracy: 0.99\n",
      "iteration no 6989: Loss: 0.23763547921927497, accuracy: 0.99\n",
      "iteration no 6990: Loss: 0.23763388506815522, accuracy: 0.99\n",
      "iteration no 6991: Loss: 0.23763282019121357, accuracy: 0.99\n",
      "iteration no 6992: Loss: 0.2376335929198175, accuracy: 0.99\n",
      "iteration no 6993: Loss: 0.23763199379184347, accuracy: 0.99\n",
      "iteration no 6994: Loss: 0.23763174957348382, accuracy: 0.99\n",
      "iteration no 6995: Loss: 0.23763072840434696, accuracy: 0.99\n",
      "iteration no 6996: Loss: 0.2376308969710421, accuracy: 0.99\n",
      "iteration no 6997: Loss: 0.23762956042505484, accuracy: 0.99\n",
      "iteration no 6998: Loss: 0.23762856665326992, accuracy: 0.99\n",
      "iteration no 6999: Loss: 0.2376291257159273, accuracy: 0.99\n",
      "iteration no 7000: Loss: 0.2376274877379565, accuracy: 0.99\n",
      "iteration no 7001: Loss: 0.23762719151578388, accuracy: 0.99\n",
      "iteration no 7002: Loss: 0.23762685131787153, accuracy: 0.99\n",
      "iteration no 7003: Loss: 0.23762648416584686, accuracy: 0.99\n",
      "iteration no 7004: Loss: 0.23762526859869426, accuracy: 0.99\n",
      "iteration no 7005: Loss: 0.23762428080461673, accuracy: 0.99\n",
      "iteration no 7006: Loss: 0.23762498314689423, accuracy: 0.99\n",
      "iteration no 7007: Loss: 0.23762330157608047, accuracy: 0.99\n",
      "iteration no 7008: Loss: 0.2376229225646852, accuracy: 0.99\n",
      "iteration no 7009: Loss: 0.23762260301195948, accuracy: 0.99\n",
      "iteration no 7010: Loss: 0.23762203689222977, accuracy: 0.99\n",
      "iteration no 7011: Loss: 0.23762080944650027, accuracy: 0.99\n",
      "iteration no 7012: Loss: 0.23762036450290885, accuracy: 0.99\n",
      "iteration no 7013: Loss: 0.237620844838845, accuracy: 0.99\n",
      "iteration no 7014: Loss: 0.23761895040506864, accuracy: 0.99\n",
      "iteration no 7015: Loss: 0.23761859834516236, accuracy: 0.99\n",
      "iteration no 7016: Loss: 0.2376186665887785, accuracy: 0.99\n",
      "iteration no 7017: Loss: 0.237617572215636, accuracy: 0.99\n",
      "iteration no 7018: Loss: 0.23761679019784643, accuracy: 0.99\n",
      "iteration no 7019: Loss: 0.23761622916582778, accuracy: 0.99\n",
      "iteration no 7020: Loss: 0.23761663340681938, accuracy: 0.99\n",
      "iteration no 7021: Loss: 0.23761458288034348, accuracy: 0.99\n",
      "iteration no 7022: Loss: 0.2376145863546492, accuracy: 0.99\n",
      "iteration no 7023: Loss: 0.23761444701134707, accuracy: 0.99\n",
      "iteration no 7024: Loss: 0.23761317514537755, accuracy: 0.99\n",
      "iteration no 7025: Loss: 0.23761238807042598, accuracy: 0.99\n",
      "iteration no 7026: Loss: 0.23761232710356697, accuracy: 0.99\n",
      "iteration no 7027: Loss: 0.23761205141229497, accuracy: 0.99\n",
      "iteration no 7028: Loss: 0.23761040083099905, accuracy: 0.99\n",
      "iteration no 7029: Loss: 0.237610800531668, accuracy: 0.99\n",
      "iteration no 7030: Loss: 0.2376100536435919, accuracy: 0.99\n",
      "iteration no 7031: Loss: 0.2376089917186513, accuracy: 0.99\n",
      "iteration no 7032: Loss: 0.23760832991279796, accuracy: 0.99\n",
      "iteration no 7033: Loss: 0.23760823723422625, accuracy: 0.99\n",
      "iteration no 7034: Loss: 0.23760799012755518, accuracy: 0.99\n",
      "iteration no 7035: Loss: 0.2376061601949352, accuracy: 0.99\n",
      "iteration no 7036: Loss: 0.23760660082794496, accuracy: 0.99\n",
      "iteration no 7037: Loss: 0.23760581189712968, accuracy: 0.99\n",
      "iteration no 7038: Loss: 0.23760455865020425, accuracy: 0.99\n",
      "iteration no 7039: Loss: 0.23760453690237415, accuracy: 0.99\n",
      "iteration no 7040: Loss: 0.2376039218798436, accuracy: 0.99\n",
      "iteration no 7041: Loss: 0.2376035302055012, accuracy: 0.99\n",
      "iteration no 7042: Loss: 0.23760162070617052, accuracy: 0.99\n",
      "iteration no 7043: Loss: 0.23760298942472907, accuracy: 0.99\n",
      "iteration no 7044: Loss: 0.23760152417742317, accuracy: 0.99\n",
      "iteration no 7045: Loss: 0.23760056479498565, accuracy: 0.99\n",
      "iteration no 7046: Loss: 0.23760064751539178, accuracy: 0.99\n",
      "iteration no 7047: Loss: 0.23759974883251603, accuracy: 0.99\n",
      "iteration no 7048: Loss: 0.23759916309418502, accuracy: 0.99\n",
      "iteration no 7049: Loss: 0.2375981086403401, accuracy: 0.99\n",
      "iteration no 7050: Loss: 0.2375986072644675, accuracy: 0.99\n",
      "iteration no 7051: Loss: 0.23759729477158728, accuracy: 0.99\n",
      "iteration no 7052: Loss: 0.23759615090846653, accuracy: 0.99\n",
      "iteration no 7053: Loss: 0.2375968216253711, accuracy: 0.99\n",
      "iteration no 7054: Loss: 0.23759547412261434, accuracy: 0.99\n",
      "iteration no 7055: Loss: 0.23759503996313297, accuracy: 0.99\n",
      "iteration no 7056: Loss: 0.2375941549995597, accuracy: 0.99\n",
      "iteration no 7057: Loss: 0.23759457448422888, accuracy: 0.99\n",
      "iteration no 7058: Loss: 0.23759308014611857, accuracy: 0.99\n",
      "iteration no 7059: Loss: 0.23759240587189684, accuracy: 0.99\n",
      "iteration no 7060: Loss: 0.23759282707700274, accuracy: 0.99\n",
      "iteration no 7061: Loss: 0.23759123229998952, accuracy: 0.99\n",
      "iteration no 7062: Loss: 0.23759075169097124, accuracy: 0.99\n",
      "iteration no 7063: Loss: 0.23759051927705466, accuracy: 0.99\n",
      "iteration no 7064: Loss: 0.2375903516997098, accuracy: 0.99\n",
      "iteration no 7065: Loss: 0.23758876789734518, accuracy: 0.99\n",
      "iteration no 7066: Loss: 0.2375887186980471, accuracy: 0.99\n",
      "iteration no 7067: Loss: 0.2375885660283021, accuracy: 0.99\n",
      "iteration no 7068: Loss: 0.23758698316165883, accuracy: 0.99\n",
      "iteration no 7069: Loss: 0.2375867886104553, accuracy: 0.99\n",
      "iteration no 7070: Loss: 0.23758649545764385, accuracy: 0.99\n",
      "iteration no 7071: Loss: 0.23758598881415827, accuracy: 0.99\n",
      "iteration no 7072: Loss: 0.2375848232501156, accuracy: 0.99\n",
      "iteration no 7073: Loss: 0.23758528046911692, accuracy: 0.99\n",
      "iteration no 7074: Loss: 0.2375838712057846, accuracy: 0.99\n",
      "iteration no 7075: Loss: 0.23758318462448955, accuracy: 0.99\n",
      "iteration no 7076: Loss: 0.2375835586752108, accuracy: 0.99\n",
      "iteration no 7077: Loss: 0.2375818582667561, accuracy: 0.99\n",
      "iteration no 7078: Loss: 0.23758217393170147, accuracy: 0.99\n",
      "iteration no 7079: Loss: 0.23758157203715163, accuracy: 0.99\n",
      "iteration no 7080: Loss: 0.2375803769022347, accuracy: 0.99\n",
      "iteration no 7081: Loss: 0.237580130411309, accuracy: 0.99\n",
      "iteration no 7082: Loss: 0.23757982903976232, accuracy: 0.99\n",
      "iteration no 7083: Loss: 0.23757909180527387, accuracy: 0.99\n",
      "iteration no 7084: Loss: 0.23757821211040006, accuracy: 0.99\n",
      "iteration no 7085: Loss: 0.23757871679152254, accuracy: 0.99\n",
      "iteration no 7086: Loss: 0.23757651415168535, accuracy: 0.99\n",
      "iteration no 7087: Loss: 0.23757699842909796, accuracy: 0.99\n",
      "iteration no 7088: Loss: 0.2375764867676351, accuracy: 0.99\n",
      "iteration no 7089: Loss: 0.23757515606409957, accuracy: 0.99\n",
      "iteration no 7090: Loss: 0.23757557288339515, accuracy: 0.99\n",
      "iteration no 7091: Loss: 0.23757476422757026, accuracy: 0.99\n",
      "iteration no 7092: Loss: 0.23757379023220768, accuracy: 0.99\n",
      "iteration no 7093: Loss: 0.23757330227034873, accuracy: 0.99\n",
      "iteration no 7094: Loss: 0.23757352208810872, accuracy: 0.99\n",
      "iteration no 7095: Loss: 0.23757175021166993, accuracy: 0.99\n",
      "iteration no 7096: Loss: 0.2375720178323013, accuracy: 0.99\n",
      "iteration no 7097: Loss: 0.23757177568404703, accuracy: 0.99\n",
      "iteration no 7098: Loss: 0.23756978234532589, accuracy: 0.99\n",
      "iteration no 7099: Loss: 0.23757108466554622, accuracy: 0.99\n",
      "iteration no 7100: Loss: 0.23756939811192923, accuracy: 0.99\n",
      "iteration no 7101: Loss: 0.23756896970501903, accuracy: 0.99\n",
      "iteration no 7102: Loss: 0.23756909267376308, accuracy: 0.99\n",
      "iteration no 7103: Loss: 0.2375678415872656, accuracy: 0.99\n",
      "iteration no 7104: Loss: 0.2375675585722093, accuracy: 0.99\n",
      "iteration no 7105: Loss: 0.2375671047863594, accuracy: 0.99\n",
      "iteration no 7106: Loss: 0.23756597291389647, accuracy: 0.99\n",
      "iteration no 7107: Loss: 0.23756618414817532, accuracy: 0.99\n",
      "iteration no 7108: Loss: 0.2375658609473391, accuracy: 0.99\n",
      "iteration no 7109: Loss: 0.23756412934840676, accuracy: 0.99\n",
      "iteration no 7110: Loss: 0.2375650595330463, accuracy: 0.99\n",
      "iteration no 7111: Loss: 0.2375632445063547, accuracy: 0.99\n",
      "iteration no 7112: Loss: 0.23756363118123247, accuracy: 0.99\n",
      "iteration no 7113: Loss: 0.23756307487029515, accuracy: 0.99\n",
      "iteration no 7114: Loss: 0.237561317237436, accuracy: 0.99\n",
      "iteration no 7115: Loss: 0.23756276542401605, accuracy: 0.99\n",
      "iteration no 7116: Loss: 0.23756037207656353, accuracy: 0.99\n",
      "iteration no 7117: Loss: 0.23756111581915165, accuracy: 0.99\n",
      "iteration no 7118: Loss: 0.2375601988500194, accuracy: 0.99\n",
      "iteration no 7119: Loss: 0.237558822221725, accuracy: 0.99\n",
      "iteration no 7120: Loss: 0.23755975474705893, accuracy: 0.99\n",
      "iteration no 7121: Loss: 0.2375583553088822, accuracy: 0.99\n",
      "iteration no 7122: Loss: 0.2375579256653106, accuracy: 0.99\n",
      "iteration no 7123: Loss: 0.23755746359291385, accuracy: 0.99\n",
      "iteration no 7124: Loss: 0.23755681221941177, accuracy: 0.99\n",
      "iteration no 7125: Loss: 0.23755644407633233, accuracy: 0.99\n",
      "iteration no 7126: Loss: 0.23755603262453695, accuracy: 0.99\n",
      "iteration no 7127: Loss: 0.23755522179503302, accuracy: 0.99\n",
      "iteration no 7128: Loss: 0.23755496387361585, accuracy: 0.99\n",
      "iteration no 7129: Loss: 0.23755404106058625, accuracy: 0.99\n",
      "iteration no 7130: Loss: 0.23755428371830195, accuracy: 0.99\n",
      "iteration no 7131: Loss: 0.23755322762405107, accuracy: 0.99\n",
      "iteration no 7132: Loss: 0.23755265297876876, accuracy: 0.99\n",
      "iteration no 7133: Loss: 0.2375526195700476, accuracy: 0.99\n",
      "iteration no 7134: Loss: 0.23755112042847637, accuracy: 0.99\n",
      "iteration no 7135: Loss: 0.23755173009945102, accuracy: 0.99\n",
      "iteration no 7136: Loss: 0.23755021974449775, accuracy: 0.99\n",
      "iteration no 7137: Loss: 0.23755036271291363, accuracy: 0.99\n",
      "iteration no 7138: Loss: 0.23754972374869887, accuracy: 0.99\n",
      "iteration no 7139: Loss: 0.2375486922588072, accuracy: 0.99\n",
      "iteration no 7140: Loss: 0.2375491133529055, accuracy: 0.99\n",
      "iteration no 7141: Loss: 0.23754704394579468, accuracy: 0.99\n",
      "iteration no 7142: Loss: 0.23754862104242114, accuracy: 0.99\n",
      "iteration no 7143: Loss: 0.2375465352036264, accuracy: 0.99\n",
      "iteration no 7144: Loss: 0.23754680689717417, accuracy: 0.99\n",
      "iteration no 7145: Loss: 0.23754616563901376, accuracy: 0.99\n",
      "iteration no 7146: Loss: 0.23754491494467012, accuracy: 0.99\n",
      "iteration no 7147: Loss: 0.2375457862413342, accuracy: 0.99\n",
      "iteration no 7148: Loss: 0.23754356082237466, accuracy: 0.99\n",
      "iteration no 7149: Loss: 0.23754491489536572, accuracy: 0.99\n",
      "iteration no 7150: Loss: 0.23754355407404767, accuracy: 0.99\n",
      "iteration no 7151: Loss: 0.2375423689094395, accuracy: 0.99\n",
      "iteration no 7152: Loss: 0.2375431224169512, accuracy: 0.99\n",
      "iteration no 7153: Loss: 0.23754132855500462, accuracy: 0.99\n",
      "iteration no 7154: Loss: 0.23754192993088818, accuracy: 0.99\n",
      "iteration no 7155: Loss: 0.23754073868838982, accuracy: 0.99\n",
      "iteration no 7156: Loss: 0.23754058013494594, accuracy: 0.99\n",
      "iteration no 7157: Loss: 0.23753987037783036, accuracy: 0.99\n",
      "iteration no 7158: Loss: 0.23753913253517675, accuracy: 0.99\n",
      "iteration no 7159: Loss: 0.23753923069287936, accuracy: 0.99\n",
      "iteration no 7160: Loss: 0.23753863056589541, accuracy: 0.99\n",
      "iteration no 7161: Loss: 0.237537374689928, accuracy: 0.99\n",
      "iteration no 7162: Loss: 0.23753791015721198, accuracy: 0.99\n",
      "iteration no 7163: Loss: 0.23753659214606587, accuracy: 0.99\n",
      "iteration no 7164: Loss: 0.23753637423862733, accuracy: 0.99\n",
      "iteration no 7165: Loss: 0.23753643908126854, accuracy: 0.99\n",
      "iteration no 7166: Loss: 0.23753425065090766, accuracy: 0.99\n",
      "iteration no 7167: Loss: 0.23753600376952827, accuracy: 0.99\n",
      "iteration no 7168: Loss: 0.23753447602249772, accuracy: 0.99\n",
      "iteration no 7169: Loss: 0.23753359139647803, accuracy: 0.99\n",
      "iteration no 7170: Loss: 0.23753381062632273, accuracy: 0.99\n",
      "iteration no 7171: Loss: 0.2375321828184426, accuracy: 0.99\n",
      "iteration no 7172: Loss: 0.23753305840197908, accuracy: 0.99\n",
      "iteration no 7173: Loss: 0.2375316385950743, accuracy: 0.99\n",
      "iteration no 7174: Loss: 0.2375313508184393, accuracy: 0.99\n",
      "iteration no 7175: Loss: 0.23753121512562908, accuracy: 0.99\n",
      "iteration no 7176: Loss: 0.23753021150145748, accuracy: 0.99\n",
      "iteration no 7177: Loss: 0.23753018274772053, accuracy: 0.99\n",
      "iteration no 7178: Loss: 0.2375296818233158, accuracy: 0.99\n",
      "iteration no 7179: Loss: 0.23752824679639684, accuracy: 0.99\n",
      "iteration no 7180: Loss: 0.23752907060526351, accuracy: 0.99\n",
      "iteration no 7181: Loss: 0.23752834049807448, accuracy: 0.99\n",
      "iteration no 7182: Loss: 0.237526568219819, accuracy: 0.99\n",
      "iteration no 7183: Loss: 0.23752775368089707, accuracy: 0.99\n",
      "iteration no 7184: Loss: 0.23752550530265482, accuracy: 0.99\n",
      "iteration no 7185: Loss: 0.237526724589196, accuracy: 0.99\n",
      "iteration no 7186: Loss: 0.23752541011456693, accuracy: 0.99\n",
      "iteration no 7187: Loss: 0.2375243501457373, accuracy: 0.99\n",
      "iteration no 7188: Loss: 0.23752519501000113, accuracy: 0.99\n",
      "iteration no 7189: Loss: 0.23752339740314166, accuracy: 0.99\n",
      "iteration no 7190: Loss: 0.23752393595497137, accuracy: 0.99\n",
      "iteration no 7191: Loss: 0.23752297357093374, accuracy: 0.99\n",
      "iteration no 7192: Loss: 0.23752213186932664, accuracy: 0.99\n",
      "iteration no 7193: Loss: 0.23752247466143755, accuracy: 0.99\n",
      "iteration no 7194: Loss: 0.23752211477810864, accuracy: 0.99\n",
      "iteration no 7195: Loss: 0.237520439293136, accuracy: 0.99\n",
      "iteration no 7196: Loss: 0.23752138341457812, accuracy: 0.99\n",
      "iteration no 7197: Loss: 0.23751998508514702, accuracy: 0.99\n",
      "iteration no 7198: Loss: 0.237519459183037, accuracy: 0.99\n",
      "iteration no 7199: Loss: 0.23751981794748178, accuracy: 0.99\n",
      "iteration no 7200: Loss: 0.23751789435491671, accuracy: 0.99\n",
      "iteration no 7201: Loss: 0.23751904147932623, accuracy: 0.99\n",
      "iteration no 7202: Loss: 0.23751754775710257, accuracy: 0.99\n",
      "iteration no 7203: Loss: 0.23751671200532726, accuracy: 0.99\n",
      "iteration no 7204: Loss: 0.23751676292935625, accuracy: 0.99\n",
      "iteration no 7205: Loss: 0.23751642287870073, accuracy: 0.99\n",
      "iteration no 7206: Loss: 0.23751508170050137, accuracy: 0.99\n",
      "iteration no 7207: Loss: 0.23751527057698932, accuracy: 0.99\n",
      "iteration no 7208: Loss: 0.23751538075793566, accuracy: 0.99\n",
      "iteration no 7209: Loss: 0.2375133555580026, accuracy: 0.99\n",
      "iteration no 7210: Loss: 0.23751469941333053, accuracy: 0.99\n",
      "iteration no 7211: Loss: 0.23751291064676694, accuracy: 0.99\n",
      "iteration no 7212: Loss: 0.23751247063598915, accuracy: 0.99\n",
      "iteration no 7213: Loss: 0.23751300361491284, accuracy: 0.99\n",
      "iteration no 7214: Loss: 0.2375119719076187, accuracy: 0.99\n",
      "iteration no 7215: Loss: 0.2375112165836477, accuracy: 0.99\n",
      "iteration no 7216: Loss: 0.2375114736983235, accuracy: 0.99\n",
      "iteration no 7217: Loss: 0.23751009319213018, accuracy: 0.99\n",
      "iteration no 7218: Loss: 0.23751000610107564, accuracy: 0.99\n",
      "iteration no 7219: Loss: 0.23751008246676727, accuracy: 0.99\n",
      "iteration no 7220: Loss: 0.23750817700312296, accuracy: 0.99\n",
      "iteration no 7221: Loss: 0.23750920418127103, accuracy: 0.99\n",
      "iteration no 7222: Loss: 0.23750831034628872, accuracy: 0.99\n",
      "iteration no 7223: Loss: 0.23750737659012705, accuracy: 0.99\n",
      "iteration no 7224: Loss: 0.23750699809738582, accuracy: 0.99\n",
      "iteration no 7225: Loss: 0.2375071925186759, accuracy: 0.99\n",
      "iteration no 7226: Loss: 0.2375058367275425, accuracy: 0.99\n",
      "iteration no 7227: Loss: 0.23750534519562697, accuracy: 0.99\n",
      "iteration no 7228: Loss: 0.2375062931256068, accuracy: 0.99\n",
      "iteration no 7229: Loss: 0.23750403016555294, accuracy: 0.99\n",
      "iteration no 7230: Loss: 0.23750441579105347, accuracy: 0.99\n",
      "iteration no 7231: Loss: 0.237504153985761, accuracy: 0.99\n",
      "iteration no 7232: Loss: 0.23750315379658166, accuracy: 0.99\n",
      "iteration no 7233: Loss: 0.23750250253082028, accuracy: 0.99\n",
      "iteration no 7234: Loss: 0.2375031765638252, accuracy: 0.99\n",
      "iteration no 7235: Loss: 0.23750181778507246, accuracy: 0.99\n",
      "iteration no 7236: Loss: 0.23750094361067514, accuracy: 0.99\n",
      "iteration no 7237: Loss: 0.23750192882689355, accuracy: 0.99\n",
      "iteration no 7238: Loss: 0.2374999284159167, accuracy: 0.99\n",
      "iteration no 7239: Loss: 0.23749986034963533, accuracy: 0.99\n",
      "iteration no 7240: Loss: 0.23749990943337435, accuracy: 0.99\n",
      "iteration no 7241: Loss: 0.23749941734720806, accuracy: 0.99\n",
      "iteration no 7242: Loss: 0.2374979682649389, accuracy: 0.99\n",
      "iteration no 7243: Loss: 0.23749936975021332, accuracy: 0.99\n",
      "iteration no 7244: Loss: 0.23749755886310486, accuracy: 0.99\n",
      "iteration no 7245: Loss: 0.2374967191930175, accuracy: 0.99\n",
      "iteration no 7246: Loss: 0.23749737521335657, accuracy: 0.99\n",
      "iteration no 7247: Loss: 0.23749647645382155, accuracy: 0.99\n",
      "iteration no 7248: Loss: 0.23749582016684156, accuracy: 0.99\n",
      "iteration no 7249: Loss: 0.23749551595670543, accuracy: 0.99\n",
      "iteration no 7250: Loss: 0.23749572304242977, accuracy: 0.99\n",
      "iteration no 7251: Loss: 0.23749374265758605, accuracy: 0.99\n",
      "iteration no 7252: Loss: 0.2374946216477299, accuracy: 0.99\n",
      "iteration no 7253: Loss: 0.23749381803577552, accuracy: 0.99\n",
      "iteration no 7254: Loss: 0.23749303812852812, accuracy: 0.99\n",
      "iteration no 7255: Loss: 0.23749280507363735, accuracy: 0.99\n",
      "iteration no 7256: Loss: 0.2374927343271126, accuracy: 0.99\n",
      "iteration no 7257: Loss: 0.23749184746630295, accuracy: 0.99\n",
      "iteration no 7258: Loss: 0.23749102623216134, accuracy: 0.99\n",
      "iteration no 7259: Loss: 0.2374917341762667, accuracy: 0.99\n",
      "iteration no 7260: Loss: 0.23748978894723888, accuracy: 0.99\n",
      "iteration no 7261: Loss: 0.23749005455631317, accuracy: 0.99\n",
      "iteration no 7262: Loss: 0.23749002507177913, accuracy: 0.99\n",
      "iteration no 7263: Loss: 0.23748911506274017, accuracy: 0.99\n",
      "iteration no 7264: Loss: 0.2374881846171653, accuracy: 0.99\n",
      "iteration no 7265: Loss: 0.23748888426464138, accuracy: 0.99\n",
      "iteration no 7266: Loss: 0.23748766229449933, accuracy: 0.99\n",
      "iteration no 7267: Loss: 0.2374865553960147, accuracy: 0.99\n",
      "iteration no 7268: Loss: 0.23748798423034873, accuracy: 0.99\n",
      "iteration no 7269: Loss: 0.23748598207394156, accuracy: 0.99\n",
      "iteration no 7270: Loss: 0.2374858814921359, accuracy: 0.99\n",
      "iteration no 7271: Loss: 0.2374857267734194, accuracy: 0.99\n",
      "iteration no 7272: Loss: 0.23748560534861812, accuracy: 0.99\n",
      "iteration no 7273: Loss: 0.23748391384023984, accuracy: 0.99\n",
      "iteration no 7274: Loss: 0.2374846895688934, accuracy: 0.99\n",
      "iteration no 7275: Loss: 0.23748392636120402, accuracy: 0.99\n",
      "iteration no 7276: Loss: 0.2374829604744231, accuracy: 0.99\n",
      "iteration no 7277: Loss: 0.23748320104940301, accuracy: 0.99\n",
      "iteration no 7278: Loss: 0.23748252902188116, accuracy: 0.99\n",
      "iteration no 7279: Loss: 0.23748194699368474, accuracy: 0.99\n",
      "iteration no 7280: Loss: 0.23748125319290897, accuracy: 0.99\n",
      "iteration no 7281: Loss: 0.23748161841100396, accuracy: 0.99\n",
      "iteration no 7282: Loss: 0.23748001900226376, accuracy: 0.99\n",
      "iteration no 7283: Loss: 0.2374805725939923, accuracy: 0.99\n",
      "iteration no 7284: Loss: 0.23747989389222696, accuracy: 0.99\n",
      "iteration no 7285: Loss: 0.23747932363462895, accuracy: 0.99\n",
      "iteration no 7286: Loss: 0.23747843346048886, accuracy: 0.99\n",
      "iteration no 7287: Loss: 0.2374789779990439, accuracy: 0.99\n",
      "iteration no 7288: Loss: 0.237477699667585, accuracy: 0.99\n",
      "iteration no 7289: Loss: 0.23747702713417565, accuracy: 0.99\n",
      "iteration no 7290: Loss: 0.2374777161279414, accuracy: 0.99\n",
      "iteration no 7291: Loss: 0.23747629354754754, accuracy: 0.99\n",
      "iteration no 7292: Loss: 0.23747600983326406, accuracy: 0.99\n",
      "iteration no 7293: Loss: 0.23747597101702983, accuracy: 0.99\n",
      "iteration no 7294: Loss: 0.2374755012964424, accuracy: 0.99\n",
      "iteration no 7295: Loss: 0.23747406601084653, accuracy: 0.99\n",
      "iteration no 7296: Loss: 0.23747530911861767, accuracy: 0.99\n",
      "iteration no 7297: Loss: 0.23747370768753392, accuracy: 0.99\n",
      "iteration no 7298: Loss: 0.23747336061030322, accuracy: 0.99\n",
      "iteration no 7299: Loss: 0.23747362288976032, accuracy: 0.99\n",
      "iteration no 7300: Loss: 0.23747285678002758, accuracy: 0.99\n",
      "iteration no 7301: Loss: 0.23747230436468109, accuracy: 0.99\n",
      "iteration no 7302: Loss: 0.2374712148420729, accuracy: 0.99\n",
      "iteration no 7303: Loss: 0.2374721569002782, accuracy: 0.99\n",
      "iteration no 7304: Loss: 0.23747063127914386, accuracy: 0.99\n",
      "iteration no 7305: Loss: 0.23747051570824473, accuracy: 0.99\n",
      "iteration no 7306: Loss: 0.23747011837061438, accuracy: 0.99\n",
      "iteration no 7307: Loss: 0.2374703566137642, accuracy: 0.99\n",
      "iteration no 7308: Loss: 0.23746897616054205, accuracy: 0.99\n",
      "iteration no 7309: Loss: 0.23746828238897044, accuracy: 0.99\n",
      "iteration no 7310: Loss: 0.237468867538509, accuracy: 0.99\n",
      "iteration no 7311: Loss: 0.2374679723740759, accuracy: 0.99\n",
      "iteration no 7312: Loss: 0.23746736234468116, accuracy: 0.99\n",
      "iteration no 7313: Loss: 0.23746676936013805, accuracy: 0.99\n",
      "iteration no 7314: Loss: 0.23746756382431047, accuracy: 0.99\n",
      "iteration no 7315: Loss: 0.23746576002603548, accuracy: 0.99\n",
      "iteration no 7316: Loss: 0.23746595660843406, accuracy: 0.99\n",
      "iteration no 7317: Loss: 0.23746517306191944, accuracy: 0.99\n",
      "iteration no 7318: Loss: 0.23746545390819868, accuracy: 0.99\n",
      "iteration no 7319: Loss: 0.23746422999471103, accuracy: 0.99\n",
      "iteration no 7320: Loss: 0.2374636543743941, accuracy: 0.99\n",
      "iteration no 7321: Loss: 0.23746439811082654, accuracy: 0.99\n",
      "iteration no 7322: Loss: 0.237463499960318, accuracy: 0.99\n",
      "iteration no 7323: Loss: 0.23746265183837625, accuracy: 0.99\n",
      "iteration no 7324: Loss: 0.2374620554942959, accuracy: 0.99\n",
      "iteration no 7325: Loss: 0.23746261616101463, accuracy: 0.99\n",
      "iteration no 7326: Loss: 0.23746102883918224, accuracy: 0.99\n",
      "iteration no 7327: Loss: 0.23746097060563354, accuracy: 0.99\n",
      "iteration no 7328: Loss: 0.23746073781148558, accuracy: 0.99\n",
      "iteration no 7329: Loss: 0.2374608610556485, accuracy: 0.99\n",
      "iteration no 7330: Loss: 0.23745943108228867, accuracy: 0.99\n",
      "iteration no 7331: Loss: 0.23745939477573558, accuracy: 0.99\n",
      "iteration no 7332: Loss: 0.23745926723029093, accuracy: 0.99\n",
      "iteration no 7333: Loss: 0.23745852828092895, accuracy: 0.99\n",
      "iteration no 7334: Loss: 0.23745742927104196, accuracy: 0.99\n",
      "iteration no 7335: Loss: 0.23745814044601865, accuracy: 0.99\n",
      "iteration no 7336: Loss: 0.23745779874800377, accuracy: 0.99\n",
      "iteration no 7337: Loss: 0.23745630940511, accuracy: 0.99\n",
      "iteration no 7338: Loss: 0.23745658976298942, accuracy: 0.99\n",
      "iteration no 7339: Loss: 0.23745602631661822, accuracy: 0.99\n",
      "iteration no 7340: Loss: 0.23745574161360533, accuracy: 0.99\n",
      "iteration no 7341: Loss: 0.23745426734028802, accuracy: 0.99\n",
      "iteration no 7342: Loss: 0.23745546701866876, accuracy: 0.99\n",
      "iteration no 7343: Loss: 0.23745425506652545, accuracy: 0.99\n",
      "iteration no 7344: Loss: 0.23745390425915017, accuracy: 0.99\n",
      "iteration no 7345: Loss: 0.23745316730924226, accuracy: 0.99\n",
      "iteration no 7346: Loss: 0.23745348430743665, accuracy: 0.99\n",
      "iteration no 7347: Loss: 0.23745251801806525, accuracy: 0.99\n",
      "iteration no 7348: Loss: 0.23745137712411438, accuracy: 0.99\n",
      "iteration no 7349: Loss: 0.23745241016555232, accuracy: 0.99\n",
      "iteration no 7350: Loss: 0.23745138084783377, accuracy: 0.99\n",
      "iteration no 7351: Loss: 0.23745090221250714, accuracy: 0.99\n",
      "iteration no 7352: Loss: 0.2374501545812439, accuracy: 0.99\n",
      "iteration no 7353: Loss: 0.2374506966354663, accuracy: 0.99\n",
      "iteration no 7354: Loss: 0.23744907383523148, accuracy: 0.99\n",
      "iteration no 7355: Loss: 0.23744893143045284, accuracy: 0.99\n",
      "iteration no 7356: Loss: 0.23744902376610763, accuracy: 0.99\n",
      "iteration no 7357: Loss: 0.23744854192019604, accuracy: 0.99\n",
      "iteration no 7358: Loss: 0.23744747880306943, accuracy: 0.99\n",
      "iteration no 7359: Loss: 0.23744779061816496, accuracy: 0.99\n",
      "iteration no 7360: Loss: 0.23744741455654161, accuracy: 0.99\n",
      "iteration no 7361: Loss: 0.23744648766854592, accuracy: 0.99\n",
      "iteration no 7362: Loss: 0.23744592264333075, accuracy: 0.99\n",
      "iteration no 7363: Loss: 0.23744617693507009, accuracy: 0.99\n",
      "iteration no 7364: Loss: 0.23744559004578195, accuracy: 0.99\n",
      "iteration no 7365: Loss: 0.2374440985027496, accuracy: 0.99\n",
      "iteration no 7366: Loss: 0.23744540708222506, accuracy: 0.99\n",
      "iteration no 7367: Loss: 0.2374441042776907, accuracy: 0.99\n",
      "iteration no 7368: Loss: 0.23744379702099133, accuracy: 0.99\n",
      "iteration no 7369: Loss: 0.2374428367299234, accuracy: 0.99\n",
      "iteration no 7370: Loss: 0.23744369682269967, accuracy: 0.99\n",
      "iteration no 7371: Loss: 0.23744203831811214, accuracy: 0.99\n",
      "iteration no 7372: Loss: 0.23744179257609987, accuracy: 0.99\n",
      "iteration no 7373: Loss: 0.2374420904789547, accuracy: 0.99\n",
      "iteration no 7374: Loss: 0.23744132686469405, accuracy: 0.99\n",
      "iteration no 7375: Loss: 0.23744068682055597, accuracy: 0.99\n",
      "iteration no 7376: Loss: 0.23744021637860652, accuracy: 0.99\n",
      "iteration no 7377: Loss: 0.23744061194248178, accuracy: 0.99\n",
      "iteration no 7378: Loss: 0.23743925490195247, accuracy: 0.99\n",
      "iteration no 7379: Loss: 0.23743875739043524, accuracy: 0.99\n",
      "iteration no 7380: Loss: 0.23743912738634787, accuracy: 0.99\n",
      "iteration no 7381: Loss: 0.23743885849742175, accuracy: 0.99\n",
      "iteration no 7382: Loss: 0.2374370880894931, accuracy: 0.99\n",
      "iteration no 7383: Loss: 0.23743819555610046, accuracy: 0.99\n",
      "iteration no 7384: Loss: 0.23743717839422993, accuracy: 0.99\n",
      "iteration no 7385: Loss: 0.2374367748356327, accuracy: 0.99\n",
      "iteration no 7386: Loss: 0.23743576719223525, accuracy: 0.99\n",
      "iteration no 7387: Loss: 0.2374364899153632, accuracy: 0.99\n",
      "iteration no 7388: Loss: 0.23743554525384303, accuracy: 0.99\n",
      "iteration no 7389: Loss: 0.2374345740353726, accuracy: 0.99\n",
      "iteration no 7390: Loss: 0.23743508599504137, accuracy: 0.99\n",
      "iteration no 7391: Loss: 0.23743452272423105, accuracy: 0.99\n",
      "iteration no 7392: Loss: 0.2374337428030652, accuracy: 0.99\n",
      "iteration no 7393: Loss: 0.23743294961664402, accuracy: 0.99\n",
      "iteration no 7394: Loss: 0.237433810565598, accuracy: 0.99\n",
      "iteration no 7395: Loss: 0.2374320365605178, accuracy: 0.99\n",
      "iteration no 7396: Loss: 0.237432199620833, accuracy: 0.99\n",
      "iteration no 7397: Loss: 0.23743199618643032, accuracy: 0.99\n",
      "iteration no 7398: Loss: 0.2374319578006373, accuracy: 0.99\n",
      "iteration no 7399: Loss: 0.23743082248507824, accuracy: 0.99\n",
      "iteration no 7400: Loss: 0.23743085679259168, accuracy: 0.99\n",
      "iteration no 7401: Loss: 0.23743053875412523, accuracy: 0.99\n",
      "iteration no 7402: Loss: 0.23742985553085477, accuracy: 0.99\n",
      "iteration no 7403: Loss: 0.23742892036117905, accuracy: 0.99\n",
      "iteration no 7404: Loss: 0.23742963610226192, accuracy: 0.99\n",
      "iteration no 7405: Loss: 0.2374289467334401, accuracy: 0.99\n",
      "iteration no 7406: Loss: 0.23742754049874756, accuracy: 0.99\n",
      "iteration no 7407: Loss: 0.2374285382367314, accuracy: 0.99\n",
      "iteration no 7408: Loss: 0.2374275966410505, accuracy: 0.99\n",
      "iteration no 7409: Loss: 0.2374272323290863, accuracy: 0.99\n",
      "iteration no 7410: Loss: 0.23742598081307048, accuracy: 0.99\n",
      "iteration no 7411: Loss: 0.23742703979757707, accuracy: 0.99\n",
      "iteration no 7412: Loss: 0.2374256421748599, accuracy: 0.99\n",
      "iteration no 7413: Loss: 0.23742512658041875, accuracy: 0.99\n",
      "iteration no 7414: Loss: 0.237425320524727, accuracy: 0.99\n",
      "iteration no 7415: Loss: 0.23742509902752185, accuracy: 0.99\n",
      "iteration no 7416: Loss: 0.23742379498488775, accuracy: 0.99\n",
      "iteration no 7417: Loss: 0.23742369169249394, accuracy: 0.99\n",
      "iteration no 7418: Loss: 0.23742397623225553, accuracy: 0.99\n",
      "iteration no 7419: Loss: 0.2374226192368068, accuracy: 0.99\n",
      "iteration no 7420: Loss: 0.23742224685781937, accuracy: 0.99\n",
      "iteration no 7421: Loss: 0.23742261107273038, accuracy: 0.99\n",
      "iteration no 7422: Loss: 0.23742215164319538, accuracy: 0.99\n",
      "iteration no 7423: Loss: 0.23742066229502695, accuracy: 0.99\n",
      "iteration no 7424: Loss: 0.23742169239579958, accuracy: 0.99\n",
      "iteration no 7425: Loss: 0.2374203653617077, accuracy: 0.99\n",
      "iteration no 7426: Loss: 0.23742032841030755, accuracy: 0.99\n",
      "iteration no 7427: Loss: 0.237419297503093, accuracy: 0.99\n",
      "iteration no 7428: Loss: 0.23741992087678115, accuracy: 0.99\n",
      "iteration no 7429: Loss: 0.23741882474720788, accuracy: 0.99\n",
      "iteration no 7430: Loss: 0.23741805710860608, accuracy: 0.99\n",
      "iteration no 7431: Loss: 0.2374188706952614, accuracy: 0.99\n",
      "iteration no 7432: Loss: 0.2374180570218862, accuracy: 0.99\n",
      "iteration no 7433: Loss: 0.23741725030123856, accuracy: 0.99\n",
      "iteration no 7434: Loss: 0.23741675903526382, accuracy: 0.99\n",
      "iteration no 7435: Loss: 0.23741718558801814, accuracy: 0.99\n",
      "iteration no 7436: Loss: 0.23741555431063222, accuracy: 0.99\n",
      "iteration no 7437: Loss: 0.23741568011961464, accuracy: 0.99\n",
      "iteration no 7438: Loss: 0.23741551568868047, accuracy: 0.99\n",
      "iteration no 7439: Loss: 0.2374153616773209, accuracy: 0.99\n",
      "iteration no 7440: Loss: 0.2374139978254709, accuracy: 0.99\n",
      "iteration no 7441: Loss: 0.23741465659646752, accuracy: 0.99\n",
      "iteration no 7442: Loss: 0.23741386504569137, accuracy: 0.99\n",
      "iteration no 7443: Loss: 0.23741329218081353, accuracy: 0.99\n",
      "iteration no 7444: Loss: 0.23741266799027338, accuracy: 0.99\n",
      "iteration no 7445: Loss: 0.2374130395187357, accuracy: 0.99\n",
      "iteration no 7446: Loss: 0.2374123231216706, accuracy: 0.99\n",
      "iteration no 7447: Loss: 0.23741091118085575, accuracy: 0.99\n",
      "iteration no 7448: Loss: 0.23741227370316725, accuracy: 0.99\n",
      "iteration no 7449: Loss: 0.2374108970985967, accuracy: 0.99\n",
      "iteration no 7450: Loss: 0.23741040311631417, accuracy: 0.99\n",
      "iteration no 7451: Loss: 0.23740998319566198, accuracy: 0.99\n",
      "iteration no 7452: Loss: 0.23741062725458148, accuracy: 0.99\n",
      "iteration no 7453: Loss: 0.23740873326322975, accuracy: 0.99\n",
      "iteration no 7454: Loss: 0.23740918261751415, accuracy: 0.99\n",
      "iteration no 7455: Loss: 0.23740892728511598, accuracy: 0.99\n",
      "iteration no 7456: Loss: 0.23740828177573386, accuracy: 0.99\n",
      "iteration no 7457: Loss: 0.23740727754493224, accuracy: 0.99\n",
      "iteration no 7458: Loss: 0.23740786556849727, accuracy: 0.99\n",
      "iteration no 7459: Loss: 0.23740726898006886, accuracy: 0.99\n",
      "iteration no 7460: Loss: 0.23740623125933802, accuracy: 0.99\n",
      "iteration no 7461: Loss: 0.2374063281314572, accuracy: 0.99\n",
      "iteration no 7462: Loss: 0.23740613988047826, accuracy: 0.99\n",
      "iteration no 7463: Loss: 0.23740565706789452, accuracy: 0.99\n",
      "iteration no 7464: Loss: 0.23740426092357947, accuracy: 0.99\n",
      "iteration no 7465: Loss: 0.23740564470961847, accuracy: 0.99\n",
      "iteration no 7466: Loss: 0.2374038383311125, accuracy: 0.99\n",
      "iteration no 7467: Loss: 0.23740376869501828, accuracy: 0.99\n",
      "iteration no 7468: Loss: 0.23740371343659444, accuracy: 0.99\n",
      "iteration no 7469: Loss: 0.2374036188501525, accuracy: 0.99\n",
      "iteration no 7470: Loss: 0.23740230504978618, accuracy: 0.99\n",
      "iteration no 7471: Loss: 0.23740247397267034, accuracy: 0.99\n",
      "iteration no 7472: Loss: 0.23740223339548966, accuracy: 0.99\n",
      "iteration no 7473: Loss: 0.23740143359018145, accuracy: 0.99\n",
      "iteration no 7474: Loss: 0.23740081533187218, accuracy: 0.99\n",
      "iteration no 7475: Loss: 0.2374011155465, accuracy: 0.99\n",
      "iteration no 7476: Loss: 0.23740073695608843, accuracy: 0.99\n",
      "iteration no 7477: Loss: 0.23739932764970967, accuracy: 0.99\n",
      "iteration no 7478: Loss: 0.23740023673770425, accuracy: 0.99\n",
      "iteration no 7479: Loss: 0.23739927783681608, accuracy: 0.99\n",
      "iteration no 7480: Loss: 0.23739891938622165, accuracy: 0.99\n",
      "iteration no 7481: Loss: 0.23739790261849417, accuracy: 0.99\n",
      "iteration no 7482: Loss: 0.23739893191469907, accuracy: 0.99\n",
      "iteration no 7483: Loss: 0.23739714009127702, accuracy: 0.99\n",
      "iteration no 7484: Loss: 0.23739697638335897, accuracy: 0.99\n",
      "iteration no 7485: Loss: 0.23739732682791984, accuracy: 0.99\n",
      "iteration no 7486: Loss: 0.23739654368023677, accuracy: 0.99\n",
      "iteration no 7487: Loss: 0.23739596500399304, accuracy: 0.99\n",
      "iteration no 7488: Loss: 0.23739515168205855, accuracy: 0.99\n",
      "iteration no 7489: Loss: 0.23739447216672005, accuracy: 0.99\n",
      "iteration no 7490: Loss: 0.23739464316586756, accuracy: 0.99\n",
      "iteration no 7491: Loss: 0.2373942682734435, accuracy: 0.99\n",
      "iteration no 7492: Loss: 0.23739302273298246, accuracy: 0.99\n",
      "iteration no 7493: Loss: 0.23739311286837156, accuracy: 0.99\n",
      "iteration no 7494: Loss: 0.23739161310062912, accuracy: 0.99\n",
      "iteration no 7495: Loss: 0.2373925219216812, accuracy: 0.99\n",
      "iteration no 7496: Loss: 0.23739130675543976, accuracy: 0.99\n",
      "iteration no 7497: Loss: 0.23739109288681018, accuracy: 0.99\n",
      "iteration no 7498: Loss: 0.23739006064881107, accuracy: 0.99\n",
      "iteration no 7499: Loss: 0.23738972021881888, accuracy: 0.99\n",
      "iteration no 7500: Loss: 0.2373887609599812, accuracy: 0.99\n",
      "iteration no 7501: Loss: 0.23738884066147992, accuracy: 0.99\n",
      "iteration no 7502: Loss: 0.23738841362885066, accuracy: 0.99\n",
      "iteration no 7503: Loss: 0.23738784748383857, accuracy: 0.99\n",
      "iteration no 7504: Loss: 0.23738722269811174, accuracy: 0.99\n",
      "iteration no 7505: Loss: 0.23738619252637733, accuracy: 0.99\n",
      "iteration no 7506: Loss: 0.23738605257305348, accuracy: 0.99\n",
      "iteration no 7507: Loss: 0.2373854296119977, accuracy: 0.99\n",
      "iteration no 7508: Loss: 0.23738582549841536, accuracy: 0.99\n",
      "iteration no 7509: Loss: 0.23738439791089655, accuracy: 0.99\n",
      "iteration no 7510: Loss: 0.23738433192379532, accuracy: 0.99\n",
      "iteration no 7511: Loss: 0.23738324516285578, accuracy: 0.99\n",
      "iteration no 7512: Loss: 0.23738300147766805, accuracy: 0.99\n",
      "iteration no 7513: Loss: 0.23738264177811252, accuracy: 0.99\n",
      "iteration no 7514: Loss: 0.23738278055005224, accuracy: 0.99\n",
      "iteration no 7515: Loss: 0.23738179265046228, accuracy: 0.99\n",
      "iteration no 7516: Loss: 0.23738099626534925, accuracy: 0.99\n",
      "iteration no 7517: Loss: 0.23738056132184576, accuracy: 0.99\n",
      "iteration no 7518: Loss: 0.23737945095801313, accuracy: 0.99\n",
      "iteration no 7519: Loss: 0.23738010635070267, accuracy: 0.99\n",
      "iteration no 7520: Loss: 0.23737926120222907, accuracy: 0.99\n",
      "iteration no 7521: Loss: 0.23737900611737578, accuracy: 0.99\n",
      "iteration no 7522: Loss: 0.23737776869031846, accuracy: 0.99\n",
      "iteration no 7523: Loss: 0.23737790466083394, accuracy: 0.99\n",
      "iteration no 7524: Loss: 0.23737655273285974, accuracy: 0.99\n",
      "iteration no 7525: Loss: 0.2373774338566261, accuracy: 0.99\n",
      "iteration no 7526: Loss: 0.23737614034065227, accuracy: 0.99\n",
      "iteration no 7527: Loss: 0.23737603143947678, accuracy: 0.99\n",
      "iteration no 7528: Loss: 0.23737483937387227, accuracy: 0.99\n",
      "iteration no 7529: Loss: 0.23737460636868965, accuracy: 0.99\n",
      "iteration no 7530: Loss: 0.2373742136770987, accuracy: 0.99\n",
      "iteration no 7531: Loss: 0.23737407410172823, accuracy: 0.99\n",
      "iteration no 7532: Loss: 0.23737373529682454, accuracy: 0.99\n",
      "iteration no 7533: Loss: 0.2373728407145857, accuracy: 0.99\n",
      "iteration no 7534: Loss: 0.2373723653559855, accuracy: 0.99\n",
      "iteration no 7535: Loss: 0.2373712322954244, accuracy: 0.99\n",
      "iteration no 7536: Loss: 0.23737194412807097, accuracy: 0.99\n",
      "iteration no 7537: Loss: 0.23737091901350524, accuracy: 0.99\n",
      "iteration no 7538: Loss: 0.23737101366117652, accuracy: 0.99\n",
      "iteration no 7539: Loss: 0.23736951885761584, accuracy: 0.99\n",
      "iteration no 7540: Loss: 0.2373695256018093, accuracy: 0.99\n",
      "iteration no 7541: Loss: 0.23736842398470903, accuracy: 0.99\n",
      "iteration no 7542: Loss: 0.23736906245313394, accuracy: 0.99\n",
      "iteration no 7543: Loss: 0.23736828250313877, accuracy: 0.99\n",
      "iteration no 7544: Loss: 0.23736788288131896, accuracy: 0.99\n",
      "iteration no 7545: Loss: 0.23736703796389902, accuracy: 0.99\n",
      "iteration no 7546: Loss: 0.2373663065292256, accuracy: 0.99\n",
      "iteration no 7547: Loss: 0.2373660865876055, accuracy: 0.99\n",
      "iteration no 7548: Loss: 0.23736583646792783, accuracy: 0.99\n",
      "iteration no 7549: Loss: 0.23736573482612844, accuracy: 0.99\n",
      "iteration no 7550: Loss: 0.23736457004615785, accuracy: 0.99\n",
      "iteration no 7551: Loss: 0.23736439635499346, accuracy: 0.99\n",
      "iteration no 7552: Loss: 0.23736296555291705, accuracy: 0.99\n",
      "iteration no 7553: Loss: 0.2373637261797829, accuracy: 0.99\n",
      "iteration no 7554: Loss: 0.2373629716187693, accuracy: 0.99\n",
      "iteration no 7555: Loss: 0.23736274999963242, accuracy: 0.99\n",
      "iteration no 7556: Loss: 0.23736160824296784, accuracy: 0.99\n",
      "iteration no 7557: Loss: 0.23736149457593897, accuracy: 0.99\n",
      "iteration no 7558: Loss: 0.23736051797969182, accuracy: 0.99\n",
      "iteration no 7559: Loss: 0.23736109696335275, accuracy: 0.99\n",
      "iteration no 7560: Loss: 0.2373603805188104, accuracy: 0.99\n",
      "iteration no 7561: Loss: 0.23735936224956372, accuracy: 0.99\n",
      "iteration no 7562: Loss: 0.23735893709244416, accuracy: 0.99\n",
      "iteration no 7563: Loss: 0.23735828507258544, accuracy: 0.99\n",
      "iteration no 7564: Loss: 0.23735839657794802, accuracy: 0.99\n",
      "iteration no 7565: Loss: 0.2373577594807635, accuracy: 0.99\n",
      "iteration no 7566: Loss: 0.23735786288689925, accuracy: 0.99\n",
      "iteration no 7567: Loss: 0.2373561420342307, accuracy: 0.99\n",
      "iteration no 7568: Loss: 0.23735649031987854, accuracy: 0.99\n",
      "iteration no 7569: Loss: 0.23735514753113612, accuracy: 0.99\n",
      "iteration no 7570: Loss: 0.2373560298430027, accuracy: 0.99\n",
      "iteration no 7571: Loss: 0.23735510091712492, accuracy: 0.99\n",
      "iteration no 7572: Loss: 0.23735456931443166, accuracy: 0.99\n",
      "iteration no 7573: Loss: 0.23735365642225148, accuracy: 0.99\n",
      "iteration no 7574: Loss: 0.2373532833107071, accuracy: 0.99\n",
      "iteration no 7575: Loss: 0.23735311359956324, accuracy: 0.99\n",
      "iteration no 7576: Loss: 0.23735300264182463, accuracy: 0.99\n",
      "iteration no 7577: Loss: 0.23735262721372372, accuracy: 0.99\n",
      "iteration no 7578: Loss: 0.23735131018183114, accuracy: 0.99\n",
      "iteration no 7579: Loss: 0.23735105329996214, accuracy: 0.99\n",
      "iteration no 7580: Loss: 0.2373502698315782, accuracy: 0.99\n",
      "iteration no 7581: Loss: 0.23735094490759706, accuracy: 0.99\n",
      "iteration no 7582: Loss: 0.23734961309889296, accuracy: 0.99\n",
      "iteration no 7583: Loss: 0.237349904939457, accuracy: 0.99\n",
      "iteration no 7584: Loss: 0.23734838552733534, accuracy: 0.99\n",
      "iteration no 7585: Loss: 0.2373484676543275, accuracy: 0.99\n",
      "iteration no 7586: Loss: 0.23734808285983625, accuracy: 0.99\n",
      "iteration no 7587: Loss: 0.23734786590759827, accuracy: 0.99\n",
      "iteration no 7588: Loss: 0.23734735019620268, accuracy: 0.99\n",
      "iteration no 7589: Loss: 0.23734658002262649, accuracy: 0.99\n",
      "iteration no 7590: Loss: 0.23734605613599574, accuracy: 0.99\n",
      "iteration no 7591: Loss: 0.23734570484574513, accuracy: 0.99\n",
      "iteration no 7592: Loss: 0.23734594416264088, accuracy: 0.99\n",
      "iteration no 7593: Loss: 0.23734483762082642, accuracy: 0.99\n",
      "iteration no 7594: Loss: 0.2373446617823883, accuracy: 0.99\n",
      "iteration no 7595: Loss: 0.23734329882643868, accuracy: 0.99\n",
      "iteration no 7596: Loss: 0.23734344298850776, accuracy: 0.99\n",
      "iteration no 7597: Loss: 0.23734343245989353, accuracy: 0.99\n",
      "iteration no 7598: Loss: 0.23734400094119573, accuracy: 0.99\n",
      "iteration no 7599: Loss: 0.2373427397428886, accuracy: 0.99\n",
      "iteration no 7600: Loss: 0.23734256100380133, accuracy: 0.99\n",
      "iteration no 7601: Loss: 0.23734129325681258, accuracy: 0.99\n",
      "iteration no 7602: Loss: 0.23734091051109085, accuracy: 0.99\n",
      "iteration no 7603: Loss: 0.23734011841096583, accuracy: 0.99\n",
      "iteration no 7604: Loss: 0.237339534090499, accuracy: 0.99\n",
      "iteration no 7605: Loss: 0.23733949581622707, accuracy: 0.99\n",
      "iteration no 7606: Loss: 0.2373399867237167, accuracy: 0.99\n",
      "iteration no 7607: Loss: 0.23733963358249618, accuracy: 0.99\n",
      "iteration no 7608: Loss: 0.2373384018214713, accuracy: 0.99\n",
      "iteration no 7609: Loss: 0.23733827317423062, accuracy: 0.99\n",
      "iteration no 7610: Loss: 0.23733689298317867, accuracy: 0.99\n",
      "iteration no 7611: Loss: 0.2373368317216354, accuracy: 0.99\n",
      "iteration no 7612: Loss: 0.2373355037946781, accuracy: 0.99\n",
      "iteration no 7613: Loss: 0.23733564290019757, accuracy: 0.99\n",
      "iteration no 7614: Loss: 0.237335505722617, accuracy: 0.99\n",
      "iteration no 7615: Loss: 0.23733520638950123, accuracy: 0.99\n",
      "iteration no 7616: Loss: 0.23733407117189081, accuracy: 0.99\n",
      "iteration no 7617: Loss: 0.2373335335725827, accuracy: 0.99\n",
      "iteration no 7618: Loss: 0.23733271663164726, accuracy: 0.99\n",
      "iteration no 7619: Loss: 0.23733378266968497, accuracy: 0.99\n",
      "iteration no 7620: Loss: 0.23733363779744468, accuracy: 0.99\n",
      "iteration no 7621: Loss: 0.23733261102038772, accuracy: 0.99\n",
      "iteration no 7622: Loss: 0.23733240668082511, accuracy: 0.99\n",
      "iteration no 7623: Loss: 0.2373308995875163, accuracy: 0.99\n",
      "iteration no 7624: Loss: 0.23733076569483433, accuracy: 0.99\n",
      "iteration no 7625: Loss: 0.23732947439175056, accuracy: 0.99\n",
      "iteration no 7626: Loss: 0.23732978169586666, accuracy: 0.99\n",
      "iteration no 7627: Loss: 0.23732916529050463, accuracy: 0.99\n",
      "iteration no 7628: Loss: 0.23733026798104506, accuracy: 0.99\n",
      "iteration no 7629: Loss: 0.23732893750499615, accuracy: 0.99\n",
      "iteration no 7630: Loss: 0.23732846839347493, accuracy: 0.99\n",
      "iteration no 7631: Loss: 0.2373275062718863, accuracy: 0.99\n",
      "iteration no 7632: Loss: 0.2373268765027827, accuracy: 0.99\n",
      "iteration no 7633: Loss: 0.2373264090472988, accuracy: 0.99\n",
      "iteration no 7634: Loss: 0.2373253952055951, accuracy: 0.99\n",
      "iteration no 7635: Loss: 0.23732572559573506, accuracy: 0.99\n",
      "iteration no 7636: Loss: 0.23732612855221863, accuracy: 0.99\n",
      "iteration no 7637: Loss: 0.23732572950875813, accuracy: 0.99\n",
      "iteration no 7638: Loss: 0.23732433259248617, accuracy: 0.99\n",
      "iteration no 7639: Loss: 0.2373242843329959, accuracy: 0.99\n",
      "iteration no 7640: Loss: 0.23732288809000396, accuracy: 0.99\n",
      "iteration no 7641: Loss: 0.23732292348834194, accuracy: 0.99\n",
      "iteration no 7642: Loss: 0.23732162942556773, accuracy: 0.99\n",
      "iteration no 7643: Loss: 0.23732218969285215, accuracy: 0.99\n",
      "iteration no 7644: Loss: 0.2373223773047335, accuracy: 0.99\n",
      "iteration no 7645: Loss: 0.23732179088409242, accuracy: 0.99\n",
      "iteration no 7646: Loss: 0.23732105378490692, accuracy: 0.99\n",
      "iteration no 7647: Loss: 0.23732020965357592, accuracy: 0.99\n",
      "iteration no 7648: Loss: 0.23731963968093145, accuracy: 0.99\n",
      "iteration no 7649: Loss: 0.23731877203897683, accuracy: 0.99\n",
      "iteration no 7650: Loss: 0.23731844918310374, accuracy: 0.99\n",
      "iteration no 7651: Loss: 0.2373176833371775, accuracy: 0.99\n",
      "iteration no 7652: Loss: 0.2373191413913635, accuracy: 0.99\n",
      "iteration no 7653: Loss: 0.23731778766710054, accuracy: 0.99\n",
      "iteration no 7654: Loss: 0.23731765199105703, accuracy: 0.99\n",
      "iteration no 7655: Loss: 0.2373165158887332, accuracy: 0.99\n",
      "iteration no 7656: Loss: 0.23731626453613597, accuracy: 0.99\n",
      "iteration no 7657: Loss: 0.23731508254283173, accuracy: 0.99\n",
      "iteration no 7658: Loss: 0.2373148538458104, accuracy: 0.99\n",
      "iteration no 7659: Loss: 0.2373145775916669, accuracy: 0.99\n",
      "iteration no 7660: Loss: 0.23731527357486804, accuracy: 0.99\n",
      "iteration no 7661: Loss: 0.23731481979299787, accuracy: 0.99\n",
      "iteration no 7662: Loss: 0.23731345150369038, accuracy: 0.99\n",
      "iteration no 7663: Loss: 0.2373132405557689, accuracy: 0.99\n",
      "iteration no 7664: Loss: 0.23731213963781086, accuracy: 0.99\n",
      "iteration no 7665: Loss: 0.2373118239478469, accuracy: 0.99\n",
      "iteration no 7666: Loss: 0.23731067118027338, accuracy: 0.99\n",
      "iteration no 7667: Loss: 0.23731170158938542, accuracy: 0.99\n",
      "iteration no 7668: Loss: 0.2373114979821624, accuracy: 0.99\n",
      "iteration no 7669: Loss: 0.23731136685452275, accuracy: 0.99\n",
      "iteration no 7670: Loss: 0.23731016102416708, accuracy: 0.99\n",
      "iteration no 7671: Loss: 0.2373095752896203, accuracy: 0.99\n",
      "iteration no 7672: Loss: 0.2373090417458425, accuracy: 0.99\n",
      "iteration no 7673: Loss: 0.2373080512255918, accuracy: 0.99\n",
      "iteration no 7674: Loss: 0.23730755325251462, accuracy: 0.99\n",
      "iteration no 7675: Loss: 0.2373072338057621, accuracy: 0.99\n",
      "iteration no 7676: Loss: 0.23730720264456598, accuracy: 0.99\n",
      "iteration no 7677: Loss: 0.23730613577704107, accuracy: 0.99\n",
      "iteration no 7678: Loss: 0.23730627585336816, accuracy: 0.99\n",
      "iteration no 7679: Loss: 0.23730522030692808, accuracy: 0.99\n",
      "iteration no 7680: Loss: 0.23730567107890382, accuracy: 0.99\n",
      "iteration no 7681: Loss: 0.2373044632839208, accuracy: 0.99\n",
      "iteration no 7682: Loss: 0.2373039825780365, accuracy: 0.99\n",
      "iteration no 7683: Loss: 0.23730376223212352, accuracy: 0.99\n",
      "iteration no 7684: Loss: 0.23730383248478926, accuracy: 0.99\n",
      "iteration no 7685: Loss: 0.23730293816577822, accuracy: 0.99\n",
      "iteration no 7686: Loss: 0.2373023827296225, accuracy: 0.99\n",
      "iteration no 7687: Loss: 0.23730198240215805, accuracy: 0.99\n",
      "iteration no 7688: Loss: 0.23730137826372405, accuracy: 0.99\n",
      "iteration no 7689: Loss: 0.23730203744037676, accuracy: 0.99\n",
      "iteration no 7690: Loss: 0.23730053683314467, accuracy: 0.99\n",
      "iteration no 7691: Loss: 0.2373008296167478, accuracy: 0.99\n",
      "iteration no 7692: Loss: 0.2372994739934064, accuracy: 0.99\n",
      "iteration no 7693: Loss: 0.23729996466719466, accuracy: 0.99\n",
      "iteration no 7694: Loss: 0.237299390921806, accuracy: 0.99\n",
      "iteration no 7695: Loss: 0.23729905719443006, accuracy: 0.99\n",
      "iteration no 7696: Loss: 0.23729813067185462, accuracy: 0.99\n",
      "iteration no 7697: Loss: 0.23729819437542488, accuracy: 0.99\n",
      "iteration no 7698: Loss: 0.23729759632810937, accuracy: 0.99\n",
      "iteration no 7699: Loss: 0.2372974574398059, accuracy: 0.99\n",
      "iteration no 7700: Loss: 0.23729705199191353, accuracy: 0.99\n",
      "iteration no 7701: Loss: 0.23729617359329036, accuracy: 0.99\n",
      "iteration no 7702: Loss: 0.23729637412840182, accuracy: 0.99\n",
      "iteration no 7703: Loss: 0.237295229638415, accuracy: 0.99\n",
      "iteration no 7704: Loss: 0.2372955412279944, accuracy: 0.99\n",
      "iteration no 7705: Loss: 0.23729475951035742, accuracy: 0.99\n",
      "iteration no 7706: Loss: 0.23729491387127363, accuracy: 0.99\n",
      "iteration no 7707: Loss: 0.237293697266238, accuracy: 0.99\n",
      "iteration no 7708: Loss: 0.2372940301024266, accuracy: 0.99\n",
      "iteration no 7709: Loss: 0.23729292951125036, accuracy: 0.99\n",
      "iteration no 7710: Loss: 0.23729282842305727, accuracy: 0.99\n",
      "iteration no 7711: Loss: 0.2372926075091507, accuracy: 0.99\n",
      "iteration no 7712: Loss: 0.23729219720644823, accuracy: 0.99\n",
      "iteration no 7713: Loss: 0.23729172149623767, accuracy: 0.99\n",
      "iteration no 7714: Loss: 0.23729110115054097, accuracy: 0.99\n",
      "iteration no 7715: Loss: 0.23729107581864223, accuracy: 0.99\n",
      "iteration no 7716: Loss: 0.23729004179310847, accuracy: 0.99\n",
      "iteration no 7717: Loss: 0.23729043253731805, accuracy: 0.99\n",
      "iteration no 7718: Loss: 0.2372895289399306, accuracy: 0.99\n",
      "iteration no 7719: Loss: 0.23728969562040553, accuracy: 0.99\n",
      "iteration no 7720: Loss: 0.23728845849908975, accuracy: 0.99\n",
      "iteration no 7721: Loss: 0.23728876938143423, accuracy: 0.99\n",
      "iteration no 7722: Loss: 0.23728788203187162, accuracy: 0.99\n",
      "iteration no 7723: Loss: 0.2372877446614362, accuracy: 0.99\n",
      "iteration no 7724: Loss: 0.2372871133307667, accuracy: 0.99\n",
      "iteration no 7725: Loss: 0.2372870484037617, accuracy: 0.99\n",
      "iteration no 7726: Loss: 0.23728665025765627, accuracy: 0.99\n",
      "iteration no 7727: Loss: 0.23728618963684078, accuracy: 0.99\n",
      "iteration no 7728: Loss: 0.23728590496517635, accuracy: 0.99\n",
      "iteration no 7729: Loss: 0.23728518317934227, accuracy: 0.99\n",
      "iteration no 7730: Loss: 0.2372852469330199, accuracy: 0.99\n",
      "iteration no 7731: Loss: 0.2372841203207779, accuracy: 0.99\n",
      "iteration no 7732: Loss: 0.23728433372716112, accuracy: 0.99\n",
      "iteration no 7733: Loss: 0.23728358716631787, accuracy: 0.99\n",
      "iteration no 7734: Loss: 0.23728376671182422, accuracy: 0.99\n",
      "iteration no 7735: Loss: 0.23728281090780395, accuracy: 0.99\n",
      "iteration no 7736: Loss: 0.23728278167925898, accuracy: 0.99\n",
      "iteration no 7737: Loss: 0.23728199706149072, accuracy: 0.99\n",
      "iteration no 7738: Loss: 0.23728188407973488, accuracy: 0.99\n",
      "iteration no 7739: Loss: 0.23728121487648232, accuracy: 0.99\n",
      "iteration no 7740: Loss: 0.23728117528426773, accuracy: 0.99\n",
      "iteration no 7741: Loss: 0.2372809081268833, accuracy: 0.99\n",
      "iteration no 7742: Loss: 0.2372800817207471, accuracy: 0.99\n",
      "iteration no 7743: Loss: 0.23728010297005026, accuracy: 0.99\n",
      "iteration no 7744: Loss: 0.23727916104504465, accuracy: 0.99\n",
      "iteration no 7745: Loss: 0.23727918261665187, accuracy: 0.99\n",
      "iteration no 7746: Loss: 0.23727834932495273, accuracy: 0.99\n",
      "iteration no 7747: Loss: 0.23727879923641887, accuracy: 0.99\n",
      "iteration no 7748: Loss: 0.23727765476752133, accuracy: 0.99\n",
      "iteration no 7749: Loss: 0.23727784246491085, accuracy: 0.99\n",
      "iteration no 7750: Loss: 0.2372767728486352, accuracy: 0.99\n",
      "iteration no 7751: Loss: 0.23727666358743726, accuracy: 0.99\n",
      "iteration no 7752: Loss: 0.23727644275805515, accuracy: 0.99\n",
      "iteration no 7753: Loss: 0.23727591019332114, accuracy: 0.99\n",
      "iteration no 7754: Loss: 0.23727571153841912, accuracy: 0.99\n",
      "iteration no 7755: Loss: 0.23727498684098638, accuracy: 0.99\n",
      "iteration no 7756: Loss: 0.2372746893516972, accuracy: 0.99\n",
      "iteration no 7757: Loss: 0.23727404381676664, accuracy: 0.99\n",
      "iteration no 7758: Loss: 0.23727451004809416, accuracy: 0.99\n",
      "iteration no 7759: Loss: 0.23727303541098416, accuracy: 0.99\n",
      "iteration no 7760: Loss: 0.2372735302578709, accuracy: 0.99\n",
      "iteration no 7761: Loss: 0.23727228363575997, accuracy: 0.99\n",
      "iteration no 7762: Loss: 0.23727258520429292, accuracy: 0.99\n",
      "iteration no 7763: Loss: 0.23727200949933372, accuracy: 0.99\n",
      "iteration no 7764: Loss: 0.2372716779971471, accuracy: 0.99\n",
      "iteration no 7765: Loss: 0.23727090749939928, accuracy: 0.99\n",
      "iteration no 7766: Loss: 0.2372706445591582, accuracy: 0.99\n",
      "iteration no 7767: Loss: 0.23727057494313758, accuracy: 0.99\n",
      "iteration no 7768: Loss: 0.23726986378066342, accuracy: 0.99\n",
      "iteration no 7769: Loss: 0.23726997067167027, accuracy: 0.99\n",
      "iteration no 7770: Loss: 0.23726877110159453, accuracy: 0.99\n",
      "iteration no 7771: Loss: 0.2372689443927125, accuracy: 0.99\n",
      "iteration no 7772: Loss: 0.2372682681484611, accuracy: 0.99\n",
      "iteration no 7773: Loss: 0.23726842279453442, accuracy: 0.99\n",
      "iteration no 7774: Loss: 0.2372671949868755, accuracy: 0.99\n",
      "iteration no 7775: Loss: 0.23726739638340677, accuracy: 0.99\n",
      "iteration no 7776: Loss: 0.23726689155715341, accuracy: 0.99\n",
      "iteration no 7777: Loss: 0.23726677287017195, accuracy: 0.99\n",
      "iteration no 7778: Loss: 0.23726577867057935, accuracy: 0.99\n",
      "iteration no 7779: Loss: 0.23726543973964898, accuracy: 0.99\n",
      "iteration no 7780: Loss: 0.23726562762577005, accuracy: 0.99\n",
      "iteration no 7781: Loss: 0.23726492372467553, accuracy: 0.99\n",
      "iteration no 7782: Loss: 0.23726472546636995, accuracy: 0.99\n",
      "iteration no 7783: Loss: 0.2372636651317294, accuracy: 0.99\n",
      "iteration no 7784: Loss: 0.23726389320137833, accuracy: 0.99\n",
      "iteration no 7785: Loss: 0.2372631145925113, accuracy: 0.99\n",
      "iteration no 7786: Loss: 0.23726352528099043, accuracy: 0.99\n",
      "iteration no 7787: Loss: 0.2372623146296095, accuracy: 0.99\n",
      "iteration no 7788: Loss: 0.23726233529165247, accuracy: 0.99\n",
      "iteration no 7789: Loss: 0.23726180081913695, accuracy: 0.99\n",
      "iteration no 7790: Loss: 0.2372615699797248, accuracy: 0.99\n",
      "iteration no 7791: Loss: 0.23726084590615432, accuracy: 0.99\n",
      "iteration no 7792: Loss: 0.2372605051840572, accuracy: 0.99\n",
      "iteration no 7793: Loss: 0.23726056195880646, accuracy: 0.99\n",
      "iteration no 7794: Loss: 0.23725973310990878, accuracy: 0.99\n",
      "iteration no 7795: Loss: 0.23725948894585083, accuracy: 0.99\n",
      "iteration no 7796: Loss: 0.237258577660572, accuracy: 0.99\n",
      "iteration no 7797: Loss: 0.23725896504567068, accuracy: 0.99\n",
      "iteration no 7798: Loss: 0.2372582481871193, accuracy: 0.99\n",
      "iteration no 7799: Loss: 0.23725833778814798, accuracy: 0.99\n",
      "iteration no 7800: Loss: 0.23725689967428615, accuracy: 0.99\n",
      "iteration no 7801: Loss: 0.23725731422878515, accuracy: 0.99\n",
      "iteration no 7802: Loss: 0.23725661851843727, accuracy: 0.99\n",
      "iteration no 7803: Loss: 0.23725678139080666, accuracy: 0.99\n",
      "iteration no 7804: Loss: 0.2372557949068101, accuracy: 0.99\n",
      "iteration no 7805: Loss: 0.23725544686444788, accuracy: 0.99\n",
      "iteration no 7806: Loss: 0.23725530034131076, accuracy: 0.99\n",
      "iteration no 7807: Loss: 0.2372547528199116, accuracy: 0.99\n",
      "iteration no 7808: Loss: 0.2372545277896122, accuracy: 0.99\n",
      "iteration no 7809: Loss: 0.2372537392320641, accuracy: 0.99\n",
      "iteration no 7810: Loss: 0.23725415411746417, accuracy: 0.99\n",
      "iteration no 7811: Loss: 0.2372529870852862, accuracy: 0.99\n",
      "iteration no 7812: Loss: 0.23725303333376668, accuracy: 0.99\n",
      "iteration no 7813: Loss: 0.23725179453776596, accuracy: 0.99\n",
      "iteration no 7814: Loss: 0.23725286830645176, accuracy: 0.99\n",
      "iteration no 7815: Loss: 0.2372514987987835, accuracy: 0.99\n",
      "iteration no 7816: Loss: 0.23725139647058135, accuracy: 0.99\n",
      "iteration no 7817: Loss: 0.23725054065110773, accuracy: 0.99\n",
      "iteration no 7818: Loss: 0.237250691875524, accuracy: 0.99\n",
      "iteration no 7819: Loss: 0.23725051194148253, accuracy: 0.99\n",
      "iteration no 7820: Loss: 0.23724968375210828, accuracy: 0.99\n",
      "iteration no 7821: Loss: 0.2372492148186941, accuracy: 0.99\n",
      "iteration no 7822: Loss: 0.23724881368711326, accuracy: 0.99\n",
      "iteration no 7823: Loss: 0.23724902048097535, accuracy: 0.99\n",
      "iteration no 7824: Loss: 0.23724794580411607, accuracy: 0.99\n",
      "iteration no 7825: Loss: 0.23724815529266063, accuracy: 0.99\n",
      "iteration no 7826: Loss: 0.23724702306063294, accuracy: 0.99\n",
      "iteration no 7827: Loss: 0.23724764032018208, accuracy: 0.99\n",
      "iteration no 7828: Loss: 0.23724626465749615, accuracy: 0.99\n",
      "iteration no 7829: Loss: 0.23724643950651125, accuracy: 0.99\n",
      "iteration no 7830: Loss: 0.23724592107090203, accuracy: 0.99\n",
      "iteration no 7831: Loss: 0.23724594464020893, accuracy: 0.99\n",
      "iteration no 7832: Loss: 0.23724511734586123, accuracy: 0.99\n",
      "iteration no 7833: Loss: 0.23724458909276874, accuracy: 0.99\n",
      "iteration no 7834: Loss: 0.23724436472286892, accuracy: 0.99\n",
      "iteration no 7835: Loss: 0.23724427627611172, accuracy: 0.99\n",
      "iteration no 7836: Loss: 0.2372440853225341, accuracy: 0.99\n",
      "iteration no 7837: Loss: 0.23724296998295336, accuracy: 0.99\n",
      "iteration no 7838: Loss: 0.23724307274801104, accuracy: 0.99\n",
      "iteration no 7839: Loss: 0.23724233480408483, accuracy: 0.99\n",
      "iteration no 7840: Loss: 0.23724252013080036, accuracy: 0.99\n",
      "iteration no 7841: Loss: 0.2372414846288485, accuracy: 0.99\n",
      "iteration no 7842: Loss: 0.23724175994677854, accuracy: 0.99\n",
      "iteration no 7843: Loss: 0.23724102375847989, accuracy: 0.99\n",
      "iteration no 7844: Loss: 0.2372408396580062, accuracy: 0.99\n",
      "iteration no 7845: Loss: 0.2372400207314532, accuracy: 0.99\n",
      "iteration no 7846: Loss: 0.23723991820671358, accuracy: 0.99\n",
      "iteration no 7847: Loss: 0.2372397901410082, accuracy: 0.99\n",
      "iteration no 7848: Loss: 0.23723917815664738, accuracy: 0.99\n",
      "iteration no 7849: Loss: 0.237238860313823, accuracy: 0.99\n",
      "iteration no 7850: Loss: 0.2372380667377727, accuracy: 0.99\n",
      "iteration no 7851: Loss: 0.23723858740302256, accuracy: 0.99\n",
      "iteration no 7852: Loss: 0.23723734909217423, accuracy: 0.99\n",
      "iteration no 7853: Loss: 0.23723777317602554, accuracy: 0.99\n",
      "iteration no 7854: Loss: 0.23723645461109705, accuracy: 0.99\n",
      "iteration no 7855: Loss: 0.23723723340419126, accuracy: 0.99\n",
      "iteration no 7856: Loss: 0.23723586193021634, accuracy: 0.99\n",
      "iteration no 7857: Loss: 0.23723616186020435, accuracy: 0.99\n",
      "iteration no 7858: Loss: 0.2372351507193406, accuracy: 0.99\n",
      "iteration no 7859: Loss: 0.23723554632453764, accuracy: 0.99\n",
      "iteration no 7860: Loss: 0.2372347274923412, accuracy: 0.99\n",
      "iteration no 7861: Loss: 0.23723429379487654, accuracy: 0.99\n",
      "iteration no 7862: Loss: 0.23723403401677612, accuracy: 0.99\n",
      "iteration no 7863: Loss: 0.23723365842253535, accuracy: 0.99\n",
      "iteration no 7864: Loss: 0.23723351893393857, accuracy: 0.99\n",
      "iteration no 7865: Loss: 0.23723267244478757, accuracy: 0.99\n",
      "iteration no 7866: Loss: 0.23723284522860452, accuracy: 0.99\n",
      "iteration no 7867: Loss: 0.23723216628661897, accuracy: 0.99\n",
      "iteration no 7868: Loss: 0.23723217120851192, accuracy: 0.99\n",
      "iteration no 7869: Loss: 0.23723102933206971, accuracy: 0.99\n",
      "iteration no 7870: Loss: 0.2372313765768262, accuracy: 0.99\n",
      "iteration no 7871: Loss: 0.23723072202955298, accuracy: 0.99\n",
      "iteration no 7872: Loss: 0.23723075442071698, accuracy: 0.99\n",
      "iteration no 7873: Loss: 0.2372297730441753, accuracy: 0.99\n",
      "iteration no 7874: Loss: 0.23722959809420535, accuracy: 0.99\n",
      "iteration no 7875: Loss: 0.23722949142409822, accuracy: 0.99\n",
      "iteration no 7876: Loss: 0.23722886374992097, accuracy: 0.99\n",
      "iteration no 7877: Loss: 0.23722867123553842, accuracy: 0.99\n",
      "iteration no 7878: Loss: 0.23722808219778524, accuracy: 0.99\n",
      "iteration no 7879: Loss: 0.23722818976457646, accuracy: 0.99\n",
      "iteration no 7880: Loss: 0.23722714362743164, accuracy: 0.99\n",
      "iteration no 7881: Loss: 0.23722732787032316, accuracy: 0.99\n",
      "iteration no 7882: Loss: 0.23722638430556825, accuracy: 0.99\n",
      "iteration no 7883: Loss: 0.23722709035846937, accuracy: 0.99\n",
      "iteration no 7884: Loss: 0.2372258074314052, accuracy: 0.99\n",
      "iteration no 7885: Loss: 0.2372257586231529, accuracy: 0.99\n",
      "iteration no 7886: Loss: 0.23722531831708205, accuracy: 0.99\n",
      "iteration no 7887: Loss: 0.23722518047381042, accuracy: 0.99\n",
      "iteration no 7888: Loss: 0.23722480139063118, accuracy: 0.99\n",
      "iteration no 7889: Loss: 0.237224003871901, accuracy: 0.99\n",
      "iteration no 7890: Loss: 0.23722410315993375, accuracy: 0.99\n",
      "iteration no 7891: Loss: 0.23722344524810368, accuracy: 0.99\n",
      "iteration no 7892: Loss: 0.2372233969571257, accuracy: 0.99\n",
      "iteration no 7893: Loss: 0.23722243435010038, accuracy: 0.99\n",
      "iteration no 7894: Loss: 0.2372229201495999, accuracy: 0.99\n",
      "iteration no 7895: Loss: 0.23722179448232322, accuracy: 0.99\n",
      "iteration no 7896: Loss: 0.23722230509045555, accuracy: 0.99\n",
      "iteration no 7897: Loss: 0.23722081038231996, accuracy: 0.99\n",
      "iteration no 7898: Loss: 0.2372215399105383, accuracy: 0.99\n",
      "iteration no 7899: Loss: 0.2372206772621452, accuracy: 0.99\n",
      "iteration no 7900: Loss: 0.23722039433117187, accuracy: 0.99\n",
      "iteration no 7901: Loss: 0.23721969290516773, accuracy: 0.99\n",
      "iteration no 7902: Loss: 0.23721986083206587, accuracy: 0.99\n",
      "iteration no 7903: Loss: 0.23721947707970256, accuracy: 0.99\n",
      "iteration no 7904: Loss: 0.23721864893178746, accuracy: 0.99\n",
      "iteration no 7905: Loss: 0.23721856462836788, accuracy: 0.99\n",
      "iteration no 7906: Loss: 0.23721812346406765, accuracy: 0.99\n",
      "iteration no 7907: Loss: 0.2372182917391814, accuracy: 0.99\n",
      "iteration no 7908: Loss: 0.23721720338224395, accuracy: 0.99\n",
      "iteration no 7909: Loss: 0.23721746254203036, accuracy: 0.99\n",
      "iteration no 7910: Loss: 0.23721653649860328, accuracy: 0.99\n",
      "iteration no 7911: Loss: 0.23721680698434028, accuracy: 0.99\n",
      "iteration no 7912: Loss: 0.23721573493881615, accuracy: 0.99\n",
      "iteration no 7913: Loss: 0.2372160621587374, accuracy: 0.99\n",
      "iteration no 7914: Loss: 0.23721548776468304, accuracy: 0.99\n",
      "iteration no 7915: Loss: 0.23721503459706983, accuracy: 0.99\n",
      "iteration no 7916: Loss: 0.23721442898468537, accuracy: 0.99\n",
      "iteration no 7917: Loss: 0.23721435375977606, accuracy: 0.99\n",
      "iteration no 7918: Loss: 0.23721430395877022, accuracy: 0.99\n",
      "iteration no 7919: Loss: 0.23721356042383113, accuracy: 0.99\n",
      "iteration no 7920: Loss: 0.23721325525632864, accuracy: 0.99\n",
      "iteration no 7921: Loss: 0.2372127305776842, accuracy: 0.99\n",
      "iteration no 7922: Loss: 0.23721296912132217, accuracy: 0.99\n",
      "iteration no 7923: Loss: 0.23721187810062816, accuracy: 0.99\n",
      "iteration no 7924: Loss: 0.2372123740745804, accuracy: 0.99\n",
      "iteration no 7925: Loss: 0.2372114255608444, accuracy: 0.99\n",
      "iteration no 7926: Loss: 0.23721147455709662, accuracy: 0.99\n",
      "iteration no 7927: Loss: 0.23721045902056967, accuracy: 0.99\n",
      "iteration no 7928: Loss: 0.23721058566590308, accuracy: 0.99\n",
      "iteration no 7929: Loss: 0.23721051443525576, accuracy: 0.99\n",
      "iteration no 7930: Loss: 0.23720986036324965, accuracy: 0.99\n",
      "iteration no 7931: Loss: 0.23720934233782995, accuracy: 0.99\n",
      "iteration no 7932: Loss: 0.23720896863338886, accuracy: 0.99\n",
      "iteration no 7933: Loss: 0.2372089942850782, accuracy: 0.99\n",
      "iteration no 7934: Loss: 0.2372083371409648, accuracy: 0.99\n",
      "iteration no 7935: Loss: 0.23720815415195565, accuracy: 0.99\n",
      "iteration no 7936: Loss: 0.2372075098981481, accuracy: 0.99\n",
      "iteration no 7937: Loss: 0.23720775003348826, accuracy: 0.99\n",
      "iteration no 7938: Loss: 0.23720652723428193, accuracy: 0.99\n",
      "iteration no 7939: Loss: 0.2372070074860757, accuracy: 0.99\n",
      "iteration no 7940: Loss: 0.2372064704444355, accuracy: 0.99\n",
      "iteration no 7941: Loss: 0.2372062054749769, accuracy: 0.99\n",
      "iteration no 7942: Loss: 0.23720536632840422, accuracy: 0.99\n",
      "iteration no 7943: Loss: 0.23720528236141636, accuracy: 0.99\n",
      "iteration no 7944: Loss: 0.23720526478959336, accuracy: 0.99\n",
      "iteration no 7945: Loss: 0.2372046152473602, accuracy: 0.99\n",
      "iteration no 7946: Loss: 0.2372042679986504, accuracy: 0.99\n",
      "iteration no 7947: Loss: 0.23720387220652045, accuracy: 0.99\n",
      "iteration no 7948: Loss: 0.23720386212534075, accuracy: 0.99\n",
      "iteration no 7949: Loss: 0.23720288747772944, accuracy: 0.99\n",
      "iteration no 7950: Loss: 0.23720319457281386, accuracy: 0.99\n",
      "iteration no 7951: Loss: 0.237202455034735, accuracy: 0.99\n",
      "iteration no 7952: Loss: 0.23720274568552618, accuracy: 0.99\n",
      "iteration no 7953: Loss: 0.23720144271101012, accuracy: 0.99\n",
      "iteration no 7954: Loss: 0.23720172614246676, accuracy: 0.99\n",
      "iteration no 7955: Loss: 0.2372013534860391, accuracy: 0.99\n",
      "iteration no 7956: Loss: 0.23720099573526632, accuracy: 0.99\n",
      "iteration no 7957: Loss: 0.23720021675195066, accuracy: 0.99\n",
      "iteration no 7958: Loss: 0.23720009876351592, accuracy: 0.99\n",
      "iteration no 7959: Loss: 0.23719990421041454, accuracy: 0.99\n",
      "iteration no 7960: Loss: 0.23719931705619351, accuracy: 0.99\n",
      "iteration no 7961: Loss: 0.23719898112186416, accuracy: 0.99\n",
      "iteration no 7962: Loss: 0.23719858971374186, accuracy: 0.99\n",
      "iteration no 7963: Loss: 0.2371985562252713, accuracy: 0.99\n",
      "iteration no 7964: Loss: 0.2371973576434837, accuracy: 0.99\n",
      "iteration no 7965: Loss: 0.23719786033046553, accuracy: 0.99\n",
      "iteration no 7966: Loss: 0.23719703118675112, accuracy: 0.99\n",
      "iteration no 7967: Loss: 0.23719717584295524, accuracy: 0.99\n",
      "iteration no 7968: Loss: 0.2371959061447339, accuracy: 0.99\n",
      "iteration no 7969: Loss: 0.23719629101257816, accuracy: 0.99\n",
      "iteration no 7970: Loss: 0.23719585028899356, accuracy: 0.99\n",
      "iteration no 7971: Loss: 0.23719535551743948, accuracy: 0.99\n",
      "iteration no 7972: Loss: 0.23719467377170855, accuracy: 0.99\n",
      "iteration no 7973: Loss: 0.23719479189796197, accuracy: 0.99\n",
      "iteration no 7974: Loss: 0.23719442606986246, accuracy: 0.99\n",
      "iteration no 7975: Loss: 0.23719370223023348, accuracy: 0.99\n",
      "iteration no 7976: Loss: 0.2371936087299364, accuracy: 0.99\n",
      "iteration no 7977: Loss: 0.23719316014221486, accuracy: 0.99\n",
      "iteration no 7978: Loss: 0.23719313720152818, accuracy: 0.99\n",
      "iteration no 7979: Loss: 0.23719181951650797, accuracy: 0.99\n",
      "iteration no 7980: Loss: 0.23719266245599943, accuracy: 0.99\n",
      "iteration no 7981: Loss: 0.23719155142255816, accuracy: 0.99\n",
      "iteration no 7982: Loss: 0.23719162392637783, accuracy: 0.99\n",
      "iteration no 7983: Loss: 0.2371905042836467, accuracy: 0.99\n",
      "iteration no 7984: Loss: 0.23719109070868946, accuracy: 0.99\n",
      "iteration no 7985: Loss: 0.23719032171711957, accuracy: 0.99\n",
      "iteration no 7986: Loss: 0.23718980652340327, accuracy: 0.99\n",
      "iteration no 7987: Loss: 0.2371895357523473, accuracy: 0.99\n",
      "iteration no 7988: Loss: 0.23718930712666414, accuracy: 0.99\n",
      "iteration no 7989: Loss: 0.23718902348271492, accuracy: 0.99\n",
      "iteration no 7990: Loss: 0.23718817557009844, accuracy: 0.99\n",
      "iteration no 7991: Loss: 0.23718860109678738, accuracy: 0.99\n",
      "iteration no 7992: Loss: 0.2371876436781406, accuracy: 0.99\n",
      "iteration no 7993: Loss: 0.23718765282557402, accuracy: 0.99\n",
      "iteration no 7994: Loss: 0.2371865082969291, accuracy: 0.99\n",
      "iteration no 7995: Loss: 0.23718736953911204, accuracy: 0.99\n",
      "iteration no 7996: Loss: 0.2371860808175698, accuracy: 0.99\n",
      "iteration no 7997: Loss: 0.2371861378946958, accuracy: 0.99\n",
      "iteration no 7998: Loss: 0.23718552181331456, accuracy: 0.99\n",
      "iteration no 7999: Loss: 0.23718558845595566, accuracy: 0.99\n",
      "iteration no 8000: Loss: 0.23718488130177523, accuracy: 0.99\n",
      "iteration no 8001: Loss: 0.23718444063043362, accuracy: 0.99\n",
      "iteration no 8002: Loss: 0.23718451668639645, accuracy: 0.99\n",
      "iteration no 8003: Loss: 0.23718379944894344, accuracy: 0.99\n",
      "iteration no 8004: Loss: 0.23718350889186635, accuracy: 0.99\n",
      "iteration no 8005: Loss: 0.23718313199724955, accuracy: 0.99\n",
      "iteration no 8006: Loss: 0.2371831999875505, accuracy: 0.99\n",
      "iteration no 8007: Loss: 0.2371821088654532, accuracy: 0.99\n",
      "iteration no 8008: Loss: 0.23718229948978353, accuracy: 0.99\n",
      "iteration no 8009: Loss: 0.23718158445293358, accuracy: 0.99\n",
      "iteration no 8010: Loss: 0.23718198012039615, accuracy: 0.99\n",
      "iteration no 8011: Loss: 0.2371807435862024, accuracy: 0.99\n",
      "iteration no 8012: Loss: 0.23718108666053112, accuracy: 0.99\n",
      "iteration no 8013: Loss: 0.23718042623513902, accuracy: 0.99\n",
      "iteration no 8014: Loss: 0.23718015059207043, accuracy: 0.99\n",
      "iteration no 8015: Loss: 0.23717958241353895, accuracy: 0.99\n",
      "iteration no 8016: Loss: 0.23717969172324388, accuracy: 0.99\n",
      "iteration no 8017: Loss: 0.23717937624228042, accuracy: 0.99\n",
      "iteration no 8018: Loss: 0.23717854965148766, accuracy: 0.99\n",
      "iteration no 8019: Loss: 0.2371787276747841, accuracy: 0.99\n",
      "iteration no 8020: Loss: 0.23717810060287395, accuracy: 0.99\n",
      "iteration no 8021: Loss: 0.23717800409410794, accuracy: 0.99\n",
      "iteration no 8022: Loss: 0.2371769132862218, accuracy: 0.99\n",
      "iteration no 8023: Loss: 0.2371777424439725, accuracy: 0.99\n",
      "iteration no 8024: Loss: 0.2371764005502282, accuracy: 0.99\n",
      "iteration no 8025: Loss: 0.23717672122669053, accuracy: 0.99\n",
      "iteration no 8026: Loss: 0.23717591161784302, accuracy: 0.99\n",
      "iteration no 8027: Loss: 0.23717625298798956, accuracy: 0.99\n",
      "iteration no 8028: Loss: 0.23717523378497454, accuracy: 0.99\n",
      "iteration no 8029: Loss: 0.23717499783404403, accuracy: 0.99\n",
      "iteration no 8030: Loss: 0.23717512575374772, accuracy: 0.99\n",
      "iteration no 8031: Loss: 0.23717444620924355, accuracy: 0.99\n",
      "iteration no 8032: Loss: 0.2371741807168326, accuracy: 0.99\n",
      "iteration no 8033: Loss: 0.23717379197613642, accuracy: 0.99\n",
      "iteration no 8034: Loss: 0.23717380916130237, accuracy: 0.99\n",
      "iteration no 8035: Loss: 0.23717295429627186, accuracy: 0.99\n",
      "iteration no 8036: Loss: 0.23717284434188418, accuracy: 0.99\n",
      "iteration no 8037: Loss: 0.23717256134098225, accuracy: 0.99\n",
      "iteration no 8038: Loss: 0.23717263761770543, accuracy: 0.99\n",
      "iteration no 8039: Loss: 0.2371712083172528, accuracy: 0.99\n",
      "iteration no 8040: Loss: 0.23717205451544981, accuracy: 0.99\n",
      "iteration no 8041: Loss: 0.23717104235159553, accuracy: 0.99\n",
      "iteration no 8042: Loss: 0.23717125210485268, accuracy: 0.99\n",
      "iteration no 8043: Loss: 0.2371701544439862, accuracy: 0.99\n",
      "iteration no 8044: Loss: 0.23717061055044422, accuracy: 0.99\n",
      "iteration no 8045: Loss: 0.23716991075704175, accuracy: 0.99\n",
      "iteration no 8046: Loss: 0.23716941740899283, accuracy: 0.99\n",
      "iteration no 8047: Loss: 0.23716951399893704, accuracy: 0.99\n",
      "iteration no 8048: Loss: 0.23716908221523164, accuracy: 0.99\n",
      "iteration no 8049: Loss: 0.23716868031930835, accuracy: 0.99\n",
      "iteration no 8050: Loss: 0.23716809103282782, accuracy: 0.99\n",
      "iteration no 8051: Loss: 0.23716837858657006, accuracy: 0.99\n",
      "iteration no 8052: Loss: 0.23716750979680973, accuracy: 0.99\n",
      "iteration no 8053: Loss: 0.237167726396923, accuracy: 0.99\n",
      "iteration no 8054: Loss: 0.23716683225290663, accuracy: 0.99\n",
      "iteration no 8055: Loss: 0.237167267531843, accuracy: 0.99\n",
      "iteration no 8056: Loss: 0.23716604932628987, accuracy: 0.99\n",
      "iteration no 8057: Loss: 0.23716640269016276, accuracy: 0.99\n",
      "iteration no 8058: Loss: 0.23716588903373365, accuracy: 0.99\n",
      "iteration no 8059: Loss: 0.23716576031994646, accuracy: 0.99\n",
      "iteration no 8060: Loss: 0.23716492676627854, accuracy: 0.99\n",
      "iteration no 8061: Loss: 0.23716522308422844, accuracy: 0.99\n",
      "iteration no 8062: Loss: 0.23716463354871636, accuracy: 0.99\n",
      "iteration no 8063: Loss: 0.23716424086391294, accuracy: 0.99\n",
      "iteration no 8064: Loss: 0.23716406964099, accuracy: 0.99\n",
      "iteration no 8065: Loss: 0.23716386196937794, accuracy: 0.99\n",
      "iteration no 8066: Loss: 0.23716349679360843, accuracy: 0.99\n",
      "iteration no 8067: Loss: 0.23716260758968413, accuracy: 0.99\n",
      "iteration no 8068: Loss: 0.2371634369625722, accuracy: 0.99\n",
      "iteration no 8069: Loss: 0.2371622245966989, accuracy: 0.99\n",
      "iteration no 8070: Loss: 0.2371623811489723, accuracy: 0.99\n",
      "iteration no 8071: Loss: 0.23716149082168925, accuracy: 0.99\n",
      "iteration no 8072: Loss: 0.23716204045408174, accuracy: 0.99\n",
      "iteration no 8073: Loss: 0.23716093336956756, accuracy: 0.99\n",
      "iteration no 8074: Loss: 0.237160979505111, accuracy: 0.99\n",
      "iteration no 8075: Loss: 0.23716077573126754, accuracy: 0.99\n",
      "iteration no 8076: Loss: 0.2371605524350815, accuracy: 0.99\n",
      "iteration no 8077: Loss: 0.23715970191656355, accuracy: 0.99\n",
      "iteration no 8078: Loss: 0.23715980482557422, accuracy: 0.99\n",
      "iteration no 8079: Loss: 0.23715989359843098, accuracy: 0.99\n",
      "iteration no 8080: Loss: 0.23715906557308858, accuracy: 0.99\n",
      "iteration no 8081: Loss: 0.23715870780299225, accuracy: 0.99\n",
      "iteration no 8082: Loss: 0.23715853929021496, accuracy: 0.99\n",
      "iteration no 8083: Loss: 0.23715857355180567, accuracy: 0.99\n",
      "iteration no 8084: Loss: 0.23715736129824666, accuracy: 0.99\n",
      "iteration no 8085: Loss: 0.2371580647873509, accuracy: 0.99\n",
      "iteration no 8086: Loss: 0.23715709877459124, accuracy: 0.99\n",
      "iteration no 8087: Loss: 0.23715719610866565, accuracy: 0.99\n",
      "iteration no 8088: Loss: 0.23715620096506473, accuracy: 0.99\n",
      "iteration no 8089: Loss: 0.23715707258936175, accuracy: 0.99\n",
      "iteration no 8090: Loss: 0.23715585419518598, accuracy: 0.99\n",
      "iteration no 8091: Loss: 0.23715572194190743, accuracy: 0.99\n",
      "iteration no 8092: Loss: 0.23715544216087425, accuracy: 0.99\n",
      "iteration no 8093: Loss: 0.23715532888415336, accuracy: 0.99\n",
      "iteration no 8094: Loss: 0.23715482387837306, accuracy: 0.99\n",
      "iteration no 8095: Loss: 0.237154608999193, accuracy: 0.99\n",
      "iteration no 8096: Loss: 0.2371545347612044, accuracy: 0.99\n",
      "iteration no 8097: Loss: 0.2371536708810531, accuracy: 0.99\n",
      "iteration no 8098: Loss: 0.237153551458756, accuracy: 0.99\n",
      "iteration no 8099: Loss: 0.2371532231317809, accuracy: 0.99\n",
      "iteration no 8100: Loss: 0.2371534797183345, accuracy: 0.99\n",
      "iteration no 8101: Loss: 0.23715222572813402, accuracy: 0.99\n",
      "iteration no 8102: Loss: 0.23715280400528363, accuracy: 0.99\n",
      "iteration no 8103: Loss: 0.23715178512914098, accuracy: 0.99\n",
      "iteration no 8104: Loss: 0.2371521144480941, accuracy: 0.99\n",
      "iteration no 8105: Loss: 0.23715114341182042, accuracy: 0.99\n",
      "iteration no 8106: Loss: 0.2371516587987272, accuracy: 0.99\n",
      "iteration no 8107: Loss: 0.23715065647098316, accuracy: 0.99\n",
      "iteration no 8108: Loss: 0.23715041956617655, accuracy: 0.99\n",
      "iteration no 8109: Loss: 0.23715047101883277, accuracy: 0.99\n",
      "iteration no 8110: Loss: 0.23715017052526105, accuracy: 0.99\n",
      "iteration no 8111: Loss: 0.2371496864185442, accuracy: 0.99\n",
      "iteration no 8112: Loss: 0.23714925837693734, accuracy: 0.99\n",
      "iteration no 8113: Loss: 0.2371492403113522, accuracy: 0.99\n",
      "iteration no 8114: Loss: 0.23714844788560369, accuracy: 0.99\n",
      "iteration no 8115: Loss: 0.23714868899978764, accuracy: 0.99\n",
      "iteration no 8116: Loss: 0.23714804102576817, accuracy: 0.99\n",
      "iteration no 8117: Loss: 0.23714803817593255, accuracy: 0.99\n",
      "iteration no 8118: Loss: 0.2371468851928228, accuracy: 0.99\n",
      "iteration no 8119: Loss: 0.23714790388792178, accuracy: 0.99\n",
      "iteration no 8120: Loss: 0.2371467748251525, accuracy: 0.99\n",
      "iteration no 8121: Loss: 0.23714688001804368, accuracy: 0.99\n",
      "iteration no 8122: Loss: 0.23714607646030011, accuracy: 0.99\n",
      "iteration no 8123: Loss: 0.23714636894777769, accuracy: 0.99\n",
      "iteration no 8124: Loss: 0.23714549117297934, accuracy: 0.99\n",
      "iteration no 8125: Loss: 0.23714550534815187, accuracy: 0.99\n",
      "iteration no 8126: Loss: 0.2371454596263559, accuracy: 0.99\n",
      "iteration no 8127: Loss: 0.23714477686892543, accuracy: 0.99\n",
      "iteration no 8128: Loss: 0.2371442784028847, accuracy: 0.99\n",
      "iteration no 8129: Loss: 0.2371443343854721, accuracy: 0.99\n",
      "iteration no 8130: Loss: 0.23714428529548331, accuracy: 0.99\n",
      "iteration no 8131: Loss: 0.23714321031900717, accuracy: 0.99\n",
      "iteration no 8132: Loss: 0.23714349681179137, accuracy: 0.99\n",
      "iteration no 8133: Loss: 0.23714282376852097, accuracy: 0.99\n",
      "iteration no 8134: Loss: 0.23714299463485605, accuracy: 0.99\n",
      "iteration no 8135: Loss: 0.23714196364524076, accuracy: 0.99\n",
      "iteration no 8136: Loss: 0.23714288672626413, accuracy: 0.99\n",
      "iteration no 8137: Loss: 0.2371413225275088, accuracy: 0.99\n",
      "iteration no 8138: Loss: 0.2371416731246339, accuracy: 0.99\n",
      "iteration no 8139: Loss: 0.2371412466769418, accuracy: 0.99\n",
      "iteration no 8140: Loss: 0.237141190693846, accuracy: 0.99\n",
      "iteration no 8141: Loss: 0.23714049984456714, accuracy: 0.99\n",
      "iteration no 8142: Loss: 0.23714071228134026, accuracy: 0.99\n",
      "iteration no 8143: Loss: 0.23713983897280638, accuracy: 0.99\n",
      "iteration no 8144: Loss: 0.23713974786198794, accuracy: 0.99\n",
      "iteration no 8145: Loss: 0.23713988555232657, accuracy: 0.99\n",
      "iteration no 8146: Loss: 0.23713893444789835, accuracy: 0.99\n",
      "iteration no 8147: Loss: 0.23713908763135144, accuracy: 0.99\n",
      "iteration no 8148: Loss: 0.23713859136842513, accuracy: 0.99\n",
      "iteration no 8149: Loss: 0.23713853275833033, accuracy: 0.99\n",
      "iteration no 8150: Loss: 0.23713771600972253, accuracy: 0.99\n",
      "iteration no 8151: Loss: 0.23713836899948346, accuracy: 0.99\n",
      "iteration no 8152: Loss: 0.2371368197767259, accuracy: 0.99\n",
      "iteration no 8153: Loss: 0.23713751820468787, accuracy: 0.99\n",
      "iteration no 8154: Loss: 0.2371367160049379, accuracy: 0.99\n",
      "iteration no 8155: Loss: 0.23713679100104434, accuracy: 0.99\n",
      "iteration no 8156: Loss: 0.23713608876829825, accuracy: 0.99\n",
      "iteration no 8157: Loss: 0.2371363576081772, accuracy: 0.99\n",
      "iteration no 8158: Loss: 0.23713525849834138, accuracy: 0.99\n",
      "iteration no 8159: Loss: 0.2371356702291275, accuracy: 0.99\n",
      "iteration no 8160: Loss: 0.23713533965283598, accuracy: 0.99\n",
      "iteration no 8161: Loss: 0.23713452322450207, accuracy: 0.99\n",
      "iteration no 8162: Loss: 0.23713474742454754, accuracy: 0.99\n",
      "iteration no 8163: Loss: 0.23713429240860734, accuracy: 0.99\n",
      "iteration no 8164: Loss: 0.23713393178956474, accuracy: 0.99\n",
      "iteration no 8165: Loss: 0.23713368158184556, accuracy: 0.99\n",
      "iteration no 8166: Loss: 0.23713379783091487, accuracy: 0.99\n",
      "iteration no 8167: Loss: 0.2371325344789519, accuracy: 0.99\n",
      "iteration no 8168: Loss: 0.23713343040712795, accuracy: 0.99\n",
      "iteration no 8169: Loss: 0.23713207488013044, accuracy: 0.99\n",
      "iteration no 8170: Loss: 0.2371327668789494, accuracy: 0.99\n",
      "iteration no 8171: Loss: 0.23713183004401409, accuracy: 0.99\n",
      "iteration no 8172: Loss: 0.23713173154141462, accuracy: 0.99\n",
      "iteration no 8173: Loss: 0.23713142107985763, accuracy: 0.99\n",
      "iteration no 8174: Loss: 0.23713148404635484, accuracy: 0.99\n",
      "iteration no 8175: Loss: 0.23713043211183124, accuracy: 0.99\n",
      "iteration no 8176: Loss: 0.23713094570583493, accuracy: 0.99\n",
      "iteration no 8177: Loss: 0.2371302015967562, accuracy: 0.99\n",
      "iteration no 8178: Loss: 0.23713009953829767, accuracy: 0.99\n",
      "iteration no 8179: Loss: 0.23712987535226587, accuracy: 0.99\n",
      "iteration no 8180: Loss: 0.237128974035533, accuracy: 0.99\n",
      "iteration no 8181: Loss: 0.237129697188505, accuracy: 0.99\n",
      "iteration no 8182: Loss: 0.2371284470492767, accuracy: 0.99\n",
      "iteration no 8183: Loss: 0.23712885768967523, accuracy: 0.99\n",
      "iteration no 8184: Loss: 0.23712812278527118, accuracy: 0.99\n",
      "iteration no 8185: Loss: 0.23712824792179213, accuracy: 0.99\n",
      "iteration no 8186: Loss: 0.23712754017319043, accuracy: 0.99\n",
      "iteration no 8187: Loss: 0.2371277555869717, accuracy: 0.99\n",
      "iteration no 8188: Loss: 0.23712711981230328, accuracy: 0.99\n",
      "iteration no 8189: Loss: 0.2371271367169288, accuracy: 0.99\n",
      "iteration no 8190: Loss: 0.23712627469389866, accuracy: 0.99\n",
      "iteration no 8191: Loss: 0.23712675277437156, accuracy: 0.99\n",
      "iteration no 8192: Loss: 0.23712573092015501, accuracy: 0.99\n",
      "iteration no 8193: Loss: 0.23712619748419272, accuracy: 0.99\n",
      "iteration no 8194: Loss: 0.23712531976700768, accuracy: 0.99\n",
      "iteration no 8195: Loss: 0.23712523129411262, accuracy: 0.99\n",
      "iteration no 8196: Loss: 0.2371251001343781, accuracy: 0.99\n",
      "iteration no 8197: Loss: 0.23712428968216884, accuracy: 0.99\n",
      "iteration no 8198: Loss: 0.2371248887105305, accuracy: 0.99\n",
      "iteration no 8199: Loss: 0.23712364900943894, accuracy: 0.99\n",
      "iteration no 8200: Loss: 0.23712426855349636, accuracy: 0.99\n",
      "iteration no 8201: Loss: 0.23712320929486203, accuracy: 0.99\n",
      "iteration no 8202: Loss: 0.23712371825211911, accuracy: 0.99\n",
      "iteration no 8203: Loss: 0.23712280872580863, accuracy: 0.99\n",
      "iteration no 8204: Loss: 0.23712284157125268, accuracy: 0.99\n",
      "iteration no 8205: Loss: 0.23712236297200964, accuracy: 0.99\n",
      "iteration no 8206: Loss: 0.23712242026909575, accuracy: 0.99\n",
      "iteration no 8207: Loss: 0.23712181361039958, accuracy: 0.99\n",
      "iteration no 8208: Loss: 0.23712187093243692, accuracy: 0.99\n",
      "iteration no 8209: Loss: 0.23712098717937366, accuracy: 0.99\n",
      "iteration no 8210: Loss: 0.23712136829333702, accuracy: 0.99\n",
      "iteration no 8211: Loss: 0.23712059979365874, accuracy: 0.99\n",
      "iteration no 8212: Loss: 0.23712064336189942, accuracy: 0.99\n",
      "iteration no 8213: Loss: 0.2371203579625734, accuracy: 0.99\n",
      "iteration no 8214: Loss: 0.2371196308214159, accuracy: 0.99\n",
      "iteration no 8215: Loss: 0.23712009369122278, accuracy: 0.99\n",
      "iteration no 8216: Loss: 0.23711900568081018, accuracy: 0.99\n",
      "iteration no 8217: Loss: 0.23711987481837832, accuracy: 0.99\n",
      "iteration no 8218: Loss: 0.23711839541458524, accuracy: 0.99\n",
      "iteration no 8219: Loss: 0.23711894182877336, accuracy: 0.99\n",
      "iteration no 8220: Loss: 0.23711810310552056, accuracy: 0.99\n",
      "iteration no 8221: Loss: 0.2371182083546537, accuracy: 0.99\n",
      "iteration no 8222: Loss: 0.23711777901779107, accuracy: 0.99\n",
      "iteration no 8223: Loss: 0.23711743730798646, accuracy: 0.99\n",
      "iteration no 8224: Loss: 0.23711724436762088, accuracy: 0.99\n",
      "iteration no 8225: Loss: 0.23711709228357897, accuracy: 0.99\n",
      "iteration no 8226: Loss: 0.23711653838599747, accuracy: 0.99\n",
      "iteration no 8227: Loss: 0.23711651166315068, accuracy: 0.99\n",
      "iteration no 8228: Loss: 0.23711591933204562, accuracy: 0.99\n",
      "iteration no 8229: Loss: 0.23711611563190668, accuracy: 0.99\n",
      "iteration no 8230: Loss: 0.23711561250045293, accuracy: 0.99\n",
      "iteration no 8231: Loss: 0.23711516659930273, accuracy: 0.99\n",
      "iteration no 8232: Loss: 0.23711553550131978, accuracy: 0.99\n",
      "iteration no 8233: Loss: 0.23711428993140823, accuracy: 0.99\n",
      "iteration no 8234: Loss: 0.2371149913847204, accuracy: 0.99\n",
      "iteration no 8235: Loss: 0.23711360828043504, accuracy: 0.99\n",
      "iteration no 8236: Loss: 0.23711453955355216, accuracy: 0.99\n",
      "iteration no 8237: Loss: 0.23711332533033036, accuracy: 0.99\n",
      "iteration no 8238: Loss: 0.23711364959154974, accuracy: 0.99\n",
      "iteration no 8239: Loss: 0.23711309775021275, accuracy: 0.99\n",
      "iteration no 8240: Loss: 0.23711271778723383, accuracy: 0.99\n",
      "iteration no 8241: Loss: 0.2371129199616353, accuracy: 0.99\n",
      "iteration no 8242: Loss: 0.23711228473849594, accuracy: 0.99\n",
      "iteration no 8243: Loss: 0.23711234125524938, accuracy: 0.99\n",
      "iteration no 8244: Loss: 0.23711159250414193, accuracy: 0.99\n",
      "iteration no 8245: Loss: 0.23711166073295142, accuracy: 0.99\n",
      "iteration no 8246: Loss: 0.2371114205421292, accuracy: 0.99\n",
      "iteration no 8247: Loss: 0.23711101918527663, accuracy: 0.99\n",
      "iteration no 8248: Loss: 0.23711065623705965, accuracy: 0.99\n",
      "iteration no 8249: Loss: 0.237110718779957, accuracy: 0.99\n",
      "iteration no 8250: Loss: 0.23710994854248707, accuracy: 0.99\n",
      "iteration no 8251: Loss: 0.237110423396377, accuracy: 0.99\n",
      "iteration no 8252: Loss: 0.23710913427563685, accuracy: 0.99\n",
      "iteration no 8253: Loss: 0.23710991851259988, accuracy: 0.99\n",
      "iteration no 8254: Loss: 0.23710870661931543, accuracy: 0.99\n",
      "iteration no 8255: Loss: 0.23710912080903512, accuracy: 0.99\n",
      "iteration no 8256: Loss: 0.23710852291553738, accuracy: 0.99\n",
      "iteration no 8257: Loss: 0.23710832032705279, accuracy: 0.99\n",
      "iteration no 8258: Loss: 0.2371082651744651, accuracy: 0.99\n",
      "iteration no 8259: Loss: 0.23710734846818926, accuracy: 0.99\n",
      "iteration no 8260: Loss: 0.23710798856035323, accuracy: 0.99\n",
      "iteration no 8261: Loss: 0.2371068053243856, accuracy: 0.99\n",
      "iteration no 8262: Loss: 0.2371075083593951, accuracy: 0.99\n",
      "iteration no 8263: Loss: 0.23710632994284253, accuracy: 0.99\n",
      "iteration no 8264: Loss: 0.23710674818473887, accuracy: 0.99\n",
      "iteration no 8265: Loss: 0.23710585560688585, accuracy: 0.99\n",
      "iteration no 8266: Loss: 0.2371060052149119, accuracy: 0.99\n",
      "iteration no 8267: Loss: 0.23710562924329837, accuracy: 0.99\n",
      "iteration no 8268: Loss: 0.23710538166984163, accuracy: 0.99\n",
      "iteration no 8269: Loss: 0.23710498260219076, accuracy: 0.99\n",
      "iteration no 8270: Loss: 0.2371049456534261, accuracy: 0.99\n",
      "iteration no 8271: Loss: 0.23710451305031266, accuracy: 0.99\n",
      "iteration no 8272: Loss: 0.23710438064243977, accuracy: 0.99\n",
      "iteration no 8273: Loss: 0.2371037497539373, accuracy: 0.99\n",
      "iteration no 8274: Loss: 0.23710370260417887, accuracy: 0.99\n",
      "iteration no 8275: Loss: 0.23710347484355226, accuracy: 0.99\n",
      "iteration no 8276: Loss: 0.2371028266841522, accuracy: 0.99\n",
      "iteration no 8277: Loss: 0.23710320160596524, accuracy: 0.99\n",
      "iteration no 8278: Loss: 0.23710186189365107, accuracy: 0.99\n",
      "iteration no 8279: Loss: 0.23710286617441687, accuracy: 0.99\n",
      "iteration no 8280: Loss: 0.23710132131862893, accuracy: 0.99\n",
      "iteration no 8281: Loss: 0.23710222015858232, accuracy: 0.99\n",
      "iteration no 8282: Loss: 0.23710118511778888, accuracy: 0.99\n",
      "iteration no 8283: Loss: 0.2371012853735237, accuracy: 0.99\n",
      "iteration no 8284: Loss: 0.23710089459472825, accuracy: 0.99\n",
      "iteration no 8285: Loss: 0.23710028987932913, accuracy: 0.99\n",
      "iteration no 8286: Loss: 0.23710063478433768, accuracy: 0.99\n",
      "iteration no 8287: Loss: 0.23709993601990637, accuracy: 0.99\n",
      "iteration no 8288: Loss: 0.23709985175211262, accuracy: 0.99\n",
      "iteration no 8289: Loss: 0.23709933696263624, accuracy: 0.99\n",
      "iteration no 8290: Loss: 0.2370991721060569, accuracy: 0.99\n",
      "iteration no 8291: Loss: 0.23709903090549583, accuracy: 0.99\n",
      "iteration no 8292: Loss: 0.23709878684521016, accuracy: 0.99\n",
      "iteration no 8293: Loss: 0.2370981526208058, accuracy: 0.99\n",
      "iteration no 8294: Loss: 0.23709854569524713, accuracy: 0.99\n",
      "iteration no 8295: Loss: 0.2370973264717523, accuracy: 0.99\n",
      "iteration no 8296: Loss: 0.23709812288243193, accuracy: 0.99\n",
      "iteration no 8297: Loss: 0.2370968063430134, accuracy: 0.99\n",
      "iteration no 8298: Loss: 0.2370975782316028, accuracy: 0.99\n",
      "iteration no 8299: Loss: 0.23709657959388905, accuracy: 0.99\n",
      "iteration no 8300: Loss: 0.23709651520388397, accuracy: 0.99\n",
      "iteration no 8301: Loss: 0.23709644131472543, accuracy: 0.99\n",
      "iteration no 8302: Loss: 0.23709561457517836, accuracy: 0.99\n",
      "iteration no 8303: Loss: 0.23709612219725032, accuracy: 0.99\n",
      "iteration no 8304: Loss: 0.23709494035584766, accuracy: 0.99\n",
      "iteration no 8305: Loss: 0.23709555407226873, accuracy: 0.99\n",
      "iteration no 8306: Loss: 0.2370945020097426, accuracy: 0.99\n",
      "iteration no 8307: Loss: 0.23709502457257553, accuracy: 0.99\n",
      "iteration no 8308: Loss: 0.2370940859644945, accuracy: 0.99\n",
      "iteration no 8309: Loss: 0.2370941166069481, accuracy: 0.99\n",
      "iteration no 8310: Loss: 0.23709387473440918, accuracy: 0.99\n",
      "iteration no 8311: Loss: 0.23709366545255411, accuracy: 0.99\n",
      "iteration no 8312: Loss: 0.2370932904514337, accuracy: 0.99\n",
      "iteration no 8313: Loss: 0.237093218802355, accuracy: 0.99\n",
      "iteration no 8314: Loss: 0.23709260685847622, accuracy: 0.99\n",
      "iteration no 8315: Loss: 0.23709266238458077, accuracy: 0.99\n",
      "iteration no 8316: Loss: 0.23709208629779777, accuracy: 0.99\n",
      "iteration no 8317: Loss: 0.2370921156233923, accuracy: 0.99\n",
      "iteration no 8318: Loss: 0.2370919142038761, accuracy: 0.99\n",
      "iteration no 8319: Loss: 0.23709110842568354, accuracy: 0.99\n",
      "iteration no 8320: Loss: 0.23709152362910918, accuracy: 0.99\n",
      "iteration no 8321: Loss: 0.23709012589434497, accuracy: 0.99\n",
      "iteration no 8322: Loss: 0.23709129999354278, accuracy: 0.99\n",
      "iteration no 8323: Loss: 0.2370898015816901, accuracy: 0.99\n",
      "iteration no 8324: Loss: 0.23709067512212048, accuracy: 0.99\n",
      "iteration no 8325: Loss: 0.23708953440994895, accuracy: 0.99\n",
      "iteration no 8326: Loss: 0.23708960627106526, accuracy: 0.99\n",
      "iteration no 8327: Loss: 0.23708938836429674, accuracy: 0.99\n",
      "iteration no 8328: Loss: 0.2370887905445937, accuracy: 0.99\n",
      "iteration no 8329: Loss: 0.23708908600227172, accuracy: 0.99\n",
      "iteration no 8330: Loss: 0.23708819534494768, accuracy: 0.99\n",
      "iteration no 8331: Loss: 0.23708849138274926, accuracy: 0.99\n",
      "iteration no 8332: Loss: 0.2370877695109887, accuracy: 0.99\n",
      "iteration no 8333: Loss: 0.23708784922290954, accuracy: 0.99\n",
      "iteration no 8334: Loss: 0.2370873609687688, accuracy: 0.99\n",
      "iteration no 8335: Loss: 0.23708736542490733, accuracy: 0.99\n",
      "iteration no 8336: Loss: 0.23708674203083963, accuracy: 0.99\n",
      "iteration no 8337: Loss: 0.23708697037437143, accuracy: 0.99\n",
      "iteration no 8338: Loss: 0.23708610445920764, accuracy: 0.99\n",
      "iteration no 8339: Loss: 0.23708652426727644, accuracy: 0.99\n",
      "iteration no 8340: Loss: 0.2370855407565836, accuracy: 0.99\n",
      "iteration no 8341: Loss: 0.23708595661795245, accuracy: 0.99\n",
      "iteration no 8342: Loss: 0.23708496319623112, accuracy: 0.99\n",
      "iteration no 8343: Loss: 0.2370853393663347, accuracy: 0.99\n",
      "iteration no 8344: Loss: 0.23708490201708698, accuracy: 0.99\n",
      "iteration no 8345: Loss: 0.23708436085279833, accuracy: 0.99\n",
      "iteration no 8346: Loss: 0.23708449883609292, accuracy: 0.99\n",
      "iteration no 8347: Loss: 0.2370836172116726, accuracy: 0.99\n",
      "iteration no 8348: Loss: 0.23708447841664873, accuracy: 0.99\n",
      "iteration no 8349: Loss: 0.23708290788471803, accuracy: 0.99\n",
      "iteration no 8350: Loss: 0.23708388164434002, accuracy: 0.99\n",
      "iteration no 8351: Loss: 0.23708260552171923, accuracy: 0.99\n",
      "iteration no 8352: Loss: 0.23708299940790406, accuracy: 0.99\n",
      "iteration no 8353: Loss: 0.23708231988251965, accuracy: 0.99\n",
      "iteration no 8354: Loss: 0.23708218717832993, accuracy: 0.99\n",
      "iteration no 8355: Loss: 0.2370822091079684, accuracy: 0.99\n",
      "iteration no 8356: Loss: 0.2370814900856395, accuracy: 0.99\n",
      "iteration no 8357: Loss: 0.23708157243656025, accuracy: 0.99\n",
      "iteration no 8358: Loss: 0.23708107577183768, accuracy: 0.99\n",
      "iteration no 8359: Loss: 0.23708119647844844, accuracy: 0.99\n",
      "iteration no 8360: Loss: 0.23708066907414727, accuracy: 0.99\n",
      "iteration no 8361: Loss: 0.2370803356498991, accuracy: 0.99\n",
      "iteration no 8362: Loss: 0.23708010191155265, accuracy: 0.99\n",
      "iteration no 8363: Loss: 0.2370800501545569, accuracy: 0.99\n",
      "iteration no 8364: Loss: 0.23707947941865332, accuracy: 0.99\n",
      "iteration no 8365: Loss: 0.2370797465989136, accuracy: 0.99\n",
      "iteration no 8366: Loss: 0.23707872469902386, accuracy: 0.99\n",
      "iteration no 8367: Loss: 0.23707927642583934, accuracy: 0.99\n",
      "iteration no 8368: Loss: 0.23707819891318038, accuracy: 0.99\n",
      "iteration no 8369: Loss: 0.2370787591342024, accuracy: 0.99\n",
      "iteration no 8370: Loss: 0.23707779463130035, accuracy: 0.99\n",
      "iteration no 8371: Loss: 0.23707800113518823, accuracy: 0.99\n",
      "iteration no 8372: Loss: 0.23707755401396938, accuracy: 0.99\n",
      "iteration no 8373: Loss: 0.237077170973841, accuracy: 0.99\n",
      "iteration no 8374: Loss: 0.23707746781565742, accuracy: 0.99\n",
      "iteration no 8375: Loss: 0.23707630181452782, accuracy: 0.99\n",
      "iteration no 8376: Loss: 0.23707705909731275, accuracy: 0.99\n",
      "iteration no 8377: Loss: 0.2370756109488748, accuracy: 0.99\n",
      "iteration no 8378: Loss: 0.2370767995614408, accuracy: 0.99\n",
      "iteration no 8379: Loss: 0.23707540588609557, accuracy: 0.99\n",
      "iteration no 8380: Loss: 0.2370759198242483, accuracy: 0.99\n",
      "iteration no 8381: Loss: 0.2370750440566898, accuracy: 0.99\n",
      "iteration no 8382: Loss: 0.23707497921243864, accuracy: 0.99\n",
      "iteration no 8383: Loss: 0.23707494885107272, accuracy: 0.99\n",
      "iteration no 8384: Loss: 0.2370744312383685, accuracy: 0.99\n",
      "iteration no 8385: Loss: 0.237074522970454, accuracy: 0.99\n",
      "iteration no 8386: Loss: 0.23707378488287484, accuracy: 0.99\n",
      "iteration no 8387: Loss: 0.23707400770542778, accuracy: 0.99\n",
      "iteration no 8388: Loss: 0.23707340361340562, accuracy: 0.99\n",
      "iteration no 8389: Loss: 0.23707344747044998, accuracy: 0.99\n",
      "iteration no 8390: Loss: 0.23707300412933902, accuracy: 0.99\n",
      "iteration no 8391: Loss: 0.23707290068444622, accuracy: 0.99\n",
      "iteration no 8392: Loss: 0.23707237634589734, accuracy: 0.99\n",
      "iteration no 8393: Loss: 0.2370724382401454, accuracy: 0.99\n",
      "iteration no 8394: Loss: 0.23707197878298825, accuracy: 0.99\n",
      "iteration no 8395: Loss: 0.23707213225840085, accuracy: 0.99\n",
      "iteration no 8396: Loss: 0.2370712772993368, accuracy: 0.99\n",
      "iteration no 8397: Loss: 0.23707149119658838, accuracy: 0.99\n",
      "iteration no 8398: Loss: 0.23707083948393443, accuracy: 0.99\n",
      "iteration no 8399: Loss: 0.23707107430383134, accuracy: 0.99\n",
      "iteration no 8400: Loss: 0.23707037575634038, accuracy: 0.99\n",
      "iteration no 8401: Loss: 0.23707028416812642, accuracy: 0.99\n",
      "iteration no 8402: Loss: 0.23707025220289185, accuracy: 0.99\n",
      "iteration no 8403: Loss: 0.23706943845727513, accuracy: 0.99\n",
      "iteration no 8404: Loss: 0.23706994739423792, accuracy: 0.99\n",
      "iteration no 8405: Loss: 0.23706861464238155, accuracy: 0.99\n",
      "iteration no 8406: Loss: 0.23706969008160428, accuracy: 0.99\n",
      "iteration no 8407: Loss: 0.237068115891647, accuracy: 0.99\n",
      "iteration no 8408: Loss: 0.2370691299476274, accuracy: 0.99\n",
      "iteration no 8409: Loss: 0.2370679606280141, accuracy: 0.99\n",
      "iteration no 8410: Loss: 0.23706829168717125, accuracy: 0.99\n",
      "iteration no 8411: Loss: 0.23706772677380866, accuracy: 0.99\n",
      "iteration no 8412: Loss: 0.23706738849660997, accuracy: 0.99\n",
      "iteration no 8413: Loss: 0.23706749038659425, accuracy: 0.99\n",
      "iteration no 8414: Loss: 0.2370666721898823, accuracy: 0.99\n",
      "iteration no 8415: Loss: 0.23706725839897708, accuracy: 0.99\n",
      "iteration no 8416: Loss: 0.23706603830309048, accuracy: 0.99\n",
      "iteration no 8417: Loss: 0.2370668102470784, accuracy: 0.99\n",
      "iteration no 8418: Loss: 0.23706563179668155, accuracy: 0.99\n",
      "iteration no 8419: Loss: 0.23706617834161497, accuracy: 0.99\n",
      "iteration no 8420: Loss: 0.23706525468664041, accuracy: 0.99\n",
      "iteration no 8421: Loss: 0.23706564420590776, accuracy: 0.99\n",
      "iteration no 8422: Loss: 0.2370649869200801, accuracy: 0.99\n",
      "iteration no 8423: Loss: 0.23706467411684634, accuracy: 0.99\n",
      "iteration no 8424: Loss: 0.2370647318212144, accuracy: 0.99\n",
      "iteration no 8425: Loss: 0.23706435669673592, accuracy: 0.99\n",
      "iteration no 8426: Loss: 0.2370642748257536, accuracy: 0.99\n",
      "iteration no 8427: Loss: 0.23706381049856332, accuracy: 0.99\n",
      "iteration no 8428: Loss: 0.2370636338130716, accuracy: 0.99\n",
      "iteration no 8429: Loss: 0.23706325449023313, accuracy: 0.99\n",
      "iteration no 8430: Loss: 0.2370631039794986, accuracy: 0.99\n",
      "iteration no 8431: Loss: 0.23706277696206773, accuracy: 0.99\n",
      "iteration no 8432: Loss: 0.2370626486447314, accuracy: 0.99\n",
      "iteration no 8433: Loss: 0.23706213265606807, accuracy: 0.99\n",
      "iteration no 8434: Loss: 0.237062453005151, accuracy: 0.99\n",
      "iteration no 8435: Loss: 0.23706161273633494, accuracy: 0.99\n",
      "iteration no 8436: Loss: 0.2370619356559472, accuracy: 0.99\n",
      "iteration no 8437: Loss: 0.23706105451478465, accuracy: 0.99\n",
      "iteration no 8438: Loss: 0.23706136932881802, accuracy: 0.99\n",
      "iteration no 8439: Loss: 0.23706048055822132, accuracy: 0.99\n",
      "iteration no 8440: Loss: 0.23706106556280204, accuracy: 0.99\n",
      "iteration no 8441: Loss: 0.2370604211417361, accuracy: 0.99\n",
      "iteration no 8442: Loss: 0.23705998663185107, accuracy: 0.99\n",
      "iteration no 8443: Loss: 0.23706004250364776, accuracy: 0.99\n",
      "iteration no 8444: Loss: 0.23705931392256985, accuracy: 0.99\n",
      "iteration no 8445: Loss: 0.23705992648289564, accuracy: 0.99\n",
      "iteration no 8446: Loss: 0.23705837940334923, accuracy: 0.99\n",
      "iteration no 8447: Loss: 0.23705957583779363, accuracy: 0.99\n",
      "iteration no 8448: Loss: 0.2370580903785926, accuracy: 0.99\n",
      "iteration no 8449: Loss: 0.2370590932275805, accuracy: 0.99\n",
      "iteration no 8450: Loss: 0.23705789335064942, accuracy: 0.99\n",
      "iteration no 8451: Loss: 0.2370581429380016, accuracy: 0.99\n",
      "iteration no 8452: Loss: 0.23705756767625863, accuracy: 0.99\n",
      "iteration no 8453: Loss: 0.2370573029819693, accuracy: 0.99\n",
      "iteration no 8454: Loss: 0.23705755645139476, accuracy: 0.99\n",
      "iteration no 8455: Loss: 0.23705660395675027, accuracy: 0.99\n",
      "iteration no 8456: Loss: 0.2370571917414983, accuracy: 0.99\n",
      "iteration no 8457: Loss: 0.23705584486291942, accuracy: 0.99\n",
      "iteration no 8458: Loss: 0.23705668801052504, accuracy: 0.99\n",
      "iteration no 8459: Loss: 0.23705554729809603, accuracy: 0.99\n",
      "iteration no 8460: Loss: 0.2370562634728607, accuracy: 0.99\n",
      "iteration no 8461: Loss: 0.23705513083046004, accuracy: 0.99\n",
      "iteration no 8462: Loss: 0.23705538636689816, accuracy: 0.99\n",
      "iteration no 8463: Loss: 0.2370550475044776, accuracy: 0.99\n",
      "iteration no 8464: Loss: 0.23705475509512666, accuracy: 0.99\n",
      "iteration no 8465: Loss: 0.2370547064984388, accuracy: 0.99\n",
      "iteration no 8466: Loss: 0.2370540147092421, accuracy: 0.99\n",
      "iteration no 8467: Loss: 0.23705430615558515, accuracy: 0.99\n",
      "iteration no 8468: Loss: 0.23705369778534646, accuracy: 0.99\n",
      "iteration no 8469: Loss: 0.23705384297829257, accuracy: 0.99\n",
      "iteration no 8470: Loss: 0.2370532411117906, accuracy: 0.99\n",
      "iteration no 8471: Loss: 0.23705319532867608, accuracy: 0.99\n",
      "iteration no 8472: Loss: 0.2370525963256445, accuracy: 0.99\n",
      "iteration no 8473: Loss: 0.23705292344779935, accuracy: 0.99\n",
      "iteration no 8474: Loss: 0.23705245516081522, accuracy: 0.99\n",
      "iteration no 8475: Loss: 0.2370523124777803, accuracy: 0.99\n",
      "iteration no 8476: Loss: 0.23705174938836326, accuracy: 0.99\n",
      "iteration no 8477: Loss: 0.23705171649267737, accuracy: 0.99\n",
      "iteration no 8478: Loss: 0.2370515447780855, accuracy: 0.99\n",
      "iteration no 8479: Loss: 0.2370513778528559, accuracy: 0.99\n",
      "iteration no 8480: Loss: 0.23705082400227057, accuracy: 0.99\n",
      "iteration no 8481: Loss: 0.2370507936220808, accuracy: 0.99\n",
      "iteration no 8482: Loss: 0.23705042476364546, accuracy: 0.99\n",
      "iteration no 8483: Loss: 0.2370505431235297, accuracy: 0.99\n",
      "iteration no 8484: Loss: 0.2370501364787149, accuracy: 0.99\n",
      "iteration no 8485: Loss: 0.2370494967054616, accuracy: 0.99\n",
      "iteration no 8486: Loss: 0.2370497847172292, accuracy: 0.99\n",
      "iteration no 8487: Loss: 0.23704892486091866, accuracy: 0.99\n",
      "iteration no 8488: Loss: 0.23704949819288984, accuracy: 0.99\n",
      "iteration no 8489: Loss: 0.23704840907683822, accuracy: 0.99\n",
      "iteration no 8490: Loss: 0.23704893187229553, accuracy: 0.99\n",
      "iteration no 8491: Loss: 0.23704780697907585, accuracy: 0.99\n",
      "iteration no 8492: Loss: 0.2370483581741652, accuracy: 0.99\n",
      "iteration no 8493: Loss: 0.23704797688611107, accuracy: 0.99\n",
      "iteration no 8494: Loss: 0.2370477607038365, accuracy: 0.99\n",
      "iteration no 8495: Loss: 0.23704735723517043, accuracy: 0.99\n",
      "iteration no 8496: Loss: 0.23704684522237263, accuracy: 0.99\n",
      "iteration no 8497: Loss: 0.23704726416963817, accuracy: 0.99\n",
      "iteration no 8498: Loss: 0.23704614303125926, accuracy: 0.99\n",
      "iteration no 8499: Loss: 0.23704690896627473, accuracy: 0.99\n",
      "iteration no 8500: Loss: 0.23704549178612971, accuracy: 0.99\n",
      "iteration no 8501: Loss: 0.23704662427138323, accuracy: 0.99\n",
      "iteration no 8502: Loss: 0.2370452327785784, accuracy: 0.99\n",
      "iteration no 8503: Loss: 0.23704591887376875, accuracy: 0.99\n",
      "iteration no 8504: Loss: 0.23704498713678537, accuracy: 0.99\n",
      "iteration no 8505: Loss: 0.23704501628078972, accuracy: 0.99\n",
      "iteration no 8506: Loss: 0.2370448608501301, accuracy: 0.99\n",
      "iteration no 8507: Loss: 0.23704432597072683, accuracy: 0.99\n",
      "iteration no 8508: Loss: 0.23704467228724382, accuracy: 0.99\n",
      "iteration no 8509: Loss: 0.23704352644586524, accuracy: 0.99\n",
      "iteration no 8510: Loss: 0.23704433707624933, accuracy: 0.99\n",
      "iteration no 8511: Loss: 0.23704287604370866, accuracy: 0.99\n",
      "iteration no 8512: Loss: 0.23704405427977465, accuracy: 0.99\n",
      "iteration no 8513: Loss: 0.23704244272123814, accuracy: 0.99\n",
      "iteration no 8514: Loss: 0.237043043758351, accuracy: 0.99\n",
      "iteration no 8515: Loss: 0.23704200143681525, accuracy: 0.99\n",
      "iteration no 8516: Loss: 0.23704201786998896, accuracy: 0.99\n",
      "iteration no 8517: Loss: 0.23704150507638713, accuracy: 0.99\n",
      "iteration no 8518: Loss: 0.23704095515910245, accuracy: 0.99\n",
      "iteration no 8519: Loss: 0.23704117508849423, accuracy: 0.99\n",
      "iteration no 8520: Loss: 0.23704006719735546, accuracy: 0.99\n",
      "iteration no 8521: Loss: 0.23704054901405597, accuracy: 0.99\n",
      "iteration no 8522: Loss: 0.23703937272509507, accuracy: 0.99\n",
      "iteration no 8523: Loss: 0.23703980202054584, accuracy: 0.99\n",
      "iteration no 8524: Loss: 0.237038644866977, accuracy: 0.99\n",
      "iteration no 8525: Loss: 0.23703907690864603, accuracy: 0.99\n",
      "iteration no 8526: Loss: 0.23703822839275762, accuracy: 0.99\n",
      "iteration no 8527: Loss: 0.23703814207502882, accuracy: 0.99\n",
      "iteration no 8528: Loss: 0.23703768369709333, accuracy: 0.99\n",
      "iteration no 8529: Loss: 0.23703716428082833, accuracy: 0.99\n",
      "iteration no 8530: Loss: 0.23703708957837238, accuracy: 0.99\n",
      "iteration no 8531: Loss: 0.2370364960228959, accuracy: 0.99\n",
      "iteration no 8532: Loss: 0.23703637881103734, accuracy: 0.99\n",
      "iteration no 8533: Loss: 0.23703592776180243, accuracy: 0.99\n",
      "iteration no 8534: Loss: 0.23703568308331308, accuracy: 0.99\n",
      "iteration no 8535: Loss: 0.23703518414229496, accuracy: 0.99\n",
      "iteration no 8536: Loss: 0.2370348405941451, accuracy: 0.99\n",
      "iteration no 8537: Loss: 0.23703449803797305, accuracy: 0.99\n",
      "iteration no 8538: Loss: 0.23703442035720573, accuracy: 0.99\n",
      "iteration no 8539: Loss: 0.23703368571265768, accuracy: 0.99\n",
      "iteration no 8540: Loss: 0.23703387748521537, accuracy: 0.99\n",
      "iteration no 8541: Loss: 0.23703312365941448, accuracy: 0.99\n",
      "iteration no 8542: Loss: 0.23703330937265132, accuracy: 0.99\n",
      "iteration no 8543: Loss: 0.23703227165812457, accuracy: 0.99\n",
      "iteration no 8544: Loss: 0.2370304894595292, accuracy: 0.99\n",
      "iteration no 8545: Loss: 0.23702783549236767, accuracy: 0.99\n",
      "iteration no 8546: Loss: 0.23702610980874164, accuracy: 0.99\n",
      "iteration no 8547: Loss: 0.23702352175347163, accuracy: 0.99\n",
      "iteration no 8548: Loss: 0.23702198269484398, accuracy: 0.99\n",
      "iteration no 8549: Loss: 0.23701956995956402, accuracy: 0.99\n",
      "iteration no 8550: Loss: 0.23701808515650785, accuracy: 0.99\n",
      "iteration no 8551: Loss: 0.2370156718007247, accuracy: 0.99\n",
      "iteration no 8552: Loss: 0.23701440819641437, accuracy: 0.99\n",
      "iteration no 8553: Loss: 0.23701264024134633, accuracy: 0.99\n",
      "iteration no 8554: Loss: 0.23701044922188177, accuracy: 0.99\n",
      "iteration no 8555: Loss: 0.23700928209971234, accuracy: 0.99\n",
      "iteration no 8556: Loss: 0.23700668950156517, accuracy: 0.99\n",
      "iteration no 8557: Loss: 0.2370060752216857, accuracy: 0.99\n",
      "iteration no 8558: Loss: 0.2370037533857111, accuracy: 0.99\n",
      "iteration no 8559: Loss: 0.2370021144939111, accuracy: 0.99\n",
      "iteration no 8560: Loss: 0.23700078069365332, accuracy: 0.99\n",
      "iteration no 8561: Loss: 0.23699857697976315, accuracy: 0.99\n",
      "iteration no 8562: Loss: 0.23699791529123465, accuracy: 0.99\n",
      "iteration no 8563: Loss: 0.23699555302158232, accuracy: 0.99\n",
      "iteration no 8564: Loss: 0.2369941808772695, accuracy: 0.99\n",
      "iteration no 8565: Loss: 0.23699294363065887, accuracy: 0.99\n",
      "iteration no 8566: Loss: 0.23699130617049696, accuracy: 0.99\n",
      "iteration no 8567: Loss: 0.23698956675071473, accuracy: 0.99\n",
      "iteration no 8568: Loss: 0.23698834763186377, accuracy: 0.99\n",
      "iteration no 8569: Loss: 0.23698669368495962, accuracy: 0.99\n",
      "iteration no 8570: Loss: 0.23698500173853682, accuracy: 0.99\n",
      "iteration no 8571: Loss: 0.2369843119827788, accuracy: 0.99\n",
      "iteration no 8572: Loss: 0.23698188273623025, accuracy: 0.99\n",
      "iteration no 8573: Loss: 0.236982081109775, accuracy: 0.99\n",
      "iteration no 8574: Loss: 0.2369809851843535, accuracy: 0.99\n",
      "iteration no 8575: Loss: 0.2369798803465918, accuracy: 0.99\n",
      "iteration no 8576: Loss: 0.23698009444003976, accuracy: 0.99\n",
      "iteration no 8577: Loss: 0.236978630945544, accuracy: 0.99\n",
      "iteration no 8578: Loss: 0.23697815525297178, accuracy: 0.99\n",
      "iteration no 8579: Loss: 0.2369778364100849, accuracy: 0.99\n",
      "iteration no 8580: Loss: 0.2369770491825161, accuracy: 0.99\n",
      "iteration no 8581: Loss: 0.23697645889227476, accuracy: 0.99\n",
      "iteration no 8582: Loss: 0.23697561485505242, accuracy: 0.99\n",
      "iteration no 8583: Loss: 0.23697502146489835, accuracy: 0.99\n",
      "iteration no 8584: Loss: 0.23697457711924455, accuracy: 0.99\n",
      "iteration no 8585: Loss: 0.2369739109487928, accuracy: 0.99\n",
      "iteration no 8586: Loss: 0.2369731559729239, accuracy: 0.99\n",
      "iteration no 8587: Loss: 0.23697299013287115, accuracy: 0.99\n",
      "iteration no 8588: Loss: 0.23697192837331255, accuracy: 0.99\n",
      "iteration no 8589: Loss: 0.236971503060609, accuracy: 0.99\n",
      "iteration no 8590: Loss: 0.23697097597862435, accuracy: 0.99\n",
      "iteration no 8591: Loss: 0.23697039248905133, accuracy: 0.99\n",
      "iteration no 8592: Loss: 0.23697005790346312, accuracy: 0.99\n",
      "iteration no 8593: Loss: 0.2369688752268136, accuracy: 0.99\n",
      "iteration no 8594: Loss: 0.23696861949406198, accuracy: 0.99\n",
      "iteration no 8595: Loss: 0.23696834208454787, accuracy: 0.99\n",
      "iteration no 8596: Loss: 0.23696748264678447, accuracy: 0.99\n",
      "iteration no 8597: Loss: 0.23696724303174493, accuracy: 0.99\n",
      "iteration no 8598: Loss: 0.23696601504855513, accuracy: 0.99\n",
      "iteration no 8599: Loss: 0.2369660737535448, accuracy: 0.99\n",
      "iteration no 8600: Loss: 0.23696543588221985, accuracy: 0.99\n",
      "iteration no 8601: Loss: 0.23696471194983618, accuracy: 0.99\n",
      "iteration no 8602: Loss: 0.23696432397635458, accuracy: 0.99\n",
      "iteration no 8603: Loss: 0.23696353654796654, accuracy: 0.99\n",
      "iteration no 8604: Loss: 0.23696314119271922, accuracy: 0.99\n",
      "iteration no 8605: Loss: 0.23696252524213102, accuracy: 0.99\n",
      "iteration no 8606: Loss: 0.2369619986843343, accuracy: 0.99\n",
      "iteration no 8607: Loss: 0.23696159909646813, accuracy: 0.99\n",
      "iteration no 8608: Loss: 0.23696083017595998, accuracy: 0.99\n",
      "iteration no 8609: Loss: 0.23696005537698445, accuracy: 0.99\n",
      "iteration no 8610: Loss: 0.2369593335057924, accuracy: 0.99\n",
      "iteration no 8611: Loss: 0.23695920415566285, accuracy: 0.99\n",
      "iteration no 8612: Loss: 0.2369583093919388, accuracy: 0.99\n",
      "iteration no 8613: Loss: 0.23695786364976992, accuracy: 0.99\n",
      "iteration no 8614: Loss: 0.23695709256565461, accuracy: 0.99\n",
      "iteration no 8615: Loss: 0.23695673243198936, accuracy: 0.99\n",
      "iteration no 8616: Loss: 0.2369561376919704, accuracy: 0.99\n",
      "iteration no 8617: Loss: 0.2369553300411436, accuracy: 0.99\n",
      "iteration no 8618: Loss: 0.23695501260069476, accuracy: 0.99\n",
      "iteration no 8619: Loss: 0.23695412688046014, accuracy: 0.99\n",
      "iteration no 8620: Loss: 0.23695387613225657, accuracy: 0.99\n",
      "iteration no 8621: Loss: 0.23695302901184676, accuracy: 0.99\n",
      "iteration no 8622: Loss: 0.23695254050247205, accuracy: 0.99\n",
      "iteration no 8623: Loss: 0.23695223563530682, accuracy: 0.99\n",
      "iteration no 8624: Loss: 0.23695114442256426, accuracy: 0.99\n",
      "iteration no 8625: Loss: 0.23695120839024617, accuracy: 0.99\n",
      "iteration no 8626: Loss: 0.23694988199461253, accuracy: 0.99\n",
      "iteration no 8627: Loss: 0.23694996925022, accuracy: 0.99\n",
      "iteration no 8628: Loss: 0.23694927203348767, accuracy: 0.99\n",
      "iteration no 8629: Loss: 0.23694828956024233, accuracy: 0.99\n",
      "iteration no 8630: Loss: 0.23694861927566863, accuracy: 0.99\n",
      "iteration no 8631: Loss: 0.23694691383083522, accuracy: 0.99\n",
      "iteration no 8632: Loss: 0.23694727786412811, accuracy: 0.99\n",
      "iteration no 8633: Loss: 0.236946207913588, accuracy: 0.99\n",
      "iteration no 8634: Loss: 0.23694562132445646, accuracy: 0.99\n",
      "iteration no 8635: Loss: 0.2369455837944089, accuracy: 0.99\n",
      "iteration no 8636: Loss: 0.236943948360992, accuracy: 0.99\n",
      "iteration no 8637: Loss: 0.23694479880024386, accuracy: 0.99\n",
      "iteration no 8638: Loss: 0.23694328289347616, accuracy: 0.99\n",
      "iteration no 8639: Loss: 0.23694337947267838, accuracy: 0.99\n",
      "iteration no 8640: Loss: 0.23694238455527522, accuracy: 0.99\n",
      "iteration no 8641: Loss: 0.2369421294399859, accuracy: 0.99\n",
      "iteration no 8642: Loss: 0.23694149218602556, accuracy: 0.99\n",
      "iteration no 8643: Loss: 0.2369414018591394, accuracy: 0.99\n",
      "iteration no 8644: Loss: 0.2369401043824821, accuracy: 0.99\n",
      "iteration no 8645: Loss: 0.2369405391340777, accuracy: 0.99\n",
      "iteration no 8646: Loss: 0.23693887644809292, accuracy: 0.99\n",
      "iteration no 8647: Loss: 0.23693961178163145, accuracy: 0.99\n",
      "iteration no 8648: Loss: 0.23693808515514858, accuracy: 0.99\n",
      "iteration no 8649: Loss: 0.23693831572722462, accuracy: 0.99\n",
      "iteration no 8650: Loss: 0.23693718036320988, accuracy: 0.99\n",
      "iteration no 8651: Loss: 0.23693683368371105, accuracy: 0.99\n",
      "iteration no 8652: Loss: 0.23693645211386066, accuracy: 0.99\n",
      "iteration no 8653: Loss: 0.23693586723164695, accuracy: 0.99\n",
      "iteration no 8654: Loss: 0.2369354743057138, accuracy: 0.99\n",
      "iteration no 8655: Loss: 0.23693486161265204, accuracy: 0.99\n",
      "iteration no 8656: Loss: 0.23693421483062885, accuracy: 0.99\n",
      "iteration no 8657: Loss: 0.23693377728660886, accuracy: 0.99\n",
      "iteration no 8658: Loss: 0.2369335072514418, accuracy: 0.99\n",
      "iteration no 8659: Loss: 0.23693295174341383, accuracy: 0.99\n",
      "iteration no 8660: Loss: 0.23693243272419623, accuracy: 0.99\n",
      "iteration no 8661: Loss: 0.236931297854166, accuracy: 0.99\n",
      "iteration no 8662: Loss: 0.23693199089050504, accuracy: 0.99\n",
      "iteration no 8663: Loss: 0.23693025877109775, accuracy: 0.99\n",
      "iteration no 8664: Loss: 0.23693096743492914, accuracy: 0.99\n",
      "iteration no 8665: Loss: 0.23692952867018136, accuracy: 0.99\n",
      "iteration no 8666: Loss: 0.23692975928905508, accuracy: 0.99\n",
      "iteration no 8667: Loss: 0.23692893462179487, accuracy: 0.99\n",
      "iteration no 8668: Loss: 0.23692816971667358, accuracy: 0.99\n",
      "iteration no 8669: Loss: 0.236928128606095, accuracy: 0.99\n",
      "iteration no 8670: Loss: 0.23692734203730037, accuracy: 0.99\n",
      "iteration no 8671: Loss: 0.23692721073675616, accuracy: 0.99\n",
      "iteration no 8672: Loss: 0.2369263669411099, accuracy: 0.99\n",
      "iteration no 8673: Loss: 0.23692594275868542, accuracy: 0.99\n",
      "iteration no 8674: Loss: 0.23692553589849244, accuracy: 0.99\n",
      "iteration no 8675: Loss: 0.23692522719971304, accuracy: 0.99\n",
      "iteration no 8676: Loss: 0.23692435037730641, accuracy: 0.99\n",
      "iteration no 8677: Loss: 0.23692418140340737, accuracy: 0.99\n",
      "iteration no 8678: Loss: 0.2369233455646035, accuracy: 0.99\n",
      "iteration no 8679: Loss: 0.23692327197513618, accuracy: 0.99\n",
      "iteration no 8680: Loss: 0.23692265155321846, accuracy: 0.99\n",
      "iteration no 8681: Loss: 0.23692177262294367, accuracy: 0.99\n",
      "iteration no 8682: Loss: 0.23692255199191314, accuracy: 0.99\n",
      "iteration no 8683: Loss: 0.23692086945441948, accuracy: 0.99\n",
      "iteration no 8684: Loss: 0.23692177615098348, accuracy: 0.99\n",
      "iteration no 8685: Loss: 0.23692026858537596, accuracy: 0.99\n",
      "iteration no 8686: Loss: 0.23692037589245693, accuracy: 0.99\n",
      "iteration no 8687: Loss: 0.23692012048237582, accuracy: 0.99\n",
      "iteration no 8688: Loss: 0.23691920754198093, accuracy: 0.99\n",
      "iteration no 8689: Loss: 0.2369197220375539, accuracy: 0.99\n",
      "iteration no 8690: Loss: 0.2369181678446298, accuracy: 0.99\n",
      "iteration no 8691: Loss: 0.23691892755196475, accuracy: 0.99\n",
      "iteration no 8692: Loss: 0.2369177332561596, accuracy: 0.99\n",
      "iteration no 8693: Loss: 0.2369176926960177, accuracy: 0.99\n",
      "iteration no 8694: Loss: 0.2369175308739355, accuracy: 0.99\n",
      "iteration no 8695: Loss: 0.23691662816992393, accuracy: 0.99\n",
      "iteration no 8696: Loss: 0.23691680009817534, accuracy: 0.99\n",
      "iteration no 8697: Loss: 0.23691573365893903, accuracy: 0.99\n",
      "iteration no 8698: Loss: 0.23691590031801552, accuracy: 0.99\n",
      "iteration no 8699: Loss: 0.23691558419788478, accuracy: 0.99\n",
      "iteration no 8700: Loss: 0.2369152123550766, accuracy: 0.99\n",
      "iteration no 8701: Loss: 0.23691516092338166, accuracy: 0.99\n",
      "iteration no 8702: Loss: 0.23691470164932896, accuracy: 0.99\n",
      "iteration no 8703: Loss: 0.23691415992389367, accuracy: 0.99\n",
      "iteration no 8704: Loss: 0.23691426698545118, accuracy: 0.99\n",
      "iteration no 8705: Loss: 0.23691383224548082, accuracy: 0.99\n",
      "iteration no 8706: Loss: 0.23691348330737866, accuracy: 0.99\n",
      "iteration no 8707: Loss: 0.236913626524608, accuracy: 0.99\n",
      "iteration no 8708: Loss: 0.23691244003997391, accuracy: 0.99\n",
      "iteration no 8709: Loss: 0.23691312842691298, accuracy: 0.99\n",
      "iteration no 8710: Loss: 0.23691222840214582, accuracy: 0.99\n",
      "iteration no 8711: Loss: 0.2369121253605279, accuracy: 0.99\n",
      "iteration no 8712: Loss: 0.23691224597862956, accuracy: 0.99\n",
      "iteration no 8713: Loss: 0.23691093112915296, accuracy: 0.99\n",
      "iteration no 8714: Loss: 0.2369118851600035, accuracy: 0.99\n",
      "iteration no 8715: Loss: 0.23691060822604654, accuracy: 0.99\n",
      "iteration no 8716: Loss: 0.23691086473766912, accuracy: 0.99\n",
      "iteration no 8717: Loss: 0.23691062087387743, accuracy: 0.99\n",
      "iteration no 8718: Loss: 0.2369097649705426, accuracy: 0.99\n",
      "iteration no 8719: Loss: 0.23691036700709536, accuracy: 0.99\n",
      "iteration no 8720: Loss: 0.2369090882586538, accuracy: 0.99\n",
      "iteration no 8721: Loss: 0.23690970615839693, accuracy: 0.99\n",
      "iteration no 8722: Loss: 0.2369089859707762, accuracy: 0.99\n",
      "iteration no 8723: Loss: 0.23690862581008296, accuracy: 0.99\n",
      "iteration no 8724: Loss: 0.2369086447751559, accuracy: 0.99\n",
      "iteration no 8725: Loss: 0.2369077401249774, accuracy: 0.99\n",
      "iteration no 8726: Loss: 0.2369083516403413, accuracy: 0.99\n",
      "iteration no 8727: Loss: 0.23690730990840825, accuracy: 0.99\n",
      "iteration no 8728: Loss: 0.23690762109948146, accuracy: 0.99\n",
      "iteration no 8729: Loss: 0.23690703352201142, accuracy: 0.99\n",
      "iteration no 8730: Loss: 0.2369065416853518, accuracy: 0.99\n",
      "iteration no 8731: Loss: 0.2369068883030896, accuracy: 0.99\n",
      "iteration no 8732: Loss: 0.23690592762223556, accuracy: 0.99\n",
      "iteration no 8733: Loss: 0.23690649706557232, accuracy: 0.99\n",
      "iteration no 8734: Loss: 0.23690544942295727, accuracy: 0.99\n",
      "iteration no 8735: Loss: 0.2369053770003085, accuracy: 0.99\n",
      "iteration no 8736: Loss: 0.23690498571219942, accuracy: 0.99\n",
      "iteration no 8737: Loss: 0.23690466818775197, accuracy: 0.99\n",
      "iteration no 8738: Loss: 0.23690459618415935, accuracy: 0.99\n",
      "iteration no 8739: Loss: 0.23690381521141174, accuracy: 0.99\n",
      "iteration no 8740: Loss: 0.23690396332153302, accuracy: 0.99\n",
      "iteration no 8741: Loss: 0.23690341311181906, accuracy: 0.99\n",
      "iteration no 8742: Loss: 0.2369030469136308, accuracy: 0.99\n",
      "iteration no 8743: Loss: 0.23690287123980136, accuracy: 0.99\n",
      "iteration no 8744: Loss: 0.23690226462743358, accuracy: 0.99\n",
      "iteration no 8745: Loss: 0.23690233038716088, accuracy: 0.99\n",
      "iteration no 8746: Loss: 0.23690185571483566, accuracy: 0.99\n",
      "iteration no 8747: Loss: 0.23690159924330031, accuracy: 0.99\n",
      "iteration no 8748: Loss: 0.23690146340120413, accuracy: 0.99\n",
      "iteration no 8749: Loss: 0.2369008694237149, accuracy: 0.99\n",
      "iteration no 8750: Loss: 0.23690083287619979, accuracy: 0.99\n",
      "iteration no 8751: Loss: 0.23690033157861062, accuracy: 0.99\n",
      "iteration no 8752: Loss: 0.23690044216757003, accuracy: 0.99\n",
      "iteration no 8753: Loss: 0.2368997201019924, accuracy: 0.99\n",
      "iteration no 8754: Loss: 0.23689994901760178, accuracy: 0.99\n",
      "iteration no 8755: Loss: 0.236898939300463, accuracy: 0.99\n",
      "iteration no 8756: Loss: 0.23689934086444248, accuracy: 0.99\n",
      "iteration no 8757: Loss: 0.2368986025124688, accuracy: 0.99\n",
      "iteration no 8758: Loss: 0.23689850831536335, accuracy: 0.99\n",
      "iteration no 8759: Loss: 0.23689849057927959, accuracy: 0.99\n",
      "iteration no 8760: Loss: 0.2368977564406009, accuracy: 0.99\n",
      "iteration no 8761: Loss: 0.23689785260515273, accuracy: 0.99\n",
      "iteration no 8762: Loss: 0.2368973777393793, accuracy: 0.99\n",
      "iteration no 8763: Loss: 0.23689687243806273, accuracy: 0.99\n",
      "iteration no 8764: Loss: 0.2368973906269573, accuracy: 0.99\n",
      "iteration no 8765: Loss: 0.2368959857944102, accuracy: 0.99\n",
      "iteration no 8766: Loss: 0.23689688276773146, accuracy: 0.99\n",
      "iteration no 8767: Loss: 0.23689572551817428, accuracy: 0.99\n",
      "iteration no 8768: Loss: 0.23689574581427492, accuracy: 0.99\n",
      "iteration no 8769: Loss: 0.2368957018550959, accuracy: 0.99\n",
      "iteration no 8770: Loss: 0.23689465050180383, accuracy: 0.99\n",
      "iteration no 8771: Loss: 0.23689564988536677, accuracy: 0.99\n",
      "iteration no 8772: Loss: 0.23689427586718773, accuracy: 0.99\n",
      "iteration no 8773: Loss: 0.23689455759719888, accuracy: 0.99\n",
      "iteration no 8774: Loss: 0.23689419339642734, accuracy: 0.99\n",
      "iteration no 8775: Loss: 0.23689344048125044, accuracy: 0.99\n",
      "iteration no 8776: Loss: 0.23689413787380484, accuracy: 0.99\n",
      "iteration no 8777: Loss: 0.23689284640375396, accuracy: 0.99\n",
      "iteration no 8778: Loss: 0.23689347181809917, accuracy: 0.99\n",
      "iteration no 8779: Loss: 0.23689284982878056, accuracy: 0.99\n",
      "iteration no 8780: Loss: 0.23689222409190863, accuracy: 0.99\n",
      "iteration no 8781: Loss: 0.23689276119842798, accuracy: 0.99\n",
      "iteration no 8782: Loss: 0.23689151710548703, accuracy: 0.99\n",
      "iteration no 8783: Loss: 0.23689234478685683, accuracy: 0.99\n",
      "iteration no 8784: Loss: 0.2368911255104799, accuracy: 0.99\n",
      "iteration no 8785: Loss: 0.23689145099831055, accuracy: 0.99\n",
      "iteration no 8786: Loss: 0.23689103291634406, accuracy: 0.99\n",
      "iteration no 8787: Loss: 0.23689069958602335, accuracy: 0.99\n",
      "iteration no 8788: Loss: 0.2368904906613688, accuracy: 0.99\n",
      "iteration no 8789: Loss: 0.2368901267258892, accuracy: 0.99\n",
      "iteration no 8790: Loss: 0.2368899395062247, accuracy: 0.99\n",
      "iteration no 8791: Loss: 0.23688992380524038, accuracy: 0.99\n",
      "iteration no 8792: Loss: 0.2368894874849205, accuracy: 0.99\n",
      "iteration no 8793: Loss: 0.23688940594434266, accuracy: 0.99\n",
      "iteration no 8794: Loss: 0.23688882619117496, accuracy: 0.99\n",
      "iteration no 8795: Loss: 0.23688874057313675, accuracy: 0.99\n",
      "iteration no 8796: Loss: 0.23688831358488435, accuracy: 0.99\n",
      "iteration no 8797: Loss: 0.23688865812666737, accuracy: 0.99\n",
      "iteration no 8798: Loss: 0.23688775895326578, accuracy: 0.99\n",
      "iteration no 8799: Loss: 0.23688811996935638, accuracy: 0.99\n",
      "iteration no 8800: Loss: 0.23688715916643835, accuracy: 0.99\n",
      "iteration no 8801: Loss: 0.23688748852321018, accuracy: 0.99\n",
      "iteration no 8802: Loss: 0.23688708869970887, accuracy: 0.99\n",
      "iteration no 8803: Loss: 0.2368865083665864, accuracy: 0.99\n",
      "iteration no 8804: Loss: 0.23688706128017822, accuracy: 0.99\n",
      "iteration no 8805: Loss: 0.23688579153975375, accuracy: 0.99\n",
      "iteration no 8806: Loss: 0.2368865365628308, accuracy: 0.99\n",
      "iteration no 8807: Loss: 0.23688561315913526, accuracy: 0.99\n",
      "iteration no 8808: Loss: 0.23688547077392413, accuracy: 0.99\n",
      "iteration no 8809: Loss: 0.23688553875602264, accuracy: 0.99\n",
      "iteration no 8810: Loss: 0.23688449967643774, accuracy: 0.99\n",
      "iteration no 8811: Loss: 0.2368853545370449, accuracy: 0.99\n",
      "iteration no 8812: Loss: 0.23688433735223816, accuracy: 0.99\n",
      "iteration no 8813: Loss: 0.23688453927306158, accuracy: 0.99\n",
      "iteration no 8814: Loss: 0.236884078032025, accuracy: 0.99\n",
      "iteration no 8815: Loss: 0.23688389745567465, accuracy: 0.99\n",
      "iteration no 8816: Loss: 0.23688376353664942, accuracy: 0.99\n",
      "iteration no 8817: Loss: 0.23688321488606554, accuracy: 0.99\n",
      "iteration no 8818: Loss: 0.23688328536564124, accuracy: 0.99\n",
      "iteration no 8819: Loss: 0.23688283233126844, accuracy: 0.99\n",
      "iteration no 8820: Loss: 0.23688316259779782, accuracy: 0.99\n",
      "iteration no 8821: Loss: 0.23688210667325704, accuracy: 0.99\n",
      "iteration no 8822: Loss: 0.23688260732737149, accuracy: 0.99\n",
      "iteration no 8823: Loss: 0.23688136046457442, accuracy: 0.99\n",
      "iteration no 8824: Loss: 0.23688210646458563, accuracy: 0.99\n",
      "iteration no 8825: Loss: 0.2368813034245215, accuracy: 0.99\n",
      "iteration no 8826: Loss: 0.23688124054507323, accuracy: 0.99\n",
      "iteration no 8827: Loss: 0.2368813494943786, accuracy: 0.99\n",
      "iteration no 8828: Loss: 0.23688050423301704, accuracy: 0.99\n",
      "iteration no 8829: Loss: 0.2368810102893814, accuracy: 0.99\n",
      "iteration no 8830: Loss: 0.23688007018991264, accuracy: 0.99\n",
      "iteration no 8831: Loss: 0.23688019764492005, accuracy: 0.99\n",
      "iteration no 8832: Loss: 0.23687988877021263, accuracy: 0.99\n",
      "iteration no 8833: Loss: 0.2368792865433908, accuracy: 0.99\n",
      "iteration no 8834: Loss: 0.2368796373878075, accuracy: 0.99\n",
      "iteration no 8835: Loss: 0.23687886424942853, accuracy: 0.99\n",
      "iteration no 8836: Loss: 0.23687908349961379, accuracy: 0.99\n",
      "iteration no 8837: Loss: 0.23687840893646211, accuracy: 0.99\n",
      "iteration no 8838: Loss: 0.23687862732592208, accuracy: 0.99\n",
      "iteration no 8839: Loss: 0.23687811945299703, accuracy: 0.99\n",
      "iteration no 8840: Loss: 0.23687803871664911, accuracy: 0.99\n",
      "iteration no 8841: Loss: 0.2368776295459318, accuracy: 0.99\n",
      "iteration no 8842: Loss: 0.23687768001209988, accuracy: 0.99\n",
      "iteration no 8843: Loss: 0.23687754666118005, accuracy: 0.99\n",
      "iteration no 8844: Loss: 0.23687695998951824, accuracy: 0.99\n",
      "iteration no 8845: Loss: 0.23687707822437185, accuracy: 0.99\n",
      "iteration no 8846: Loss: 0.2368759551122554, accuracy: 0.99\n",
      "iteration no 8847: Loss: 0.23687693195654144, accuracy: 0.99\n",
      "iteration no 8848: Loss: 0.23687586754452966, accuracy: 0.99\n",
      "iteration no 8849: Loss: 0.23687592234587657, accuracy: 0.99\n",
      "iteration no 8850: Loss: 0.23687568173946563, accuracy: 0.99\n",
      "iteration no 8851: Loss: 0.2368749864561986, accuracy: 0.99\n",
      "iteration no 8852: Loss: 0.23687567785764502, accuracy: 0.99\n",
      "iteration no 8853: Loss: 0.23687456091826703, accuracy: 0.99\n",
      "iteration no 8854: Loss: 0.23687496156317206, accuracy: 0.99\n",
      "iteration no 8855: Loss: 0.23687447272889528, accuracy: 0.99\n",
      "iteration no 8856: Loss: 0.2368741012849459, accuracy: 0.99\n",
      "iteration no 8857: Loss: 0.23687424475443988, accuracy: 0.99\n",
      "iteration no 8858: Loss: 0.23687368430643269, accuracy: 0.99\n",
      "iteration no 8859: Loss: 0.23687363374283005, accuracy: 0.99\n",
      "iteration no 8860: Loss: 0.23687351778285698, accuracy: 0.99\n",
      "iteration no 8861: Loss: 0.23687334096436705, accuracy: 0.99\n",
      "iteration no 8862: Loss: 0.23687269587860155, accuracy: 0.99\n",
      "iteration no 8863: Loss: 0.23687334168906637, accuracy: 0.99\n",
      "iteration no 8864: Loss: 0.23687176765768492, accuracy: 0.99\n",
      "iteration no 8865: Loss: 0.2368729507565966, accuracy: 0.99\n",
      "iteration no 8866: Loss: 0.23687159346486328, accuracy: 0.99\n",
      "iteration no 8867: Loss: 0.23687192431572945, accuracy: 0.99\n",
      "iteration no 8868: Loss: 0.23687153655443863, accuracy: 0.99\n",
      "iteration no 8869: Loss: 0.2368710086419487, accuracy: 0.99\n",
      "iteration no 8870: Loss: 0.23687147602528055, accuracy: 0.99\n",
      "iteration no 8871: Loss: 0.23687069225679985, accuracy: 0.99\n",
      "iteration no 8872: Loss: 0.2368707655150813, accuracy: 0.99\n",
      "iteration no 8873: Loss: 0.23687033609145494, accuracy: 0.99\n",
      "iteration no 8874: Loss: 0.23687019936233156, accuracy: 0.99\n",
      "iteration no 8875: Loss: 0.23687005324669538, accuracy: 0.99\n",
      "iteration no 8876: Loss: 0.23686973770874, accuracy: 0.99\n",
      "iteration no 8877: Loss: 0.23686949502977314, accuracy: 0.99\n",
      "iteration no 8878: Loss: 0.23686942530614086, accuracy: 0.99\n",
      "iteration no 8879: Loss: 0.23686951294019445, accuracy: 0.99\n",
      "iteration no 8880: Loss: 0.2368686633513023, accuracy: 0.99\n",
      "iteration no 8881: Loss: 0.23686916276796022, accuracy: 0.99\n",
      "iteration no 8882: Loss: 0.2368676413411177, accuracy: 0.99\n",
      "iteration no 8883: Loss: 0.23686883226620375, accuracy: 0.99\n",
      "iteration no 8884: Loss: 0.2368675703408054, accuracy: 0.99\n",
      "iteration no 8885: Loss: 0.2368678722166311, accuracy: 0.99\n",
      "iteration no 8886: Loss: 0.23686762483387053, accuracy: 0.99\n",
      "iteration no 8887: Loss: 0.2368669184669385, accuracy: 0.99\n",
      "iteration no 8888: Loss: 0.23686749020174785, accuracy: 0.99\n",
      "iteration no 8889: Loss: 0.23686658633706514, accuracy: 0.99\n",
      "iteration no 8890: Loss: 0.2368668177164444, accuracy: 0.99\n",
      "iteration no 8891: Loss: 0.23686634738579643, accuracy: 0.99\n",
      "iteration no 8892: Loss: 0.23686619926287553, accuracy: 0.99\n",
      "iteration no 8893: Loss: 0.23686588412344128, accuracy: 0.99\n",
      "iteration no 8894: Loss: 0.23686594983260295, accuracy: 0.99\n",
      "iteration no 8895: Loss: 0.2368652987834007, accuracy: 0.99\n",
      "iteration no 8896: Loss: 0.2368656417959274, accuracy: 0.99\n",
      "iteration no 8897: Loss: 0.2368653098481988, accuracy: 0.99\n",
      "iteration no 8898: Loss: 0.23686471176346485, accuracy: 0.99\n",
      "iteration no 8899: Loss: 0.2368651729311308, accuracy: 0.99\n",
      "iteration no 8900: Loss: 0.23686368893139112, accuracy: 0.99\n",
      "iteration no 8901: Loss: 0.23686501328520126, accuracy: 0.99\n",
      "iteration no 8902: Loss: 0.23686367941182995, accuracy: 0.99\n",
      "iteration no 8903: Loss: 0.2368639201929974, accuracy: 0.99\n",
      "iteration no 8904: Loss: 0.2368636642874702, accuracy: 0.99\n",
      "iteration no 8905: Loss: 0.23686292597435593, accuracy: 0.99\n",
      "iteration no 8906: Loss: 0.23686359452035444, accuracy: 0.99\n",
      "iteration no 8907: Loss: 0.2368626450801991, accuracy: 0.99\n",
      "iteration no 8908: Loss: 0.23686285178656574, accuracy: 0.99\n",
      "iteration no 8909: Loss: 0.23686250690710067, accuracy: 0.99\n",
      "iteration no 8910: Loss: 0.23686228869984288, accuracy: 0.99\n",
      "iteration no 8911: Loss: 0.2368620546015253, accuracy: 0.99\n",
      "iteration no 8912: Loss: 0.23686210551182474, accuracy: 0.99\n",
      "iteration no 8913: Loss: 0.23686146402788247, accuracy: 0.99\n",
      "iteration no 8914: Loss: 0.23686161923811477, accuracy: 0.99\n",
      "iteration no 8915: Loss: 0.23686122844186974, accuracy: 0.99\n",
      "iteration no 8916: Loss: 0.23686081838621081, accuracy: 0.99\n",
      "iteration no 8917: Loss: 0.23686132301823606, accuracy: 0.99\n",
      "iteration no 8918: Loss: 0.23685987938587816, accuracy: 0.99\n",
      "iteration no 8919: Loss: 0.2368611792050303, accuracy: 0.99\n",
      "iteration no 8920: Loss: 0.2368596864134713, accuracy: 0.99\n",
      "iteration no 8921: Loss: 0.23686017999444045, accuracy: 0.99\n",
      "iteration no 8922: Loss: 0.23685972306802258, accuracy: 0.99\n",
      "iteration no 8923: Loss: 0.23685921841587987, accuracy: 0.99\n",
      "iteration no 8924: Loss: 0.23685962696385673, accuracy: 0.99\n",
      "iteration no 8925: Loss: 0.23685877624301768, accuracy: 0.99\n",
      "iteration no 8926: Loss: 0.23685901758739766, accuracy: 0.99\n",
      "iteration no 8927: Loss: 0.23685873663804458, accuracy: 0.99\n",
      "iteration no 8928: Loss: 0.23685845949741474, accuracy: 0.99\n",
      "iteration no 8929: Loss: 0.2368582447403687, accuracy: 0.99\n",
      "iteration no 8930: Loss: 0.2368582583169329, accuracy: 0.99\n",
      "iteration no 8931: Loss: 0.23685766562957394, accuracy: 0.99\n",
      "iteration no 8932: Loss: 0.23685785453057506, accuracy: 0.99\n",
      "iteration no 8933: Loss: 0.2368574409444899, accuracy: 0.99\n",
      "iteration no 8934: Loss: 0.2368570678313633, accuracy: 0.99\n",
      "iteration no 8935: Loss: 0.23685745595052599, accuracy: 0.99\n",
      "iteration no 8936: Loss: 0.23685621217736552, accuracy: 0.99\n",
      "iteration no 8937: Loss: 0.23685752497264756, accuracy: 0.99\n",
      "iteration no 8938: Loss: 0.23685591626767155, accuracy: 0.99\n",
      "iteration no 8939: Loss: 0.23685674343410268, accuracy: 0.99\n",
      "iteration no 8940: Loss: 0.236855686782303, accuracy: 0.99\n",
      "iteration no 8941: Loss: 0.23685582827237364, accuracy: 0.99\n",
      "iteration no 8942: Loss: 0.23685554033586123, accuracy: 0.99\n",
      "iteration no 8943: Loss: 0.23685533769117736, accuracy: 0.99\n",
      "iteration no 8944: Loss: 0.23685507545904383, accuracy: 0.99\n",
      "iteration no 8945: Loss: 0.236854990862991, accuracy: 0.99\n",
      "iteration no 8946: Loss: 0.2368547489596627, accuracy: 0.99\n",
      "iteration no 8947: Loss: 0.23685436816731942, accuracy: 0.99\n",
      "iteration no 8948: Loss: 0.23685455557658805, accuracy: 0.99\n",
      "iteration no 8949: Loss: 0.23685381546062734, accuracy: 0.99\n",
      "iteration no 8950: Loss: 0.23685421899390635, accuracy: 0.99\n",
      "iteration no 8951: Loss: 0.23685362435794477, accuracy: 0.99\n",
      "iteration no 8952: Loss: 0.2368534489323414, accuracy: 0.99\n",
      "iteration no 8953: Loss: 0.2368535883038292, accuracy: 0.99\n",
      "iteration no 8954: Loss: 0.236852481503461, accuracy: 0.99\n",
      "iteration no 8955: Loss: 0.2368535216780397, accuracy: 0.99\n",
      "iteration no 8956: Loss: 0.23685213974129304, accuracy: 0.99\n",
      "iteration no 8957: Loss: 0.23685295742841916, accuracy: 0.99\n",
      "iteration no 8958: Loss: 0.23685201732512629, accuracy: 0.99\n",
      "iteration no 8959: Loss: 0.2368520149488429, accuracy: 0.99\n",
      "iteration no 8960: Loss: 0.236851969568986, accuracy: 0.99\n",
      "iteration no 8961: Loss: 0.23685170393388572, accuracy: 0.99\n",
      "iteration no 8962: Loss: 0.2368513839942505, accuracy: 0.99\n",
      "iteration no 8963: Loss: 0.23685119067677304, accuracy: 0.99\n",
      "iteration no 8964: Loss: 0.2368510207165829, accuracy: 0.99\n",
      "iteration no 8965: Loss: 0.23685071888069614, accuracy: 0.99\n",
      "iteration no 8966: Loss: 0.23685098776410468, accuracy: 0.99\n",
      "iteration no 8967: Loss: 0.23685016293023803, accuracy: 0.99\n",
      "iteration no 8968: Loss: 0.23685047936007675, accuracy: 0.99\n",
      "iteration no 8969: Loss: 0.23685004141521843, accuracy: 0.99\n",
      "iteration no 8970: Loss: 0.23684978285871378, accuracy: 0.99\n",
      "iteration no 8971: Loss: 0.23685001978956402, accuracy: 0.99\n",
      "iteration no 8972: Loss: 0.23684876975485636, accuracy: 0.99\n",
      "iteration no 8973: Loss: 0.23684990671294442, accuracy: 0.99\n",
      "iteration no 8974: Loss: 0.23684848611969195, accuracy: 0.99\n",
      "iteration no 8975: Loss: 0.23684915226433445, accuracy: 0.99\n",
      "iteration no 8976: Loss: 0.23684852267958817, accuracy: 0.99\n",
      "iteration no 8977: Loss: 0.23684828674050676, accuracy: 0.99\n",
      "iteration no 8978: Loss: 0.23684848217169557, accuracy: 0.99\n",
      "iteration no 8979: Loss: 0.23684798459078654, accuracy: 0.99\n",
      "iteration no 8980: Loss: 0.23684778847874327, accuracy: 0.99\n",
      "iteration no 8981: Loss: 0.23684763432218786, accuracy: 0.99\n",
      "iteration no 8982: Loss: 0.2368474588140875, accuracy: 0.99\n",
      "iteration no 8983: Loss: 0.23684708198184118, accuracy: 0.99\n",
      "iteration no 8984: Loss: 0.23684728782561254, accuracy: 0.99\n",
      "iteration no 8985: Loss: 0.23684642390011923, accuracy: 0.99\n",
      "iteration no 8986: Loss: 0.23684697376891659, accuracy: 0.99\n",
      "iteration no 8987: Loss: 0.23684636251116564, accuracy: 0.99\n",
      "iteration no 8988: Loss: 0.23684630916806815, accuracy: 0.99\n",
      "iteration no 8989: Loss: 0.23684648004122388, accuracy: 0.99\n",
      "iteration no 8990: Loss: 0.2368452349102214, accuracy: 0.99\n",
      "iteration no 8991: Loss: 0.23684639269107205, accuracy: 0.99\n",
      "iteration no 8992: Loss: 0.23684487000888715, accuracy: 0.99\n",
      "iteration no 8993: Loss: 0.23684571199637774, accuracy: 0.99\n",
      "iteration no 8994: Loss: 0.23684487369484827, accuracy: 0.99\n",
      "iteration no 8995: Loss: 0.23684467630185801, accuracy: 0.99\n",
      "iteration no 8996: Loss: 0.2368448991065936, accuracy: 0.99\n",
      "iteration no 8997: Loss: 0.2368443726873713, accuracy: 0.99\n",
      "iteration no 8998: Loss: 0.23684429989285177, accuracy: 0.99\n",
      "iteration no 8999: Loss: 0.2368440443610978, accuracy: 0.99\n",
      "iteration no 9000: Loss: 0.2368436762903141, accuracy: 0.99\n",
      "iteration no 9001: Loss: 0.23684321339662762, accuracy: 0.99\n",
      "iteration no 9002: Loss: 0.23684336521005322, accuracy: 0.99\n",
      "iteration no 9003: Loss: 0.23684245849833965, accuracy: 0.99\n",
      "iteration no 9004: Loss: 0.2368427255304844, accuracy: 0.99\n",
      "iteration no 9005: Loss: 0.23684188237189577, accuracy: 0.99\n",
      "iteration no 9006: Loss: 0.23684172694143563, accuracy: 0.99\n",
      "iteration no 9007: Loss: 0.23684164666088225, accuracy: 0.99\n",
      "iteration no 9008: Loss: 0.23684064466440835, accuracy: 0.99\n",
      "iteration no 9009: Loss: 0.23684142106131642, accuracy: 0.99\n",
      "iteration no 9010: Loss: 0.23683991842123964, accuracy: 0.99\n",
      "iteration no 9011: Loss: 0.2368404226960604, accuracy: 0.99\n",
      "iteration no 9012: Loss: 0.2368396810286591, accuracy: 0.99\n",
      "iteration no 9013: Loss: 0.23683935534933861, accuracy: 0.99\n",
      "iteration no 9014: Loss: 0.23683932012338516, accuracy: 0.99\n",
      "iteration no 9015: Loss: 0.2368387419168586, accuracy: 0.99\n",
      "iteration no 9016: Loss: 0.23683839452893013, accuracy: 0.99\n",
      "iteration no 9017: Loss: 0.23683813449124058, accuracy: 0.99\n",
      "iteration no 9018: Loss: 0.23683797032306764, accuracy: 0.99\n",
      "iteration no 9019: Loss: 0.23683714143208512, accuracy: 0.99\n",
      "iteration no 9020: Loss: 0.23683779730588964, accuracy: 0.99\n",
      "iteration no 9021: Loss: 0.23683638792353542, accuracy: 0.99\n",
      "iteration no 9022: Loss: 0.23683708455556357, accuracy: 0.99\n",
      "iteration no 9023: Loss: 0.236836170574927, accuracy: 0.99\n",
      "iteration no 9024: Loss: 0.23683589330348087, accuracy: 0.99\n",
      "iteration no 9025: Loss: 0.2368361001704326, accuracy: 0.99\n",
      "iteration no 9026: Loss: 0.2368349359063449, accuracy: 0.99\n",
      "iteration no 9027: Loss: 0.23683546279264975, accuracy: 0.99\n",
      "iteration no 9028: Loss: 0.23683446828267266, accuracy: 0.99\n",
      "iteration no 9029: Loss: 0.23683450949460355, accuracy: 0.99\n",
      "iteration no 9030: Loss: 0.2368341757108054, accuracy: 0.99\n",
      "iteration no 9031: Loss: 0.2368338081594764, accuracy: 0.99\n",
      "iteration no 9032: Loss: 0.23683340186129936, accuracy: 0.99\n",
      "iteration no 9033: Loss: 0.23683327345026195, accuracy: 0.99\n",
      "iteration no 9034: Loss: 0.23683261041547787, accuracy: 0.99\n",
      "iteration no 9035: Loss: 0.23683260983221216, accuracy: 0.99\n",
      "iteration no 9036: Loss: 0.23683239947756202, accuracy: 0.99\n",
      "iteration no 9037: Loss: 0.23683142503888033, accuracy: 0.99\n",
      "iteration no 9038: Loss: 0.2368321662976312, accuracy: 0.99\n",
      "iteration no 9039: Loss: 0.23683074382259794, accuracy: 0.99\n",
      "iteration no 9040: Loss: 0.23683154790921268, accuracy: 0.99\n",
      "iteration no 9041: Loss: 0.2368305177403901, accuracy: 0.99\n",
      "iteration no 9042: Loss: 0.2368303646862322, accuracy: 0.99\n",
      "iteration no 9043: Loss: 0.23683032528382358, accuracy: 0.99\n",
      "iteration no 9044: Loss: 0.2368298104138536, accuracy: 0.99\n",
      "iteration no 9045: Loss: 0.236829441321101, accuracy: 0.99\n",
      "iteration no 9046: Loss: 0.23682923262207045, accuracy: 0.99\n",
      "iteration no 9047: Loss: 0.23682899140199806, accuracy: 0.99\n",
      "iteration no 9048: Loss: 0.23682834960921734, accuracy: 0.99\n",
      "iteration no 9049: Loss: 0.23682854785247928, accuracy: 0.99\n",
      "iteration no 9050: Loss: 0.23682753350783853, accuracy: 0.99\n",
      "iteration no 9051: Loss: 0.2368280223118014, accuracy: 0.99\n",
      "iteration no 9052: Loss: 0.23682730151656012, accuracy: 0.99\n",
      "iteration no 9053: Loss: 0.23682676814543557, accuracy: 0.99\n",
      "iteration no 9054: Loss: 0.23682704327102488, accuracy: 0.99\n",
      "iteration no 9055: Loss: 0.23682601369574885, accuracy: 0.99\n",
      "iteration no 9056: Loss: 0.23682643464512912, accuracy: 0.99\n",
      "iteration no 9057: Loss: 0.23682552996286604, accuracy: 0.99\n",
      "iteration no 9058: Loss: 0.23682562235117266, accuracy: 0.99\n",
      "iteration no 9059: Loss: 0.23682491872302466, accuracy: 0.99\n",
      "iteration no 9060: Loss: 0.2368253930431406, accuracy: 0.99\n",
      "iteration no 9061: Loss: 0.2368240227962005, accuracy: 0.99\n",
      "iteration no 9062: Loss: 0.23682501532031727, accuracy: 0.99\n",
      "iteration no 9063: Loss: 0.23682368604292603, accuracy: 0.99\n",
      "iteration no 9064: Loss: 0.23682390189717897, accuracy: 0.99\n",
      "iteration no 9065: Loss: 0.23682345136958916, accuracy: 0.99\n",
      "iteration no 9066: Loss: 0.23682276168249217, accuracy: 0.99\n",
      "iteration no 9067: Loss: 0.23682324201501048, accuracy: 0.99\n",
      "iteration no 9068: Loss: 0.2368224286972862, accuracy: 0.99\n",
      "iteration no 9069: Loss: 0.23682220109963847, accuracy: 0.99\n",
      "iteration no 9070: Loss: 0.23682195642501325, accuracy: 0.99\n",
      "iteration no 9071: Loss: 0.2368216685203966, accuracy: 0.99\n",
      "iteration no 9072: Loss: 0.23682133492765212, accuracy: 0.99\n",
      "iteration no 9073: Loss: 0.23682126861595743, accuracy: 0.99\n",
      "iteration no 9074: Loss: 0.23682057505537873, accuracy: 0.99\n",
      "iteration no 9075: Loss: 0.2368206291631445, accuracy: 0.99\n",
      "iteration no 9076: Loss: 0.23682030592178754, accuracy: 0.99\n",
      "iteration no 9077: Loss: 0.23681951296000364, accuracy: 0.99\n",
      "iteration no 9078: Loss: 0.23682013653413028, accuracy: 0.99\n",
      "iteration no 9079: Loss: 0.23681886779734723, accuracy: 0.99\n",
      "iteration no 9080: Loss: 0.23681941171863338, accuracy: 0.99\n",
      "iteration no 9081: Loss: 0.2368183690484965, accuracy: 0.99\n",
      "iteration no 9082: Loss: 0.23681868619510466, accuracy: 0.99\n",
      "iteration no 9083: Loss: 0.23681802021228515, accuracy: 0.99\n",
      "iteration no 9084: Loss: 0.23681813300263765, accuracy: 0.99\n",
      "iteration no 9085: Loss: 0.23681720136215736, accuracy: 0.99\n",
      "iteration no 9086: Loss: 0.23681759305962688, accuracy: 0.99\n",
      "iteration no 9087: Loss: 0.23681687746037589, accuracy: 0.99\n",
      "iteration no 9088: Loss: 0.23681652846998258, accuracy: 0.99\n",
      "iteration no 9089: Loss: 0.23681667749480606, accuracy: 0.99\n",
      "iteration no 9090: Loss: 0.23681564331350505, accuracy: 0.99\n",
      "iteration no 9091: Loss: 0.23681623856890602, accuracy: 0.99\n",
      "iteration no 9092: Loss: 0.23681524006263216, accuracy: 0.99\n",
      "iteration no 9093: Loss: 0.2368153514746018, accuracy: 0.99\n",
      "iteration no 9094: Loss: 0.2368148525841099, accuracy: 0.99\n",
      "iteration no 9095: Loss: 0.2368149672933123, accuracy: 0.99\n",
      "iteration no 9096: Loss: 0.23681411784363374, accuracy: 0.99\n",
      "iteration no 9097: Loss: 0.23681453561672194, accuracy: 0.99\n",
      "iteration no 9098: Loss: 0.23681357528319624, accuracy: 0.99\n",
      "iteration no 9099: Loss: 0.23681371221022152, accuracy: 0.99\n",
      "iteration no 9100: Loss: 0.23681340119229932, accuracy: 0.99\n",
      "iteration no 9101: Loss: 0.23681255971000925, accuracy: 0.99\n",
      "iteration no 9102: Loss: 0.2368132419140742, accuracy: 0.99\n",
      "iteration no 9103: Loss: 0.23681213409878454, accuracy: 0.99\n",
      "iteration no 9104: Loss: 0.2368123154252318, accuracy: 0.99\n",
      "iteration no 9105: Loss: 0.23681180012580003, accuracy: 0.99\n",
      "iteration no 9106: Loss: 0.23681145647465213, accuracy: 0.99\n",
      "iteration no 9107: Loss: 0.23681144470333862, accuracy: 0.99\n",
      "iteration no 9108: Loss: 0.23681106150125955, accuracy: 0.99\n",
      "iteration no 9109: Loss: 0.23681052514703105, accuracy: 0.99\n",
      "iteration no 9110: Loss: 0.23681062271624853, accuracy: 0.99\n",
      "iteration no 9111: Loss: 0.23681025745334966, accuracy: 0.99\n",
      "iteration no 9112: Loss: 0.2368094639902999, accuracy: 0.99\n",
      "iteration no 9113: Loss: 0.23681020144133935, accuracy: 0.99\n",
      "iteration no 9114: Loss: 0.23680875548366487, accuracy: 0.99\n",
      "iteration no 9115: Loss: 0.2368095763569319, accuracy: 0.99\n",
      "iteration no 9116: Loss: 0.23680837573690255, accuracy: 0.99\n",
      "iteration no 9117: Loss: 0.2368087921189157, accuracy: 0.99\n",
      "iteration no 9118: Loss: 0.23680803824845786, accuracy: 0.99\n",
      "iteration no 9119: Loss: 0.23680840590784638, accuracy: 0.99\n",
      "iteration no 9120: Loss: 0.2368072049461825, accuracy: 0.99\n",
      "iteration no 9121: Loss: 0.23680781375137064, accuracy: 0.99\n",
      "iteration no 9122: Loss: 0.23680696318123612, accuracy: 0.99\n",
      "iteration no 9123: Loss: 0.23680679182498876, accuracy: 0.99\n",
      "iteration no 9124: Loss: 0.23680680854165734, accuracy: 0.99\n",
      "iteration no 9125: Loss: 0.23680599193395097, accuracy: 0.99\n",
      "iteration no 9126: Loss: 0.2368063207890264, accuracy: 0.99\n",
      "iteration no 9127: Loss: 0.23680556636440833, accuracy: 0.99\n",
      "iteration no 9128: Loss: 0.23680542915567582, accuracy: 0.99\n",
      "iteration no 9129: Loss: 0.23680516805782773, accuracy: 0.99\n",
      "iteration no 9130: Loss: 0.23680524484106297, accuracy: 0.99\n",
      "iteration no 9131: Loss: 0.23680441517387524, accuracy: 0.99\n",
      "iteration no 9132: Loss: 0.23680484401085788, accuracy: 0.99\n",
      "iteration no 9133: Loss: 0.23680379344483393, accuracy: 0.99\n",
      "iteration no 9134: Loss: 0.23680413350986423, accuracy: 0.99\n",
      "iteration no 9135: Loss: 0.23680368278446773, accuracy: 0.99\n",
      "iteration no 9136: Loss: 0.23680301640851437, accuracy: 0.99\n",
      "iteration no 9137: Loss: 0.23680353625585443, accuracy: 0.99\n",
      "iteration no 9138: Loss: 0.23680258080503067, accuracy: 0.99\n",
      "iteration no 9139: Loss: 0.23680259806285223, accuracy: 0.99\n",
      "iteration no 9140: Loss: 0.23680219120991164, accuracy: 0.99\n",
      "iteration no 9141: Loss: 0.2368021509104904, accuracy: 0.99\n",
      "iteration no 9142: Loss: 0.23680156433671914, accuracy: 0.99\n",
      "iteration no 9143: Loss: 0.23680174209954974, accuracy: 0.99\n",
      "iteration no 9144: Loss: 0.23680083539657054, accuracy: 0.99\n",
      "iteration no 9145: Loss: 0.23680108665590227, accuracy: 0.99\n",
      "iteration no 9146: Loss: 0.2368006791823796, accuracy: 0.99\n",
      "iteration no 9147: Loss: 0.2368000663023309, accuracy: 0.99\n",
      "iteration no 9148: Loss: 0.23680054572508472, accuracy: 0.99\n",
      "iteration no 9149: Loss: 0.23679948601820372, accuracy: 0.99\n",
      "iteration no 9150: Loss: 0.2367997986267542, accuracy: 0.99\n",
      "iteration no 9151: Loss: 0.23679904126994916, accuracy: 0.99\n",
      "iteration no 9152: Loss: 0.2367992684503049, accuracy: 0.99\n",
      "iteration no 9153: Loss: 0.2367984142078713, accuracy: 0.99\n",
      "iteration no 9154: Loss: 0.23679916387776942, accuracy: 0.99\n",
      "iteration no 9155: Loss: 0.23679744385011642, accuracy: 0.99\n",
      "iteration no 9156: Loss: 0.23679874413815255, accuracy: 0.99\n",
      "iteration no 9157: Loss: 0.2367972913732953, accuracy: 0.99\n",
      "iteration no 9158: Loss: 0.23679768470826396, accuracy: 0.99\n",
      "iteration no 9159: Loss: 0.23679725261683376, accuracy: 0.99\n",
      "iteration no 9160: Loss: 0.236796758381835, accuracy: 0.99\n",
      "iteration no 9161: Loss: 0.23679681613162534, accuracy: 0.99\n",
      "iteration no 9162: Loss: 0.23679636786862168, accuracy: 0.99\n",
      "iteration no 9163: Loss: 0.2367959972072641, accuracy: 0.99\n",
      "iteration no 9164: Loss: 0.23679601455628901, accuracy: 0.99\n",
      "iteration no 9165: Loss: 0.2367958118672206, accuracy: 0.99\n",
      "iteration no 9166: Loss: 0.2367948352105358, accuracy: 0.99\n",
      "iteration no 9167: Loss: 0.23679568638086224, accuracy: 0.99\n",
      "iteration no 9168: Loss: 0.23679424287827977, accuracy: 0.99\n",
      "iteration no 9169: Loss: 0.23679472316652694, accuracy: 0.99\n",
      "iteration no 9170: Loss: 0.23679372539875093, accuracy: 0.99\n",
      "iteration no 9171: Loss: 0.23679331586763375, accuracy: 0.99\n",
      "iteration no 9172: Loss: 0.2367929709748637, accuracy: 0.99\n",
      "iteration no 9173: Loss: 0.23679257437627024, accuracy: 0.99\n",
      "iteration no 9174: Loss: 0.23679168640952444, accuracy: 0.99\n",
      "iteration no 9175: Loss: 0.23679174289443697, accuracy: 0.99\n",
      "iteration no 9176: Loss: 0.2367910197060456, accuracy: 0.99\n",
      "iteration no 9177: Loss: 0.23679019193464912, accuracy: 0.99\n",
      "iteration no 9178: Loss: 0.23679051825064584, accuracy: 0.99\n",
      "iteration no 9179: Loss: 0.23678901547851033, accuracy: 0.99\n",
      "iteration no 9180: Loss: 0.23678959278821765, accuracy: 0.99\n",
      "iteration no 9181: Loss: 0.2367885018951945, accuracy: 0.99\n",
      "iteration no 9182: Loss: 0.23678801077430153, accuracy: 0.99\n",
      "iteration no 9183: Loss: 0.23678788103323756, accuracy: 0.99\n",
      "iteration no 9184: Loss: 0.23678714315613295, accuracy: 0.99\n",
      "iteration no 9185: Loss: 0.2367866338778446, accuracy: 0.99\n",
      "iteration no 9186: Loss: 0.23678639694364137, accuracy: 0.99\n",
      "iteration no 9187: Loss: 0.23678596327987494, accuracy: 0.99\n",
      "iteration no 9188: Loss: 0.23678500730443014, accuracy: 0.99\n",
      "iteration no 9189: Loss: 0.23678535239786996, accuracy: 0.99\n",
      "iteration no 9190: Loss: 0.2367838365515455, accuracy: 0.99\n",
      "iteration no 9191: Loss: 0.23678449039314425, accuracy: 0.99\n",
      "iteration no 9192: Loss: 0.2367831680957333, accuracy: 0.99\n",
      "iteration no 9193: Loss: 0.23678331507677813, accuracy: 0.99\n",
      "iteration no 9194: Loss: 0.23678247440927788, accuracy: 0.99\n",
      "iteration no 9195: Loss: 0.23678249050011813, accuracy: 0.99\n",
      "iteration no 9196: Loss: 0.23678110044342965, accuracy: 0.99\n",
      "iteration no 9197: Loss: 0.2367817642778777, accuracy: 0.99\n",
      "iteration no 9198: Loss: 0.2367805031036989, accuracy: 0.99\n",
      "iteration no 9199: Loss: 0.23678034881355675, accuracy: 0.99\n",
      "iteration no 9200: Loss: 0.23678005060995455, accuracy: 0.99\n",
      "iteration no 9201: Loss: 0.23677900684436376, accuracy: 0.99\n",
      "iteration no 9202: Loss: 0.23677922764161363, accuracy: 0.99\n",
      "iteration no 9203: Loss: 0.23677827809754776, accuracy: 0.99\n",
      "iteration no 9204: Loss: 0.236778093032892, accuracy: 0.99\n",
      "iteration no 9205: Loss: 0.23677740676195108, accuracy: 0.99\n",
      "iteration no 9206: Loss: 0.2367775868856517, accuracy: 0.99\n",
      "iteration no 9207: Loss: 0.2367760698515206, accuracy: 0.99\n",
      "iteration no 9208: Loss: 0.2367770396452263, accuracy: 0.99\n",
      "iteration no 9209: Loss: 0.23677566208698358, accuracy: 0.99\n",
      "iteration no 9210: Loss: 0.23677585554200029, accuracy: 0.99\n",
      "iteration no 9211: Loss: 0.23677515331235427, accuracy: 0.99\n",
      "iteration no 9212: Loss: 0.23677475469239126, accuracy: 0.99\n",
      "iteration no 9213: Loss: 0.236774340416869, accuracy: 0.99\n",
      "iteration no 9214: Loss: 0.23677412959631927, accuracy: 0.99\n",
      "iteration no 9215: Loss: 0.23677327435936574, accuracy: 0.99\n",
      "iteration no 9216: Loss: 0.23677350246964646, accuracy: 0.99\n",
      "iteration no 9217: Loss: 0.23677278717022815, accuracy: 0.99\n",
      "iteration no 9218: Loss: 0.23677199762650117, accuracy: 0.99\n",
      "iteration no 9219: Loss: 0.23677238205204118, accuracy: 0.99\n",
      "iteration no 9220: Loss: 0.23677118580452347, accuracy: 0.99\n",
      "iteration no 9221: Loss: 0.23677143272445392, accuracy: 0.99\n",
      "iteration no 9222: Loss: 0.23677069736073106, accuracy: 0.99\n",
      "iteration no 9223: Loss: 0.236770531745635, accuracy: 0.99\n",
      "iteration no 9224: Loss: 0.23676994616949504, accuracy: 0.99\n",
      "iteration no 9225: Loss: 0.23676993061303514, accuracy: 0.99\n",
      "iteration no 9226: Loss: 0.2367689614137537, accuracy: 0.99\n",
      "iteration no 9227: Loss: 0.23676931729454656, accuracy: 0.99\n",
      "iteration no 9228: Loss: 0.23676872276185434, accuracy: 0.99\n",
      "iteration no 9229: Loss: 0.23676807932268107, accuracy: 0.99\n",
      "iteration no 9230: Loss: 0.23676837553634655, accuracy: 0.99\n",
      "iteration no 9231: Loss: 0.23676752334025314, accuracy: 0.99\n",
      "iteration no 9232: Loss: 0.2367673003706814, accuracy: 0.99\n",
      "iteration no 9233: Loss: 0.23676721060072276, accuracy: 0.99\n",
      "iteration no 9234: Loss: 0.23676686074548214, accuracy: 0.99\n",
      "iteration no 9235: Loss: 0.23676594766759315, accuracy: 0.99\n",
      "iteration no 9236: Loss: 0.23676638607646283, accuracy: 0.99\n",
      "iteration no 9237: Loss: 0.2367655833877368, accuracy: 0.99\n",
      "iteration no 9238: Loss: 0.23676540413574593, accuracy: 0.99\n",
      "iteration no 9239: Loss: 0.23676468950640495, accuracy: 0.99\n",
      "iteration no 9240: Loss: 0.23676484370110945, accuracy: 0.99\n",
      "iteration no 9241: Loss: 0.23676432530891073, accuracy: 0.99\n",
      "iteration no 9242: Loss: 0.23676365555824386, accuracy: 0.99\n",
      "iteration no 9243: Loss: 0.23676357199476106, accuracy: 0.99\n",
      "iteration no 9244: Loss: 0.23676337154142535, accuracy: 0.99\n",
      "iteration no 9245: Loss: 0.2367629891334537, accuracy: 0.99\n",
      "iteration no 9246: Loss: 0.23676212888141607, accuracy: 0.99\n",
      "iteration no 9247: Loss: 0.23676249408985084, accuracy: 0.99\n",
      "iteration no 9248: Loss: 0.23676168481758664, accuracy: 0.99\n",
      "iteration no 9249: Loss: 0.23676169867096825, accuracy: 0.99\n",
      "iteration no 9250: Loss: 0.2367604808343275, accuracy: 0.99\n",
      "iteration no 9251: Loss: 0.23676142070263193, accuracy: 0.99\n",
      "iteration no 9252: Loss: 0.23676004076065774, accuracy: 0.99\n",
      "iteration no 9253: Loss: 0.2367603094679957, accuracy: 0.99\n",
      "iteration no 9254: Loss: 0.2367592474548405, accuracy: 0.99\n",
      "iteration no 9255: Loss: 0.2367598374362383, accuracy: 0.99\n",
      "iteration no 9256: Loss: 0.23675890978792583, accuracy: 0.99\n",
      "iteration no 9257: Loss: 0.23675853999564267, accuracy: 0.99\n",
      "iteration no 9258: Loss: 0.23675842310051415, accuracy: 0.99\n",
      "iteration no 9259: Loss: 0.2367579768616344, accuracy: 0.99\n",
      "iteration no 9260: Loss: 0.23675777363776135, accuracy: 0.99\n",
      "iteration no 9261: Loss: 0.2367567464145604, accuracy: 0.99\n",
      "iteration no 9262: Loss: 0.2367575484105524, accuracy: 0.99\n",
      "iteration no 9263: Loss: 0.23675630707426873, accuracy: 0.99\n",
      "iteration no 9264: Loss: 0.2367565808168141, accuracy: 0.99\n",
      "iteration no 9265: Loss: 0.23675519732760597, accuracy: 0.99\n",
      "iteration no 9266: Loss: 0.23675637321496482, accuracy: 0.99\n",
      "iteration no 9267: Loss: 0.23675481013552102, accuracy: 0.99\n",
      "iteration no 9268: Loss: 0.23675519543706927, accuracy: 0.99\n",
      "iteration no 9269: Loss: 0.23675429799705855, accuracy: 0.99\n",
      "iteration no 9270: Loss: 0.23675465278129676, accuracy: 0.99\n",
      "iteration no 9271: Loss: 0.23675366230539602, accuracy: 0.99\n",
      "iteration no 9272: Loss: 0.23675352944288217, accuracy: 0.99\n",
      "iteration no 9273: Loss: 0.23675338450261052, accuracy: 0.99\n",
      "iteration no 9274: Loss: 0.23675302651735924, accuracy: 0.99\n",
      "iteration no 9275: Loss: 0.23675244744108181, accuracy: 0.99\n",
      "iteration no 9276: Loss: 0.2367519362165169, accuracy: 0.99\n",
      "iteration no 9277: Loss: 0.23675229232556327, accuracy: 0.99\n",
      "iteration no 9278: Loss: 0.23675140197366812, accuracy: 0.99\n",
      "iteration no 9279: Loss: 0.23675127167546053, accuracy: 0.99\n",
      "iteration no 9280: Loss: 0.2367506052880966, accuracy: 0.99\n",
      "iteration no 9281: Loss: 0.23675105156578985, accuracy: 0.99\n",
      "iteration no 9282: Loss: 0.23674973517808434, accuracy: 0.99\n",
      "iteration no 9283: Loss: 0.23675013382731136, accuracy: 0.99\n",
      "iteration no 9284: Loss: 0.2367494743829284, accuracy: 0.99\n",
      "iteration no 9285: Loss: 0.23674966892584892, accuracy: 0.99\n",
      "iteration no 9286: Loss: 0.23674853189658243, accuracy: 0.99\n",
      "iteration no 9287: Loss: 0.23674867829748047, accuracy: 0.99\n",
      "iteration no 9288: Loss: 0.23674838109901308, accuracy: 0.99\n",
      "iteration no 9289: Loss: 0.23674805667406973, accuracy: 0.99\n",
      "iteration no 9290: Loss: 0.23674737392671769, accuracy: 0.99\n",
      "iteration no 9291: Loss: 0.23674735639258826, accuracy: 0.99\n",
      "iteration no 9292: Loss: 0.23674702299898182, accuracy: 0.99\n",
      "iteration no 9293: Loss: 0.23674639259762253, accuracy: 0.99\n",
      "iteration no 9294: Loss: 0.23674617036877058, accuracy: 0.99\n",
      "iteration no 9295: Loss: 0.23674522112198682, accuracy: 0.99\n",
      "iteration no 9296: Loss: 0.23674532732285136, accuracy: 0.99\n",
      "iteration no 9297: Loss: 0.2367445160102215, accuracy: 0.99\n",
      "iteration no 9298: Loss: 0.2367448002630232, accuracy: 0.99\n",
      "iteration no 9299: Loss: 0.2367437803861343, accuracy: 0.99\n",
      "iteration no 9300: Loss: 0.2367436907431676, accuracy: 0.99\n",
      "iteration no 9301: Loss: 0.23674293431112747, accuracy: 0.99\n",
      "iteration no 9302: Loss: 0.23674256633716023, accuracy: 0.99\n",
      "iteration no 9303: Loss: 0.23674200690560007, accuracy: 0.99\n",
      "iteration no 9304: Loss: 0.23674186608374467, accuracy: 0.99\n",
      "iteration no 9305: Loss: 0.23674155319466883, accuracy: 0.99\n",
      "iteration no 9306: Loss: 0.23674085464867584, accuracy: 0.99\n",
      "iteration no 9307: Loss: 0.23674057906225773, accuracy: 0.99\n",
      "iteration no 9308: Loss: 0.23673970644576006, accuracy: 0.99\n",
      "iteration no 9309: Loss: 0.23673973922971808, accuracy: 0.99\n",
      "iteration no 9310: Loss: 0.23673880820440932, accuracy: 0.99\n",
      "iteration no 9311: Loss: 0.2367391345155896, accuracy: 0.99\n",
      "iteration no 9312: Loss: 0.23673830795661674, accuracy: 0.99\n",
      "iteration no 9313: Loss: 0.2367381556427009, accuracy: 0.99\n",
      "iteration no 9314: Loss: 0.23673756451044847, accuracy: 0.99\n",
      "iteration no 9315: Loss: 0.2367371615387424, accuracy: 0.99\n",
      "iteration no 9316: Loss: 0.23673678900058587, accuracy: 0.99\n",
      "iteration no 9317: Loss: 0.23673614785829034, accuracy: 0.99\n",
      "iteration no 9318: Loss: 0.2367362474563095, accuracy: 0.99\n",
      "iteration no 9319: Loss: 0.23673568744462137, accuracy: 0.99\n",
      "iteration no 9320: Loss: 0.2367355904524872, accuracy: 0.99\n",
      "iteration no 9321: Loss: 0.23673468426141567, accuracy: 0.99\n",
      "iteration no 9322: Loss: 0.23673472740306423, accuracy: 0.99\n",
      "iteration no 9323: Loss: 0.23673369898726176, accuracy: 0.99\n",
      "iteration no 9324: Loss: 0.23673387760660194, accuracy: 0.99\n",
      "iteration no 9325: Loss: 0.23673319818883787, accuracy: 0.99\n",
      "iteration no 9326: Loss: 0.23673347348378215, accuracy: 0.99\n",
      "iteration no 9327: Loss: 0.23673239090662135, accuracy: 0.99\n",
      "iteration no 9328: Loss: 0.23673246641317813, accuracy: 0.99\n",
      "iteration no 9329: Loss: 0.23673159401512434, accuracy: 0.99\n",
      "iteration no 9330: Loss: 0.23673155869716697, accuracy: 0.99\n",
      "iteration no 9331: Loss: 0.2367309015333734, accuracy: 0.99\n",
      "iteration no 9332: Loss: 0.23673090435871336, accuracy: 0.99\n",
      "iteration no 9333: Loss: 0.2367304710493281, accuracy: 0.99\n",
      "iteration no 9334: Loss: 0.23672986800935708, accuracy: 0.99\n",
      "iteration no 9335: Loss: 0.2367297560806837, accuracy: 0.99\n",
      "iteration no 9336: Loss: 0.23672889384596835, accuracy: 0.99\n",
      "iteration no 9337: Loss: 0.23672897199165957, accuracy: 0.99\n",
      "iteration no 9338: Loss: 0.23672804772076145, accuracy: 0.99\n",
      "iteration no 9339: Loss: 0.2367286506120471, accuracy: 0.99\n",
      "iteration no 9340: Loss: 0.23672735343114978, accuracy: 0.99\n",
      "iteration no 9341: Loss: 0.23672796635511256, accuracy: 0.99\n",
      "iteration no 9342: Loss: 0.23672653216611694, accuracy: 0.99\n",
      "iteration no 9343: Loss: 0.23672693957192328, accuracy: 0.99\n",
      "iteration no 9344: Loss: 0.23672573653651835, accuracy: 0.99\n",
      "iteration no 9345: Loss: 0.23672626834301344, accuracy: 0.99\n",
      "iteration no 9346: Loss: 0.23672549618992958, accuracy: 0.99\n",
      "iteration no 9347: Loss: 0.236725343082687, accuracy: 0.99\n",
      "iteration no 9348: Loss: 0.2367246883853359, accuracy: 0.99\n",
      "iteration no 9349: Loss: 0.23672437324602283, accuracy: 0.99\n",
      "iteration no 9350: Loss: 0.2367240250757312, accuracy: 0.99\n",
      "iteration no 9351: Loss: 0.236723435536763, accuracy: 0.99\n",
      "iteration no 9352: Loss: 0.23672366804052805, accuracy: 0.99\n",
      "iteration no 9353: Loss: 0.23672280212325547, accuracy: 0.99\n",
      "iteration no 9354: Loss: 0.23672293258825383, accuracy: 0.99\n",
      "iteration no 9355: Loss: 0.23672184392667125, accuracy: 0.99\n",
      "iteration no 9356: Loss: 0.23672222993574132, accuracy: 0.99\n",
      "iteration no 9357: Loss: 0.2367209179771524, accuracy: 0.99\n",
      "iteration no 9358: Loss: 0.23672164304065332, accuracy: 0.99\n",
      "iteration no 9359: Loss: 0.23672060025773917, accuracy: 0.99\n",
      "iteration no 9360: Loss: 0.23672095131300647, accuracy: 0.99\n",
      "iteration no 9361: Loss: 0.23671988879508, accuracy: 0.99\n",
      "iteration no 9362: Loss: 0.23671996485402702, accuracy: 0.99\n",
      "iteration no 9363: Loss: 0.2367190671429934, accuracy: 0.99\n",
      "iteration no 9364: Loss: 0.23671901484289462, accuracy: 0.99\n",
      "iteration no 9365: Loss: 0.23671879399657636, accuracy: 0.99\n",
      "iteration no 9366: Loss: 0.2367184310615133, accuracy: 0.99\n",
      "iteration no 9367: Loss: 0.2367180630188967, accuracy: 0.99\n",
      "iteration no 9368: Loss: 0.23671734784277632, accuracy: 0.99\n",
      "iteration no 9369: Loss: 0.23671744662840205, accuracy: 0.99\n",
      "iteration no 9370: Loss: 0.2367164618693244, accuracy: 0.99\n",
      "iteration no 9371: Loss: 0.23671702319668497, accuracy: 0.99\n",
      "iteration no 9372: Loss: 0.23671611986546653, accuracy: 0.99\n",
      "iteration no 9373: Loss: 0.23671655529266783, accuracy: 0.99\n",
      "iteration no 9374: Loss: 0.2367155766810854, accuracy: 0.99\n",
      "iteration no 9375: Loss: 0.23671579584546065, accuracy: 0.99\n",
      "iteration no 9376: Loss: 0.23671497026613408, accuracy: 0.99\n",
      "iteration no 9377: Loss: 0.23671495122064856, accuracy: 0.99\n",
      "iteration no 9378: Loss: 0.2367147889947405, accuracy: 0.99\n",
      "iteration no 9379: Loss: 0.23671459954762936, accuracy: 0.99\n",
      "iteration no 9380: Loss: 0.23671421348308203, accuracy: 0.99\n",
      "iteration no 9381: Loss: 0.23671372193706436, accuracy: 0.99\n",
      "iteration no 9382: Loss: 0.23671357071582394, accuracy: 0.99\n",
      "iteration no 9383: Loss: 0.236712876990563, accuracy: 0.99\n",
      "iteration no 9384: Loss: 0.2367133539309867, accuracy: 0.99\n",
      "iteration no 9385: Loss: 0.23671257472040697, accuracy: 0.99\n",
      "iteration no 9386: Loss: 0.23671274836375697, accuracy: 0.99\n",
      "iteration no 9387: Loss: 0.23671185589729368, accuracy: 0.99\n",
      "iteration no 9388: Loss: 0.23671204993274364, accuracy: 0.99\n",
      "iteration no 9389: Loss: 0.23671135171121566, accuracy: 0.99\n",
      "iteration no 9390: Loss: 0.2367114316796441, accuracy: 0.99\n",
      "iteration no 9391: Loss: 0.2367112021706005, accuracy: 0.99\n",
      "iteration no 9392: Loss: 0.2367108128412907, accuracy: 0.99\n",
      "iteration no 9393: Loss: 0.2367105958416877, accuracy: 0.99\n",
      "iteration no 9394: Loss: 0.2367100847816324, accuracy: 0.99\n",
      "iteration no 9395: Loss: 0.23670996132845606, accuracy: 0.99\n",
      "iteration no 9396: Loss: 0.23670943795929877, accuracy: 0.99\n",
      "iteration no 9397: Loss: 0.23670983550325453, accuracy: 0.99\n",
      "iteration no 9398: Loss: 0.23670901470139616, accuracy: 0.99\n",
      "iteration no 9399: Loss: 0.23670915148678945, accuracy: 0.99\n",
      "iteration no 9400: Loss: 0.23670817452483506, accuracy: 0.99\n",
      "iteration no 9401: Loss: 0.23670846758817443, accuracy: 0.99\n",
      "iteration no 9402: Loss: 0.23670779239214212, accuracy: 0.99\n",
      "iteration no 9403: Loss: 0.23670812980289718, accuracy: 0.99\n",
      "iteration no 9404: Loss: 0.23670743555002938, accuracy: 0.99\n",
      "iteration no 9405: Loss: 0.23670721547255147, accuracy: 0.99\n",
      "iteration no 9406: Loss: 0.2367068855905714, accuracy: 0.99\n",
      "iteration no 9407: Loss: 0.23670642970699485, accuracy: 0.99\n",
      "iteration no 9408: Loss: 0.23670655092650103, accuracy: 0.99\n",
      "iteration no 9409: Loss: 0.23670606677816947, accuracy: 0.99\n",
      "iteration no 9410: Loss: 0.2367062724422485, accuracy: 0.99\n",
      "iteration no 9411: Loss: 0.2367054012025801, accuracy: 0.99\n",
      "iteration no 9412: Loss: 0.23670565210142896, accuracy: 0.99\n",
      "iteration no 9413: Loss: 0.2367045360139356, accuracy: 0.99\n",
      "iteration no 9414: Loss: 0.23670517829124546, accuracy: 0.99\n",
      "iteration no 9415: Loss: 0.2367043102181722, accuracy: 0.99\n",
      "iteration no 9416: Loss: 0.23670483173595677, accuracy: 0.99\n",
      "iteration no 9417: Loss: 0.23670371816302133, accuracy: 0.99\n",
      "iteration no 9418: Loss: 0.2367039458431759, accuracy: 0.99\n",
      "iteration no 9419: Loss: 0.2367031424355304, accuracy: 0.99\n",
      "iteration no 9420: Loss: 0.2367033283937221, accuracy: 0.99\n",
      "iteration no 9421: Loss: 0.23670316060563173, accuracy: 0.99\n",
      "iteration no 9422: Loss: 0.23670263973022054, accuracy: 0.99\n",
      "iteration no 9423: Loss: 0.2367025508262473, accuracy: 0.99\n",
      "iteration no 9424: Loss: 0.23670188165423273, accuracy: 0.99\n",
      "iteration no 9425: Loss: 0.23670207307476146, accuracy: 0.99\n",
      "iteration no 9426: Loss: 0.23670124047461616, accuracy: 0.99\n",
      "iteration no 9427: Loss: 0.23670196398525445, accuracy: 0.99\n",
      "iteration no 9428: Loss: 0.23670071547948376, accuracy: 0.99\n",
      "iteration no 9429: Loss: 0.23670137946883046, accuracy: 0.99\n",
      "iteration no 9430: Loss: 0.23670000283715698, accuracy: 0.99\n",
      "iteration no 9431: Loss: 0.2367006351931384, accuracy: 0.99\n",
      "iteration no 9432: Loss: 0.23669985569073565, accuracy: 0.99\n",
      "iteration no 9433: Loss: 0.236700261624483, accuracy: 0.99\n",
      "iteration no 9434: Loss: 0.23669954209139854, accuracy: 0.99\n",
      "iteration no 9435: Loss: 0.23669933989986508, accuracy: 0.99\n",
      "iteration no 9436: Loss: 0.23669894611240738, accuracy: 0.99\n",
      "iteration no 9437: Loss: 0.2366985958946355, accuracy: 0.99\n",
      "iteration no 9438: Loss: 0.23669882866310943, accuracy: 0.99\n",
      "iteration no 9439: Loss: 0.23669823600128548, accuracy: 0.99\n",
      "iteration no 9440: Loss: 0.2366982731735069, accuracy: 0.99\n",
      "iteration no 9441: Loss: 0.2366975151777434, accuracy: 0.99\n",
      "iteration no 9442: Loss: 0.2366977518161737, accuracy: 0.99\n",
      "iteration no 9443: Loss: 0.2366967810879177, accuracy: 0.99\n",
      "iteration no 9444: Loss: 0.23669760744531804, accuracy: 0.99\n",
      "iteration no 9445: Loss: 0.2366967852170924, accuracy: 0.99\n",
      "iteration no 9446: Loss: 0.23669684364540788, accuracy: 0.99\n",
      "iteration no 9447: Loss: 0.23669619356135604, accuracy: 0.99\n",
      "iteration no 9448: Loss: 0.23669609128305208, accuracy: 0.99\n",
      "iteration no 9449: Loss: 0.2366957403599252, accuracy: 0.99\n",
      "iteration no 9450: Loss: 0.23669591365556147, accuracy: 0.99\n",
      "iteration no 9451: Loss: 0.2366955544029087, accuracy: 0.99\n",
      "iteration no 9452: Loss: 0.2366950683652364, accuracy: 0.99\n",
      "iteration no 9453: Loss: 0.23669492309559712, accuracy: 0.99\n",
      "iteration no 9454: Loss: 0.23669440615256382, accuracy: 0.99\n",
      "iteration no 9455: Loss: 0.23669460902837675, accuracy: 0.99\n",
      "iteration no 9456: Loss: 0.23669408903919809, accuracy: 0.99\n",
      "iteration no 9457: Loss: 0.23669419018616544, accuracy: 0.99\n",
      "iteration no 9458: Loss: 0.23669364317056552, accuracy: 0.99\n",
      "iteration no 9459: Loss: 0.23669344387924315, accuracy: 0.99\n",
      "iteration no 9460: Loss: 0.23669304066528501, accuracy: 0.99\n",
      "iteration no 9461: Loss: 0.23669304175951664, accuracy: 0.99\n",
      "iteration no 9462: Loss: 0.2366929863182551, accuracy: 0.99\n",
      "iteration no 9463: Loss: 0.2366924552933175, accuracy: 0.99\n",
      "iteration no 9464: Loss: 0.236692308102211, accuracy: 0.99\n",
      "iteration no 9465: Loss: 0.2366917156076287, accuracy: 0.99\n",
      "iteration no 9466: Loss: 0.23669180634899625, accuracy: 0.99\n",
      "iteration no 9467: Loss: 0.23669148970764364, accuracy: 0.99\n",
      "iteration no 9468: Loss: 0.2366917044629075, accuracy: 0.99\n",
      "iteration no 9469: Loss: 0.23669076013093035, accuracy: 0.99\n",
      "iteration no 9470: Loss: 0.2366910813773035, accuracy: 0.99\n",
      "iteration no 9471: Loss: 0.23669031374476365, accuracy: 0.99\n",
      "iteration no 9472: Loss: 0.2366903226927652, accuracy: 0.99\n",
      "iteration no 9473: Loss: 0.2366902905078751, accuracy: 0.99\n",
      "iteration no 9474: Loss: 0.2366899026572417, accuracy: 0.99\n",
      "iteration no 9475: Loss: 0.2366898128616876, accuracy: 0.99\n",
      "iteration no 9476: Loss: 0.23668909594341475, accuracy: 0.99\n",
      "iteration no 9477: Loss: 0.23668925350172054, accuracy: 0.99\n",
      "iteration no 9478: Loss: 0.23668858802024667, accuracy: 0.99\n",
      "iteration no 9479: Loss: 0.23668924063340077, accuracy: 0.99\n",
      "iteration no 9480: Loss: 0.23668815503201007, accuracy: 0.99\n",
      "iteration no 9481: Loss: 0.23668861471645092, accuracy: 0.99\n",
      "iteration no 9482: Loss: 0.23668747914334398, accuracy: 0.99\n",
      "iteration no 9483: Loss: 0.2366880826454877, accuracy: 0.99\n",
      "iteration no 9484: Loss: 0.23668741729871345, accuracy: 0.99\n",
      "iteration no 9485: Loss: 0.23668770408650247, accuracy: 0.99\n",
      "iteration no 9486: Loss: 0.23668695418064606, accuracy: 0.99\n",
      "iteration no 9487: Loss: 0.23668697182284718, accuracy: 0.99\n",
      "iteration no 9488: Loss: 0.23668649363319932, accuracy: 0.99\n",
      "iteration no 9489: Loss: 0.23668617076964454, accuracy: 0.99\n",
      "iteration no 9490: Loss: 0.23668645913553232, accuracy: 0.99\n",
      "iteration no 9491: Loss: 0.23668592367566518, accuracy: 0.99\n",
      "iteration no 9492: Loss: 0.23668593212620775, accuracy: 0.99\n",
      "iteration no 9493: Loss: 0.23668509817967986, accuracy: 0.99\n",
      "iteration no 9494: Loss: 0.23668534217028994, accuracy: 0.99\n",
      "iteration no 9495: Loss: 0.23668470187474014, accuracy: 0.99\n",
      "iteration no 9496: Loss: 0.23668539197111535, accuracy: 0.99\n",
      "iteration no 9497: Loss: 0.23668422689536667, accuracy: 0.99\n",
      "iteration no 9498: Loss: 0.23668471409228642, accuracy: 0.99\n",
      "iteration no 9499: Loss: 0.23668374788270702, accuracy: 0.99\n",
      "iteration no 9500: Loss: 0.23668405189643052, accuracy: 0.99\n",
      "iteration no 9501: Loss: 0.23668361393776538, accuracy: 0.99\n",
      "iteration no 9502: Loss: 0.23668372586924308, accuracy: 0.99\n",
      "iteration no 9503: Loss: 0.23668324860533535, accuracy: 0.99\n",
      "iteration no 9504: Loss: 0.23668299879760024, accuracy: 0.99\n",
      "iteration no 9505: Loss: 0.23668264799031064, accuracy: 0.99\n",
      "iteration no 9506: Loss: 0.23668228157958546, accuracy: 0.99\n",
      "iteration no 9507: Loss: 0.2366827216938467, accuracy: 0.99\n",
      "iteration no 9508: Loss: 0.23668195012563753, accuracy: 0.99\n",
      "iteration no 9509: Loss: 0.23668216040840076, accuracy: 0.99\n",
      "iteration no 9510: Loss: 0.2366811869539422, accuracy: 0.99\n",
      "iteration no 9511: Loss: 0.23668170462845428, accuracy: 0.99\n",
      "iteration no 9512: Loss: 0.23668109025045223, accuracy: 0.99\n",
      "iteration no 9513: Loss: 0.23668141003715082, accuracy: 0.99\n",
      "iteration no 9514: Loss: 0.23668070373927597, accuracy: 0.99\n",
      "iteration no 9515: Loss: 0.23668072481714023, accuracy: 0.99\n",
      "iteration no 9516: Loss: 0.23668023225481338, accuracy: 0.99\n",
      "iteration no 9517: Loss: 0.2366799838245992, accuracy: 0.99\n",
      "iteration no 9518: Loss: 0.2366800765718344, accuracy: 0.99\n",
      "iteration no 9519: Loss: 0.23667978980903692, accuracy: 0.99\n",
      "iteration no 9520: Loss: 0.2366795604513637, accuracy: 0.99\n",
      "iteration no 9521: Loss: 0.23667901231470234, accuracy: 0.99\n",
      "iteration no 9522: Loss: 0.2366791076517052, accuracy: 0.99\n",
      "iteration no 9523: Loss: 0.23667878951710025, accuracy: 0.99\n",
      "iteration no 9524: Loss: 0.23667893637695622, accuracy: 0.99\n",
      "iteration no 9525: Loss: 0.23667835498839723, accuracy: 0.99\n",
      "iteration no 9526: Loss: 0.23667816532871894, accuracy: 0.99\n",
      "iteration no 9527: Loss: 0.2366779063924599, accuracy: 0.99\n",
      "iteration no 9528: Loss: 0.23667756060676615, accuracy: 0.99\n",
      "iteration no 9529: Loss: 0.2366777849506646, accuracy: 0.99\n",
      "iteration no 9530: Loss: 0.23667726700404626, accuracy: 0.99\n",
      "iteration no 9531: Loss: 0.23667724453595682, accuracy: 0.99\n",
      "iteration no 9532: Loss: 0.23667666703933654, accuracy: 0.99\n",
      "iteration no 9533: Loss: 0.23667664612915684, accuracy: 0.99\n",
      "iteration no 9534: Loss: 0.2366762524396935, accuracy: 0.99\n",
      "iteration no 9535: Loss: 0.23667675562712412, accuracy: 0.99\n",
      "iteration no 9536: Loss: 0.23667583035675308, accuracy: 0.99\n",
      "iteration no 9537: Loss: 0.2366759606505629, accuracy: 0.99\n",
      "iteration no 9538: Loss: 0.23667528574125746, accuracy: 0.99\n",
      "iteration no 9539: Loss: 0.2366753416182277, accuracy: 0.99\n",
      "iteration no 9540: Loss: 0.2366753537910844, accuracy: 0.99\n",
      "iteration no 9541: Loss: 0.23667493645554516, accuracy: 0.99\n",
      "iteration no 9542: Loss: 0.2366747697565057, accuracy: 0.99\n",
      "iteration no 9543: Loss: 0.23667430690231261, accuracy: 0.99\n",
      "iteration no 9544: Loss: 0.23667443856051112, accuracy: 0.99\n",
      "iteration no 9545: Loss: 0.23667390734493438, accuracy: 0.99\n",
      "iteration no 9546: Loss: 0.23667437831253524, accuracy: 0.99\n",
      "iteration no 9547: Loss: 0.236673424194715, accuracy: 0.99\n",
      "iteration no 9548: Loss: 0.2366738433084188, accuracy: 0.99\n",
      "iteration no 9549: Loss: 0.23667269037191513, accuracy: 0.99\n",
      "iteration no 9550: Loss: 0.23667329244665042, accuracy: 0.99\n",
      "iteration no 9551: Loss: 0.2366727965123521, accuracy: 0.99\n",
      "iteration no 9552: Loss: 0.2366729427251839, accuracy: 0.99\n",
      "iteration no 9553: Loss: 0.23667216959038517, accuracy: 0.99\n",
      "iteration no 9554: Loss: 0.2366721761999624, accuracy: 0.99\n",
      "iteration no 9555: Loss: 0.2366718447385671, accuracy: 0.99\n",
      "iteration no 9556: Loss: 0.23667186401147475, accuracy: 0.99\n",
      "iteration no 9557: Loss: 0.23667183842611475, accuracy: 0.99\n",
      "iteration no 9558: Loss: 0.23667108487119115, accuracy: 0.99\n",
      "iteration no 9559: Loss: 0.23667141475744052, accuracy: 0.99\n",
      "iteration no 9560: Loss: 0.2366703258552149, accuracy: 0.99\n",
      "iteration no 9561: Loss: 0.2366711261830246, accuracy: 0.99\n",
      "iteration no 9562: Loss: 0.2366701114010643, accuracy: 0.99\n",
      "iteration no 9563: Loss: 0.23667093014391624, accuracy: 0.99\n",
      "iteration no 9564: Loss: 0.2366696096868004, accuracy: 0.99\n",
      "iteration no 9565: Loss: 0.23667008344173873, accuracy: 0.99\n",
      "iteration no 9566: Loss: 0.2366692371881138, accuracy: 0.99\n",
      "iteration no 9567: Loss: 0.23666989513561176, accuracy: 0.99\n",
      "iteration no 9568: Loss: 0.23666925629912117, accuracy: 0.99\n",
      "iteration no 9569: Loss: 0.23666900835163274, accuracy: 0.99\n",
      "iteration no 9570: Loss: 0.23666878203942024, accuracy: 0.99\n",
      "iteration no 9571: Loss: 0.23666830924451704, accuracy: 0.99\n",
      "iteration no 9572: Loss: 0.23666876987972385, accuracy: 0.99\n",
      "iteration no 9573: Loss: 0.23666797356254476, accuracy: 0.99\n",
      "iteration no 9574: Loss: 0.23666838997847855, accuracy: 0.99\n",
      "iteration no 9575: Loss: 0.236667263623242, accuracy: 0.99\n",
      "iteration no 9576: Loss: 0.23666789186813905, accuracy: 0.99\n",
      "iteration no 9577: Loss: 0.23666704694987106, accuracy: 0.99\n",
      "iteration no 9578: Loss: 0.23666771841774978, accuracy: 0.99\n",
      "iteration no 9579: Loss: 0.23666685314909314, accuracy: 0.99\n",
      "iteration no 9580: Loss: 0.23666699489844395, accuracy: 0.99\n",
      "iteration no 9581: Loss: 0.23666634153448957, accuracy: 0.99\n",
      "iteration no 9582: Loss: 0.23666632129032106, accuracy: 0.99\n",
      "iteration no 9583: Loss: 0.23666639244891285, accuracy: 0.99\n",
      "iteration no 9584: Loss: 0.236666035311875, accuracy: 0.99\n",
      "iteration no 9585: Loss: 0.23666579599661447, accuracy: 0.99\n",
      "iteration no 9586: Loss: 0.23666525797346388, accuracy: 0.99\n",
      "iteration no 9587: Loss: 0.2366654031222212, accuracy: 0.99\n",
      "iteration no 9588: Loss: 0.23666494547001582, accuracy: 0.99\n",
      "iteration no 9589: Loss: 0.23666542856337497, accuracy: 0.99\n",
      "iteration no 9590: Loss: 0.23666452306639996, accuracy: 0.99\n",
      "iteration no 9591: Loss: 0.236664783675442, accuracy: 0.99\n",
      "iteration no 9592: Loss: 0.23666404943470887, accuracy: 0.99\n",
      "iteration no 9593: Loss: 0.2366643029972877, accuracy: 0.99\n",
      "iteration no 9594: Loss: 0.23666405502119703, accuracy: 0.99\n",
      "iteration no 9595: Loss: 0.23666388441140168, accuracy: 0.99\n",
      "iteration no 9596: Loss: 0.2366635638375283, accuracy: 0.99\n",
      "iteration no 9597: Loss: 0.23666302307578754, accuracy: 0.99\n",
      "iteration no 9598: Loss: 0.2366633357856487, accuracy: 0.99\n",
      "iteration no 9599: Loss: 0.23666298740508074, accuracy: 0.99\n",
      "iteration no 9600: Loss: 0.23666310360128384, accuracy: 0.99\n",
      "iteration no 9601: Loss: 0.23666216981469534, accuracy: 0.99\n",
      "iteration no 9602: Loss: 0.23666255099859718, accuracy: 0.99\n",
      "iteration no 9603: Loss: 0.23666189217762967, accuracy: 0.99\n",
      "iteration no 9604: Loss: 0.23666231756686096, accuracy: 0.99\n",
      "iteration no 9605: Loss: 0.23666178686959732, accuracy: 0.99\n",
      "iteration no 9606: Loss: 0.2366614783168666, accuracy: 0.99\n",
      "iteration no 9607: Loss: 0.23666137781304553, accuracy: 0.99\n",
      "iteration no 9608: Loss: 0.2366608763071636, accuracy: 0.99\n",
      "iteration no 9609: Loss: 0.2366613583140918, accuracy: 0.99\n",
      "iteration no 9610: Loss: 0.2366606241846195, accuracy: 0.99\n",
      "iteration no 9611: Loss: 0.2366608940968966, accuracy: 0.99\n",
      "iteration no 9612: Loss: 0.23666004198527507, accuracy: 0.99\n",
      "iteration no 9613: Loss: 0.2366603009043241, accuracy: 0.99\n",
      "iteration no 9614: Loss: 0.23665975985904472, accuracy: 0.99\n",
      "iteration no 9615: Loss: 0.2366602578919225, accuracy: 0.99\n",
      "iteration no 9616: Loss: 0.23665942572937065, accuracy: 0.99\n",
      "iteration no 9617: Loss: 0.23665953973222414, accuracy: 0.99\n",
      "iteration no 9618: Loss: 0.23665885827820474, accuracy: 0.99\n",
      "iteration no 9619: Loss: 0.23665908533547636, accuracy: 0.99\n",
      "iteration no 9620: Loss: 0.23665906501240302, accuracy: 0.99\n",
      "iteration no 9621: Loss: 0.23665850830056864, accuracy: 0.99\n",
      "iteration no 9622: Loss: 0.23665850510833225, accuracy: 0.99\n",
      "iteration no 9623: Loss: 0.23665775426294605, accuracy: 0.99\n",
      "iteration no 9624: Loss: 0.23665843045665197, accuracy: 0.99\n",
      "iteration no 9625: Loss: 0.23665766325570675, accuracy: 0.99\n",
      "iteration no 9626: Loss: 0.23665809970342427, accuracy: 0.99\n",
      "iteration no 9627: Loss: 0.23665697213134163, accuracy: 0.99\n",
      "iteration no 9628: Loss: 0.2366575481974172, accuracy: 0.99\n",
      "iteration no 9629: Loss: 0.23665663329685777, accuracy: 0.99\n",
      "iteration no 9630: Loss: 0.2366573780795786, accuracy: 0.99\n",
      "iteration no 9631: Loss: 0.23665652647762245, accuracy: 0.99\n",
      "iteration no 9632: Loss: 0.23665663168831363, accuracy: 0.99\n",
      "iteration no 9633: Loss: 0.23665612835182323, accuracy: 0.99\n",
      "iteration no 9634: Loss: 0.23665592963333654, accuracy: 0.99\n",
      "iteration no 9635: Loss: 0.2366562277049193, accuracy: 0.99\n",
      "iteration no 9636: Loss: 0.23665560885681988, accuracy: 0.99\n",
      "iteration no 9637: Loss: 0.2366557825455155, accuracy: 0.99\n",
      "iteration no 9638: Loss: 0.23665476429876592, accuracy: 0.99\n",
      "iteration no 9639: Loss: 0.23665545724142356, accuracy: 0.99\n",
      "iteration no 9640: Loss: 0.23665466560551773, accuracy: 0.99\n",
      "iteration no 9641: Loss: 0.23665537327355746, accuracy: 0.99\n",
      "iteration no 9642: Loss: 0.2366540782434478, accuracy: 0.99\n",
      "iteration no 9643: Loss: 0.23665469708467857, accuracy: 0.99\n",
      "iteration no 9644: Loss: 0.23665384823157112, accuracy: 0.99\n",
      "iteration no 9645: Loss: 0.23665441227512668, accuracy: 0.99\n",
      "iteration no 9646: Loss: 0.2366538017170714, accuracy: 0.99\n",
      "iteration no 9647: Loss: 0.2366536328086248, accuracy: 0.99\n",
      "iteration no 9648: Loss: 0.2366534390127359, accuracy: 0.99\n",
      "iteration no 9649: Loss: 0.2366530092020281, accuracy: 0.99\n",
      "iteration no 9650: Loss: 0.23665346576999366, accuracy: 0.99\n",
      "iteration no 9651: Loss: 0.23665270634901747, accuracy: 0.99\n",
      "iteration no 9652: Loss: 0.23665302299425653, accuracy: 0.99\n",
      "iteration no 9653: Loss: 0.23665205631396596, accuracy: 0.99\n",
      "iteration no 9654: Loss: 0.23665258511127263, accuracy: 0.99\n",
      "iteration no 9655: Loss: 0.23665193837177698, accuracy: 0.99\n",
      "iteration no 9656: Loss: 0.23665250019754352, accuracy: 0.99\n",
      "iteration no 9657: Loss: 0.2366516136860108, accuracy: 0.99\n",
      "iteration no 9658: Loss: 0.23665171145242458, accuracy: 0.99\n",
      "iteration no 9659: Loss: 0.23665111227128738, accuracy: 0.99\n",
      "iteration no 9660: Loss: 0.23665167022354067, accuracy: 0.99\n",
      "iteration no 9661: Loss: 0.23665114677057073, accuracy: 0.99\n",
      "iteration no 9662: Loss: 0.23665096019710896, accuracy: 0.99\n",
      "iteration no 9663: Loss: 0.23665055463819382, accuracy: 0.99\n",
      "iteration no 9664: Loss: 0.23665030624314565, accuracy: 0.99\n",
      "iteration no 9665: Loss: 0.23665073669742853, accuracy: 0.99\n",
      "iteration no 9666: Loss: 0.23664998610297933, accuracy: 0.99\n",
      "iteration no 9667: Loss: 0.23665019666586873, accuracy: 0.99\n",
      "iteration no 9668: Loss: 0.23664930724848482, accuracy: 0.99\n",
      "iteration no 9669: Loss: 0.23664988310949017, accuracy: 0.99\n",
      "iteration no 9670: Loss: 0.23664951040925702, accuracy: 0.99\n",
      "iteration no 9671: Loss: 0.23664953728730198, accuracy: 0.99\n",
      "iteration no 9672: Loss: 0.2366489406385779, accuracy: 0.99\n",
      "iteration no 9673: Loss: 0.23664889455205917, accuracy: 0.99\n",
      "iteration no 9674: Loss: 0.23664864806861866, accuracy: 0.99\n",
      "iteration no 9675: Loss: 0.23664877641439616, accuracy: 0.99\n",
      "iteration no 9676: Loss: 0.23664848826494977, accuracy: 0.99\n",
      "iteration no 9677: Loss: 0.23664809298633982, accuracy: 0.99\n",
      "iteration no 9678: Loss: 0.23664797099528695, accuracy: 0.99\n",
      "iteration no 9679: Loss: 0.2366475740553502, accuracy: 0.99\n",
      "iteration no 9680: Loss: 0.23664800541412837, accuracy: 0.99\n",
      "iteration no 9681: Loss: 0.23664737681364711, accuracy: 0.99\n",
      "iteration no 9682: Loss: 0.23664744794518539, accuracy: 0.99\n",
      "iteration no 9683: Loss: 0.23664689398775704, accuracy: 0.99\n",
      "iteration no 9684: Loss: 0.23664693188606828, accuracy: 0.99\n",
      "iteration no 9685: Loss: 0.2366469678297931, accuracy: 0.99\n",
      "iteration no 9686: Loss: 0.23664666917785127, accuracy: 0.99\n",
      "iteration no 9687: Loss: 0.23664641404444342, accuracy: 0.99\n",
      "iteration no 9688: Loss: 0.23664594696620977, accuracy: 0.99\n",
      "iteration no 9689: Loss: 0.2366461437928845, accuracy: 0.99\n",
      "iteration no 9690: Loss: 0.23664596611086627, accuracy: 0.99\n",
      "iteration no 9691: Loss: 0.23664589815508522, accuracy: 0.99\n",
      "iteration no 9692: Loss: 0.2366451988784381, accuracy: 0.99\n",
      "iteration no 9693: Loss: 0.23664539588526237, accuracy: 0.99\n",
      "iteration no 9694: Loss: 0.23664509909421802, accuracy: 0.99\n",
      "iteration no 9695: Loss: 0.23664529157804512, accuracy: 0.99\n",
      "iteration no 9696: Loss: 0.23664477239567092, accuracy: 0.99\n",
      "iteration no 9697: Loss: 0.23664452501979744, accuracy: 0.99\n",
      "iteration no 9698: Loss: 0.2366444403455556, accuracy: 0.99\n",
      "iteration no 9699: Loss: 0.2366442436762006, accuracy: 0.99\n",
      "iteration no 9700: Loss: 0.23664440898071784, accuracy: 0.99\n",
      "iteration no 9701: Loss: 0.23664378260880384, accuracy: 0.99\n",
      "iteration no 9702: Loss: 0.23664394985707693, accuracy: 0.99\n",
      "iteration no 9703: Loss: 0.2366432221753143, accuracy: 0.99\n",
      "iteration no 9704: Loss: 0.23664377096085393, accuracy: 0.99\n",
      "iteration no 9705: Loss: 0.2366430762831348, accuracy: 0.99\n",
      "iteration no 9706: Loss: 0.23664354372927615, accuracy: 0.99\n",
      "iteration no 9707: Loss: 0.2366425782233016, accuracy: 0.99\n",
      "iteration no 9708: Loss: 0.23664280854809078, accuracy: 0.99\n",
      "iteration no 9709: Loss: 0.23664251507471948, accuracy: 0.99\n",
      "iteration no 9710: Loss: 0.2366426396417966, accuracy: 0.99\n",
      "iteration no 9711: Loss: 0.2366422314960664, accuracy: 0.99\n",
      "iteration no 9712: Loss: 0.2366418772129033, accuracy: 0.99\n",
      "iteration no 9713: Loss: 0.23664176176571128, accuracy: 0.99\n",
      "iteration no 9714: Loss: 0.23664164588393938, accuracy: 0.99\n",
      "iteration no 9715: Loss: 0.23664196101377066, accuracy: 0.99\n",
      "iteration no 9716: Loss: 0.2366409714201344, accuracy: 0.99\n",
      "iteration no 9717: Loss: 0.23664154101735096, accuracy: 0.99\n",
      "iteration no 9718: Loss: 0.2366403694773002, accuracy: 0.99\n",
      "iteration no 9719: Loss: 0.23664159066533313, accuracy: 0.99\n",
      "iteration no 9720: Loss: 0.23664046171487652, accuracy: 0.99\n",
      "iteration no 9721: Loss: 0.23664085279532066, accuracy: 0.99\n",
      "iteration no 9722: Loss: 0.2366399721635058, accuracy: 0.99\n",
      "iteration no 9723: Loss: 0.23664022779060104, accuracy: 0.99\n",
      "iteration no 9724: Loss: 0.23664012048584918, accuracy: 0.99\n",
      "iteration no 9725: Loss: 0.23663994281385978, accuracy: 0.99\n",
      "iteration no 9726: Loss: 0.23663964586497538, accuracy: 0.99\n",
      "iteration no 9727: Loss: 0.236639281657805, accuracy: 0.99\n",
      "iteration no 9728: Loss: 0.23663943269312174, accuracy: 0.99\n",
      "iteration no 9729: Loss: 0.23663904797473845, accuracy: 0.99\n",
      "iteration no 9730: Loss: 0.23663936891214937, accuracy: 0.99\n",
      "iteration no 9731: Loss: 0.23663837408095856, accuracy: 0.99\n",
      "iteration no 9732: Loss: 0.23663900444699065, accuracy: 0.99\n",
      "iteration no 9733: Loss: 0.23663802274721757, accuracy: 0.99\n",
      "iteration no 9734: Loss: 0.23663893333396618, accuracy: 0.99\n",
      "iteration no 9735: Loss: 0.23663785448855695, accuracy: 0.99\n",
      "iteration no 9736: Loss: 0.23663826434609708, accuracy: 0.99\n",
      "iteration no 9737: Loss: 0.2366374176170924, accuracy: 0.99\n",
      "iteration no 9738: Loss: 0.23663789248652575, accuracy: 0.99\n",
      "iteration no 9739: Loss: 0.23663759026843567, accuracy: 0.99\n",
      "iteration no 9740: Loss: 0.23663732057756565, accuracy: 0.99\n",
      "iteration no 9741: Loss: 0.23663720658051435, accuracy: 0.99\n",
      "iteration no 9742: Loss: 0.23663652690362275, accuracy: 0.99\n",
      "iteration no 9743: Loss: 0.23663728903989711, accuracy: 0.99\n",
      "iteration no 9744: Loss: 0.23663638034795945, accuracy: 0.99\n",
      "iteration no 9745: Loss: 0.23663699796947393, accuracy: 0.99\n",
      "iteration no 9746: Loss: 0.236635733303651, accuracy: 0.99\n",
      "iteration no 9747: Loss: 0.23663645831846397, accuracy: 0.99\n",
      "iteration no 9748: Loss: 0.23663593180415882, accuracy: 0.99\n",
      "iteration no 9749: Loss: 0.23663625407705746, accuracy: 0.99\n",
      "iteration no 9750: Loss: 0.23663547807519808, accuracy: 0.99\n",
      "iteration no 9751: Loss: 0.23663550645357023, accuracy: 0.99\n",
      "iteration no 9752: Loss: 0.23663529259645372, accuracy: 0.99\n",
      "iteration no 9753: Loss: 0.23663537560547782, accuracy: 0.99\n",
      "iteration no 9754: Loss: 0.23663517807733062, accuracy: 0.99\n",
      "iteration no 9755: Loss: 0.23663461797487342, accuracy: 0.99\n",
      "iteration no 9756: Loss: 0.23663482115455342, accuracy: 0.99\n",
      "iteration no 9757: Loss: 0.23663426770553325, accuracy: 0.99\n",
      "iteration no 9758: Loss: 0.2366349128277218, accuracy: 0.99\n",
      "iteration no 9759: Loss: 0.23663388958216625, accuracy: 0.99\n",
      "iteration no 9760: Loss: 0.2366344401020132, accuracy: 0.99\n",
      "iteration no 9761: Loss: 0.236633530582899, accuracy: 0.99\n",
      "iteration no 9762: Loss: 0.23663412386694727, accuracy: 0.99\n",
      "iteration no 9763: Loss: 0.23663362711534935, accuracy: 0.99\n",
      "iteration no 9764: Loss: 0.23663356800653162, accuracy: 0.99\n",
      "iteration no 9765: Loss: 0.2366332354242967, accuracy: 0.99\n",
      "iteration no 9766: Loss: 0.23663292275720538, accuracy: 0.99\n",
      "iteration no 9767: Loss: 0.23663326655014982, accuracy: 0.99\n",
      "iteration no 9768: Loss: 0.23663279862530637, accuracy: 0.99\n",
      "iteration no 9769: Loss: 0.23663287190914842, accuracy: 0.99\n",
      "iteration no 9770: Loss: 0.23663213959033413, accuracy: 0.99\n",
      "iteration no 9771: Loss: 0.23663259514075222, accuracy: 0.99\n",
      "iteration no 9772: Loss: 0.23663215693853945, accuracy: 0.99\n",
      "iteration no 9773: Loss: 0.2366323514834807, accuracy: 0.99\n",
      "iteration no 9774: Loss: 0.2366317082634818, accuracy: 0.99\n",
      "iteration no 9775: Loss: 0.23663163693176964, accuracy: 0.99\n",
      "iteration no 9776: Loss: 0.23663165621824758, accuracy: 0.99\n",
      "iteration no 9777: Loss: 0.23663157979105426, accuracy: 0.99\n",
      "iteration no 9778: Loss: 0.23663145335612826, accuracy: 0.99\n",
      "iteration no 9779: Loss: 0.23663083056737766, accuracy: 0.99\n",
      "iteration no 9780: Loss: 0.23663090806498782, accuracy: 0.99\n",
      "iteration no 9781: Loss: 0.23663073293983278, accuracy: 0.99\n",
      "iteration no 9782: Loss: 0.2366310550672523, accuracy: 0.99\n",
      "iteration no 9783: Loss: 0.23663014686065492, accuracy: 0.99\n",
      "iteration no 9784: Loss: 0.23663044661669858, accuracy: 0.99\n",
      "iteration no 9785: Loss: 0.23662982289138984, accuracy: 0.99\n",
      "iteration no 9786: Loss: 0.2366303593924283, accuracy: 0.99\n",
      "iteration no 9787: Loss: 0.23662996213283183, accuracy: 0.99\n",
      "iteration no 9788: Loss: 0.23662952722841457, accuracy: 0.99\n",
      "iteration no 9789: Loss: 0.23662950018590106, accuracy: 0.99\n",
      "iteration no 9790: Loss: 0.23662928734030064, accuracy: 0.99\n",
      "iteration no 9791: Loss: 0.236629666917342, accuracy: 0.99\n",
      "iteration no 9792: Loss: 0.23662886991800347, accuracy: 0.99\n",
      "iteration no 9793: Loss: 0.23662904859626643, accuracy: 0.99\n",
      "iteration no 9794: Loss: 0.2366282887723345, accuracy: 0.99\n",
      "iteration no 9795: Loss: 0.23662904467265677, accuracy: 0.99\n",
      "iteration no 9796: Loss: 0.23662832358983815, accuracy: 0.99\n",
      "iteration no 9797: Loss: 0.23662862832858986, accuracy: 0.99\n",
      "iteration no 9798: Loss: 0.2366278384570103, accuracy: 0.99\n",
      "iteration no 9799: Loss: 0.23662801290173324, accuracy: 0.99\n",
      "iteration no 9800: Loss: 0.23662802571427613, accuracy: 0.99\n",
      "iteration no 9801: Loss: 0.23662775754510035, accuracy: 0.99\n",
      "iteration no 9802: Loss: 0.23662760862992063, accuracy: 0.99\n",
      "iteration no 9803: Loss: 0.23662709006114954, accuracy: 0.99\n",
      "iteration no 9804: Loss: 0.23662744481448064, accuracy: 0.99\n",
      "iteration no 9805: Loss: 0.23662695198483596, accuracy: 0.99\n",
      "iteration no 9806: Loss: 0.236627299916349, accuracy: 0.99\n",
      "iteration no 9807: Loss: 0.23662645416880024, accuracy: 0.99\n",
      "iteration no 9808: Loss: 0.23662676980899722, accuracy: 0.99\n",
      "iteration no 9809: Loss: 0.23662619969112195, accuracy: 0.99\n",
      "iteration no 9810: Loss: 0.23662675581408643, accuracy: 0.99\n",
      "iteration no 9811: Loss: 0.23662587951076347, accuracy: 0.99\n",
      "iteration no 9812: Loss: 0.2366261043688067, accuracy: 0.99\n",
      "iteration no 9813: Loss: 0.23662545295394496, accuracy: 0.99\n",
      "iteration no 9814: Loss: 0.23662585008247236, accuracy: 0.99\n",
      "iteration no 9815: Loss: 0.23662549419910534, accuracy: 0.99\n",
      "iteration no 9816: Loss: 0.2366251313416235, accuracy: 0.99\n",
      "iteration no 9817: Loss: 0.23662505337110548, accuracy: 0.99\n",
      "iteration no 9818: Loss: 0.23662466276210647, accuracy: 0.99\n",
      "iteration no 9819: Loss: 0.23662515411599783, accuracy: 0.99\n",
      "iteration no 9820: Loss: 0.23662426037935447, accuracy: 0.99\n",
      "iteration no 9821: Loss: 0.2366246460326239, accuracy: 0.99\n",
      "iteration no 9822: Loss: 0.2366235549811691, accuracy: 0.99\n",
      "iteration no 9823: Loss: 0.23662472923746797, accuracy: 0.99\n",
      "iteration no 9824: Loss: 0.2366237296851737, accuracy: 0.99\n",
      "iteration no 9825: Loss: 0.23662404192788522, accuracy: 0.99\n",
      "iteration no 9826: Loss: 0.23662314824445227, accuracy: 0.99\n",
      "iteration no 9827: Loss: 0.23662344545314007, accuracy: 0.99\n",
      "iteration no 9828: Loss: 0.23662338300778157, accuracy: 0.99\n",
      "iteration no 9829: Loss: 0.23662317537012434, accuracy: 0.99\n",
      "iteration no 9830: Loss: 0.23662277992982883, accuracy: 0.99\n",
      "iteration no 9831: Loss: 0.23662238384183076, accuracy: 0.99\n",
      "iteration no 9832: Loss: 0.2366228290991749, accuracy: 0.99\n",
      "iteration no 9833: Loss: 0.2366222878581708, accuracy: 0.99\n",
      "iteration no 9834: Loss: 0.23662255442738528, accuracy: 0.99\n",
      "iteration no 9835: Loss: 0.23662143608053854, accuracy: 0.99\n",
      "iteration no 9836: Loss: 0.23662221996255492, accuracy: 0.99\n",
      "iteration no 9837: Loss: 0.2366214950304706, accuracy: 0.99\n",
      "iteration no 9838: Loss: 0.2366221133684402, accuracy: 0.99\n",
      "iteration no 9839: Loss: 0.236620956916408, accuracy: 0.99\n",
      "iteration no 9840: Loss: 0.23662138849250788, accuracy: 0.99\n",
      "iteration no 9841: Loss: 0.23662089053747726, accuracy: 0.99\n",
      "iteration no 9842: Loss: 0.23662119416295813, accuracy: 0.99\n",
      "iteration no 9843: Loss: 0.23662068837310243, accuracy: 0.99\n",
      "iteration no 9844: Loss: 0.23662042844714715, accuracy: 0.99\n",
      "iteration no 9845: Loss: 0.23662035077666044, accuracy: 0.99\n",
      "iteration no 9846: Loss: 0.2366202215127539, accuracy: 0.99\n",
      "iteration no 9847: Loss: 0.23662045912732144, accuracy: 0.99\n",
      "iteration no 9848: Loss: 0.2366194570946194, accuracy: 0.99\n",
      "iteration no 9849: Loss: 0.2366200743458954, accuracy: 0.99\n",
      "iteration no 9850: Loss: 0.236619067104746, accuracy: 0.99\n",
      "iteration no 9851: Loss: 0.23662008045286537, accuracy: 0.99\n",
      "iteration no 9852: Loss: 0.23661897087845368, accuracy: 0.99\n",
      "iteration no 9853: Loss: 0.23661940047134333, accuracy: 0.99\n",
      "iteration no 9854: Loss: 0.236618513726491, accuracy: 0.99\n",
      "iteration no 9855: Loss: 0.23661914735968603, accuracy: 0.99\n",
      "iteration no 9856: Loss: 0.23661875423331233, accuracy: 0.99\n",
      "iteration no 9857: Loss: 0.23661851771677536, accuracy: 0.99\n",
      "iteration no 9858: Loss: 0.23661824205620827, accuracy: 0.99\n",
      "iteration no 9859: Loss: 0.23661783958869467, accuracy: 0.99\n",
      "iteration no 9860: Loss: 0.23661839346472982, accuracy: 0.99\n",
      "iteration no 9861: Loss: 0.23661757617686707, accuracy: 0.99\n",
      "iteration no 9862: Loss: 0.23661802100151252, accuracy: 0.99\n",
      "iteration no 9863: Loss: 0.2366168431403054, accuracy: 0.99\n",
      "iteration no 9864: Loss: 0.2366179034343074, accuracy: 0.99\n",
      "iteration no 9865: Loss: 0.23661707156164363, accuracy: 0.99\n",
      "iteration no 9866: Loss: 0.2366172715599048, accuracy: 0.99\n",
      "iteration no 9867: Loss: 0.23661657746397505, accuracy: 0.99\n",
      "iteration no 9868: Loss: 0.23661669908339936, accuracy: 0.99\n",
      "iteration no 9869: Loss: 0.23661679009078396, accuracy: 0.99\n",
      "iteration no 9870: Loss: 0.23661644390091324, accuracy: 0.99\n",
      "iteration no 9871: Loss: 0.2366162116237902, accuracy: 0.99\n",
      "iteration no 9872: Loss: 0.23661570718487546, accuracy: 0.99\n",
      "iteration no 9873: Loss: 0.23661631530840826, accuracy: 0.99\n",
      "iteration no 9874: Loss: 0.236615618322694, accuracy: 0.99\n",
      "iteration no 9875: Loss: 0.23661592142118118, accuracy: 0.99\n",
      "iteration no 9876: Loss: 0.23661497001821327, accuracy: 0.99\n",
      "iteration no 9877: Loss: 0.23661550258918707, accuracy: 0.99\n",
      "iteration no 9878: Loss: 0.23661520313671636, accuracy: 0.99\n",
      "iteration no 9879: Loss: 0.23661514481293086, accuracy: 0.99\n",
      "iteration no 9880: Loss: 0.23661478391592275, accuracy: 0.99\n",
      "iteration no 9881: Loss: 0.23661451055619742, accuracy: 0.99\n",
      "iteration no 9882: Loss: 0.23661476264942888, accuracy: 0.99\n",
      "iteration no 9883: Loss: 0.23661438737708015, accuracy: 0.99\n",
      "iteration no 9884: Loss: 0.2366143614951381, accuracy: 0.99\n",
      "iteration no 9885: Loss: 0.23661383393885244, accuracy: 0.99\n",
      "iteration no 9886: Loss: 0.23661397455056488, accuracy: 0.99\n",
      "iteration no 9887: Loss: 0.2366137091905093, accuracy: 0.99\n",
      "iteration no 9888: Loss: 0.23661387355040836, accuracy: 0.99\n",
      "iteration no 9889: Loss: 0.2366132760623189, accuracy: 0.99\n",
      "iteration no 9890: Loss: 0.23661316360307794, accuracy: 0.99\n",
      "iteration no 9891: Loss: 0.2366132459597996, accuracy: 0.99\n",
      "iteration no 9892: Loss: 0.23661309586931606, accuracy: 0.99\n",
      "iteration no 9893: Loss: 0.23661294591565352, accuracy: 0.99\n",
      "iteration no 9894: Loss: 0.23661242791671863, accuracy: 0.99\n",
      "iteration no 9895: Loss: 0.23661257429531724, accuracy: 0.99\n",
      "iteration no 9896: Loss: 0.23661242017516715, accuracy: 0.99\n",
      "iteration no 9897: Loss: 0.2366124832130421, accuracy: 0.99\n",
      "iteration no 9898: Loss: 0.2366117093978471, accuracy: 0.99\n",
      "iteration no 9899: Loss: 0.2366118627517968, accuracy: 0.99\n",
      "iteration no 9900: Loss: 0.23661157027193663, accuracy: 0.99\n",
      "iteration no 9901: Loss: 0.2366120657949086, accuracy: 0.99\n",
      "iteration no 9902: Loss: 0.23661126797374407, accuracy: 0.99\n",
      "iteration no 9903: Loss: 0.23661119566080968, accuracy: 0.99\n",
      "iteration no 9904: Loss: 0.23661109499581146, accuracy: 0.99\n",
      "iteration no 9905: Loss: 0.23661111900641868, accuracy: 0.99\n",
      "iteration no 9906: Loss: 0.23661105655221365, accuracy: 0.99\n",
      "iteration no 9907: Loss: 0.23661036673403066, accuracy: 0.99\n",
      "iteration no 9908: Loss: 0.23661062938650812, accuracy: 0.99\n",
      "iteration no 9909: Loss: 0.23661042078346112, accuracy: 0.99\n",
      "iteration no 9910: Loss: 0.23661067291710464, accuracy: 0.99\n",
      "iteration no 9911: Loss: 0.2366097847797392, accuracy: 0.99\n",
      "iteration no 9912: Loss: 0.23661014037180528, accuracy: 0.99\n",
      "iteration no 9913: Loss: 0.23660956454780058, accuracy: 0.99\n",
      "iteration no 9914: Loss: 0.23661024725635516, accuracy: 0.99\n",
      "iteration no 9915: Loss: 0.2366094187694222, accuracy: 0.99\n",
      "iteration no 9916: Loss: 0.23660946427024965, accuracy: 0.99\n",
      "iteration no 9917: Loss: 0.23660901120093508, accuracy: 0.99\n",
      "iteration no 9918: Loss: 0.23660939805747244, accuracy: 0.99\n",
      "iteration no 9919: Loss: 0.2366091122774577, accuracy: 0.99\n",
      "iteration no 9920: Loss: 0.2366086181618262, accuracy: 0.99\n",
      "iteration no 9921: Loss: 0.23660871418523277, accuracy: 0.99\n",
      "iteration no 9922: Loss: 0.23660833256594713, accuracy: 0.99\n",
      "iteration no 9923: Loss: 0.23660884749694827, accuracy: 0.99\n",
      "iteration no 9924: Loss: 0.2366079266438856, accuracy: 0.99\n",
      "iteration no 9925: Loss: 0.23660839461319108, accuracy: 0.99\n",
      "iteration no 9926: Loss: 0.23660745652449972, accuracy: 0.99\n",
      "iteration no 9927: Loss: 0.23660843157735362, accuracy: 0.99\n",
      "iteration no 9928: Loss: 0.23660747945749241, accuracy: 0.99\n",
      "iteration no 9929: Loss: 0.23660780121199868, accuracy: 0.99\n",
      "iteration no 9930: Loss: 0.2366070709582068, accuracy: 0.99\n",
      "iteration no 9931: Loss: 0.23660750312188988, accuracy: 0.99\n",
      "iteration no 9932: Loss: 0.23660724484570242, accuracy: 0.99\n",
      "iteration no 9933: Loss: 0.23660694039440955, accuracy: 0.99\n",
      "iteration no 9934: Loss: 0.23660682791494583, accuracy: 0.99\n",
      "iteration no 9935: Loss: 0.2366063987227416, accuracy: 0.99\n",
      "iteration no 9936: Loss: 0.23660708617830795, accuracy: 0.99\n",
      "iteration no 9937: Loss: 0.23660612305489087, accuracy: 0.99\n",
      "iteration no 9938: Loss: 0.2366066322783582, accuracy: 0.99\n",
      "iteration no 9939: Loss: 0.23660545282092665, accuracy: 0.99\n",
      "iteration no 9940: Loss: 0.2366067365690556, accuracy: 0.99\n",
      "iteration no 9941: Loss: 0.23660567583562525, accuracy: 0.99\n",
      "iteration no 9942: Loss: 0.2366059953981993, accuracy: 0.99\n",
      "iteration no 9943: Loss: 0.23660518911387407, accuracy: 0.99\n",
      "iteration no 9944: Loss: 0.23660570678197812, accuracy: 0.99\n",
      "iteration no 9945: Loss: 0.2366054295090692, accuracy: 0.99\n",
      "iteration no 9946: Loss: 0.23660522730913602, accuracy: 0.99\n",
      "iteration no 9947: Loss: 0.23660497208839992, accuracy: 0.99\n",
      "iteration no 9948: Loss: 0.23660462743116611, accuracy: 0.99\n",
      "iteration no 9949: Loss: 0.23660526124858114, accuracy: 0.99\n",
      "iteration no 9950: Loss: 0.23660432123503516, accuracy: 0.99\n",
      "iteration no 9951: Loss: 0.23660480942249207, accuracy: 0.99\n",
      "iteration no 9952: Loss: 0.23660363771320386, accuracy: 0.99\n",
      "iteration no 9953: Loss: 0.23660503734738675, accuracy: 0.99\n",
      "iteration no 9954: Loss: 0.23660382080297326, accuracy: 0.99\n",
      "iteration no 9955: Loss: 0.2366043225640676, accuracy: 0.99\n",
      "iteration no 9956: Loss: 0.23660337700974754, accuracy: 0.99\n",
      "iteration no 9957: Loss: 0.23660398428977275, accuracy: 0.99\n",
      "iteration no 9958: Loss: 0.23660365679053647, accuracy: 0.99\n",
      "iteration no 9959: Loss: 0.23660339113307594, accuracy: 0.99\n",
      "iteration no 9960: Loss: 0.23660323364474803, accuracy: 0.99\n",
      "iteration no 9961: Loss: 0.23660284927905473, accuracy: 0.99\n",
      "iteration no 9962: Loss: 0.2366035006537105, accuracy: 0.99\n",
      "iteration no 9963: Loss: 0.2366025221805369, accuracy: 0.99\n",
      "iteration no 9964: Loss: 0.23660304007139354, accuracy: 0.99\n",
      "iteration no 9965: Loss: 0.23660193369232035, accuracy: 0.99\n",
      "iteration no 9966: Loss: 0.23660311479831314, accuracy: 0.99\n",
      "iteration no 9967: Loss: 0.23660213946127856, accuracy: 0.99\n",
      "iteration no 9968: Loss: 0.2366024304446232, accuracy: 0.99\n",
      "iteration no 9969: Loss: 0.23660179714682059, accuracy: 0.99\n",
      "iteration no 9970: Loss: 0.23660208780958147, accuracy: 0.99\n",
      "iteration no 9971: Loss: 0.23660192685847106, accuracy: 0.99\n",
      "iteration no 9972: Loss: 0.2366017203934709, accuracy: 0.99\n",
      "iteration no 9973: Loss: 0.23660153460740696, accuracy: 0.99\n",
      "iteration no 9974: Loss: 0.23660105157231137, accuracy: 0.99\n",
      "iteration no 9975: Loss: 0.23660168370055185, accuracy: 0.99\n",
      "iteration no 9976: Loss: 0.23660083781720126, accuracy: 0.99\n",
      "iteration no 9977: Loss: 0.2366013173350271, accuracy: 0.99\n",
      "iteration no 9978: Loss: 0.23660028788763943, accuracy: 0.99\n",
      "iteration no 9979: Loss: 0.23660127248470184, accuracy: 0.99\n",
      "iteration no 9980: Loss: 0.2366004762041476, accuracy: 0.99\n",
      "iteration no 9981: Loss: 0.23660071148921924, accuracy: 0.99\n",
      "iteration no 9982: Loss: 0.23660006785852797, accuracy: 0.99\n",
      "iteration no 9983: Loss: 0.23660029710888655, accuracy: 0.99\n",
      "iteration no 9984: Loss: 0.23660024091812962, accuracy: 0.99\n",
      "iteration no 9985: Loss: 0.23659997991457815, accuracy: 0.99\n",
      "iteration no 9986: Loss: 0.2365997969634655, accuracy: 0.99\n",
      "iteration no 9987: Loss: 0.23659939868804017, accuracy: 0.99\n",
      "iteration no 9988: Loss: 0.23659990869150666, accuracy: 0.99\n",
      "iteration no 9989: Loss: 0.23659923615539608, accuracy: 0.99\n",
      "iteration no 9990: Loss: 0.2365994389254772, accuracy: 0.99\n",
      "iteration no 9991: Loss: 0.23659860889559609, accuracy: 0.99\n",
      "iteration no 9992: Loss: 0.23659944139985212, accuracy: 0.99\n",
      "iteration no 9993: Loss: 0.23659888547147875, accuracy: 0.99\n",
      "iteration no 9994: Loss: 0.23659878106863647, accuracy: 0.99\n",
      "iteration no 9995: Loss: 0.2365984212460228, accuracy: 0.99\n",
      "iteration no 9996: Loss: 0.23659860967502366, accuracy: 0.99\n",
      "iteration no 9997: Loss: 0.2365985526636396, accuracy: 0.99\n",
      "iteration no 9998: Loss: 0.23659814734242854, accuracy: 0.99\n",
      "iteration no 9999: Loss: 0.23659795948628146, accuracy: 0.99\n",
      "iteration no 10000: Loss: 0.23659778845300913, accuracy: 0.99\n"
     ]
    }
   ],
   "source": [
    "# hyperparamters\n",
    "h = 100   # number of hidden states\n",
    "iterations = 10000\n",
    "step_size = 0.5\n",
    "reg = 1e-3\n",
    "\n",
    "# constants\n",
    "n_examples = X.shape[0]\n",
    "\n",
    "# initialize the weights\n",
    "W1 = np.random.randn(D,h)*0.01\n",
    "W2 = np.random.randn(h,K)*0.01\n",
    "\n",
    "b1 = np.zeros((1,h))\n",
    "b2 = np.zeros((1,K))\n",
    "\n",
    "# training loop\n",
    "for iter in range(iterations):\n",
    "\n",
    "    # calculate the scores\n",
    "    hidden_scores = np.dot(X, W1) + b1\n",
    "    hidden_scores = np.maximum(hidden_scores, 0) # relu activation function\n",
    "    scores = np.dot(hidden_scores, W2) + b2\n",
    "\n",
    "    # applying softmax\n",
    "    exp_scores = np.exp(scores)\n",
    "    probs = exp_scores/np.sum(exp_scores, axis=1, keepdims=True)\n",
    "\n",
    "    # loss calculation\n",
    "    loss = -np.sum(np.log(probs[range(n_examples), y]))/n_examples + 0.5*reg*np.sum(W1*W1) + 0.5*reg*np.sum(W2*W2)\n",
    "\n",
    "    # calculate gradients\n",
    "    dscores2 = probs\n",
    "    dscores2[range(n_examples), y]-=1\n",
    "    dscores2 = dscores2/n_examples\n",
    "\n",
    "    db2 = np.sum(dscores2, axis=0, keepdims=True)\n",
    "    dW2 = np.dot(hidden_scores.T, dscores2) \n",
    "\n",
    "    dscores1 = np.dot(dscores2, W2.T)\n",
    "    dscores1[hidden_scores==0] = 0       # backprop relu activation non-linearity\n",
    "\n",
    "    db1 = np.sum(dscores1, axis=0, keepdims=True)\n",
    "    dW1 = np.dot(X.T, dscores1)\n",
    "\n",
    "    dW2 += reg*W2\n",
    "    dW1 += reg*W1\n",
    "\n",
    "    # performing gradient descent\n",
    "    b2 = b2 - step_size * db2\n",
    "    W2 = W2 - step_size * dW2\n",
    "    b1 = b1 - step_size * db1\n",
    "    W1 = W1 - step_size * dW1\n",
    "\n",
    "\n",
    "    # accuracy calculation\n",
    "    hidden_scores = np.dot(X, W1) + b1\n",
    "    hidden_scores = np.maximum(hidden_scores, 0) # relu activation function\n",
    "    scores = np.dot(hidden_scores, W2) + b2\n",
    "    predictions = np.argmax(scores, axis=1)\n",
    "\n",
    "    accuracy = np.sum(predictions == y)/n_examples\n",
    "\n",
    "    print(f'iteration no {iter+1}: Loss: {loss}, accuracy: {accuracy}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<center><b>ACHIEVED ACCURACY OF 99% </b><center>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

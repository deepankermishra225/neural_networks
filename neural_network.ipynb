{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><b>TRAINING A NEURAL NETWORK USING NUMPY</b><br><center>\n",
    "<hr>\n",
    "<ul>\n",
    "<li>https://cs231n.github.io/neural-networks-case-study/#grad</li>\n",
    "<li>https://dustinstansbury.github.io/theclevermachine/derivation-backpropagation</li>\n",
    "<li>https://ai.plainenglish.io/gradient-descent-update-rule-for-multiclass-logistic-regression-4bf3033cac10</li>\n",
    "<li>https://blog.yani.ai/backpropagation/</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><b>GENERATING SAMPLE DATA</b></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "N = 100 # number of points per class \n",
    "D = 2 # dimensionality\n",
    "K = 3 # number of classes\n",
    "\n",
    "X = np.zeros((N*K,D))\n",
    "y = np.zeros(N*K, dtype='uint8')\n",
    "for j in range(K):\n",
    "  ix = range(N*j,N*(j+1))\n",
    "  r = np.linspace(0.0,1,N) # radius\n",
    "  t = np.linspace(j*4,(j+1)*4,N) + np.random.randn(N)*0.2 # theta\n",
    "  X[ix] = np.c_[r*np.sin(t), r*np.cos(t)]\n",
    "  y[ix] = j\n",
    "\n",
    "## visualize data\n",
    "data = {\n",
    "    'x1': X[:,0],\n",
    "    'x2': X[:,1],\n",
    "    'y': y\n",
    "}\n",
    "plt.rcParams['axes.facecolor'] = '#bea3c2'\n",
    "sns.scatterplot(x='x1', y='x2', hue='y', data=data)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><b>TRAINING A NEURAL NETWORK WITH ONE HIDDEN LAYER</b></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration no 1: Loss: 1.0986566683088836, accuracy: 0.4633333333333333\n",
      "iteration no 2: Loss: 1.098513067400682, accuracy: 0.57\n",
      "iteration no 3: Loss: 1.098366617531513, accuracy: 0.5866666666666667\n",
      "iteration no 4: Loss: 1.09822088429559, accuracy: 0.6\n",
      "iteration no 5: Loss: 1.0980741969176464, accuracy: 0.6033333333333334\n",
      "iteration no 6: Loss: 1.0979261052010585, accuracy: 0.5933333333333334\n",
      "iteration no 7: Loss: 1.0977726014775817, accuracy: 0.5866666666666667\n",
      "iteration no 8: Loss: 1.0976116627674766, accuracy: 0.57\n",
      "iteration no 9: Loss: 1.0974404601304604, accuracy: 0.5733333333333334\n",
      "iteration no 10: Loss: 1.0972555945526827, accuracy: 0.5833333333333334\n",
      "iteration no 11: Loss: 1.0970533788233796, accuracy: 0.5766666666666667\n",
      "iteration no 12: Loss: 1.0968315349155175, accuracy: 0.57\n",
      "iteration no 13: Loss: 1.096591107456824, accuracy: 0.5633333333333334\n",
      "iteration no 14: Loss: 1.0963307274855407, accuracy: 0.55\n",
      "iteration no 15: Loss: 1.0960465914864146, accuracy: 0.5366666666666666\n",
      "iteration no 16: Loss: 1.0957340891168115, accuracy: 0.52\n",
      "iteration no 17: Loss: 1.0953919589979206, accuracy: 0.5133333333333333\n",
      "iteration no 18: Loss: 1.0950154178994518, accuracy: 0.5133333333333333\n",
      "iteration no 19: Loss: 1.0946003073652584, accuracy: 0.5133333333333333\n",
      "iteration no 20: Loss: 1.0941429512088563, accuracy: 0.51\n",
      "iteration no 21: Loss: 1.0936378901063755, accuracy: 0.5133333333333333\n",
      "iteration no 22: Loss: 1.0930803046482909, accuracy: 0.5133333333333333\n",
      "iteration no 23: Loss: 1.0924641553907215, accuracy: 0.5133333333333333\n",
      "iteration no 24: Loss: 1.0917827527277206, accuracy: 0.5133333333333333\n",
      "iteration no 25: Loss: 1.0910299970838726, accuracy: 0.5133333333333333\n",
      "iteration no 26: Loss: 1.0901975935008235, accuracy: 0.5133333333333333\n",
      "iteration no 27: Loss: 1.0892770154565938, accuracy: 0.5166666666666667\n",
      "iteration no 28: Loss: 1.0882592183819568, accuracy: 0.5166666666666667\n",
      "iteration no 29: Loss: 1.0871340616767398, accuracy: 0.5166666666666667\n",
      "iteration no 30: Loss: 1.0858918405545361, accuracy: 0.5166666666666667\n",
      "iteration no 31: Loss: 1.0845205120675099, accuracy: 0.5166666666666667\n",
      "iteration no 32: Loss: 1.083007975361326, accuracy: 0.52\n",
      "iteration no 33: Loss: 1.081343504566244, accuracy: 0.52\n",
      "iteration no 34: Loss: 1.0795138215371907, accuracy: 0.52\n",
      "iteration no 35: Loss: 1.0775059652293968, accuracy: 0.5166666666666667\n",
      "iteration no 36: Loss: 1.075301167918244, accuracy: 0.5233333333333333\n",
      "iteration no 37: Loss: 1.0728837834915852, accuracy: 0.52\n",
      "iteration no 38: Loss: 1.0702380341439557, accuracy: 0.5166666666666667\n",
      "iteration no 39: Loss: 1.0673479280954625, accuracy: 0.52\n",
      "iteration no 40: Loss: 1.0641971536517496, accuracy: 0.52\n",
      "iteration no 41: Loss: 1.0607664867944833, accuracy: 0.52\n",
      "iteration no 42: Loss: 1.0570393595242642, accuracy: 0.5166666666666667\n",
      "iteration no 43: Loss: 1.0529997545065126, accuracy: 0.51\n",
      "iteration no 44: Loss: 1.0486306107038614, accuracy: 0.51\n",
      "iteration no 45: Loss: 1.0439187472447415, accuracy: 0.51\n",
      "iteration no 46: Loss: 1.0388487712493426, accuracy: 0.51\n",
      "iteration no 47: Loss: 1.0334130920359157, accuracy: 0.51\n",
      "iteration no 48: Loss: 1.02760820388889, accuracy: 0.5133333333333333\n",
      "iteration no 49: Loss: 1.02143320468895, accuracy: 0.5133333333333333\n",
      "iteration no 50: Loss: 1.0148930369142393, accuracy: 0.5133333333333333\n",
      "iteration no 51: Loss: 1.0079979846106903, accuracy: 0.5133333333333333\n",
      "iteration no 52: Loss: 1.0007608300791704, accuracy: 0.51\n",
      "iteration no 53: Loss: 0.9932031755840548, accuracy: 0.5133333333333333\n",
      "iteration no 54: Loss: 0.9853484126697107, accuracy: 0.5133333333333333\n",
      "iteration no 55: Loss: 0.9772277977482575, accuracy: 0.5166666666666667\n",
      "iteration no 56: Loss: 0.9688769329504754, accuracy: 0.5166666666666667\n",
      "iteration no 57: Loss: 0.9603340278995876, accuracy: 0.5166666666666667\n",
      "iteration no 58: Loss: 0.9516406009196247, accuracy: 0.5166666666666667\n",
      "iteration no 59: Loss: 0.9428399957103877, accuracy: 0.52\n",
      "iteration no 60: Loss: 0.933977778855889, accuracy: 0.5166666666666667\n",
      "iteration no 61: Loss: 0.9250973049449964, accuracy: 0.5166666666666667\n",
      "iteration no 62: Loss: 0.9162418422345437, accuracy: 0.5233333333333333\n",
      "iteration no 63: Loss: 0.9074513775421874, accuracy: 0.52\n",
      "iteration no 64: Loss: 0.8987649899944479, accuracy: 0.5166666666666667\n",
      "iteration no 65: Loss: 0.8902152659615904, accuracy: 0.5166666666666667\n",
      "iteration no 66: Loss: 0.8818309216384144, accuracy: 0.52\n",
      "iteration no 67: Loss: 0.8736378705188753, accuracy: 0.5233333333333333\n",
      "iteration no 68: Loss: 0.8656593193447953, accuracy: 0.52\n",
      "iteration no 69: Loss: 0.8579148254892234, accuracy: 0.52\n",
      "iteration no 70: Loss: 0.8504178444521742, accuracy: 0.5166666666666667\n",
      "iteration no 71: Loss: 0.8431821656586284, accuracy: 0.5166666666666667\n",
      "iteration no 72: Loss: 0.8362172623005505, accuracy: 0.52\n",
      "iteration no 73: Loss: 0.8295280108202816, accuracy: 0.52\n",
      "iteration no 74: Loss: 0.8231212271144354, accuracy: 0.52\n",
      "iteration no 75: Loss: 0.817002025942113, accuracy: 0.5233333333333333\n",
      "iteration no 76: Loss: 0.8111637867627611, accuracy: 0.5233333333333333\n",
      "iteration no 77: Loss: 0.8056020212242123, accuracy: 0.5266666666666666\n",
      "iteration no 78: Loss: 0.8003141020126029, accuracy: 0.5266666666666666\n",
      "iteration no 79: Loss: 0.7952990034136278, accuracy: 0.5333333333333333\n",
      "iteration no 80: Loss: 0.7905552250019684, accuracy: 0.54\n",
      "iteration no 81: Loss: 0.7860819754159288, accuracy: 0.54\n",
      "iteration no 82: Loss: 0.7818706690018032, accuracy: 0.54\n",
      "iteration no 83: Loss: 0.7779167689613622, accuracy: 0.5333333333333333\n",
      "iteration no 84: Loss: 0.7742034439903451, accuracy: 0.5333333333333333\n",
      "iteration no 85: Loss: 0.770723113685208, accuracy: 0.5333333333333333\n",
      "iteration no 86: Loss: 0.7674678222152448, accuracy: 0.5333333333333333\n",
      "iteration no 87: Loss: 0.764421945395855, accuracy: 0.5333333333333333\n",
      "iteration no 88: Loss: 0.7615721527570615, accuracy: 0.5333333333333333\n",
      "iteration no 89: Loss: 0.7589067737479189, accuracy: 0.5333333333333333\n",
      "iteration no 90: Loss: 0.7564158320386813, accuracy: 0.53\n",
      "iteration no 91: Loss: 0.7540894815300442, accuracy: 0.53\n",
      "iteration no 92: Loss: 0.751912631751694, accuracy: 0.53\n",
      "iteration no 93: Loss: 0.749875920011149, accuracy: 0.53\n",
      "iteration no 94: Loss: 0.7479682628325736, accuracy: 0.53\n",
      "iteration no 95: Loss: 0.7461790878642391, accuracy: 0.5333333333333333\n",
      "iteration no 96: Loss: 0.7444991961097757, accuracy: 0.5333333333333333\n",
      "iteration no 97: Loss: 0.742925975820924, accuracy: 0.5333333333333333\n",
      "iteration no 98: Loss: 0.7414536869163173, accuracy: 0.5366666666666666\n",
      "iteration no 99: Loss: 0.740075432905584, accuracy: 0.5366666666666666\n",
      "iteration no 100: Loss: 0.7387807791854959, accuracy: 0.5366666666666666\n",
      "iteration no 101: Loss: 0.7375618502127915, accuracy: 0.5366666666666666\n",
      "iteration no 102: Loss: 0.7364090280852297, accuracy: 0.5366666666666666\n",
      "iteration no 103: Loss: 0.7353154481602829, accuracy: 0.5366666666666666\n",
      "iteration no 104: Loss: 0.7342810741238984, accuracy: 0.5366666666666666\n",
      "iteration no 105: Loss: 0.733305593629942, accuracy: 0.5366666666666666\n",
      "iteration no 106: Loss: 0.7323855797182881, accuracy: 0.5366666666666666\n",
      "iteration no 107: Loss: 0.7315116669419128, accuracy: 0.5366666666666666\n",
      "iteration no 108: Loss: 0.7306812515881072, accuracy: 0.5366666666666666\n",
      "iteration no 109: Loss: 0.7298909127910469, accuracy: 0.5366666666666666\n",
      "iteration no 110: Loss: 0.7291338523762627, accuracy: 0.5366666666666666\n",
      "iteration no 111: Loss: 0.7284123860667616, accuracy: 0.5366666666666666\n",
      "iteration no 112: Loss: 0.7277215011056164, accuracy: 0.5366666666666666\n",
      "iteration no 113: Loss: 0.7270571369519432, accuracy: 0.5366666666666666\n",
      "iteration no 114: Loss: 0.7264160669877388, accuracy: 0.5366666666666666\n",
      "iteration no 115: Loss: 0.7258011411757864, accuracy: 0.54\n",
      "iteration no 116: Loss: 0.725206141437869, accuracy: 0.5366666666666666\n",
      "iteration no 117: Loss: 0.7246362925309483, accuracy: 0.5366666666666666\n",
      "iteration no 118: Loss: 0.7240846530201575, accuracy: 0.5366666666666666\n",
      "iteration no 119: Loss: 0.7235474786299506, accuracy: 0.54\n",
      "iteration no 120: Loss: 0.7230284048535431, accuracy: 0.54\n",
      "iteration no 121: Loss: 0.7225210274859909, accuracy: 0.54\n",
      "iteration no 122: Loss: 0.7220293722852437, accuracy: 0.54\n",
      "iteration no 123: Loss: 0.7215521146912647, accuracy: 0.54\n",
      "iteration no 124: Loss: 0.7210875053292559, accuracy: 0.5433333333333333\n",
      "iteration no 125: Loss: 0.7206340029659976, accuracy: 0.5433333333333333\n",
      "iteration no 126: Loss: 0.7201984584454877, accuracy: 0.5433333333333333\n",
      "iteration no 127: Loss: 0.719781365611645, accuracy: 0.5433333333333333\n",
      "iteration no 128: Loss: 0.7193720813975144, accuracy: 0.5433333333333333\n",
      "iteration no 129: Loss: 0.7189746517304848, accuracy: 0.5433333333333333\n",
      "iteration no 130: Loss: 0.7185864541495771, accuracy: 0.5433333333333333\n",
      "iteration no 131: Loss: 0.7182050463861479, accuracy: 0.5433333333333333\n",
      "iteration no 132: Loss: 0.7178335208856153, accuracy: 0.5433333333333333\n",
      "iteration no 133: Loss: 0.7174763328808735, accuracy: 0.5433333333333333\n",
      "iteration no 134: Loss: 0.7171314151802984, accuracy: 0.5433333333333333\n",
      "iteration no 135: Loss: 0.7167943427887118, accuracy: 0.5433333333333333\n",
      "iteration no 136: Loss: 0.7164628706458962, accuracy: 0.5433333333333333\n",
      "iteration no 137: Loss: 0.7161368246865122, accuracy: 0.5433333333333333\n",
      "iteration no 138: Loss: 0.7158136660364687, accuracy: 0.5433333333333333\n",
      "iteration no 139: Loss: 0.7154927757351841, accuracy: 0.5433333333333333\n",
      "iteration no 140: Loss: 0.7151751425030055, accuracy: 0.5433333333333333\n",
      "iteration no 141: Loss: 0.7148614963549331, accuracy: 0.5433333333333333\n",
      "iteration no 142: Loss: 0.7145534693859513, accuracy: 0.5433333333333333\n",
      "iteration no 143: Loss: 0.7142485740411993, accuracy: 0.5433333333333333\n",
      "iteration no 144: Loss: 0.7139471686540652, accuracy: 0.5433333333333333\n",
      "iteration no 145: Loss: 0.713656003700299, accuracy: 0.5466666666666666\n",
      "iteration no 146: Loss: 0.7133719802694659, accuracy: 0.5466666666666666\n",
      "iteration no 147: Loss: 0.7130928102721111, accuracy: 0.5466666666666666\n",
      "iteration no 148: Loss: 0.7128158859181501, accuracy: 0.5466666666666666\n",
      "iteration no 149: Loss: 0.7125422170315117, accuracy: 0.5466666666666666\n",
      "iteration no 150: Loss: 0.7122714714412122, accuracy: 0.5466666666666666\n",
      "iteration no 151: Loss: 0.7120046317499074, accuracy: 0.5466666666666666\n",
      "iteration no 152: Loss: 0.7117441176785724, accuracy: 0.5466666666666666\n",
      "iteration no 153: Loss: 0.7114873434270008, accuracy: 0.5466666666666666\n",
      "iteration no 154: Loss: 0.7112345360716987, accuracy: 0.5466666666666666\n",
      "iteration no 155: Loss: 0.7109840014280155, accuracy: 0.5466666666666666\n",
      "iteration no 156: Loss: 0.7107354878925529, accuracy: 0.5466666666666666\n",
      "iteration no 157: Loss: 0.7104892721781254, accuracy: 0.5466666666666666\n",
      "iteration no 158: Loss: 0.7102445357032632, accuracy: 0.5466666666666666\n",
      "iteration no 159: Loss: 0.7100018529158156, accuracy: 0.5466666666666666\n",
      "iteration no 160: Loss: 0.7097603589801006, accuracy: 0.55\n",
      "iteration no 161: Loss: 0.7095193582695237, accuracy: 0.55\n",
      "iteration no 162: Loss: 0.709280344823537, accuracy: 0.5533333333333333\n",
      "iteration no 163: Loss: 0.7090438907909454, accuracy: 0.5533333333333333\n",
      "iteration no 164: Loss: 0.7088097205033834, accuracy: 0.5566666666666666\n",
      "iteration no 165: Loss: 0.7085783239352086, accuracy: 0.5566666666666666\n",
      "iteration no 166: Loss: 0.7083486265679984, accuracy: 0.5566666666666666\n",
      "iteration no 167: Loss: 0.7081209829097361, accuracy: 0.56\n",
      "iteration no 168: Loss: 0.7078940054609539, accuracy: 0.5566666666666666\n",
      "iteration no 169: Loss: 0.7076697884835359, accuracy: 0.5566666666666666\n",
      "iteration no 170: Loss: 0.7074484293114631, accuracy: 0.56\n",
      "iteration no 171: Loss: 0.7072298057870566, accuracy: 0.56\n",
      "iteration no 172: Loss: 0.7070132043443621, accuracy: 0.56\n",
      "iteration no 173: Loss: 0.7067974863575583, accuracy: 0.56\n",
      "iteration no 174: Loss: 0.7065834111484034, accuracy: 0.56\n",
      "iteration no 175: Loss: 0.7063705523645877, accuracy: 0.56\n",
      "iteration no 176: Loss: 0.7061605231837457, accuracy: 0.56\n",
      "iteration no 177: Loss: 0.7059500214482616, accuracy: 0.56\n",
      "iteration no 178: Loss: 0.7057415464610713, accuracy: 0.56\n",
      "iteration no 179: Loss: 0.7055332135745895, accuracy: 0.56\n",
      "iteration no 180: Loss: 0.705326029789161, accuracy: 0.56\n",
      "iteration no 181: Loss: 0.7051191287741612, accuracy: 0.56\n",
      "iteration no 182: Loss: 0.7049133378708943, accuracy: 0.56\n",
      "iteration no 183: Loss: 0.7047091023718136, accuracy: 0.5633333333333334\n",
      "iteration no 184: Loss: 0.7045077061737881, accuracy: 0.5633333333333334\n",
      "iteration no 185: Loss: 0.7043075519942562, accuracy: 0.5633333333333334\n",
      "iteration no 186: Loss: 0.7041111109894687, accuracy: 0.5633333333333334\n",
      "iteration no 187: Loss: 0.7039159880173036, accuracy: 0.5633333333333334\n",
      "iteration no 188: Loss: 0.703721362706111, accuracy: 0.5633333333333334\n",
      "iteration no 189: Loss: 0.7035274301704538, accuracy: 0.5633333333333334\n",
      "iteration no 190: Loss: 0.7033340953876164, accuracy: 0.5633333333333334\n",
      "iteration no 191: Loss: 0.7031411599888988, accuracy: 0.5633333333333334\n",
      "iteration no 192: Loss: 0.7029489505901598, accuracy: 0.5633333333333334\n",
      "iteration no 193: Loss: 0.7027573874618491, accuracy: 0.5633333333333334\n",
      "iteration no 194: Loss: 0.7025670595030777, accuracy: 0.5633333333333334\n",
      "iteration no 195: Loss: 0.7023765918903566, accuracy: 0.5633333333333334\n",
      "iteration no 196: Loss: 0.7021868077083648, accuracy: 0.5633333333333334\n",
      "iteration no 197: Loss: 0.7019972731802955, accuracy: 0.5633333333333334\n",
      "iteration no 198: Loss: 0.701807496066916, accuracy: 0.5633333333333334\n",
      "iteration no 199: Loss: 0.7016178517400179, accuracy: 0.5633333333333334\n",
      "iteration no 200: Loss: 0.7014284115654609, accuracy: 0.5633333333333334\n",
      "iteration no 201: Loss: 0.7012392169203967, accuracy: 0.5633333333333334\n",
      "iteration no 202: Loss: 0.7010504737411991, accuracy: 0.5633333333333334\n",
      "iteration no 203: Loss: 0.7008626427839836, accuracy: 0.5633333333333334\n",
      "iteration no 204: Loss: 0.7006744942552856, accuracy: 0.5633333333333334\n",
      "iteration no 205: Loss: 0.7004866037464633, accuracy: 0.5633333333333334\n",
      "iteration no 206: Loss: 0.7002984151649597, accuracy: 0.5633333333333334\n",
      "iteration no 207: Loss: 0.7001111946440075, accuracy: 0.5633333333333334\n",
      "iteration no 208: Loss: 0.6999238958116667, accuracy: 0.5633333333333334\n",
      "iteration no 209: Loss: 0.6997374075533306, accuracy: 0.5633333333333334\n",
      "iteration no 210: Loss: 0.6995516423201966, accuracy: 0.5633333333333334\n",
      "iteration no 211: Loss: 0.6993655466278467, accuracy: 0.5633333333333334\n",
      "iteration no 212: Loss: 0.6991799728204195, accuracy: 0.5633333333333334\n",
      "iteration no 213: Loss: 0.6989937361193082, accuracy: 0.5666666666666667\n",
      "iteration no 214: Loss: 0.6988071894828848, accuracy: 0.5666666666666667\n",
      "iteration no 215: Loss: 0.6986198053932828, accuracy: 0.5666666666666667\n",
      "iteration no 216: Loss: 0.6984323765247794, accuracy: 0.5666666666666667\n",
      "iteration no 217: Loss: 0.6982447871705781, accuracy: 0.5666666666666667\n",
      "iteration no 218: Loss: 0.6980565600450601, accuracy: 0.5666666666666667\n",
      "iteration no 219: Loss: 0.6978683339656134, accuracy: 0.5666666666666667\n",
      "iteration no 220: Loss: 0.6976797608750378, accuracy: 0.5666666666666667\n",
      "iteration no 221: Loss: 0.6974895686161049, accuracy: 0.57\n",
      "iteration no 222: Loss: 0.6972991346662226, accuracy: 0.57\n",
      "iteration no 223: Loss: 0.6971086223007537, accuracy: 0.57\n",
      "iteration no 224: Loss: 0.6969184586615899, accuracy: 0.57\n",
      "iteration no 225: Loss: 0.6967278393259104, accuracy: 0.57\n",
      "iteration no 226: Loss: 0.6965364907170319, accuracy: 0.5733333333333334\n",
      "iteration no 227: Loss: 0.6963441132963528, accuracy: 0.5733333333333334\n",
      "iteration no 228: Loss: 0.6961523619104676, accuracy: 0.5733333333333334\n",
      "iteration no 229: Loss: 0.6959608378121006, accuracy: 0.5733333333333334\n",
      "iteration no 230: Loss: 0.6957689030889913, accuracy: 0.5733333333333334\n",
      "iteration no 231: Loss: 0.6955759898394, accuracy: 0.5733333333333334\n",
      "iteration no 232: Loss: 0.6953829982292457, accuracy: 0.5733333333333334\n",
      "iteration no 233: Loss: 0.6951894198627405, accuracy: 0.5733333333333334\n",
      "iteration no 234: Loss: 0.6949956870308766, accuracy: 0.5733333333333334\n",
      "iteration no 235: Loss: 0.6948013439412053, accuracy: 0.5733333333333334\n",
      "iteration no 236: Loss: 0.6946059911685868, accuracy: 0.5733333333333334\n",
      "iteration no 237: Loss: 0.6944104316867294, accuracy: 0.5733333333333334\n",
      "iteration no 238: Loss: 0.6942140329693652, accuracy: 0.5733333333333334\n",
      "iteration no 239: Loss: 0.6940181828944303, accuracy: 0.5733333333333334\n",
      "iteration no 240: Loss: 0.6938219443766103, accuracy: 0.5733333333333334\n",
      "iteration no 241: Loss: 0.6936256733826982, accuracy: 0.5733333333333334\n",
      "iteration no 242: Loss: 0.6934303465583658, accuracy: 0.5733333333333334\n",
      "iteration no 243: Loss: 0.6932341196417419, accuracy: 0.5733333333333334\n",
      "iteration no 244: Loss: 0.6930377778266812, accuracy: 0.5733333333333334\n",
      "iteration no 245: Loss: 0.692841061173388, accuracy: 0.5733333333333334\n",
      "iteration no 246: Loss: 0.6926438536626911, accuracy: 0.5733333333333334\n",
      "iteration no 247: Loss: 0.6924473881075279, accuracy: 0.5733333333333334\n",
      "iteration no 248: Loss: 0.6922507899679236, accuracy: 0.5733333333333334\n",
      "iteration no 249: Loss: 0.6920537963386761, accuracy: 0.5733333333333334\n",
      "iteration no 250: Loss: 0.6918571734328358, accuracy: 0.5733333333333334\n",
      "iteration no 251: Loss: 0.6916577882372699, accuracy: 0.5733333333333334\n",
      "iteration no 252: Loss: 0.6914585702221373, accuracy: 0.5733333333333334\n",
      "iteration no 253: Loss: 0.6912585499127641, accuracy: 0.5733333333333334\n",
      "iteration no 254: Loss: 0.691058360897071, accuracy: 0.5733333333333334\n",
      "iteration no 255: Loss: 0.6908573362885833, accuracy: 0.5733333333333334\n",
      "iteration no 256: Loss: 0.6906563916276056, accuracy: 0.5733333333333334\n",
      "iteration no 257: Loss: 0.6904547710915693, accuracy: 0.5733333333333334\n",
      "iteration no 258: Loss: 0.690252856640067, accuracy: 0.5733333333333334\n",
      "iteration no 259: Loss: 0.6900502192512858, accuracy: 0.5733333333333334\n",
      "iteration no 260: Loss: 0.6898474479702266, accuracy: 0.5733333333333334\n",
      "iteration no 261: Loss: 0.6896430981096912, accuracy: 0.5766666666666667\n",
      "iteration no 262: Loss: 0.6894384406368981, accuracy: 0.5766666666666667\n",
      "iteration no 263: Loss: 0.6892310256012002, accuracy: 0.58\n",
      "iteration no 264: Loss: 0.6890195081652623, accuracy: 0.5766666666666667\n",
      "iteration no 265: Loss: 0.6888072188221446, accuracy: 0.58\n",
      "iteration no 266: Loss: 0.6885951843251803, accuracy: 0.58\n",
      "iteration no 267: Loss: 0.6883827092970413, accuracy: 0.58\n",
      "iteration no 268: Loss: 0.6881690806644061, accuracy: 0.58\n",
      "iteration no 269: Loss: 0.6879544192417804, accuracy: 0.58\n",
      "iteration no 270: Loss: 0.6877400953217614, accuracy: 0.58\n",
      "iteration no 271: Loss: 0.6875260127183713, accuracy: 0.58\n",
      "iteration no 272: Loss: 0.6873094829628277, accuracy: 0.58\n",
      "iteration no 273: Loss: 0.6870924576945069, accuracy: 0.5833333333333334\n",
      "iteration no 274: Loss: 0.6868747174525471, accuracy: 0.5833333333333334\n",
      "iteration no 275: Loss: 0.6866569020133346, accuracy: 0.5833333333333334\n",
      "iteration no 276: Loss: 0.6864365495195273, accuracy: 0.5833333333333334\n",
      "iteration no 277: Loss: 0.6862126191705414, accuracy: 0.5833333333333334\n",
      "iteration no 278: Loss: 0.6859879571516052, accuracy: 0.5833333333333334\n",
      "iteration no 279: Loss: 0.6857622263929147, accuracy: 0.5833333333333334\n",
      "iteration no 280: Loss: 0.6855328936820148, accuracy: 0.5833333333333334\n",
      "iteration no 281: Loss: 0.685304180824062, accuracy: 0.5833333333333334\n",
      "iteration no 282: Loss: 0.6850740392468408, accuracy: 0.5833333333333334\n",
      "iteration no 283: Loss: 0.6848428589305071, accuracy: 0.5833333333333334\n",
      "iteration no 284: Loss: 0.6846113604098909, accuracy: 0.5833333333333334\n",
      "iteration no 285: Loss: 0.6843788031754199, accuracy: 0.5833333333333334\n",
      "iteration no 286: Loss: 0.6841461363864014, accuracy: 0.5866666666666667\n",
      "iteration no 287: Loss: 0.6839103602002632, accuracy: 0.5866666666666667\n",
      "iteration no 288: Loss: 0.6836741850476449, accuracy: 0.5866666666666667\n",
      "iteration no 289: Loss: 0.6834383780320088, accuracy: 0.5866666666666667\n",
      "iteration no 290: Loss: 0.6832012475482399, accuracy: 0.59\n",
      "iteration no 291: Loss: 0.682963722980842, accuracy: 0.59\n",
      "iteration no 292: Loss: 0.6827240254883716, accuracy: 0.59\n",
      "iteration no 293: Loss: 0.6824781525132937, accuracy: 0.59\n",
      "iteration no 294: Loss: 0.6822327307762838, accuracy: 0.5933333333333334\n",
      "iteration no 295: Loss: 0.6819865836675664, accuracy: 0.5966666666666667\n",
      "iteration no 296: Loss: 0.6817381617030921, accuracy: 0.5966666666666667\n",
      "iteration no 297: Loss: 0.6814910388195338, accuracy: 0.5966666666666667\n",
      "iteration no 298: Loss: 0.6812412101761863, accuracy: 0.5966666666666667\n",
      "iteration no 299: Loss: 0.680985163239416, accuracy: 0.5966666666666667\n",
      "iteration no 300: Loss: 0.6807279552006943, accuracy: 0.6\n",
      "iteration no 301: Loss: 0.6804689840770198, accuracy: 0.6033333333333334\n",
      "iteration no 302: Loss: 0.6802099410079067, accuracy: 0.6033333333333334\n",
      "iteration no 303: Loss: 0.6799494680315431, accuracy: 0.6033333333333334\n",
      "iteration no 304: Loss: 0.6796887956353668, accuracy: 0.6033333333333334\n",
      "iteration no 305: Loss: 0.6794271948039405, accuracy: 0.6033333333333334\n",
      "iteration no 306: Loss: 0.6791642006334617, accuracy: 0.6033333333333334\n",
      "iteration no 307: Loss: 0.678900831482511, accuracy: 0.6033333333333334\n",
      "iteration no 308: Loss: 0.6786364324465616, accuracy: 0.6033333333333334\n",
      "iteration no 309: Loss: 0.678370519384616, accuracy: 0.6033333333333334\n",
      "iteration no 310: Loss: 0.678104037444376, accuracy: 0.6033333333333334\n",
      "iteration no 311: Loss: 0.6778363668045915, accuracy: 0.6033333333333334\n",
      "iteration no 312: Loss: 0.6775682709828454, accuracy: 0.6033333333333334\n",
      "iteration no 313: Loss: 0.6772919861610778, accuracy: 0.6033333333333334\n",
      "iteration no 314: Loss: 0.6770157822332189, accuracy: 0.6033333333333334\n",
      "iteration no 315: Loss: 0.6767382531526572, accuracy: 0.6033333333333334\n",
      "iteration no 316: Loss: 0.6764588003769328, accuracy: 0.6033333333333334\n",
      "iteration no 317: Loss: 0.6761792649482297, accuracy: 0.6033333333333334\n",
      "iteration no 318: Loss: 0.6758988329311801, accuracy: 0.6033333333333334\n",
      "iteration no 319: Loss: 0.6756173197961276, accuracy: 0.6033333333333334\n",
      "iteration no 320: Loss: 0.6753354140634306, accuracy: 0.6033333333333334\n",
      "iteration no 321: Loss: 0.6750513540746838, accuracy: 0.6033333333333334\n",
      "iteration no 322: Loss: 0.6747663420885702, accuracy: 0.6033333333333334\n",
      "iteration no 323: Loss: 0.674480421580905, accuracy: 0.6033333333333334\n",
      "iteration no 324: Loss: 0.6741928714986283, accuracy: 0.6033333333333334\n",
      "iteration no 325: Loss: 0.6739042568489728, accuracy: 0.6033333333333334\n",
      "iteration no 326: Loss: 0.6736144417325516, accuracy: 0.6033333333333334\n",
      "iteration no 327: Loss: 0.6733238101666805, accuracy: 0.6033333333333334\n",
      "iteration no 328: Loss: 0.6730299636095672, accuracy: 0.6033333333333334\n",
      "iteration no 329: Loss: 0.6727313455980849, accuracy: 0.6033333333333334\n",
      "iteration no 330: Loss: 0.6724307407721831, accuracy: 0.6033333333333334\n",
      "iteration no 331: Loss: 0.6721304372290231, accuracy: 0.6033333333333334\n",
      "iteration no 332: Loss: 0.6718284643892929, accuracy: 0.6033333333333334\n",
      "iteration no 333: Loss: 0.671524759170006, accuracy: 0.6033333333333334\n",
      "iteration no 334: Loss: 0.6712204489277639, accuracy: 0.6033333333333334\n",
      "iteration no 335: Loss: 0.6709148938780611, accuracy: 0.6033333333333334\n",
      "iteration no 336: Loss: 0.6706074534465377, accuracy: 0.6033333333333334\n",
      "iteration no 337: Loss: 0.6702964218811872, accuracy: 0.6033333333333334\n",
      "iteration no 338: Loss: 0.6699805645532159, accuracy: 0.6033333333333334\n",
      "iteration no 339: Loss: 0.6696650132356342, accuracy: 0.6033333333333334\n",
      "iteration no 340: Loss: 0.6693490542104176, accuracy: 0.6033333333333334\n",
      "iteration no 341: Loss: 0.6690312186377257, accuracy: 0.6033333333333334\n",
      "iteration no 342: Loss: 0.6687124746169149, accuracy: 0.6033333333333334\n",
      "iteration no 343: Loss: 0.6683921219409269, accuracy: 0.6033333333333334\n",
      "iteration no 344: Loss: 0.6680701230158034, accuracy: 0.6033333333333334\n",
      "iteration no 345: Loss: 0.667739634512199, accuracy: 0.6033333333333334\n",
      "iteration no 346: Loss: 0.6674065659008424, accuracy: 0.6033333333333334\n",
      "iteration no 347: Loss: 0.667072823882394, accuracy: 0.6033333333333334\n",
      "iteration no 348: Loss: 0.6667378748378765, accuracy: 0.6033333333333334\n",
      "iteration no 349: Loss: 0.6664015903807824, accuracy: 0.6033333333333334\n",
      "iteration no 350: Loss: 0.6660643192073896, accuracy: 0.6033333333333334\n",
      "iteration no 351: Loss: 0.665724975360461, accuracy: 0.6066666666666667\n",
      "iteration no 352: Loss: 0.665385240361451, accuracy: 0.6066666666666667\n",
      "iteration no 353: Loss: 0.6650438197907321, accuracy: 0.6066666666666667\n",
      "iteration no 354: Loss: 0.6646972825150781, accuracy: 0.6066666666666667\n",
      "iteration no 355: Loss: 0.6643465834966725, accuracy: 0.6066666666666667\n",
      "iteration no 356: Loss: 0.6639965730377141, accuracy: 0.61\n",
      "iteration no 357: Loss: 0.663643525003742, accuracy: 0.61\n",
      "iteration no 358: Loss: 0.6632908403378576, accuracy: 0.61\n",
      "iteration no 359: Loss: 0.6629370921127166, accuracy: 0.61\n",
      "iteration no 360: Loss: 0.6625828091628729, accuracy: 0.61\n",
      "iteration no 361: Loss: 0.6622251347116335, accuracy: 0.61\n",
      "iteration no 362: Loss: 0.6618682248492519, accuracy: 0.61\n",
      "iteration no 363: Loss: 0.6615069208591637, accuracy: 0.61\n",
      "iteration no 364: Loss: 0.661147446460468, accuracy: 0.61\n",
      "iteration no 365: Loss: 0.6607826348038438, accuracy: 0.61\n",
      "iteration no 366: Loss: 0.6604178628368262, accuracy: 0.6133333333333333\n",
      "iteration no 367: Loss: 0.6600493114483482, accuracy: 0.61\n",
      "iteration no 368: Loss: 0.6596813740452231, accuracy: 0.6133333333333333\n",
      "iteration no 369: Loss: 0.6593103441123686, accuracy: 0.6133333333333333\n",
      "iteration no 370: Loss: 0.6589358588770928, accuracy: 0.6133333333333333\n",
      "iteration no 371: Loss: 0.6585630944281502, accuracy: 0.6133333333333333\n",
      "iteration no 372: Loss: 0.6581828292125448, accuracy: 0.6133333333333333\n",
      "iteration no 373: Loss: 0.6578042353018353, accuracy: 0.6133333333333333\n",
      "iteration no 374: Loss: 0.6574233623408599, accuracy: 0.6133333333333333\n",
      "iteration no 375: Loss: 0.6570399700031516, accuracy: 0.6133333333333333\n",
      "iteration no 376: Loss: 0.6566549260977441, accuracy: 0.6133333333333333\n",
      "iteration no 377: Loss: 0.6562703517801738, accuracy: 0.6133333333333333\n",
      "iteration no 378: Loss: 0.6558823241279736, accuracy: 0.6166666666666667\n",
      "iteration no 379: Loss: 0.6554946143136113, accuracy: 0.6166666666666667\n",
      "iteration no 380: Loss: 0.6551020715488957, accuracy: 0.6166666666666667\n",
      "iteration no 381: Loss: 0.6547094929623294, accuracy: 0.6166666666666667\n",
      "iteration no 382: Loss: 0.6543176795492754, accuracy: 0.6166666666666667\n",
      "iteration no 383: Loss: 0.6539213991440612, accuracy: 0.6166666666666667\n",
      "iteration no 384: Loss: 0.6535228348249027, accuracy: 0.6166666666666667\n",
      "iteration no 385: Loss: 0.65312367384356, accuracy: 0.6166666666666667\n",
      "iteration no 386: Loss: 0.652721870444408, accuracy: 0.6166666666666667\n",
      "iteration no 387: Loss: 0.6523179443308166, accuracy: 0.6166666666666667\n",
      "iteration no 388: Loss: 0.6519115876941585, accuracy: 0.6166666666666667\n",
      "iteration no 389: Loss: 0.6515001885105942, accuracy: 0.6166666666666667\n",
      "iteration no 390: Loss: 0.6510842174205801, accuracy: 0.6166666666666667\n",
      "iteration no 391: Loss: 0.6506645028540803, accuracy: 0.62\n",
      "iteration no 392: Loss: 0.650238934749845, accuracy: 0.62\n",
      "iteration no 393: Loss: 0.6498129124955376, accuracy: 0.62\n",
      "iteration no 394: Loss: 0.6493774209424232, accuracy: 0.62\n",
      "iteration no 395: Loss: 0.6489368233493477, accuracy: 0.62\n",
      "iteration no 396: Loss: 0.6484921451328931, accuracy: 0.62\n",
      "iteration no 397: Loss: 0.6480476167085474, accuracy: 0.62\n",
      "iteration no 398: Loss: 0.6475976626770897, accuracy: 0.62\n",
      "iteration no 399: Loss: 0.6471476629372638, accuracy: 0.62\n",
      "iteration no 400: Loss: 0.6466937710734093, accuracy: 0.62\n",
      "iteration no 401: Loss: 0.6462443759636454, accuracy: 0.6233333333333333\n",
      "iteration no 402: Loss: 0.6457907199331057, accuracy: 0.6233333333333333\n",
      "iteration no 403: Loss: 0.6453333789813098, accuracy: 0.6233333333333333\n",
      "iteration no 404: Loss: 0.6448816046599146, accuracy: 0.6233333333333333\n",
      "iteration no 405: Loss: 0.6444259882605615, accuracy: 0.6233333333333333\n",
      "iteration no 406: Loss: 0.6439699640588149, accuracy: 0.6233333333333333\n",
      "iteration no 407: Loss: 0.6435080718738592, accuracy: 0.6233333333333333\n",
      "iteration no 408: Loss: 0.6430468128647739, accuracy: 0.6233333333333333\n",
      "iteration no 409: Loss: 0.6425869982652673, accuracy: 0.6233333333333333\n",
      "iteration no 410: Loss: 0.642120160305722, accuracy: 0.6233333333333333\n",
      "iteration no 411: Loss: 0.6416533415343973, accuracy: 0.6233333333333333\n",
      "iteration no 412: Loss: 0.6411811079988855, accuracy: 0.6233333333333333\n",
      "iteration no 413: Loss: 0.6407138749537021, accuracy: 0.6233333333333333\n",
      "iteration no 414: Loss: 0.6402370963868009, accuracy: 0.6233333333333333\n",
      "iteration no 415: Loss: 0.6397621374301765, accuracy: 0.6233333333333333\n",
      "iteration no 416: Loss: 0.6392826878254247, accuracy: 0.6233333333333333\n",
      "iteration no 417: Loss: 0.6388015469189409, accuracy: 0.6233333333333333\n",
      "iteration no 418: Loss: 0.6383192315503999, accuracy: 0.6233333333333333\n",
      "iteration no 419: Loss: 0.6378322910402957, accuracy: 0.6233333333333333\n",
      "iteration no 420: Loss: 0.6373443193886469, accuracy: 0.6266666666666667\n",
      "iteration no 421: Loss: 0.6368505227640864, accuracy: 0.6233333333333333\n",
      "iteration no 422: Loss: 0.6363595327251721, accuracy: 0.6266666666666667\n",
      "iteration no 423: Loss: 0.6358575275330306, accuracy: 0.6266666666666667\n",
      "iteration no 424: Loss: 0.6353594279771755, accuracy: 0.6266666666666667\n",
      "iteration no 425: Loss: 0.6348525601814954, accuracy: 0.6266666666666667\n",
      "iteration no 426: Loss: 0.6343486944875912, accuracy: 0.6266666666666667\n",
      "iteration no 427: Loss: 0.633838146208435, accuracy: 0.6266666666666667\n",
      "iteration no 428: Loss: 0.6333256846758611, accuracy: 0.63\n",
      "iteration no 429: Loss: 0.6328112885452073, accuracy: 0.63\n",
      "iteration no 430: Loss: 0.6322927702843616, accuracy: 0.63\n",
      "iteration no 431: Loss: 0.63177284378437, accuracy: 0.63\n",
      "iteration no 432: Loss: 0.6312477179592345, accuracy: 0.6333333333333333\n",
      "iteration no 433: Loss: 0.6307283116119988, accuracy: 0.6366666666666667\n",
      "iteration no 434: Loss: 0.6301941330614522, accuracy: 0.6366666666666667\n",
      "iteration no 435: Loss: 0.6296646971948521, accuracy: 0.6366666666666667\n",
      "iteration no 436: Loss: 0.6291294180203004, accuracy: 0.6366666666666667\n",
      "iteration no 437: Loss: 0.6285930597380818, accuracy: 0.6366666666666667\n",
      "iteration no 438: Loss: 0.6280526187192622, accuracy: 0.64\n",
      "iteration no 439: Loss: 0.6275080703109011, accuracy: 0.64\n",
      "iteration no 440: Loss: 0.6269650865824709, accuracy: 0.64\n",
      "iteration no 441: Loss: 0.6264200427017405, accuracy: 0.64\n",
      "iteration no 442: Loss: 0.6258649675642582, accuracy: 0.64\n",
      "iteration no 443: Loss: 0.6253104209211242, accuracy: 0.64\n",
      "iteration no 444: Loss: 0.6247511467189404, accuracy: 0.64\n",
      "iteration no 445: Loss: 0.6241874704870131, accuracy: 0.64\n",
      "iteration no 446: Loss: 0.6236167353299317, accuracy: 0.64\n",
      "iteration no 447: Loss: 0.6230426191373781, accuracy: 0.64\n",
      "iteration no 448: Loss: 0.6224715589642729, accuracy: 0.6433333333333333\n",
      "iteration no 449: Loss: 0.6218865499479005, accuracy: 0.6433333333333333\n",
      "iteration no 450: Loss: 0.6213079546792454, accuracy: 0.6433333333333333\n",
      "iteration no 451: Loss: 0.6207175111949397, accuracy: 0.6466666666666666\n",
      "iteration no 452: Loss: 0.6201290804263355, accuracy: 0.6433333333333333\n",
      "iteration no 453: Loss: 0.6195249744148563, accuracy: 0.6466666666666666\n",
      "iteration no 454: Loss: 0.6189260513703523, accuracy: 0.6466666666666666\n",
      "iteration no 455: Loss: 0.6183157806008998, accuracy: 0.6466666666666666\n",
      "iteration no 456: Loss: 0.6177059586184481, accuracy: 0.6466666666666666\n",
      "iteration no 457: Loss: 0.6170893089936026, accuracy: 0.6466666666666666\n",
      "iteration no 458: Loss: 0.6164687272721792, accuracy: 0.6466666666666666\n",
      "iteration no 459: Loss: 0.6158519297262034, accuracy: 0.65\n",
      "iteration no 460: Loss: 0.615230103844696, accuracy: 0.65\n",
      "iteration no 461: Loss: 0.6146055239508412, accuracy: 0.65\n",
      "iteration no 462: Loss: 0.613978836781826, accuracy: 0.65\n",
      "iteration no 463: Loss: 0.6133546780751248, accuracy: 0.65\n",
      "iteration no 464: Loss: 0.612721699267053, accuracy: 0.65\n",
      "iteration no 465: Loss: 0.612092682963499, accuracy: 0.65\n",
      "iteration no 466: Loss: 0.6114564831602116, accuracy: 0.65\n",
      "iteration no 467: Loss: 0.6108194174964354, accuracy: 0.65\n",
      "iteration no 468: Loss: 0.6101824967579837, accuracy: 0.65\n",
      "iteration no 469: Loss: 0.6095472331549722, accuracy: 0.65\n",
      "iteration no 470: Loss: 0.6089064276789712, accuracy: 0.65\n",
      "iteration no 471: Loss: 0.6082684626387495, accuracy: 0.65\n",
      "iteration no 472: Loss: 0.6076252554216981, accuracy: 0.65\n",
      "iteration no 473: Loss: 0.6069808755912554, accuracy: 0.65\n",
      "iteration no 474: Loss: 0.6063355594501899, accuracy: 0.65\n",
      "iteration no 475: Loss: 0.6056927295776617, accuracy: 0.65\n",
      "iteration no 476: Loss: 0.6050391331742754, accuracy: 0.65\n",
      "iteration no 477: Loss: 0.6043867837488507, accuracy: 0.65\n",
      "iteration no 478: Loss: 0.6037288351158716, accuracy: 0.6533333333333333\n",
      "iteration no 479: Loss: 0.6030696075241965, accuracy: 0.6533333333333333\n",
      "iteration no 480: Loss: 0.6024036835238555, accuracy: 0.6533333333333333\n",
      "iteration no 481: Loss: 0.6017437733678412, accuracy: 0.6533333333333333\n",
      "iteration no 482: Loss: 0.6010704652392806, accuracy: 0.6533333333333333\n",
      "iteration no 483: Loss: 0.6003966109011343, accuracy: 0.6566666666666666\n",
      "iteration no 484: Loss: 0.5997248038978278, accuracy: 0.6633333333333333\n",
      "iteration no 485: Loss: 0.59904205886059, accuracy: 0.6566666666666666\n",
      "iteration no 486: Loss: 0.5983639569492203, accuracy: 0.6666666666666666\n",
      "iteration no 487: Loss: 0.5976784209956945, accuracy: 0.6666666666666666\n",
      "iteration no 488: Loss: 0.5969917920281255, accuracy: 0.67\n",
      "iteration no 489: Loss: 0.5963065740648608, accuracy: 0.67\n",
      "iteration no 490: Loss: 0.5956082710716292, accuracy: 0.67\n",
      "iteration no 491: Loss: 0.5949152414130958, accuracy: 0.67\n",
      "iteration no 492: Loss: 0.5942245018715534, accuracy: 0.68\n",
      "iteration no 493: Loss: 0.5935270634590197, accuracy: 0.6733333333333333\n",
      "iteration no 494: Loss: 0.592834103468359, accuracy: 0.6833333333333333\n",
      "iteration no 495: Loss: 0.59213645937144, accuracy: 0.6733333333333333\n",
      "iteration no 496: Loss: 0.5914399212273787, accuracy: 0.6833333333333333\n",
      "iteration no 497: Loss: 0.5907335429589383, accuracy: 0.6766666666666666\n",
      "iteration no 498: Loss: 0.5900310694589053, accuracy: 0.6833333333333333\n",
      "iteration no 499: Loss: 0.5893252656248713, accuracy: 0.6766666666666666\n",
      "iteration no 500: Loss: 0.5886178808135134, accuracy: 0.6833333333333333\n",
      "iteration no 501: Loss: 0.5878988680218077, accuracy: 0.6766666666666666\n",
      "iteration no 502: Loss: 0.5871826531683967, accuracy: 0.6833333333333333\n",
      "iteration no 503: Loss: 0.5864710985235033, accuracy: 0.6833333333333333\n",
      "iteration no 504: Loss: 0.5857604150246899, accuracy: 0.6866666666666666\n",
      "iteration no 505: Loss: 0.5850497637272462, accuracy: 0.6833333333333333\n",
      "iteration no 506: Loss: 0.5843395960096706, accuracy: 0.6866666666666666\n",
      "iteration no 507: Loss: 0.5836301039004201, accuracy: 0.6833333333333333\n",
      "iteration no 508: Loss: 0.5829203087638366, accuracy: 0.6933333333333334\n",
      "iteration no 509: Loss: 0.5821977805241064, accuracy: 0.69\n",
      "iteration no 510: Loss: 0.5814854763073664, accuracy: 0.6966666666666667\n",
      "iteration no 511: Loss: 0.5807513002410293, accuracy: 0.69\n",
      "iteration no 512: Loss: 0.5800307756823834, accuracy: 0.6966666666666667\n",
      "iteration no 513: Loss: 0.5792825987826536, accuracy: 0.6966666666666667\n",
      "iteration no 514: Loss: 0.5785639453115632, accuracy: 0.7033333333333334\n",
      "iteration no 515: Loss: 0.5778265239563397, accuracy: 0.7\n",
      "iteration no 516: Loss: 0.5771013530926913, accuracy: 0.7066666666666667\n",
      "iteration no 517: Loss: 0.576353795431733, accuracy: 0.7\n",
      "iteration no 518: Loss: 0.5756248720626325, accuracy: 0.7066666666666667\n",
      "iteration no 519: Loss: 0.5748782703103842, accuracy: 0.7066666666666667\n",
      "iteration no 520: Loss: 0.5741354460385444, accuracy: 0.71\n",
      "iteration no 521: Loss: 0.5733902987734829, accuracy: 0.7066666666666667\n",
      "iteration no 522: Loss: 0.5726619257559462, accuracy: 0.71\n",
      "iteration no 523: Loss: 0.5719210847751681, accuracy: 0.7066666666666667\n",
      "iteration no 524: Loss: 0.5711872161913502, accuracy: 0.71\n",
      "iteration no 525: Loss: 0.570457051926762, accuracy: 0.7066666666666667\n",
      "iteration no 526: Loss: 0.5697449555192305, accuracy: 0.71\n",
      "iteration no 527: Loss: 0.5690150935064816, accuracy: 0.7066666666666667\n",
      "iteration no 528: Loss: 0.5683146241349395, accuracy: 0.71\n",
      "iteration no 529: Loss: 0.5675888739864628, accuracy: 0.7066666666666667\n",
      "iteration no 530: Loss: 0.5669039638027124, accuracy: 0.71\n",
      "iteration no 531: Loss: 0.5661984926557871, accuracy: 0.7066666666666667\n",
      "iteration no 532: Loss: 0.5655246731897327, accuracy: 0.71\n",
      "iteration no 533: Loss: 0.5648625502944413, accuracy: 0.71\n",
      "iteration no 534: Loss: 0.564252231713944, accuracy: 0.7133333333333334\n",
      "iteration no 535: Loss: 0.563616544338532, accuracy: 0.7033333333333334\n",
      "iteration no 536: Loss: 0.5630274379453692, accuracy: 0.7166666666666667\n",
      "iteration no 537: Loss: 0.5624714137998911, accuracy: 0.7\n",
      "iteration no 538: Loss: 0.5619531110048899, accuracy: 0.7133333333333334\n",
      "iteration no 539: Loss: 0.5614495213573898, accuracy: 0.7033333333333334\n",
      "iteration no 540: Loss: 0.5610301795474673, accuracy: 0.7166666666666667\n",
      "iteration no 541: Loss: 0.5607134182566654, accuracy: 0.7033333333333334\n",
      "iteration no 542: Loss: 0.5604873686452466, accuracy: 0.7133333333333334\n",
      "iteration no 543: Loss: 0.5603633345739528, accuracy: 0.7066666666666667\n",
      "iteration no 544: Loss: 0.5602763168524317, accuracy: 0.7166666666666667\n",
      "iteration no 545: Loss: 0.5604889281487023, accuracy: 0.71\n",
      "iteration no 546: Loss: 0.5606259438574628, accuracy: 0.7366666666666667\n",
      "iteration no 547: Loss: 0.5612299915964817, accuracy: 0.7233333333333334\n",
      "iteration no 548: Loss: 0.5616354493154883, accuracy: 0.75\n",
      "iteration no 549: Loss: 0.5628907938368278, accuracy: 0.7233333333333334\n",
      "iteration no 550: Loss: 0.5637199241758544, accuracy: 0.7633333333333333\n",
      "iteration no 551: Loss: 0.5656686399892228, accuracy: 0.73\n",
      "iteration no 552: Loss: 0.5666657454874302, accuracy: 0.7733333333333333\n",
      "iteration no 553: Loss: 0.5697846562855744, accuracy: 0.74\n",
      "iteration no 554: Loss: 0.5707286636284951, accuracy: 0.7733333333333333\n",
      "iteration no 555: Loss: 0.574775390504432, accuracy: 0.74\n",
      "iteration no 556: Loss: 0.5755897313064038, accuracy: 0.7666666666666667\n",
      "iteration no 557: Loss: 0.580456219675863, accuracy: 0.74\n",
      "iteration no 558: Loss: 0.5803059442339178, accuracy: 0.7633333333333333\n",
      "iteration no 559: Loss: 0.5855631499952173, accuracy: 0.7433333333333333\n",
      "iteration no 560: Loss: 0.5839786084577931, accuracy: 0.7633333333333333\n",
      "iteration no 561: Loss: 0.5889069328076303, accuracy: 0.7433333333333333\n",
      "iteration no 562: Loss: 0.5856059629254243, accuracy: 0.7633333333333333\n",
      "iteration no 563: Loss: 0.589529642710392, accuracy: 0.7433333333333333\n",
      "iteration no 564: Loss: 0.584575561361434, accuracy: 0.7633333333333333\n",
      "iteration no 565: Loss: 0.5875211701147731, accuracy: 0.7433333333333333\n",
      "iteration no 566: Loss: 0.5818530797644916, accuracy: 0.7633333333333333\n",
      "iteration no 567: Loss: 0.5840985298370133, accuracy: 0.74\n",
      "iteration no 568: Loss: 0.5782660922353858, accuracy: 0.7633333333333333\n",
      "iteration no 569: Loss: 0.5801679164684747, accuracy: 0.7466666666666667\n",
      "iteration no 570: Loss: 0.5746788606334288, accuracy: 0.7666666666666667\n",
      "iteration no 571: Loss: 0.5764119212603538, accuracy: 0.7466666666666667\n",
      "iteration no 572: Loss: 0.5714459152558512, accuracy: 0.7666666666666667\n",
      "iteration no 573: Loss: 0.5729998933197319, accuracy: 0.75\n",
      "iteration no 574: Loss: 0.5684448144249887, accuracy: 0.7733333333333333\n",
      "iteration no 575: Loss: 0.5700571090813539, accuracy: 0.7533333333333333\n",
      "iteration no 576: Loss: 0.5658772916038204, accuracy: 0.7766666666666666\n",
      "iteration no 577: Loss: 0.5676279213604994, accuracy: 0.7533333333333333\n",
      "iteration no 578: Loss: 0.563765194755628, accuracy: 0.7766666666666666\n",
      "iteration no 579: Loss: 0.5656559129826726, accuracy: 0.7533333333333333\n",
      "iteration no 580: Loss: 0.5620799023697431, accuracy: 0.7766666666666666\n",
      "iteration no 581: Loss: 0.5640795606017598, accuracy: 0.7533333333333333\n",
      "iteration no 582: Loss: 0.5605673166680964, accuracy: 0.7766666666666666\n",
      "iteration no 583: Loss: 0.5626664596223052, accuracy: 0.7566666666666667\n",
      "iteration no 584: Loss: 0.5593285898974598, accuracy: 0.7766666666666666\n",
      "iteration no 585: Loss: 0.5617074273483653, accuracy: 0.76\n",
      "iteration no 586: Loss: 0.5584887579410805, accuracy: 0.7766666666666666\n",
      "iteration no 587: Loss: 0.5609363307588082, accuracy: 0.76\n",
      "iteration no 588: Loss: 0.5576712088650203, accuracy: 0.7766666666666666\n",
      "iteration no 589: Loss: 0.5603821813506461, accuracy: 0.76\n",
      "iteration no 590: Loss: 0.5570140958745546, accuracy: 0.7766666666666666\n",
      "iteration no 591: Loss: 0.5598697181498563, accuracy: 0.76\n",
      "iteration no 592: Loss: 0.5562134672551928, accuracy: 0.7766666666666666\n",
      "iteration no 593: Loss: 0.558993805428045, accuracy: 0.7633333333333333\n",
      "iteration no 594: Loss: 0.5551910899671408, accuracy: 0.78\n",
      "iteration no 595: Loss: 0.5580054636773041, accuracy: 0.7633333333333333\n",
      "iteration no 596: Loss: 0.553974304216258, accuracy: 0.78\n",
      "iteration no 597: Loss: 0.5568987249742692, accuracy: 0.7666666666666667\n",
      "iteration no 598: Loss: 0.5527423026877568, accuracy: 0.78\n",
      "iteration no 599: Loss: 0.555628586916363, accuracy: 0.7666666666666667\n",
      "iteration no 600: Loss: 0.5515742138012971, accuracy: 0.78\n",
      "iteration no 601: Loss: 0.5541955033572167, accuracy: 0.77\n",
      "iteration no 602: Loss: 0.5501802782123741, accuracy: 0.7833333333333333\n",
      "iteration no 603: Loss: 0.5532592501965002, accuracy: 0.77\n",
      "iteration no 604: Loss: 0.5490425039203962, accuracy: 0.7833333333333333\n",
      "iteration no 605: Loss: 0.552316922126567, accuracy: 0.7733333333333333\n",
      "iteration no 606: Loss: 0.5481047800666334, accuracy: 0.7833333333333333\n",
      "iteration no 607: Loss: 0.5512127018401365, accuracy: 0.7733333333333333\n",
      "iteration no 608: Loss: 0.5466248512957776, accuracy: 0.7833333333333333\n",
      "iteration no 609: Loss: 0.5499243233329795, accuracy: 0.7733333333333333\n",
      "iteration no 610: Loss: 0.5455472664190423, accuracy: 0.7833333333333333\n",
      "iteration no 611: Loss: 0.5486282244524487, accuracy: 0.7733333333333333\n",
      "iteration no 612: Loss: 0.5439571538069529, accuracy: 0.7833333333333333\n",
      "iteration no 613: Loss: 0.5471993185398147, accuracy: 0.7733333333333333\n",
      "iteration no 614: Loss: 0.5429424592502113, accuracy: 0.7833333333333333\n",
      "iteration no 615: Loss: 0.5463530015764986, accuracy: 0.7766666666666666\n",
      "iteration no 616: Loss: 0.5415077861512886, accuracy: 0.7833333333333333\n",
      "iteration no 617: Loss: 0.5449280145356585, accuracy: 0.78\n",
      "iteration no 618: Loss: 0.5404212026644077, accuracy: 0.7833333333333333\n",
      "iteration no 619: Loss: 0.5436726463676775, accuracy: 0.78\n",
      "iteration no 620: Loss: 0.5389447366443145, accuracy: 0.78\n",
      "iteration no 621: Loss: 0.542321195392476, accuracy: 0.78\n",
      "iteration no 622: Loss: 0.5375627911777081, accuracy: 0.78\n",
      "iteration no 623: Loss: 0.5409551292139543, accuracy: 0.78\n",
      "iteration no 624: Loss: 0.5362082093479966, accuracy: 0.7833333333333333\n",
      "iteration no 625: Loss: 0.5395285581566772, accuracy: 0.78\n",
      "iteration no 626: Loss: 0.5348857917268834, accuracy: 0.7833333333333333\n",
      "iteration no 627: Loss: 0.538349739182401, accuracy: 0.7833333333333333\n",
      "iteration no 628: Loss: 0.53352592681064, accuracy: 0.7866666666666666\n",
      "iteration no 629: Loss: 0.5370841156405894, accuracy: 0.7833333333333333\n",
      "iteration no 630: Loss: 0.5322624439683128, accuracy: 0.7866666666666666\n",
      "iteration no 631: Loss: 0.5355032984225407, accuracy: 0.7833333333333333\n",
      "iteration no 632: Loss: 0.5308471821886569, accuracy: 0.79\n",
      "iteration no 633: Loss: 0.5346573101665404, accuracy: 0.79\n",
      "iteration no 634: Loss: 0.5297576542763073, accuracy: 0.7933333333333333\n",
      "iteration no 635: Loss: 0.5332050121794185, accuracy: 0.79\n",
      "iteration no 636: Loss: 0.5284364693872876, accuracy: 0.7933333333333333\n",
      "iteration no 637: Loss: 0.5321467163763709, accuracy: 0.7933333333333333\n",
      "iteration no 638: Loss: 0.5273322865705221, accuracy: 0.7933333333333333\n",
      "iteration no 639: Loss: 0.5312310493355192, accuracy: 0.7933333333333333\n",
      "iteration no 640: Loss: 0.5262799219344059, accuracy: 0.7933333333333333\n",
      "iteration no 641: Loss: 0.5297226799083257, accuracy: 0.7933333333333333\n",
      "iteration no 642: Loss: 0.5249502399213262, accuracy: 0.7933333333333333\n",
      "iteration no 643: Loss: 0.5289634086255108, accuracy: 0.7933333333333333\n",
      "iteration no 644: Loss: 0.5240735998979369, accuracy: 0.7933333333333333\n",
      "iteration no 645: Loss: 0.5278495953069751, accuracy: 0.7933333333333333\n",
      "iteration no 646: Loss: 0.5227426703522281, accuracy: 0.7933333333333333\n",
      "iteration no 647: Loss: 0.5264648580371137, accuracy: 0.7933333333333333\n",
      "iteration no 648: Loss: 0.5215071899432951, accuracy: 0.7933333333333333\n",
      "iteration no 649: Loss: 0.5253877185465887, accuracy: 0.7933333333333333\n",
      "iteration no 650: Loss: 0.5203412379586194, accuracy: 0.7966666666666666\n",
      "iteration no 651: Loss: 0.5241355240956623, accuracy: 0.7966666666666666\n",
      "iteration no 652: Loss: 0.5190095335927167, accuracy: 0.7966666666666666\n",
      "iteration no 653: Loss: 0.5228576484523022, accuracy: 0.7966666666666666\n",
      "iteration no 654: Loss: 0.5178279504147496, accuracy: 0.7966666666666666\n",
      "iteration no 655: Loss: 0.5218049365112893, accuracy: 0.7966666666666666\n",
      "iteration no 656: Loss: 0.5165003109553474, accuracy: 0.7966666666666666\n",
      "iteration no 657: Loss: 0.520228354467475, accuracy: 0.7966666666666666\n",
      "iteration no 658: Loss: 0.514973070563042, accuracy: 0.7966666666666666\n",
      "iteration no 659: Loss: 0.5189686547172544, accuracy: 0.8\n",
      "iteration no 660: Loss: 0.5138521204520528, accuracy: 0.8\n",
      "iteration no 661: Loss: 0.5176248412054347, accuracy: 0.8\n",
      "iteration no 662: Loss: 0.5126212875418502, accuracy: 0.8\n",
      "iteration no 663: Loss: 0.5164429832677884, accuracy: 0.8\n",
      "iteration no 664: Loss: 0.5107730276755442, accuracy: 0.8\n",
      "iteration no 665: Loss: 0.514468428706661, accuracy: 0.8033333333333333\n",
      "iteration no 666: Loss: 0.5093743681745763, accuracy: 0.8\n",
      "iteration no 667: Loss: 0.5130561308905098, accuracy: 0.8033333333333333\n",
      "iteration no 668: Loss: 0.5078726622888446, accuracy: 0.8\n",
      "iteration no 669: Loss: 0.5115437191097417, accuracy: 0.8033333333333333\n",
      "iteration no 670: Loss: 0.5065372445509432, accuracy: 0.8\n",
      "iteration no 671: Loss: 0.5102429616140554, accuracy: 0.8033333333333333\n",
      "iteration no 672: Loss: 0.5053117739217278, accuracy: 0.8\n",
      "iteration no 673: Loss: 0.50900706853042, accuracy: 0.8033333333333333\n",
      "iteration no 674: Loss: 0.5041494218813592, accuracy: 0.8\n",
      "iteration no 675: Loss: 0.507994120871411, accuracy: 0.8033333333333333\n",
      "iteration no 676: Loss: 0.5031121592388369, accuracy: 0.8\n",
      "iteration no 677: Loss: 0.5070024783968273, accuracy: 0.8066666666666666\n",
      "iteration no 678: Loss: 0.5021390323721336, accuracy: 0.8033333333333333\n",
      "iteration no 679: Loss: 0.5059893424210838, accuracy: 0.8066666666666666\n",
      "iteration no 680: Loss: 0.5010586946105436, accuracy: 0.8033333333333333\n",
      "iteration no 681: Loss: 0.5050656130914876, accuracy: 0.8066666666666666\n",
      "iteration no 682: Loss: 0.4999310461791033, accuracy: 0.8033333333333333\n",
      "iteration no 683: Loss: 0.5038073033885638, accuracy: 0.8066666666666666\n",
      "iteration no 684: Loss: 0.49878818278411297, accuracy: 0.8033333333333333\n",
      "iteration no 685: Loss: 0.5026931561136103, accuracy: 0.8066666666666666\n",
      "iteration no 686: Loss: 0.49770844606244535, accuracy: 0.8033333333333333\n",
      "iteration no 687: Loss: 0.5015146339733826, accuracy: 0.8066666666666666\n",
      "iteration no 688: Loss: 0.4963499976202812, accuracy: 0.8033333333333333\n",
      "iteration no 689: Loss: 0.5000508336941281, accuracy: 0.8066666666666666\n",
      "iteration no 690: Loss: 0.494896527846854, accuracy: 0.8033333333333333\n",
      "iteration no 691: Loss: 0.498643727316033, accuracy: 0.8066666666666666\n",
      "iteration no 692: Loss: 0.49358090534178045, accuracy: 0.8033333333333333\n",
      "iteration no 693: Loss: 0.49715212526339814, accuracy: 0.8066666666666666\n",
      "iteration no 694: Loss: 0.4921578791936863, accuracy: 0.8033333333333333\n",
      "iteration no 695: Loss: 0.4957926324587877, accuracy: 0.8066666666666666\n",
      "iteration no 696: Loss: 0.4908448798599498, accuracy: 0.8033333333333333\n",
      "iteration no 697: Loss: 0.4943273640130529, accuracy: 0.8166666666666667\n",
      "iteration no 698: Loss: 0.48941358272825713, accuracy: 0.8033333333333333\n",
      "iteration no 699: Loss: 0.4930788944316273, accuracy: 0.8166666666666667\n",
      "iteration no 700: Loss: 0.48840447922479646, accuracy: 0.8033333333333333\n",
      "iteration no 701: Loss: 0.4919763351274483, accuracy: 0.8166666666666667\n",
      "iteration no 702: Loss: 0.48735181872442346, accuracy: 0.8033333333333333\n",
      "iteration no 703: Loss: 0.490853595941933, accuracy: 0.82\n",
      "iteration no 704: Loss: 0.48611334869051326, accuracy: 0.8033333333333333\n",
      "iteration no 705: Loss: 0.48955589392174853, accuracy: 0.82\n",
      "iteration no 706: Loss: 0.4850100795694951, accuracy: 0.8033333333333333\n",
      "iteration no 707: Loss: 0.4885766141898761, accuracy: 0.8233333333333334\n",
      "iteration no 708: Loss: 0.4841504658375424, accuracy: 0.8066666666666666\n",
      "iteration no 709: Loss: 0.4878144206288117, accuracy: 0.8233333333333334\n",
      "iteration no 710: Loss: 0.4833040821827482, accuracy: 0.8066666666666666\n",
      "iteration no 711: Loss: 0.48696373519590913, accuracy: 0.8266666666666667\n",
      "iteration no 712: Loss: 0.48231510393010846, accuracy: 0.8066666666666666\n",
      "iteration no 713: Loss: 0.4859274234413773, accuracy: 0.8266666666666667\n",
      "iteration no 714: Loss: 0.48127599019406675, accuracy: 0.8066666666666666\n",
      "iteration no 715: Loss: 0.4848160151806762, accuracy: 0.8266666666666667\n",
      "iteration no 716: Loss: 0.480260765897923, accuracy: 0.8066666666666666\n",
      "iteration no 717: Loss: 0.48377388255070947, accuracy: 0.83\n",
      "iteration no 718: Loss: 0.47915083251430507, accuracy: 0.8066666666666666\n",
      "iteration no 719: Loss: 0.48256853301616726, accuracy: 0.83\n",
      "iteration no 720: Loss: 0.4779312558527505, accuracy: 0.8066666666666666\n",
      "iteration no 721: Loss: 0.48134497432838563, accuracy: 0.83\n",
      "iteration no 722: Loss: 0.47680566205673036, accuracy: 0.81\n",
      "iteration no 723: Loss: 0.4802230251981293, accuracy: 0.83\n",
      "iteration no 724: Loss: 0.4758339651781134, accuracy: 0.8133333333333334\n",
      "iteration no 725: Loss: 0.47913137086206825, accuracy: 0.83\n",
      "iteration no 726: Loss: 0.4747386167327418, accuracy: 0.8133333333333334\n",
      "iteration no 727: Loss: 0.47808840834120914, accuracy: 0.83\n",
      "iteration no 728: Loss: 0.4736568309334455, accuracy: 0.8166666666666667\n",
      "iteration no 729: Loss: 0.47686838754038, accuracy: 0.83\n",
      "iteration no 730: Loss: 0.47262264244720337, accuracy: 0.82\n",
      "iteration no 731: Loss: 0.47592666684245205, accuracy: 0.83\n",
      "iteration no 732: Loss: 0.4717536790370118, accuracy: 0.82\n",
      "iteration no 733: Loss: 0.4750258227479154, accuracy: 0.83\n",
      "iteration no 734: Loss: 0.47069054010608397, accuracy: 0.8233333333333334\n",
      "iteration no 735: Loss: 0.4740343531533061, accuracy: 0.83\n",
      "iteration no 736: Loss: 0.469881817935237, accuracy: 0.8233333333333334\n",
      "iteration no 737: Loss: 0.4731518112674462, accuracy: 0.83\n",
      "iteration no 738: Loss: 0.46885775298143295, accuracy: 0.8233333333333334\n",
      "iteration no 739: Loss: 0.47216347289715577, accuracy: 0.83\n",
      "iteration no 740: Loss: 0.46789613690682447, accuracy: 0.83\n",
      "iteration no 741: Loss: 0.4710571875263183, accuracy: 0.83\n",
      "iteration no 742: Loss: 0.46666025107525816, accuracy: 0.83\n",
      "iteration no 743: Loss: 0.4698299074093767, accuracy: 0.83\n",
      "iteration no 744: Loss: 0.46545335939660915, accuracy: 0.83\n",
      "iteration no 745: Loss: 0.46846886067666677, accuracy: 0.8333333333333334\n",
      "iteration no 746: Loss: 0.46417922348750584, accuracy: 0.83\n",
      "iteration no 747: Loss: 0.4671953735074657, accuracy: 0.8333333333333334\n",
      "iteration no 748: Loss: 0.4630607214030499, accuracy: 0.8333333333333334\n",
      "iteration no 749: Loss: 0.4659782857938762, accuracy: 0.8333333333333334\n",
      "iteration no 750: Loss: 0.46195464229376154, accuracy: 0.8366666666666667\n",
      "iteration no 751: Loss: 0.4649356909429681, accuracy: 0.8333333333333334\n",
      "iteration no 752: Loss: 0.46095738331722813, accuracy: 0.84\n",
      "iteration no 753: Loss: 0.4638522371936998, accuracy: 0.8333333333333334\n",
      "iteration no 754: Loss: 0.4598619509569668, accuracy: 0.84\n",
      "iteration no 755: Loss: 0.46269116424750945, accuracy: 0.8333333333333334\n",
      "iteration no 756: Loss: 0.45880116280783867, accuracy: 0.84\n",
      "iteration no 757: Loss: 0.4614363427873126, accuracy: 0.8333333333333334\n",
      "iteration no 758: Loss: 0.45763584323852463, accuracy: 0.84\n",
      "iteration no 759: Loss: 0.46043562657444626, accuracy: 0.8333333333333334\n",
      "iteration no 760: Loss: 0.45671172443033464, accuracy: 0.84\n",
      "iteration no 761: Loss: 0.4593521021042549, accuracy: 0.8333333333333334\n",
      "iteration no 762: Loss: 0.45572293482798887, accuracy: 0.84\n",
      "iteration no 763: Loss: 0.45834111145415335, accuracy: 0.8333333333333334\n",
      "iteration no 764: Loss: 0.4547107375823537, accuracy: 0.84\n",
      "iteration no 765: Loss: 0.4575450443447257, accuracy: 0.8333333333333334\n",
      "iteration no 766: Loss: 0.45402545195128197, accuracy: 0.84\n",
      "iteration no 767: Loss: 0.45666259117168656, accuracy: 0.8333333333333334\n",
      "iteration no 768: Loss: 0.45308251788950016, accuracy: 0.84\n",
      "iteration no 769: Loss: 0.45573020427219957, accuracy: 0.8333333333333334\n",
      "iteration no 770: Loss: 0.4521503314011649, accuracy: 0.84\n",
      "iteration no 771: Loss: 0.4548013524457585, accuracy: 0.8333333333333334\n",
      "iteration no 772: Loss: 0.4511926716520817, accuracy: 0.84\n",
      "iteration no 773: Loss: 0.4538144996198717, accuracy: 0.8333333333333334\n",
      "iteration no 774: Loss: 0.45030391424779254, accuracy: 0.84\n",
      "iteration no 775: Loss: 0.45313296614075205, accuracy: 0.8333333333333334\n",
      "iteration no 776: Loss: 0.44960873502333565, accuracy: 0.84\n",
      "iteration no 777: Loss: 0.4521624264387505, accuracy: 0.8366666666666667\n",
      "iteration no 778: Loss: 0.44872955182988056, accuracy: 0.84\n",
      "iteration no 779: Loss: 0.45129322043675635, accuracy: 0.8366666666666667\n",
      "iteration no 780: Loss: 0.44795256601760686, accuracy: 0.84\n",
      "iteration no 781: Loss: 0.45053094018506606, accuracy: 0.8366666666666667\n",
      "iteration no 782: Loss: 0.44703014165252825, accuracy: 0.84\n",
      "iteration no 783: Loss: 0.4495946446692674, accuracy: 0.8366666666666667\n",
      "iteration no 784: Loss: 0.4462849327387742, accuracy: 0.84\n",
      "iteration no 785: Loss: 0.44910048839380784, accuracy: 0.8366666666666667\n",
      "iteration no 786: Loss: 0.44563277523678546, accuracy: 0.84\n",
      "iteration no 787: Loss: 0.44817876718553173, accuracy: 0.8366666666666667\n",
      "iteration no 788: Loss: 0.44476738876441074, accuracy: 0.84\n",
      "iteration no 789: Loss: 0.4473472431561041, accuracy: 0.8366666666666667\n",
      "iteration no 790: Loss: 0.44396526974472816, accuracy: 0.84\n",
      "iteration no 791: Loss: 0.44645338675317064, accuracy: 0.8366666666666667\n",
      "iteration no 792: Loss: 0.4430632723661024, accuracy: 0.84\n",
      "iteration no 793: Loss: 0.4454722357667876, accuracy: 0.8366666666666667\n",
      "iteration no 794: Loss: 0.4419901844435663, accuracy: 0.84\n",
      "iteration no 795: Loss: 0.44420558726356296, accuracy: 0.8366666666666667\n",
      "iteration no 796: Loss: 0.4408570940940152, accuracy: 0.84\n",
      "iteration no 797: Loss: 0.44297028033940555, accuracy: 0.8366666666666667\n",
      "iteration no 798: Loss: 0.4399472076965435, accuracy: 0.8433333333333334\n",
      "iteration no 799: Loss: 0.4421991869741358, accuracy: 0.8366666666666667\n",
      "iteration no 800: Loss: 0.43918839098140083, accuracy: 0.8433333333333334\n",
      "iteration no 801: Loss: 0.4414541029873254, accuracy: 0.8366666666666667\n",
      "iteration no 802: Loss: 0.43849990926734617, accuracy: 0.84\n",
      "iteration no 803: Loss: 0.44077701170916017, accuracy: 0.8366666666666667\n",
      "iteration no 804: Loss: 0.43797159940302827, accuracy: 0.8433333333333334\n",
      "iteration no 805: Loss: 0.4402563157922724, accuracy: 0.8366666666666667\n",
      "iteration no 806: Loss: 0.437332391423946, accuracy: 0.8433333333333334\n",
      "iteration no 807: Loss: 0.4397752483086321, accuracy: 0.8366666666666667\n",
      "iteration no 808: Loss: 0.4367579152215089, accuracy: 0.8433333333333334\n",
      "iteration no 809: Loss: 0.43907782997300887, accuracy: 0.8366666666666667\n",
      "iteration no 810: Loss: 0.43604287331313457, accuracy: 0.8433333333333334\n",
      "iteration no 811: Loss: 0.4383851792485796, accuracy: 0.8366666666666667\n",
      "iteration no 812: Loss: 0.4353992011638576, accuracy: 0.8433333333333334\n",
      "iteration no 813: Loss: 0.43775012806700253, accuracy: 0.8366666666666667\n",
      "iteration no 814: Loss: 0.4347595808748956, accuracy: 0.8433333333333334\n",
      "iteration no 815: Loss: 0.4370495377447945, accuracy: 0.8366666666666667\n",
      "iteration no 816: Loss: 0.4339690969129183, accuracy: 0.84\n",
      "iteration no 817: Loss: 0.4362318518152354, accuracy: 0.84\n",
      "iteration no 818: Loss: 0.4332474793820779, accuracy: 0.84\n",
      "iteration no 819: Loss: 0.4353982946393807, accuracy: 0.84\n",
      "iteration no 820: Loss: 0.4324995664904548, accuracy: 0.84\n",
      "iteration no 821: Loss: 0.4346348800528582, accuracy: 0.84\n",
      "iteration no 822: Loss: 0.43166577257802663, accuracy: 0.84\n",
      "iteration no 823: Loss: 0.43364378811710824, accuracy: 0.84\n",
      "iteration no 824: Loss: 0.4307576348118672, accuracy: 0.84\n",
      "iteration no 825: Loss: 0.4327794730337443, accuracy: 0.84\n",
      "iteration no 826: Loss: 0.42994134268927586, accuracy: 0.84\n",
      "iteration no 827: Loss: 0.43195415724927094, accuracy: 0.84\n",
      "iteration no 828: Loss: 0.42923915840794824, accuracy: 0.8433333333333334\n",
      "iteration no 829: Loss: 0.43120774215228, accuracy: 0.84\n",
      "iteration no 830: Loss: 0.4285074715559188, accuracy: 0.8433333333333334\n",
      "iteration no 831: Loss: 0.43027838035870053, accuracy: 0.84\n",
      "iteration no 832: Loss: 0.42760460484478535, accuracy: 0.8433333333333334\n",
      "iteration no 833: Loss: 0.4294102201363656, accuracy: 0.84\n",
      "iteration no 834: Loss: 0.4267792416201398, accuracy: 0.8433333333333334\n",
      "iteration no 835: Loss: 0.42855915241244036, accuracy: 0.84\n",
      "iteration no 836: Loss: 0.4261021065921393, accuracy: 0.8433333333333334\n",
      "iteration no 837: Loss: 0.427995271533401, accuracy: 0.84\n",
      "iteration no 838: Loss: 0.42546220328919615, accuracy: 0.8433333333333334\n",
      "iteration no 839: Loss: 0.42712109278590066, accuracy: 0.84\n",
      "iteration no 840: Loss: 0.42465854397979175, accuracy: 0.8433333333333334\n",
      "iteration no 841: Loss: 0.4264236256201998, accuracy: 0.84\n",
      "iteration no 842: Loss: 0.4239799872329656, accuracy: 0.8433333333333334\n",
      "iteration no 843: Loss: 0.42571427182987287, accuracy: 0.84\n",
      "iteration no 844: Loss: 0.42335729249203996, accuracy: 0.8433333333333334\n",
      "iteration no 845: Loss: 0.4250226551235288, accuracy: 0.84\n",
      "iteration no 846: Loss: 0.4226968564926944, accuracy: 0.8433333333333334\n",
      "iteration no 847: Loss: 0.42439804527148706, accuracy: 0.84\n",
      "iteration no 848: Loss: 0.42201280107273426, accuracy: 0.8433333333333334\n",
      "iteration no 849: Loss: 0.4236532364792444, accuracy: 0.84\n",
      "iteration no 850: Loss: 0.42140679458501346, accuracy: 0.8466666666666667\n",
      "iteration no 851: Loss: 0.4230417912116521, accuracy: 0.84\n",
      "iteration no 852: Loss: 0.42074719731752946, accuracy: 0.8466666666666667\n",
      "iteration no 853: Loss: 0.42234390446025477, accuracy: 0.84\n",
      "iteration no 854: Loss: 0.42005180466792835, accuracy: 0.8433333333333334\n",
      "iteration no 855: Loss: 0.4216265868452048, accuracy: 0.84\n",
      "iteration no 856: Loss: 0.41944489099775695, accuracy: 0.8433333333333334\n",
      "iteration no 857: Loss: 0.42101469750676346, accuracy: 0.84\n",
      "iteration no 858: Loss: 0.4188442674636108, accuracy: 0.8433333333333334\n",
      "iteration no 859: Loss: 0.42038530358855797, accuracy: 0.84\n",
      "iteration no 860: Loss: 0.418094816733966, accuracy: 0.8433333333333334\n",
      "iteration no 861: Loss: 0.41944710409377145, accuracy: 0.8366666666666667\n",
      "iteration no 862: Loss: 0.4174158935593261, accuracy: 0.8433333333333334\n",
      "iteration no 863: Loss: 0.41887730501071735, accuracy: 0.8366666666666667\n",
      "iteration no 864: Loss: 0.41679094944826717, accuracy: 0.8466666666666667\n",
      "iteration no 865: Loss: 0.41821310107647147, accuracy: 0.8366666666666667\n",
      "iteration no 866: Loss: 0.4160234863137936, accuracy: 0.8466666666666667\n",
      "iteration no 867: Loss: 0.4171525619675477, accuracy: 0.8366666666666667\n",
      "iteration no 868: Loss: 0.41522850633041286, accuracy: 0.8466666666666667\n",
      "iteration no 869: Loss: 0.41647270403755554, accuracy: 0.8366666666666667\n",
      "iteration no 870: Loss: 0.41443881230131396, accuracy: 0.8466666666666667\n",
      "iteration no 871: Loss: 0.4154557697822861, accuracy: 0.8366666666666667\n",
      "iteration no 872: Loss: 0.41366614238784105, accuracy: 0.8466666666666667\n",
      "iteration no 873: Loss: 0.4148611140135346, accuracy: 0.8366666666666667\n",
      "iteration no 874: Loss: 0.4130268229332118, accuracy: 0.85\n",
      "iteration no 875: Loss: 0.41405634705592353, accuracy: 0.8433333333333334\n",
      "iteration no 876: Loss: 0.41226417277461846, accuracy: 0.85\n",
      "iteration no 877: Loss: 0.4131177463581091, accuracy: 0.8433333333333334\n",
      "iteration no 878: Loss: 0.41142971284143026, accuracy: 0.85\n",
      "iteration no 879: Loss: 0.41244594965800413, accuracy: 0.8433333333333334\n",
      "iteration no 880: Loss: 0.41083963476145585, accuracy: 0.85\n",
      "iteration no 881: Loss: 0.4116363512124896, accuracy: 0.8433333333333334\n",
      "iteration no 882: Loss: 0.4100553687690696, accuracy: 0.8466666666666667\n",
      "iteration no 883: Loss: 0.4109210162858061, accuracy: 0.8433333333333334\n",
      "iteration no 884: Loss: 0.40943683389476965, accuracy: 0.8466666666666667\n",
      "iteration no 885: Loss: 0.41027416382162174, accuracy: 0.8466666666666667\n",
      "iteration no 886: Loss: 0.4088317751510506, accuracy: 0.8466666666666667\n",
      "iteration no 887: Loss: 0.40967515291972934, accuracy: 0.8466666666666667\n",
      "iteration no 888: Loss: 0.4082717296463408, accuracy: 0.8466666666666667\n",
      "iteration no 889: Loss: 0.4089306555544866, accuracy: 0.8466666666666667\n",
      "iteration no 890: Loss: 0.4075770160431097, accuracy: 0.8466666666666667\n",
      "iteration no 891: Loss: 0.4083790175575479, accuracy: 0.8466666666666667\n",
      "iteration no 892: Loss: 0.407091894780227, accuracy: 0.8466666666666667\n",
      "iteration no 893: Loss: 0.40783795524742084, accuracy: 0.8466666666666667\n",
      "iteration no 894: Loss: 0.4065972006619148, accuracy: 0.8466666666666667\n",
      "iteration no 895: Loss: 0.4072316143963557, accuracy: 0.85\n",
      "iteration no 896: Loss: 0.40598834873854944, accuracy: 0.8466666666666667\n",
      "iteration no 897: Loss: 0.40671634198183176, accuracy: 0.85\n",
      "iteration no 898: Loss: 0.40554810643441014, accuracy: 0.8466666666666667\n",
      "iteration no 899: Loss: 0.40628824272674474, accuracy: 0.85\n",
      "iteration no 900: Loss: 0.4051255757317383, accuracy: 0.8466666666666667\n",
      "iteration no 901: Loss: 0.4059072841015154, accuracy: 0.85\n",
      "iteration no 902: Loss: 0.40475130935375325, accuracy: 0.8466666666666667\n",
      "iteration no 903: Loss: 0.40543996671664884, accuracy: 0.85\n",
      "iteration no 904: Loss: 0.4043229218519516, accuracy: 0.8466666666666667\n",
      "iteration no 905: Loss: 0.40511458244696885, accuracy: 0.85\n",
      "iteration no 906: Loss: 0.4039835963213224, accuracy: 0.8466666666666667\n",
      "iteration no 907: Loss: 0.40472284650217794, accuracy: 0.85\n",
      "iteration no 908: Loss: 0.40358472924972666, accuracy: 0.8466666666666667\n",
      "iteration no 909: Loss: 0.40430822329597865, accuracy: 0.85\n",
      "iteration no 910: Loss: 0.4031653230277498, accuracy: 0.8466666666666667\n",
      "iteration no 911: Loss: 0.403995742089899, accuracy: 0.85\n",
      "iteration no 912: Loss: 0.4028548871706011, accuracy: 0.8466666666666667\n",
      "iteration no 913: Loss: 0.4036011762891193, accuracy: 0.85\n",
      "iteration no 914: Loss: 0.4024705940276546, accuracy: 0.8466666666666667\n",
      "iteration no 915: Loss: 0.4032003233837948, accuracy: 0.85\n",
      "iteration no 916: Loss: 0.40208396907658595, accuracy: 0.8466666666666667\n",
      "iteration no 917: Loss: 0.40287254954282914, accuracy: 0.85\n",
      "iteration no 918: Loss: 0.40175607966934135, accuracy: 0.8466666666666667\n",
      "iteration no 919: Loss: 0.4024786195306209, accuracy: 0.85\n",
      "iteration no 920: Loss: 0.4013536006118937, accuracy: 0.8466666666666667\n",
      "iteration no 921: Loss: 0.4021151163051089, accuracy: 0.85\n",
      "iteration no 922: Loss: 0.4010108541634277, accuracy: 0.85\n",
      "iteration no 923: Loss: 0.4017030363721382, accuracy: 0.85\n",
      "iteration no 924: Loss: 0.4005754062999917, accuracy: 0.85\n",
      "iteration no 925: Loss: 0.4013602894952437, accuracy: 0.8533333333333334\n",
      "iteration no 926: Loss: 0.4002437937176784, accuracy: 0.85\n",
      "iteration no 927: Loss: 0.401000121721317, accuracy: 0.8533333333333334\n",
      "iteration no 928: Loss: 0.39985824550545646, accuracy: 0.85\n",
      "iteration no 929: Loss: 0.4005303716385667, accuracy: 0.8533333333333334\n",
      "iteration no 930: Loss: 0.399419978593435, accuracy: 0.85\n",
      "iteration no 931: Loss: 0.40031035643109236, accuracy: 0.8533333333333334\n",
      "iteration no 932: Loss: 0.3991731389670399, accuracy: 0.85\n",
      "iteration no 933: Loss: 0.3998320065378401, accuracy: 0.8533333333333334\n",
      "iteration no 934: Loss: 0.3986966557645949, accuracy: 0.85\n",
      "iteration no 935: Loss: 0.3993738976403295, accuracy: 0.8533333333333334\n",
      "iteration no 936: Loss: 0.39822927209324466, accuracy: 0.85\n",
      "iteration no 937: Loss: 0.3988872368478732, accuracy: 0.8533333333333334\n",
      "iteration no 938: Loss: 0.39780016319235983, accuracy: 0.85\n",
      "iteration no 939: Loss: 0.3986081416197118, accuracy: 0.8533333333333334\n",
      "iteration no 940: Loss: 0.39757771640182726, accuracy: 0.85\n",
      "iteration no 941: Loss: 0.3982691303048154, accuracy: 0.8533333333333334\n",
      "iteration no 942: Loss: 0.39715552313753394, accuracy: 0.85\n",
      "iteration no 943: Loss: 0.39789231938232783, accuracy: 0.8533333333333334\n",
      "iteration no 944: Loss: 0.3968090966531799, accuracy: 0.85\n",
      "iteration no 945: Loss: 0.39760531388743, accuracy: 0.8533333333333334\n",
      "iteration no 946: Loss: 0.39649854558135794, accuracy: 0.85\n",
      "iteration no 947: Loss: 0.3972202529485529, accuracy: 0.8533333333333334\n",
      "iteration no 948: Loss: 0.3961391343218116, accuracy: 0.85\n",
      "iteration no 949: Loss: 0.3968610638407149, accuracy: 0.8533333333333334\n",
      "iteration no 950: Loss: 0.3957832366004412, accuracy: 0.85\n",
      "iteration no 951: Loss: 0.39648167009147034, accuracy: 0.8533333333333334\n",
      "iteration no 952: Loss: 0.39539818976280316, accuracy: 0.85\n",
      "iteration no 953: Loss: 0.39620291429405663, accuracy: 0.8533333333333334\n",
      "iteration no 954: Loss: 0.3951105332697825, accuracy: 0.85\n",
      "iteration no 955: Loss: 0.39582892981627293, accuracy: 0.8533333333333334\n",
      "iteration no 956: Loss: 0.39478625068766926, accuracy: 0.85\n",
      "iteration no 957: Loss: 0.3955456244648191, accuracy: 0.8533333333333334\n",
      "iteration no 958: Loss: 0.3944440948716286, accuracy: 0.85\n",
      "iteration no 959: Loss: 0.39520330748735094, accuracy: 0.8533333333333334\n",
      "iteration no 960: Loss: 0.3941430010177206, accuracy: 0.85\n",
      "iteration no 961: Loss: 0.3949667312355264, accuracy: 0.8533333333333334\n",
      "iteration no 962: Loss: 0.39386279421291315, accuracy: 0.85\n",
      "iteration no 963: Loss: 0.3945843890879431, accuracy: 0.8533333333333334\n",
      "iteration no 964: Loss: 0.39344014936721855, accuracy: 0.85\n",
      "iteration no 965: Loss: 0.3941770974811129, accuracy: 0.8533333333333334\n",
      "iteration no 966: Loss: 0.39303646436643364, accuracy: 0.85\n",
      "iteration no 967: Loss: 0.39383700485967577, accuracy: 0.8533333333333334\n",
      "iteration no 968: Loss: 0.3926914296441525, accuracy: 0.85\n",
      "iteration no 969: Loss: 0.3933855427210747, accuracy: 0.8533333333333334\n",
      "iteration no 970: Loss: 0.392244832745715, accuracy: 0.85\n",
      "iteration no 971: Loss: 0.392971366226148, accuracy: 0.8533333333333334\n",
      "iteration no 972: Loss: 0.3918508662310012, accuracy: 0.85\n",
      "iteration no 973: Loss: 0.39249644893184094, accuracy: 0.8533333333333334\n",
      "iteration no 974: Loss: 0.39139683457318486, accuracy: 0.85\n",
      "iteration no 975: Loss: 0.39200645192528827, accuracy: 0.8533333333333334\n",
      "iteration no 976: Loss: 0.3909278871624342, accuracy: 0.85\n",
      "iteration no 977: Loss: 0.3915805637948879, accuracy: 0.8533333333333334\n",
      "iteration no 978: Loss: 0.3904968498075842, accuracy: 0.85\n",
      "iteration no 979: Loss: 0.3910968685223368, accuracy: 0.8533333333333334\n",
      "iteration no 980: Loss: 0.39005152917262387, accuracy: 0.85\n",
      "iteration no 981: Loss: 0.39060634037813013, accuracy: 0.8533333333333334\n",
      "iteration no 982: Loss: 0.38955995229140206, accuracy: 0.85\n",
      "iteration no 983: Loss: 0.3900753069529851, accuracy: 0.8533333333333334\n",
      "iteration no 984: Loss: 0.38907840979235087, accuracy: 0.85\n",
      "iteration no 985: Loss: 0.3895827067536667, accuracy: 0.8533333333333334\n",
      "iteration no 986: Loss: 0.3885991422812865, accuracy: 0.8533333333333334\n",
      "iteration no 987: Loss: 0.3891609324075733, accuracy: 0.8533333333333334\n",
      "iteration no 988: Loss: 0.38818417851246784, accuracy: 0.8533333333333334\n",
      "iteration no 989: Loss: 0.3887238986850084, accuracy: 0.8533333333333334\n",
      "iteration no 990: Loss: 0.3877738228743114, accuracy: 0.8533333333333334\n",
      "iteration no 991: Loss: 0.3882527290863796, accuracy: 0.8566666666666667\n",
      "iteration no 992: Loss: 0.387318821315283, accuracy: 0.86\n",
      "iteration no 993: Loss: 0.3877510191851009, accuracy: 0.8566666666666667\n",
      "iteration no 994: Loss: 0.3868123244771775, accuracy: 0.86\n",
      "iteration no 995: Loss: 0.38720849876887153, accuracy: 0.8566666666666667\n",
      "iteration no 996: Loss: 0.3863159634332843, accuracy: 0.86\n",
      "iteration no 997: Loss: 0.38673627784448067, accuracy: 0.8566666666666667\n",
      "iteration no 998: Loss: 0.3858552190476171, accuracy: 0.86\n",
      "iteration no 999: Loss: 0.38640980097346345, accuracy: 0.8566666666666667\n",
      "iteration no 1000: Loss: 0.3855125338214818, accuracy: 0.86\n",
      "iteration no 1001: Loss: 0.3859489491528058, accuracy: 0.8566666666666667\n",
      "iteration no 1002: Loss: 0.3850897523677454, accuracy: 0.86\n",
      "iteration no 1003: Loss: 0.3855268778341446, accuracy: 0.8566666666666667\n",
      "iteration no 1004: Loss: 0.3846707053906805, accuracy: 0.86\n",
      "iteration no 1005: Loss: 0.38511963624256135, accuracy: 0.8566666666666667\n",
      "iteration no 1006: Loss: 0.38428211135315804, accuracy: 0.86\n",
      "iteration no 1007: Loss: 0.38471995583533114, accuracy: 0.8566666666666667\n",
      "iteration no 1008: Loss: 0.38387969761365703, accuracy: 0.86\n",
      "iteration no 1009: Loss: 0.38432065280056527, accuracy: 0.8566666666666667\n",
      "iteration no 1010: Loss: 0.3834987655970682, accuracy: 0.86\n",
      "iteration no 1011: Loss: 0.3840840905500769, accuracy: 0.8566666666666667\n",
      "iteration no 1012: Loss: 0.38324403682794433, accuracy: 0.86\n",
      "iteration no 1013: Loss: 0.38374596447819603, accuracy: 0.8566666666666667\n",
      "iteration no 1014: Loss: 0.3828967185933495, accuracy: 0.8633333333333333\n",
      "iteration no 1015: Loss: 0.38337789167671327, accuracy: 0.8566666666666667\n",
      "iteration no 1016: Loss: 0.38252902086151247, accuracy: 0.8633333333333333\n",
      "iteration no 1017: Loss: 0.3830137827791925, accuracy: 0.8566666666666667\n",
      "iteration no 1018: Loss: 0.38215822818448086, accuracy: 0.8633333333333333\n",
      "iteration no 1019: Loss: 0.3825749417362849, accuracy: 0.86\n",
      "iteration no 1020: Loss: 0.3817106749575417, accuracy: 0.8633333333333333\n",
      "iteration no 1021: Loss: 0.38217014973584584, accuracy: 0.86\n",
      "iteration no 1022: Loss: 0.3813589111099434, accuracy: 0.8633333333333333\n",
      "iteration no 1023: Loss: 0.3818885273193345, accuracy: 0.86\n",
      "iteration no 1024: Loss: 0.38109644434167844, accuracy: 0.8633333333333333\n",
      "iteration no 1025: Loss: 0.38156965931914333, accuracy: 0.86\n",
      "iteration no 1026: Loss: 0.380759514743469, accuracy: 0.8633333333333333\n",
      "iteration no 1027: Loss: 0.38124835024068104, accuracy: 0.86\n",
      "iteration no 1028: Loss: 0.38045475773901677, accuracy: 0.8633333333333333\n",
      "iteration no 1029: Loss: 0.380976111421926, accuracy: 0.86\n",
      "iteration no 1030: Loss: 0.3801432016439737, accuracy: 0.8633333333333333\n",
      "iteration no 1031: Loss: 0.38062699567175207, accuracy: 0.86\n",
      "iteration no 1032: Loss: 0.3798179912782623, accuracy: 0.8633333333333333\n",
      "iteration no 1033: Loss: 0.38042606709538995, accuracy: 0.86\n",
      "iteration no 1034: Loss: 0.37957801886072406, accuracy: 0.86\n",
      "iteration no 1035: Loss: 0.3801922800715001, accuracy: 0.86\n",
      "iteration no 1036: Loss: 0.37929898913106086, accuracy: 0.86\n",
      "iteration no 1037: Loss: 0.3798431715575491, accuracy: 0.8633333333333333\n",
      "iteration no 1038: Loss: 0.378954391123919, accuracy: 0.86\n",
      "iteration no 1039: Loss: 0.37948740998524655, accuracy: 0.8633333333333333\n",
      "iteration no 1040: Loss: 0.3786077054384721, accuracy: 0.86\n",
      "iteration no 1041: Loss: 0.37915673478849127, accuracy: 0.8633333333333333\n",
      "iteration no 1042: Loss: 0.3782710355366003, accuracy: 0.86\n",
      "iteration no 1043: Loss: 0.3788578337573025, accuracy: 0.8633333333333333\n",
      "iteration no 1044: Loss: 0.3779318959121512, accuracy: 0.8633333333333333\n",
      "iteration no 1045: Loss: 0.37852724794711085, accuracy: 0.8633333333333333\n",
      "iteration no 1046: Loss: 0.37758368286714106, accuracy: 0.8633333333333333\n",
      "iteration no 1047: Loss: 0.3781560793643441, accuracy: 0.8633333333333333\n",
      "iteration no 1048: Loss: 0.3772249732344852, accuracy: 0.8633333333333333\n",
      "iteration no 1049: Loss: 0.37785324869830517, accuracy: 0.8633333333333333\n",
      "iteration no 1050: Loss: 0.37691747812234117, accuracy: 0.8633333333333333\n",
      "iteration no 1051: Loss: 0.3775051513225618, accuracy: 0.8633333333333333\n",
      "iteration no 1052: Loss: 0.37648754629901643, accuracy: 0.8633333333333333\n",
      "iteration no 1053: Loss: 0.37707124266676906, accuracy: 0.8633333333333333\n",
      "iteration no 1054: Loss: 0.3760785214415878, accuracy: 0.8633333333333333\n",
      "iteration no 1055: Loss: 0.3766241707225463, accuracy: 0.8633333333333333\n",
      "iteration no 1056: Loss: 0.3756290860208112, accuracy: 0.8666666666666667\n",
      "iteration no 1057: Loss: 0.3762206410407929, accuracy: 0.8633333333333333\n",
      "iteration no 1058: Loss: 0.37517907589023614, accuracy: 0.8666666666666667\n",
      "iteration no 1059: Loss: 0.37569415736532913, accuracy: 0.8633333333333333\n",
      "iteration no 1060: Loss: 0.37467938516644944, accuracy: 0.8666666666666667\n",
      "iteration no 1061: Loss: 0.3751758248062768, accuracy: 0.8633333333333333\n",
      "iteration no 1062: Loss: 0.3741169016627012, accuracy: 0.8666666666666667\n",
      "iteration no 1063: Loss: 0.3745923136502331, accuracy: 0.8633333333333333\n",
      "iteration no 1064: Loss: 0.373594809365232, accuracy: 0.8666666666666667\n",
      "iteration no 1065: Loss: 0.3740481681308412, accuracy: 0.8633333333333333\n",
      "iteration no 1066: Loss: 0.3729831096360452, accuracy: 0.8666666666666667\n",
      "iteration no 1067: Loss: 0.37340528786746247, accuracy: 0.8666666666666667\n",
      "iteration no 1068: Loss: 0.37240062364403487, accuracy: 0.8666666666666667\n",
      "iteration no 1069: Loss: 0.37290175719235163, accuracy: 0.8666666666666667\n",
      "iteration no 1070: Loss: 0.37188314189457883, accuracy: 0.8666666666666667\n",
      "iteration no 1071: Loss: 0.37231460350475043, accuracy: 0.8666666666666667\n",
      "iteration no 1072: Loss: 0.3713345373707237, accuracy: 0.87\n",
      "iteration no 1073: Loss: 0.37180436404675343, accuracy: 0.8666666666666667\n",
      "iteration no 1074: Loss: 0.37085656258637834, accuracy: 0.87\n",
      "iteration no 1075: Loss: 0.37131444799922697, accuracy: 0.8666666666666667\n",
      "iteration no 1076: Loss: 0.3703539871530639, accuracy: 0.87\n",
      "iteration no 1077: Loss: 0.3707958905113315, accuracy: 0.8633333333333333\n",
      "iteration no 1078: Loss: 0.3698233093326031, accuracy: 0.87\n",
      "iteration no 1079: Loss: 0.3702591297953813, accuracy: 0.8666666666666667\n",
      "iteration no 1080: Loss: 0.36930730554346813, accuracy: 0.8733333333333333\n",
      "iteration no 1081: Loss: 0.36981165843467084, accuracy: 0.8666666666666667\n",
      "iteration no 1082: Loss: 0.3688965850009448, accuracy: 0.8733333333333333\n",
      "iteration no 1083: Loss: 0.3693614072826331, accuracy: 0.8666666666666667\n",
      "iteration no 1084: Loss: 0.3684625214858675, accuracy: 0.8733333333333333\n",
      "iteration no 1085: Loss: 0.3689126000103164, accuracy: 0.8666666666666667\n",
      "iteration no 1086: Loss: 0.3680277924429627, accuracy: 0.8733333333333333\n",
      "iteration no 1087: Loss: 0.3684730981320406, accuracy: 0.8666666666666667\n",
      "iteration no 1088: Loss: 0.36756293412779695, accuracy: 0.8733333333333333\n",
      "iteration no 1089: Loss: 0.368011625525283, accuracy: 0.8666666666666667\n",
      "iteration no 1090: Loss: 0.3671090530741181, accuracy: 0.8733333333333333\n",
      "iteration no 1091: Loss: 0.3675382640701957, accuracy: 0.87\n",
      "iteration no 1092: Loss: 0.3666062041170889, accuracy: 0.8733333333333333\n",
      "iteration no 1093: Loss: 0.36698469487905844, accuracy: 0.87\n",
      "iteration no 1094: Loss: 0.36610425115504175, accuracy: 0.8733333333333333\n",
      "iteration no 1095: Loss: 0.36646702483052784, accuracy: 0.87\n",
      "iteration no 1096: Loss: 0.36555807301960985, accuracy: 0.8733333333333333\n",
      "iteration no 1097: Loss: 0.36589551864611797, accuracy: 0.87\n",
      "iteration no 1098: Loss: 0.36499154423872865, accuracy: 0.8733333333333333\n",
      "iteration no 1099: Loss: 0.3653069712401237, accuracy: 0.87\n",
      "iteration no 1100: Loss: 0.3644399669314697, accuracy: 0.8766666666666667\n",
      "iteration no 1101: Loss: 0.3647334503874927, accuracy: 0.87\n",
      "iteration no 1102: Loss: 0.36389074178674774, accuracy: 0.8766666666666667\n",
      "iteration no 1103: Loss: 0.36417473872967515, accuracy: 0.8733333333333333\n",
      "iteration no 1104: Loss: 0.36335448249437385, accuracy: 0.8766666666666667\n",
      "iteration no 1105: Loss: 0.36362635137027755, accuracy: 0.8766666666666667\n",
      "iteration no 1106: Loss: 0.3628810249605155, accuracy: 0.8766666666666667\n",
      "iteration no 1107: Loss: 0.3631552378009539, accuracy: 0.8733333333333333\n",
      "iteration no 1108: Loss: 0.36240330490034167, accuracy: 0.88\n",
      "iteration no 1109: Loss: 0.36267001635777335, accuracy: 0.8733333333333333\n",
      "iteration no 1110: Loss: 0.3619353947039164, accuracy: 0.88\n",
      "iteration no 1111: Loss: 0.36218269801444986, accuracy: 0.8733333333333333\n",
      "iteration no 1112: Loss: 0.36146471280241055, accuracy: 0.88\n",
      "iteration no 1113: Loss: 0.3616988181059615, accuracy: 0.8733333333333333\n",
      "iteration no 1114: Loss: 0.3609938331355049, accuracy: 0.88\n",
      "iteration no 1115: Loss: 0.36122170923329855, accuracy: 0.8733333333333333\n",
      "iteration no 1116: Loss: 0.36054792366755317, accuracy: 0.88\n",
      "iteration no 1117: Loss: 0.36076674996538033, accuracy: 0.8733333333333333\n",
      "iteration no 1118: Loss: 0.36009367029476774, accuracy: 0.88\n",
      "iteration no 1119: Loss: 0.36030975200496085, accuracy: 0.8766666666666667\n",
      "iteration no 1120: Loss: 0.3596699659418442, accuracy: 0.88\n",
      "iteration no 1121: Loss: 0.35988245213321385, accuracy: 0.88\n",
      "iteration no 1122: Loss: 0.35923350199209014, accuracy: 0.88\n",
      "iteration no 1123: Loss: 0.3594543268541746, accuracy: 0.88\n",
      "iteration no 1124: Loss: 0.3588206186445961, accuracy: 0.88\n",
      "iteration no 1125: Loss: 0.3590002334634197, accuracy: 0.88\n",
      "iteration no 1126: Loss: 0.35834082524987426, accuracy: 0.88\n",
      "iteration no 1127: Loss: 0.3585142836317049, accuracy: 0.88\n",
      "iteration no 1128: Loss: 0.3579009609438547, accuracy: 0.88\n",
      "iteration no 1129: Loss: 0.3580747550613619, accuracy: 0.88\n",
      "iteration no 1130: Loss: 0.3574894677747163, accuracy: 0.88\n",
      "iteration no 1131: Loss: 0.3576767055992859, accuracy: 0.88\n",
      "iteration no 1132: Loss: 0.3571099110820671, accuracy: 0.88\n",
      "iteration no 1133: Loss: 0.3573071501231013, accuracy: 0.88\n",
      "iteration no 1134: Loss: 0.3567204909861511, accuracy: 0.88\n",
      "iteration no 1135: Loss: 0.35692575910348767, accuracy: 0.88\n",
      "iteration no 1136: Loss: 0.35635752351249467, accuracy: 0.88\n",
      "iteration no 1137: Loss: 0.35658733442995266, accuracy: 0.88\n",
      "iteration no 1138: Loss: 0.3560266495975188, accuracy: 0.88\n",
      "iteration no 1139: Loss: 0.35625248570630197, accuracy: 0.88\n",
      "iteration no 1140: Loss: 0.3556748147526975, accuracy: 0.88\n",
      "iteration no 1141: Loss: 0.35592650419709015, accuracy: 0.88\n",
      "iteration no 1142: Loss: 0.3553411590228881, accuracy: 0.88\n",
      "iteration no 1143: Loss: 0.3555981162487898, accuracy: 0.88\n",
      "iteration no 1144: Loss: 0.35500294949066624, accuracy: 0.8833333333333333\n",
      "iteration no 1145: Loss: 0.3552346284583457, accuracy: 0.88\n",
      "iteration no 1146: Loss: 0.35463916718589217, accuracy: 0.8833333333333333\n",
      "iteration no 1147: Loss: 0.35489253818695665, accuracy: 0.88\n",
      "iteration no 1148: Loss: 0.35431312223371475, accuracy: 0.8833333333333333\n",
      "iteration no 1149: Loss: 0.35456091177069377, accuracy: 0.88\n",
      "iteration no 1150: Loss: 0.35397101263916053, accuracy: 0.8833333333333333\n",
      "iteration no 1151: Loss: 0.35423565117069916, accuracy: 0.88\n",
      "iteration no 1152: Loss: 0.35360947588117275, accuracy: 0.8833333333333333\n",
      "iteration no 1153: Loss: 0.35390787314772626, accuracy: 0.88\n",
      "iteration no 1154: Loss: 0.3533030985452459, accuracy: 0.8866666666666667\n",
      "iteration no 1155: Loss: 0.3535800236833616, accuracy: 0.88\n",
      "iteration no 1156: Loss: 0.35296651992228634, accuracy: 0.8866666666666667\n",
      "iteration no 1157: Loss: 0.35323109538591757, accuracy: 0.88\n",
      "iteration no 1158: Loss: 0.35264514994829177, accuracy: 0.89\n",
      "iteration no 1159: Loss: 0.3529694205197033, accuracy: 0.88\n",
      "iteration no 1160: Loss: 0.3523766940352151, accuracy: 0.89\n",
      "iteration no 1161: Loss: 0.35268798038430454, accuracy: 0.88\n",
      "iteration no 1162: Loss: 0.3520724014555486, accuracy: 0.89\n",
      "iteration no 1163: Loss: 0.3524011854638041, accuracy: 0.88\n",
      "iteration no 1164: Loss: 0.35178500670326485, accuracy: 0.8933333333333333\n",
      "iteration no 1165: Loss: 0.3521468035680098, accuracy: 0.8833333333333333\n",
      "iteration no 1166: Loss: 0.35153074067094, accuracy: 0.8933333333333333\n",
      "iteration no 1167: Loss: 0.3518346700379238, accuracy: 0.8833333333333333\n",
      "iteration no 1168: Loss: 0.35120182195334015, accuracy: 0.8933333333333333\n",
      "iteration no 1169: Loss: 0.35156380635813667, accuracy: 0.8833333333333333\n",
      "iteration no 1170: Loss: 0.3508982344590329, accuracy: 0.8966666666666666\n",
      "iteration no 1171: Loss: 0.3511950873731948, accuracy: 0.8833333333333333\n",
      "iteration no 1172: Loss: 0.35049173949206497, accuracy: 0.8966666666666666\n",
      "iteration no 1173: Loss: 0.3508172090837135, accuracy: 0.8833333333333333\n",
      "iteration no 1174: Loss: 0.35008990270388884, accuracy: 0.8966666666666666\n",
      "iteration no 1175: Loss: 0.3503174967232432, accuracy: 0.8833333333333333\n",
      "iteration no 1176: Loss: 0.349588971462123, accuracy: 0.8966666666666666\n",
      "iteration no 1177: Loss: 0.3498657073923512, accuracy: 0.8866666666666667\n",
      "iteration no 1178: Loss: 0.34914898097652786, accuracy: 0.8933333333333333\n",
      "iteration no 1179: Loss: 0.3493450925494654, accuracy: 0.8866666666666667\n",
      "iteration no 1180: Loss: 0.3486433220911588, accuracy: 0.8933333333333333\n",
      "iteration no 1181: Loss: 0.348844121751241, accuracy: 0.8866666666666667\n",
      "iteration no 1182: Loss: 0.3481631047998637, accuracy: 0.8933333333333333\n",
      "iteration no 1183: Loss: 0.348337949889984, accuracy: 0.8866666666666667\n",
      "iteration no 1184: Loss: 0.34766583071910007, accuracy: 0.8933333333333333\n",
      "iteration no 1185: Loss: 0.34784645725787966, accuracy: 0.8866666666666667\n",
      "iteration no 1186: Loss: 0.34718370099380674, accuracy: 0.8933333333333333\n",
      "iteration no 1187: Loss: 0.3473563163796442, accuracy: 0.89\n",
      "iteration no 1188: Loss: 0.3467224191857892, accuracy: 0.8933333333333333\n",
      "iteration no 1189: Loss: 0.346877539696663, accuracy: 0.8933333333333333\n",
      "iteration no 1190: Loss: 0.3462489747822989, accuracy: 0.8933333333333333\n",
      "iteration no 1191: Loss: 0.3464063165356048, accuracy: 0.8933333333333333\n",
      "iteration no 1192: Loss: 0.34580033660190135, accuracy: 0.8966666666666666\n",
      "iteration no 1193: Loss: 0.3459215344781958, accuracy: 0.8933333333333333\n",
      "iteration no 1194: Loss: 0.34532675704510885, accuracy: 0.8966666666666666\n",
      "iteration no 1195: Loss: 0.34546824558271694, accuracy: 0.8933333333333333\n",
      "iteration no 1196: Loss: 0.34489634398631197, accuracy: 0.8966666666666666\n",
      "iteration no 1197: Loss: 0.34503423305187886, accuracy: 0.8933333333333333\n",
      "iteration no 1198: Loss: 0.34447773940723936, accuracy: 0.8966666666666666\n",
      "iteration no 1199: Loss: 0.34457571806611215, accuracy: 0.8933333333333333\n",
      "iteration no 1200: Loss: 0.3440186306545274, accuracy: 0.8966666666666666\n",
      "iteration no 1201: Loss: 0.34412699594742924, accuracy: 0.8933333333333333\n",
      "iteration no 1202: Loss: 0.34360002661423894, accuracy: 0.8966666666666666\n",
      "iteration no 1203: Loss: 0.34371940624231173, accuracy: 0.89\n",
      "iteration no 1204: Loss: 0.3432091978315502, accuracy: 0.8966666666666666\n",
      "iteration no 1205: Loss: 0.34328972417308745, accuracy: 0.89\n",
      "iteration no 1206: Loss: 0.3427799599317542, accuracy: 0.9\n",
      "iteration no 1207: Loss: 0.3428560295937933, accuracy: 0.8933333333333333\n",
      "iteration no 1208: Loss: 0.34236482852541816, accuracy: 0.9\n",
      "iteration no 1209: Loss: 0.3424263740288695, accuracy: 0.8933333333333333\n",
      "iteration no 1210: Loss: 0.3419787288933629, accuracy: 0.9\n",
      "iteration no 1211: Loss: 0.3420516749430343, accuracy: 0.8933333333333333\n",
      "iteration no 1212: Loss: 0.3415876405784528, accuracy: 0.9\n",
      "iteration no 1213: Loss: 0.3416706976394044, accuracy: 0.8933333333333333\n",
      "iteration no 1214: Loss: 0.3412125698678217, accuracy: 0.9\n",
      "iteration no 1215: Loss: 0.34126825399145516, accuracy: 0.8933333333333333\n",
      "iteration no 1216: Loss: 0.3408332761415457, accuracy: 0.9\n",
      "iteration no 1217: Loss: 0.34091186110799127, accuracy: 0.8933333333333333\n",
      "iteration no 1218: Loss: 0.34047690969049527, accuracy: 0.9\n",
      "iteration no 1219: Loss: 0.34056832244608104, accuracy: 0.8933333333333333\n",
      "iteration no 1220: Loss: 0.3401410770088074, accuracy: 0.9\n",
      "iteration no 1221: Loss: 0.34023216138234985, accuracy: 0.8933333333333333\n",
      "iteration no 1222: Loss: 0.33981795623982985, accuracy: 0.9\n",
      "iteration no 1223: Loss: 0.33989128097011184, accuracy: 0.8933333333333333\n",
      "iteration no 1224: Loss: 0.3394652692997142, accuracy: 0.9\n",
      "iteration no 1225: Loss: 0.33957331010746794, accuracy: 0.9\n",
      "iteration no 1226: Loss: 0.3391491461399604, accuracy: 0.9\n",
      "iteration no 1227: Loss: 0.3392514671649186, accuracy: 0.9\n",
      "iteration no 1228: Loss: 0.33884137327681335, accuracy: 0.9\n",
      "iteration no 1229: Loss: 0.33891100314752354, accuracy: 0.9\n",
      "iteration no 1230: Loss: 0.3385029504524004, accuracy: 0.9\n",
      "iteration no 1231: Loss: 0.3386039887397048, accuracy: 0.9\n",
      "iteration no 1232: Loss: 0.33819186460117306, accuracy: 0.9\n",
      "iteration no 1233: Loss: 0.33828729592179596, accuracy: 0.9\n",
      "iteration no 1234: Loss: 0.3378708366613977, accuracy: 0.9\n",
      "iteration no 1235: Loss: 0.3379255946760382, accuracy: 0.9\n",
      "iteration no 1236: Loss: 0.33752734836150466, accuracy: 0.9\n",
      "iteration no 1237: Loss: 0.33761127362477217, accuracy: 0.9\n",
      "iteration no 1238: Loss: 0.3371889757910621, accuracy: 0.9\n",
      "iteration no 1239: Loss: 0.33723000210738513, accuracy: 0.9\n",
      "iteration no 1240: Loss: 0.33683360184053485, accuracy: 0.9\n",
      "iteration no 1241: Loss: 0.3369059257877838, accuracy: 0.9033333333333333\n",
      "iteration no 1242: Loss: 0.3365001648882092, accuracy: 0.9\n",
      "iteration no 1243: Loss: 0.33653609285616504, accuracy: 0.9033333333333333\n",
      "iteration no 1244: Loss: 0.33614438977347577, accuracy: 0.9\n",
      "iteration no 1245: Loss: 0.3362106901273432, accuracy: 0.9033333333333333\n",
      "iteration no 1246: Loss: 0.33580434290645195, accuracy: 0.9033333333333333\n",
      "iteration no 1247: Loss: 0.33586164030327376, accuracy: 0.9033333333333333\n",
      "iteration no 1248: Loss: 0.3354678060140812, accuracy: 0.9\n",
      "iteration no 1249: Loss: 0.33548326178221005, accuracy: 0.9033333333333333\n",
      "iteration no 1250: Loss: 0.33509502004869507, accuracy: 0.9033333333333333\n",
      "iteration no 1251: Loss: 0.3351427543844225, accuracy: 0.9033333333333333\n",
      "iteration no 1252: Loss: 0.3347572323988605, accuracy: 0.9033333333333333\n",
      "iteration no 1253: Loss: 0.33476712374412365, accuracy: 0.9033333333333333\n",
      "iteration no 1254: Loss: 0.3343963470586971, accuracy: 0.9\n",
      "iteration no 1255: Loss: 0.3344004475398424, accuracy: 0.9033333333333333\n",
      "iteration no 1256: Loss: 0.3340444646202615, accuracy: 0.9\n",
      "iteration no 1257: Loss: 0.3340508109119123, accuracy: 0.9033333333333333\n",
      "iteration no 1258: Loss: 0.33369529476550747, accuracy: 0.9\n",
      "iteration no 1259: Loss: 0.333698160185038, accuracy: 0.9033333333333333\n",
      "iteration no 1260: Loss: 0.33334749385353946, accuracy: 0.9\n",
      "iteration no 1261: Loss: 0.3333400951308868, accuracy: 0.9033333333333333\n",
      "iteration no 1262: Loss: 0.33299392066883093, accuracy: 0.9\n",
      "iteration no 1263: Loss: 0.3329846592346873, accuracy: 0.9033333333333333\n",
      "iteration no 1264: Loss: 0.33264893416927044, accuracy: 0.9\n",
      "iteration no 1265: Loss: 0.3326381989113436, accuracy: 0.9066666666666666\n",
      "iteration no 1266: Loss: 0.3323115964016168, accuracy: 0.9\n",
      "iteration no 1267: Loss: 0.33230927791503073, accuracy: 0.9066666666666666\n",
      "iteration no 1268: Loss: 0.33196860455952215, accuracy: 0.9\n",
      "iteration no 1269: Loss: 0.33195029993846836, accuracy: 0.9066666666666666\n",
      "iteration no 1270: Loss: 0.33162888129761076, accuracy: 0.9\n",
      "iteration no 1271: Loss: 0.33161482464116354, accuracy: 0.9066666666666666\n",
      "iteration no 1272: Loss: 0.3312908157261083, accuracy: 0.9\n",
      "iteration no 1273: Loss: 0.33124155788822063, accuracy: 0.9066666666666666\n",
      "iteration no 1274: Loss: 0.3309297707695749, accuracy: 0.9\n",
      "iteration no 1275: Loss: 0.33091049186306787, accuracy: 0.9\n",
      "iteration no 1276: Loss: 0.3305840804733233, accuracy: 0.9033333333333333\n",
      "iteration no 1277: Loss: 0.33056277352021735, accuracy: 0.9\n",
      "iteration no 1278: Loss: 0.33026336472322604, accuracy: 0.9033333333333333\n",
      "iteration no 1279: Loss: 0.33021613309361125, accuracy: 0.9\n",
      "iteration no 1280: Loss: 0.3299068766243095, accuracy: 0.9033333333333333\n",
      "iteration no 1281: Loss: 0.32989018960448063, accuracy: 0.9\n",
      "iteration no 1282: Loss: 0.32959751137513604, accuracy: 0.9033333333333333\n",
      "iteration no 1283: Loss: 0.3295404088649085, accuracy: 0.9\n",
      "iteration no 1284: Loss: 0.32925728369188445, accuracy: 0.9033333333333333\n",
      "iteration no 1285: Loss: 0.32919530431264854, accuracy: 0.9\n",
      "iteration no 1286: Loss: 0.32891457800970225, accuracy: 0.9033333333333333\n",
      "iteration no 1287: Loss: 0.32885471666591015, accuracy: 0.9\n",
      "iteration no 1288: Loss: 0.32858321421110576, accuracy: 0.9033333333333333\n",
      "iteration no 1289: Loss: 0.32854104392756295, accuracy: 0.9\n",
      "iteration no 1290: Loss: 0.3282933164254007, accuracy: 0.9066666666666666\n",
      "iteration no 1291: Loss: 0.32826777741805097, accuracy: 0.9\n",
      "iteration no 1292: Loss: 0.3280121155250997, accuracy: 0.9066666666666666\n",
      "iteration no 1293: Loss: 0.3279716452068284, accuracy: 0.9\n",
      "iteration no 1294: Loss: 0.3277307713664797, accuracy: 0.9066666666666666\n",
      "iteration no 1295: Loss: 0.32767446494212027, accuracy: 0.9\n",
      "iteration no 1296: Loss: 0.32743890764493966, accuracy: 0.9066666666666666\n",
      "iteration no 1297: Loss: 0.3274016072853798, accuracy: 0.9\n",
      "iteration no 1298: Loss: 0.32717652882623655, accuracy: 0.9066666666666666\n",
      "iteration no 1299: Loss: 0.3271128456403637, accuracy: 0.9\n",
      "iteration no 1300: Loss: 0.3268806400255695, accuracy: 0.9066666666666666\n",
      "iteration no 1301: Loss: 0.32684548963323534, accuracy: 0.9\n",
      "iteration no 1302: Loss: 0.326623953202969, accuracy: 0.91\n",
      "iteration no 1303: Loss: 0.32655944968737294, accuracy: 0.9\n",
      "iteration no 1304: Loss: 0.326347583447226, accuracy: 0.91\n",
      "iteration no 1305: Loss: 0.32631066234097733, accuracy: 0.9\n",
      "iteration no 1306: Loss: 0.326100880942767, accuracy: 0.91\n",
      "iteration no 1307: Loss: 0.32606376469350545, accuracy: 0.9\n",
      "iteration no 1308: Loss: 0.3258725432834563, accuracy: 0.91\n",
      "iteration no 1309: Loss: 0.32583820582290546, accuracy: 0.9\n",
      "iteration no 1310: Loss: 0.3256510361018242, accuracy: 0.91\n",
      "iteration no 1311: Loss: 0.32562465227161574, accuracy: 0.9\n",
      "iteration no 1312: Loss: 0.32544348691699737, accuracy: 0.91\n",
      "iteration no 1313: Loss: 0.32538261264279206, accuracy: 0.9\n",
      "iteration no 1314: Loss: 0.32520977608594337, accuracy: 0.91\n",
      "iteration no 1315: Loss: 0.32518221038749656, accuracy: 0.9\n",
      "iteration no 1316: Loss: 0.3250011542052052, accuracy: 0.91\n",
      "iteration no 1317: Loss: 0.32496929715766903, accuracy: 0.9\n",
      "iteration no 1318: Loss: 0.3247952585000759, accuracy: 0.91\n",
      "iteration no 1319: Loss: 0.324764542534757, accuracy: 0.9\n",
      "iteration no 1320: Loss: 0.3245907765363563, accuracy: 0.91\n",
      "iteration no 1321: Loss: 0.3245316600720334, accuracy: 0.9\n",
      "iteration no 1322: Loss: 0.32434778048024343, accuracy: 0.91\n",
      "iteration no 1323: Loss: 0.32432779858554167, accuracy: 0.9\n",
      "iteration no 1324: Loss: 0.3241345625080814, accuracy: 0.91\n",
      "iteration no 1325: Loss: 0.3241088857129694, accuracy: 0.9\n",
      "iteration no 1326: Loss: 0.323921784003822, accuracy: 0.91\n",
      "iteration no 1327: Loss: 0.3239012821183978, accuracy: 0.9\n",
      "iteration no 1328: Loss: 0.32370171360581496, accuracy: 0.91\n",
      "iteration no 1329: Loss: 0.3236837244399872, accuracy: 0.9\n",
      "iteration no 1330: Loss: 0.32347988421938706, accuracy: 0.91\n",
      "iteration no 1331: Loss: 0.323462723239885, accuracy: 0.9033333333333333\n",
      "iteration no 1332: Loss: 0.32326268012142145, accuracy: 0.91\n",
      "iteration no 1333: Loss: 0.3232095661258302, accuracy: 0.9\n",
      "iteration no 1334: Loss: 0.3230101764785051, accuracy: 0.91\n",
      "iteration no 1335: Loss: 0.32301664816343983, accuracy: 0.9033333333333333\n",
      "iteration no 1336: Loss: 0.32280856122783574, accuracy: 0.9133333333333333\n",
      "iteration no 1337: Loss: 0.32281603239474344, accuracy: 0.9033333333333333\n",
      "iteration no 1338: Loss: 0.32260889520957164, accuracy: 0.9133333333333333\n",
      "iteration no 1339: Loss: 0.3225862815155547, accuracy: 0.9033333333333333\n",
      "iteration no 1340: Loss: 0.322373494260123, accuracy: 0.9133333333333333\n",
      "iteration no 1341: Loss: 0.3223796565476859, accuracy: 0.9033333333333333\n",
      "iteration no 1342: Loss: 0.3221685254239188, accuracy: 0.9133333333333333\n",
      "iteration no 1343: Loss: 0.3221544225718086, accuracy: 0.9033333333333333\n",
      "iteration no 1344: Loss: 0.32194952015481226, accuracy: 0.9133333333333333\n",
      "iteration no 1345: Loss: 0.32195120605775995, accuracy: 0.9033333333333333\n",
      "iteration no 1346: Loss: 0.3217449540975597, accuracy: 0.9133333333333333\n",
      "iteration no 1347: Loss: 0.32171424231414303, accuracy: 0.9066666666666666\n",
      "iteration no 1348: Loss: 0.3215039866286032, accuracy: 0.9133333333333333\n",
      "iteration no 1349: Loss: 0.32150867729606636, accuracy: 0.9066666666666666\n",
      "iteration no 1350: Loss: 0.32129805999270933, accuracy: 0.9133333333333333\n",
      "iteration no 1351: Loss: 0.3212914779193154, accuracy: 0.9066666666666666\n",
      "iteration no 1352: Loss: 0.3210890863167654, accuracy: 0.9133333333333333\n",
      "iteration no 1353: Loss: 0.3210526679530636, accuracy: 0.9066666666666666\n",
      "iteration no 1354: Loss: 0.3208494613058667, accuracy: 0.9166666666666666\n",
      "iteration no 1355: Loss: 0.3208383567838243, accuracy: 0.91\n",
      "iteration no 1356: Loss: 0.32061268399116205, accuracy: 0.9166666666666666\n",
      "iteration no 1357: Loss: 0.3206039718069079, accuracy: 0.91\n",
      "iteration no 1358: Loss: 0.32038515634508474, accuracy: 0.9166666666666666\n",
      "iteration no 1359: Loss: 0.3203470948010525, accuracy: 0.9066666666666666\n",
      "iteration no 1360: Loss: 0.3201230513094631, accuracy: 0.9166666666666666\n",
      "iteration no 1361: Loss: 0.32010283771748393, accuracy: 0.9066666666666666\n",
      "iteration no 1362: Loss: 0.3198853930182266, accuracy: 0.9166666666666666\n",
      "iteration no 1363: Loss: 0.3198413884614313, accuracy: 0.9066666666666666\n",
      "iteration no 1364: Loss: 0.31962756317506663, accuracy: 0.9166666666666666\n",
      "iteration no 1365: Loss: 0.31960874946186807, accuracy: 0.91\n",
      "iteration no 1366: Loss: 0.3193922226447708, accuracy: 0.9166666666666666\n",
      "iteration no 1367: Loss: 0.31934680635580237, accuracy: 0.91\n",
      "iteration no 1368: Loss: 0.31912161161083874, accuracy: 0.9166666666666666\n",
      "iteration no 1369: Loss: 0.3190992817892429, accuracy: 0.91\n",
      "iteration no 1370: Loss: 0.31889243746398116, accuracy: 0.9166666666666666\n",
      "iteration no 1371: Loss: 0.3188418302017729, accuracy: 0.91\n",
      "iteration no 1372: Loss: 0.31862267434817104, accuracy: 0.9166666666666666\n",
      "iteration no 1373: Loss: 0.3186043688413941, accuracy: 0.91\n",
      "iteration no 1374: Loss: 0.31839152408915844, accuracy: 0.9166666666666666\n",
      "iteration no 1375: Loss: 0.31834247698512325, accuracy: 0.91\n",
      "iteration no 1376: Loss: 0.3181322828219882, accuracy: 0.9166666666666666\n",
      "iteration no 1377: Loss: 0.3180849853364613, accuracy: 0.91\n",
      "iteration no 1378: Loss: 0.3178727820246183, accuracy: 0.9166666666666666\n",
      "iteration no 1379: Loss: 0.31780164921948517, accuracy: 0.91\n",
      "iteration no 1380: Loss: 0.31760140552337957, accuracy: 0.92\n",
      "iteration no 1381: Loss: 0.31755451360493403, accuracy: 0.91\n",
      "iteration no 1382: Loss: 0.3173476965570144, accuracy: 0.9233333333333333\n",
      "iteration no 1383: Loss: 0.31727893383940364, accuracy: 0.91\n",
      "iteration no 1384: Loss: 0.31709368814599953, accuracy: 0.9233333333333333\n",
      "iteration no 1385: Loss: 0.3170361506176109, accuracy: 0.91\n",
      "iteration no 1386: Loss: 0.3168476628678133, accuracy: 0.9233333333333333\n",
      "iteration no 1387: Loss: 0.31674546368153217, accuracy: 0.9066666666666666\n",
      "iteration no 1388: Loss: 0.3165603848397471, accuracy: 0.9233333333333333\n",
      "iteration no 1389: Loss: 0.31649358506089126, accuracy: 0.9066666666666666\n",
      "iteration no 1390: Loss: 0.31631516301239226, accuracy: 0.9233333333333333\n",
      "iteration no 1391: Loss: 0.31620323665403904, accuracy: 0.9066666666666666\n",
      "iteration no 1392: Loss: 0.31601222247388694, accuracy: 0.9233333333333333\n",
      "iteration no 1393: Loss: 0.31589030412272406, accuracy: 0.9066666666666666\n",
      "iteration no 1394: Loss: 0.3157104650954202, accuracy: 0.9233333333333333\n",
      "iteration no 1395: Loss: 0.31559813232024164, accuracy: 0.9066666666666666\n",
      "iteration no 1396: Loss: 0.3154191347302037, accuracy: 0.9233333333333333\n",
      "iteration no 1397: Loss: 0.3153647082002359, accuracy: 0.9066666666666666\n",
      "iteration no 1398: Loss: 0.31518391949254354, accuracy: 0.9266666666666666\n",
      "iteration no 1399: Loss: 0.31507907761655424, accuracy: 0.9066666666666666\n",
      "iteration no 1400: Loss: 0.3149034912606539, accuracy: 0.9266666666666666\n",
      "iteration no 1401: Loss: 0.3148013140385354, accuracy: 0.9066666666666666\n",
      "iteration no 1402: Loss: 0.31462916099784466, accuracy: 0.9266666666666666\n",
      "iteration no 1403: Loss: 0.3145245937620446, accuracy: 0.9066666666666666\n",
      "iteration no 1404: Loss: 0.31435337976419475, accuracy: 0.9266666666666666\n",
      "iteration no 1405: Loss: 0.31424356725368474, accuracy: 0.9066666666666666\n",
      "iteration no 1406: Loss: 0.3140799340606087, accuracy: 0.9266666666666666\n",
      "iteration no 1407: Loss: 0.31397523363142515, accuracy: 0.9066666666666666\n",
      "iteration no 1408: Loss: 0.31381224302235505, accuracy: 0.9266666666666666\n",
      "iteration no 1409: Loss: 0.3137089043949991, accuracy: 0.9033333333333333\n",
      "iteration no 1410: Loss: 0.313552358552895, accuracy: 0.9266666666666666\n",
      "iteration no 1411: Loss: 0.31345291520370655, accuracy: 0.9066666666666666\n",
      "iteration no 1412: Loss: 0.31329725476979725, accuracy: 0.9266666666666666\n",
      "iteration no 1413: Loss: 0.31319709261479994, accuracy: 0.91\n",
      "iteration no 1414: Loss: 0.3130453700980896, accuracy: 0.9266666666666666\n",
      "iteration no 1415: Loss: 0.31294888240981883, accuracy: 0.91\n",
      "iteration no 1416: Loss: 0.31279632297446014, accuracy: 0.9266666666666666\n",
      "iteration no 1417: Loss: 0.3126987599175178, accuracy: 0.91\n",
      "iteration no 1418: Loss: 0.3125493703663954, accuracy: 0.9266666666666666\n",
      "iteration no 1419: Loss: 0.31245495028421555, accuracy: 0.91\n",
      "iteration no 1420: Loss: 0.312310189267039, accuracy: 0.9266666666666666\n",
      "iteration no 1421: Loss: 0.3122166579118964, accuracy: 0.91\n",
      "iteration no 1422: Loss: 0.3120844473827933, accuracy: 0.9266666666666666\n",
      "iteration no 1423: Loss: 0.3120027583823959, accuracy: 0.91\n",
      "iteration no 1424: Loss: 0.3118904850952709, accuracy: 0.9266666666666666\n",
      "iteration no 1425: Loss: 0.31180538788031176, accuracy: 0.91\n",
      "iteration no 1426: Loss: 0.31167342788279667, accuracy: 0.9266666666666666\n",
      "iteration no 1427: Loss: 0.3115719082723251, accuracy: 0.91\n",
      "iteration no 1428: Loss: 0.3114635900114697, accuracy: 0.9266666666666666\n",
      "iteration no 1429: Loss: 0.31137661056534477, accuracy: 0.91\n",
      "iteration no 1430: Loss: 0.3112457567785297, accuracy: 0.9266666666666666\n",
      "iteration no 1431: Loss: 0.3111567188723354, accuracy: 0.91\n",
      "iteration no 1432: Loss: 0.31105562279513504, accuracy: 0.9266666666666666\n",
      "iteration no 1433: Loss: 0.31096818708347795, accuracy: 0.91\n",
      "iteration no 1434: Loss: 0.31083899106324725, accuracy: 0.93\n",
      "iteration no 1435: Loss: 0.3107479716430164, accuracy: 0.91\n",
      "iteration no 1436: Loss: 0.3106476639396532, accuracy: 0.93\n",
      "iteration no 1437: Loss: 0.3105646516882992, accuracy: 0.91\n",
      "iteration no 1438: Loss: 0.3104344453320315, accuracy: 0.93\n",
      "iteration no 1439: Loss: 0.3103360887208401, accuracy: 0.9133333333333333\n",
      "iteration no 1440: Loss: 0.3102346458912053, accuracy: 0.93\n",
      "iteration no 1441: Loss: 0.3101452311310388, accuracy: 0.91\n",
      "iteration no 1442: Loss: 0.31001952211491124, accuracy: 0.93\n",
      "iteration no 1443: Loss: 0.30993415178272044, accuracy: 0.9133333333333333\n",
      "iteration no 1444: Loss: 0.3098260344270045, accuracy: 0.9333333333333333\n",
      "iteration no 1445: Loss: 0.30973321678173293, accuracy: 0.9133333333333333\n",
      "iteration no 1446: Loss: 0.3096344174316344, accuracy: 0.9366666666666666\n",
      "iteration no 1447: Loss: 0.3095351239978856, accuracy: 0.91\n",
      "iteration no 1448: Loss: 0.30941183162859076, accuracy: 0.9366666666666666\n",
      "iteration no 1449: Loss: 0.3093227846922203, accuracy: 0.9166666666666666\n",
      "iteration no 1450: Loss: 0.30921690908222105, accuracy: 0.9366666666666666\n",
      "iteration no 1451: Loss: 0.3091309329095588, accuracy: 0.91\n",
      "iteration no 1452: Loss: 0.30901322851842666, accuracy: 0.9366666666666666\n",
      "iteration no 1453: Loss: 0.3089187898882034, accuracy: 0.91\n",
      "iteration no 1454: Loss: 0.308818830720418, accuracy: 0.9366666666666666\n",
      "iteration no 1455: Loss: 0.30873632408452856, accuracy: 0.9133333333333333\n",
      "iteration no 1456: Loss: 0.3086349757990927, accuracy: 0.9366666666666666\n",
      "iteration no 1457: Loss: 0.3085594732963511, accuracy: 0.9166666666666666\n",
      "iteration no 1458: Loss: 0.3084599068471284, accuracy: 0.9366666666666666\n",
      "iteration no 1459: Loss: 0.3083770695168865, accuracy: 0.9166666666666666\n",
      "iteration no 1460: Loss: 0.3082801180916148, accuracy: 0.9366666666666666\n",
      "iteration no 1461: Loss: 0.3082103380693279, accuracy: 0.9166666666666666\n",
      "iteration no 1462: Loss: 0.3081162669204257, accuracy: 0.9366666666666666\n",
      "iteration no 1463: Loss: 0.30803956185461406, accuracy: 0.9166666666666666\n",
      "iteration no 1464: Loss: 0.30792105134341097, accuracy: 0.9366666666666666\n",
      "iteration no 1465: Loss: 0.3078406224178636, accuracy: 0.9166666666666666\n",
      "iteration no 1466: Loss: 0.3077463012312177, accuracy: 0.9366666666666666\n",
      "iteration no 1467: Loss: 0.3076593922308804, accuracy: 0.9166666666666666\n",
      "iteration no 1468: Loss: 0.30757141914640757, accuracy: 0.9366666666666666\n",
      "iteration no 1469: Loss: 0.3074982540032176, accuracy: 0.9166666666666666\n",
      "iteration no 1470: Loss: 0.30741837755601764, accuracy: 0.9366666666666666\n",
      "iteration no 1471: Loss: 0.307353546580083, accuracy: 0.9166666666666666\n",
      "iteration no 1472: Loss: 0.3072865891280559, accuracy: 0.9366666666666666\n",
      "iteration no 1473: Loss: 0.3072166319653245, accuracy: 0.9166666666666666\n",
      "iteration no 1474: Loss: 0.30712932001679033, accuracy: 0.9366666666666666\n",
      "iteration no 1475: Loss: 0.30706721240328055, accuracy: 0.9166666666666666\n",
      "iteration no 1476: Loss: 0.3069933884854573, accuracy: 0.9366666666666666\n",
      "iteration no 1477: Loss: 0.30692355505942726, accuracy: 0.9166666666666666\n",
      "iteration no 1478: Loss: 0.30685008680907844, accuracy: 0.9366666666666666\n",
      "iteration no 1479: Loss: 0.30678650698125903, accuracy: 0.9166666666666666\n",
      "iteration no 1480: Loss: 0.30671800729854776, accuracy: 0.9366666666666666\n",
      "iteration no 1481: Loss: 0.3066561466667937, accuracy: 0.92\n",
      "iteration no 1482: Loss: 0.3065696449635801, accuracy: 0.9366666666666666\n",
      "iteration no 1483: Loss: 0.30651070600176683, accuracy: 0.9166666666666666\n",
      "iteration no 1484: Loss: 0.30644148431770346, accuracy: 0.9366666666666666\n",
      "iteration no 1485: Loss: 0.30638095251576125, accuracy: 0.92\n",
      "iteration no 1486: Loss: 0.3063082741031874, accuracy: 0.9366666666666666\n",
      "iteration no 1487: Loss: 0.30624387413901616, accuracy: 0.9166666666666666\n",
      "iteration no 1488: Loss: 0.3061606672160865, accuracy: 0.9366666666666666\n",
      "iteration no 1489: Loss: 0.3060991213482611, accuracy: 0.92\n",
      "iteration no 1490: Loss: 0.3060275368148261, accuracy: 0.9366666666666666\n",
      "iteration no 1491: Loss: 0.30596439981576645, accuracy: 0.9166666666666666\n",
      "iteration no 1492: Loss: 0.30589877307620783, accuracy: 0.9366666666666666\n",
      "iteration no 1493: Loss: 0.3058499336937804, accuracy: 0.9166666666666666\n",
      "iteration no 1494: Loss: 0.3057832124402831, accuracy: 0.9366666666666666\n",
      "iteration no 1495: Loss: 0.30573024031260826, accuracy: 0.9166666666666666\n",
      "iteration no 1496: Loss: 0.30565873350202694, accuracy: 0.9366666666666666\n",
      "iteration no 1497: Loss: 0.30560972504183326, accuracy: 0.9166666666666666\n",
      "iteration no 1498: Loss: 0.3055405754435175, accuracy: 0.9366666666666666\n",
      "iteration no 1499: Loss: 0.3054863814478902, accuracy: 0.9166666666666666\n",
      "iteration no 1500: Loss: 0.3054132489283831, accuracy: 0.9366666666666666\n",
      "iteration no 1501: Loss: 0.30536061736573544, accuracy: 0.9166666666666666\n",
      "iteration no 1502: Loss: 0.30529345094610905, accuracy: 0.9366666666666666\n",
      "iteration no 1503: Loss: 0.30523905858357453, accuracy: 0.9166666666666666\n",
      "iteration no 1504: Loss: 0.3051703453946771, accuracy: 0.9366666666666666\n",
      "iteration no 1505: Loss: 0.3051162963699568, accuracy: 0.9166666666666666\n",
      "iteration no 1506: Loss: 0.3050332355776665, accuracy: 0.9366666666666666\n",
      "iteration no 1507: Loss: 0.30498363144354707, accuracy: 0.9166666666666666\n",
      "iteration no 1508: Loss: 0.30491228262301373, accuracy: 0.9366666666666666\n",
      "iteration no 1509: Loss: 0.30485867803171113, accuracy: 0.9166666666666666\n",
      "iteration no 1510: Loss: 0.3047802461748749, accuracy: 0.9366666666666666\n",
      "iteration no 1511: Loss: 0.3047296081263973, accuracy: 0.9166666666666666\n",
      "iteration no 1512: Loss: 0.3046486205630784, accuracy: 0.9366666666666666\n",
      "iteration no 1513: Loss: 0.3045942582517064, accuracy: 0.9166666666666666\n",
      "iteration no 1514: Loss: 0.30451361149936546, accuracy: 0.9366666666666666\n",
      "iteration no 1515: Loss: 0.3044590517593202, accuracy: 0.9166666666666666\n",
      "iteration no 1516: Loss: 0.3043757627980907, accuracy: 0.9366666666666666\n",
      "iteration no 1517: Loss: 0.3043318597911424, accuracy: 0.9166666666666666\n",
      "iteration no 1518: Loss: 0.3042512914116971, accuracy: 0.9366666666666666\n",
      "iteration no 1519: Loss: 0.3041980764423745, accuracy: 0.9166666666666666\n",
      "iteration no 1520: Loss: 0.3041105721960422, accuracy: 0.9366666666666666\n",
      "iteration no 1521: Loss: 0.30406476643236974, accuracy: 0.9166666666666666\n",
      "iteration no 1522: Loss: 0.30397828707619967, accuracy: 0.9366666666666666\n",
      "iteration no 1523: Loss: 0.3039351978336715, accuracy: 0.9166666666666666\n",
      "iteration no 1524: Loss: 0.30384751327307286, accuracy: 0.9366666666666666\n",
      "iteration no 1525: Loss: 0.303792504462528, accuracy: 0.9166666666666666\n",
      "iteration no 1526: Loss: 0.3037096412084976, accuracy: 0.9366666666666666\n",
      "iteration no 1527: Loss: 0.30366263124839626, accuracy: 0.9166666666666666\n",
      "iteration no 1528: Loss: 0.3035669607430032, accuracy: 0.9366666666666666\n",
      "iteration no 1529: Loss: 0.3035229615475775, accuracy: 0.9166666666666666\n",
      "iteration no 1530: Loss: 0.30343249166440595, accuracy: 0.9366666666666666\n",
      "iteration no 1531: Loss: 0.30337686668464325, accuracy: 0.9166666666666666\n",
      "iteration no 1532: Loss: 0.3032791565913199, accuracy: 0.9366666666666666\n",
      "iteration no 1533: Loss: 0.303232663153676, accuracy: 0.9166666666666666\n",
      "iteration no 1534: Loss: 0.30313419249958595, accuracy: 0.9366666666666666\n",
      "iteration no 1535: Loss: 0.30309133379684206, accuracy: 0.92\n",
      "iteration no 1536: Loss: 0.3029884156743732, accuracy: 0.9366666666666666\n",
      "iteration no 1537: Loss: 0.30289010443209763, accuracy: 0.92\n",
      "iteration no 1538: Loss: 0.30277675054894604, accuracy: 0.9366666666666666\n",
      "iteration no 1539: Loss: 0.30270935434358, accuracy: 0.9166666666666666\n",
      "iteration no 1540: Loss: 0.3025958106530019, accuracy: 0.9366666666666666\n",
      "iteration no 1541: Loss: 0.3025370056060914, accuracy: 0.9166666666666666\n",
      "iteration no 1542: Loss: 0.3024306066337634, accuracy: 0.9366666666666666\n",
      "iteration no 1543: Loss: 0.30236228086136846, accuracy: 0.9166666666666666\n",
      "iteration no 1544: Loss: 0.30222715202918626, accuracy: 0.9366666666666666\n",
      "iteration no 1545: Loss: 0.3021122728832391, accuracy: 0.9166666666666666\n",
      "iteration no 1546: Loss: 0.30199246991461604, accuracy: 0.9366666666666666\n",
      "iteration no 1547: Loss: 0.30190840732626856, accuracy: 0.9166666666666666\n",
      "iteration no 1548: Loss: 0.30178064668335014, accuracy: 0.9366666666666666\n",
      "iteration no 1549: Loss: 0.301695672438942, accuracy: 0.9233333333333333\n",
      "iteration no 1550: Loss: 0.3015687408923014, accuracy: 0.9366666666666666\n",
      "iteration no 1551: Loss: 0.3014510236728893, accuracy: 0.9233333333333333\n",
      "iteration no 1552: Loss: 0.3013223433100472, accuracy: 0.9366666666666666\n",
      "iteration no 1553: Loss: 0.30123178533221934, accuracy: 0.9233333333333333\n",
      "iteration no 1554: Loss: 0.30111450186053007, accuracy: 0.9366666666666666\n",
      "iteration no 1555: Loss: 0.3010015737658613, accuracy: 0.9266666666666666\n",
      "iteration no 1556: Loss: 0.3008817835415129, accuracy: 0.9366666666666666\n",
      "iteration no 1557: Loss: 0.30075613627449566, accuracy: 0.9266666666666666\n",
      "iteration no 1558: Loss: 0.30063481150999616, accuracy: 0.9366666666666666\n",
      "iteration no 1559: Loss: 0.30053182455345706, accuracy: 0.9266666666666666\n",
      "iteration no 1560: Loss: 0.3004000373764886, accuracy: 0.9366666666666666\n",
      "iteration no 1561: Loss: 0.3002774565953853, accuracy: 0.9266666666666666\n",
      "iteration no 1562: Loss: 0.30015569530633696, accuracy: 0.9366666666666666\n",
      "iteration no 1563: Loss: 0.3000287765728974, accuracy: 0.9266666666666666\n",
      "iteration no 1564: Loss: 0.2999088068332974, accuracy: 0.9366666666666666\n",
      "iteration no 1565: Loss: 0.29978316309550007, accuracy: 0.9266666666666666\n",
      "iteration no 1566: Loss: 0.29965955041683073, accuracy: 0.9366666666666666\n",
      "iteration no 1567: Loss: 0.29955316423081874, accuracy: 0.93\n",
      "iteration no 1568: Loss: 0.29943983056186, accuracy: 0.9366666666666666\n",
      "iteration no 1569: Loss: 0.29934688549112787, accuracy: 0.9333333333333333\n",
      "iteration no 1570: Loss: 0.29923977710480665, accuracy: 0.9366666666666666\n",
      "iteration no 1571: Loss: 0.29913292346823217, accuracy: 0.9333333333333333\n",
      "iteration no 1572: Loss: 0.29902267098871005, accuracy: 0.9366666666666666\n",
      "iteration no 1573: Loss: 0.2989255196863283, accuracy: 0.9333333333333333\n",
      "iteration no 1574: Loss: 0.2988221879830438, accuracy: 0.9366666666666666\n",
      "iteration no 1575: Loss: 0.298713816444638, accuracy: 0.93\n",
      "iteration no 1576: Loss: 0.29861651889140706, accuracy: 0.9366666666666666\n",
      "iteration no 1577: Loss: 0.29851897084015794, accuracy: 0.9333333333333333\n",
      "iteration no 1578: Loss: 0.2984171925272902, accuracy: 0.9366666666666666\n",
      "iteration no 1579: Loss: 0.2983235111926663, accuracy: 0.93\n",
      "iteration no 1580: Loss: 0.298225998612874, accuracy: 0.9366666666666666\n",
      "iteration no 1581: Loss: 0.29813730619728784, accuracy: 0.93\n",
      "iteration no 1582: Loss: 0.2980446578421739, accuracy: 0.9366666666666666\n",
      "iteration no 1583: Loss: 0.297955232466887, accuracy: 0.93\n",
      "iteration no 1584: Loss: 0.29785834696630975, accuracy: 0.9366666666666666\n",
      "iteration no 1585: Loss: 0.2977674695781806, accuracy: 0.93\n",
      "iteration no 1586: Loss: 0.2976722520887459, accuracy: 0.9366666666666666\n",
      "iteration no 1587: Loss: 0.2975933536895934, accuracy: 0.93\n",
      "iteration no 1588: Loss: 0.29750172267254854, accuracy: 0.9366666666666666\n",
      "iteration no 1589: Loss: 0.2974184656928041, accuracy: 0.93\n",
      "iteration no 1590: Loss: 0.2973341078391559, accuracy: 0.9366666666666666\n",
      "iteration no 1591: Loss: 0.29726054643407207, accuracy: 0.93\n",
      "iteration no 1592: Loss: 0.29716881311554727, accuracy: 0.9366666666666666\n",
      "iteration no 1593: Loss: 0.29710032397964997, accuracy: 0.9333333333333333\n",
      "iteration no 1594: Loss: 0.29700819420813607, accuracy: 0.9366666666666666\n",
      "iteration no 1595: Loss: 0.29692733036431995, accuracy: 0.9333333333333333\n",
      "iteration no 1596: Loss: 0.29684161259048636, accuracy: 0.9366666666666666\n",
      "iteration no 1597: Loss: 0.29677222911277035, accuracy: 0.9333333333333333\n",
      "iteration no 1598: Loss: 0.2966883309534053, accuracy: 0.9366666666666666\n",
      "iteration no 1599: Loss: 0.29663079490149696, accuracy: 0.9333333333333333\n",
      "iteration no 1600: Loss: 0.29654331170202436, accuracy: 0.9366666666666666\n",
      "iteration no 1601: Loss: 0.2964668927706487, accuracy: 0.9333333333333333\n",
      "iteration no 1602: Loss: 0.29638140204005187, accuracy: 0.9366666666666666\n",
      "iteration no 1603: Loss: 0.29632669323892835, accuracy: 0.9333333333333333\n",
      "iteration no 1604: Loss: 0.2962409701366701, accuracy: 0.9366666666666666\n",
      "iteration no 1605: Loss: 0.29618799871485335, accuracy: 0.9333333333333333\n",
      "iteration no 1606: Loss: 0.2961064775781691, accuracy: 0.9366666666666666\n",
      "iteration no 1607: Loss: 0.2960315532155173, accuracy: 0.9333333333333333\n",
      "iteration no 1608: Loss: 0.2959563595493181, accuracy: 0.9366666666666666\n",
      "iteration no 1609: Loss: 0.2958968193093566, accuracy: 0.9333333333333333\n",
      "iteration no 1610: Loss: 0.2958141833098359, accuracy: 0.9366666666666666\n",
      "iteration no 1611: Loss: 0.29575527337426255, accuracy: 0.9333333333333333\n",
      "iteration no 1612: Loss: 0.2956744434857509, accuracy: 0.9366666666666666\n",
      "iteration no 1613: Loss: 0.2956167197186565, accuracy: 0.9333333333333333\n",
      "iteration no 1614: Loss: 0.2955391790556984, accuracy: 0.9366666666666666\n",
      "iteration no 1615: Loss: 0.29547570466891465, accuracy: 0.9333333333333333\n",
      "iteration no 1616: Loss: 0.295392520526867, accuracy: 0.9366666666666666\n",
      "iteration no 1617: Loss: 0.29532102453039283, accuracy: 0.9333333333333333\n",
      "iteration no 1618: Loss: 0.2952432634906723, accuracy: 0.9366666666666666\n",
      "iteration no 1619: Loss: 0.29517117783529867, accuracy: 0.9333333333333333\n",
      "iteration no 1620: Loss: 0.2950895705948321, accuracy: 0.9366666666666666\n",
      "iteration no 1621: Loss: 0.2950285248456785, accuracy: 0.9333333333333333\n",
      "iteration no 1622: Loss: 0.2949455542248094, accuracy: 0.9366666666666666\n",
      "iteration no 1623: Loss: 0.2948886141777931, accuracy: 0.9333333333333333\n",
      "iteration no 1624: Loss: 0.2948091092884127, accuracy: 0.9366666666666666\n",
      "iteration no 1625: Loss: 0.2947406973455932, accuracy: 0.9333333333333333\n",
      "iteration no 1626: Loss: 0.294667000700513, accuracy: 0.9366666666666666\n",
      "iteration no 1627: Loss: 0.29459793063133033, accuracy: 0.9333333333333333\n",
      "iteration no 1628: Loss: 0.2945211295498964, accuracy: 0.9366666666666666\n",
      "iteration no 1629: Loss: 0.2944482631544872, accuracy: 0.9333333333333333\n",
      "iteration no 1630: Loss: 0.29436713584920143, accuracy: 0.9366666666666666\n",
      "iteration no 1631: Loss: 0.2942910147348399, accuracy: 0.9333333333333333\n",
      "iteration no 1632: Loss: 0.2942148389018724, accuracy: 0.9366666666666666\n",
      "iteration no 1633: Loss: 0.2941523064706503, accuracy: 0.9333333333333333\n",
      "iteration no 1634: Loss: 0.2940808283943674, accuracy: 0.9366666666666666\n",
      "iteration no 1635: Loss: 0.2940121590634878, accuracy: 0.9333333333333333\n",
      "iteration no 1636: Loss: 0.2939446440390095, accuracy: 0.9366666666666666\n",
      "iteration no 1637: Loss: 0.2938672385099995, accuracy: 0.9333333333333333\n",
      "iteration no 1638: Loss: 0.2938030703902026, accuracy: 0.9366666666666666\n",
      "iteration no 1639: Loss: 0.2937420074677902, accuracy: 0.9333333333333333\n",
      "iteration no 1640: Loss: 0.2936706991054533, accuracy: 0.9366666666666666\n",
      "iteration no 1641: Loss: 0.29360155427280277, accuracy: 0.9333333333333333\n",
      "iteration no 1642: Loss: 0.29352818885506865, accuracy: 0.9366666666666666\n",
      "iteration no 1643: Loss: 0.2934688871480343, accuracy: 0.9333333333333333\n",
      "iteration no 1644: Loss: 0.2933989215110595, accuracy: 0.9366666666666666\n",
      "iteration no 1645: Loss: 0.2933366371647422, accuracy: 0.9333333333333333\n",
      "iteration no 1646: Loss: 0.29326921378343684, accuracy: 0.9366666666666666\n",
      "iteration no 1647: Loss: 0.2932018817801587, accuracy: 0.9333333333333333\n",
      "iteration no 1648: Loss: 0.2931398142762156, accuracy: 0.9366666666666666\n",
      "iteration no 1649: Loss: 0.29308105329351286, accuracy: 0.9333333333333333\n",
      "iteration no 1650: Loss: 0.2930151928976209, accuracy: 0.9366666666666666\n",
      "iteration no 1651: Loss: 0.29295314313772997, accuracy: 0.9366666666666666\n",
      "iteration no 1652: Loss: 0.29288514733636145, accuracy: 0.9366666666666666\n",
      "iteration no 1653: Loss: 0.29282926538993514, accuracy: 0.9366666666666666\n",
      "iteration no 1654: Loss: 0.29276339709039834, accuracy: 0.9366666666666666\n",
      "iteration no 1655: Loss: 0.29270723724355696, accuracy: 0.9366666666666666\n",
      "iteration no 1656: Loss: 0.29264688550404466, accuracy: 0.94\n",
      "iteration no 1657: Loss: 0.29257988334757185, accuracy: 0.9366666666666666\n",
      "iteration no 1658: Loss: 0.29252094628961134, accuracy: 0.94\n",
      "iteration no 1659: Loss: 0.29246537941578593, accuracy: 0.9366666666666666\n",
      "iteration no 1660: Loss: 0.2924023349283518, accuracy: 0.94\n",
      "iteration no 1661: Loss: 0.2923487116104409, accuracy: 0.9366666666666666\n",
      "iteration no 1662: Loss: 0.2922830782474488, accuracy: 0.94\n",
      "iteration no 1663: Loss: 0.2922327880425252, accuracy: 0.9366666666666666\n",
      "iteration no 1664: Loss: 0.2921646189433919, accuracy: 0.94\n",
      "iteration no 1665: Loss: 0.29209660246204916, accuracy: 0.9366666666666666\n",
      "iteration no 1666: Loss: 0.29203338372126664, accuracy: 0.94\n",
      "iteration no 1667: Loss: 0.29196667314116204, accuracy: 0.9366666666666666\n",
      "iteration no 1668: Loss: 0.2919001033331617, accuracy: 0.94\n",
      "iteration no 1669: Loss: 0.29184601451160047, accuracy: 0.9366666666666666\n",
      "iteration no 1670: Loss: 0.2917778987320951, accuracy: 0.94\n",
      "iteration no 1671: Loss: 0.2917231650681083, accuracy: 0.9366666666666666\n",
      "iteration no 1672: Loss: 0.2916622741611101, accuracy: 0.94\n",
      "iteration no 1673: Loss: 0.29160749220381504, accuracy: 0.9366666666666666\n",
      "iteration no 1674: Loss: 0.2915426045404726, accuracy: 0.94\n",
      "iteration no 1675: Loss: 0.2914746463398398, accuracy: 0.9366666666666666\n",
      "iteration no 1676: Loss: 0.2914114023557784, accuracy: 0.94\n",
      "iteration no 1677: Loss: 0.29135822109094933, accuracy: 0.9366666666666666\n",
      "iteration no 1678: Loss: 0.29129785712097356, accuracy: 0.94\n",
      "iteration no 1679: Loss: 0.2912388925321158, accuracy: 0.9366666666666666\n",
      "iteration no 1680: Loss: 0.29118299227851685, accuracy: 0.94\n",
      "iteration no 1681: Loss: 0.29112912697961546, accuracy: 0.9366666666666666\n",
      "iteration no 1682: Loss: 0.29106980885933903, accuracy: 0.94\n",
      "iteration no 1683: Loss: 0.2910199100720692, accuracy: 0.9366666666666666\n",
      "iteration no 1684: Loss: 0.2909535543577473, accuracy: 0.94\n",
      "iteration no 1685: Loss: 0.2908950019427912, accuracy: 0.9366666666666666\n",
      "iteration no 1686: Loss: 0.2908374706697372, accuracy: 0.94\n",
      "iteration no 1687: Loss: 0.2907840183009747, accuracy: 0.9366666666666666\n",
      "iteration no 1688: Loss: 0.29072805480293695, accuracy: 0.94\n",
      "iteration no 1689: Loss: 0.2906695265449282, accuracy: 0.9366666666666666\n",
      "iteration no 1690: Loss: 0.2906132578281353, accuracy: 0.94\n",
      "iteration no 1691: Loss: 0.2905628249111984, accuracy: 0.9366666666666666\n",
      "iteration no 1692: Loss: 0.2905026266917411, accuracy: 0.94\n",
      "iteration no 1693: Loss: 0.2904476393851267, accuracy: 0.9366666666666666\n",
      "iteration no 1694: Loss: 0.290389530805776, accuracy: 0.94\n",
      "iteration no 1695: Loss: 0.2903322669382024, accuracy: 0.9366666666666666\n",
      "iteration no 1696: Loss: 0.29027811146689053, accuracy: 0.94\n",
      "iteration no 1697: Loss: 0.29022187616743844, accuracy: 0.9366666666666666\n",
      "iteration no 1698: Loss: 0.2901702246586592, accuracy: 0.94\n",
      "iteration no 1699: Loss: 0.2901198544709965, accuracy: 0.9366666666666666\n",
      "iteration no 1700: Loss: 0.2900627949236728, accuracy: 0.94\n",
      "iteration no 1701: Loss: 0.2900133922250029, accuracy: 0.94\n",
      "iteration no 1702: Loss: 0.2899539218245977, accuracy: 0.94\n",
      "iteration no 1703: Loss: 0.28990388765577707, accuracy: 0.9433333333333334\n",
      "iteration no 1704: Loss: 0.28984766363299136, accuracy: 0.94\n",
      "iteration no 1705: Loss: 0.28979616547769615, accuracy: 0.94\n",
      "iteration no 1706: Loss: 0.2897448069885829, accuracy: 0.94\n",
      "iteration no 1707: Loss: 0.28969017414979836, accuracy: 0.9433333333333334\n",
      "iteration no 1708: Loss: 0.28963941974812074, accuracy: 0.94\n",
      "iteration no 1709: Loss: 0.2895874843728056, accuracy: 0.9433333333333334\n",
      "iteration no 1710: Loss: 0.2895324882718342, accuracy: 0.94\n",
      "iteration no 1711: Loss: 0.2894815708901917, accuracy: 0.9433333333333334\n",
      "iteration no 1712: Loss: 0.28942668710469216, accuracy: 0.94\n",
      "iteration no 1713: Loss: 0.2893825244345051, accuracy: 0.9433333333333334\n",
      "iteration no 1714: Loss: 0.28932929823364906, accuracy: 0.94\n",
      "iteration no 1715: Loss: 0.28927688117701505, accuracy: 0.9433333333333334\n",
      "iteration no 1716: Loss: 0.2892292923397349, accuracy: 0.9433333333333334\n",
      "iteration no 1717: Loss: 0.2891778774595496, accuracy: 0.9433333333333334\n",
      "iteration no 1718: Loss: 0.2891267502537742, accuracy: 0.9433333333333334\n",
      "iteration no 1719: Loss: 0.28908338773748166, accuracy: 0.9433333333333334\n",
      "iteration no 1720: Loss: 0.28902934948422243, accuracy: 0.9433333333333334\n",
      "iteration no 1721: Loss: 0.28898194662110904, accuracy: 0.9433333333333334\n",
      "iteration no 1722: Loss: 0.28893015299445785, accuracy: 0.9433333333333334\n",
      "iteration no 1723: Loss: 0.2888847074404376, accuracy: 0.9433333333333334\n",
      "iteration no 1724: Loss: 0.28883521291425973, accuracy: 0.9433333333333334\n",
      "iteration no 1725: Loss: 0.28878501361150694, accuracy: 0.9433333333333334\n",
      "iteration no 1726: Loss: 0.2887401815133902, accuracy: 0.9433333333333334\n",
      "iteration no 1727: Loss: 0.2886938934530825, accuracy: 0.9433333333333334\n",
      "iteration no 1728: Loss: 0.2886455949775571, accuracy: 0.9433333333333334\n",
      "iteration no 1729: Loss: 0.28859839506995677, accuracy: 0.9466666666666667\n",
      "iteration no 1730: Loss: 0.2885499405878867, accuracy: 0.9433333333333334\n",
      "iteration no 1731: Loss: 0.28850708187581403, accuracy: 0.9466666666666667\n",
      "iteration no 1732: Loss: 0.288458524402349, accuracy: 0.9433333333333334\n",
      "iteration no 1733: Loss: 0.28841692902406657, accuracy: 0.9466666666666667\n",
      "iteration no 1734: Loss: 0.2883708523954547, accuracy: 0.9433333333333334\n",
      "iteration no 1735: Loss: 0.28832695513408557, accuracy: 0.9466666666666667\n",
      "iteration no 1736: Loss: 0.288279755659493, accuracy: 0.9433333333333334\n",
      "iteration no 1737: Loss: 0.2882342535365861, accuracy: 0.9466666666666667\n",
      "iteration no 1738: Loss: 0.2881859450582911, accuracy: 0.9433333333333334\n",
      "iteration no 1739: Loss: 0.2881393864541837, accuracy: 0.9466666666666667\n",
      "iteration no 1740: Loss: 0.2880899962688257, accuracy: 0.9433333333333334\n",
      "iteration no 1741: Loss: 0.2880458978085325, accuracy: 0.9466666666666667\n",
      "iteration no 1742: Loss: 0.287997880739441, accuracy: 0.95\n",
      "iteration no 1743: Loss: 0.2879551118495857, accuracy: 0.9466666666666667\n",
      "iteration no 1744: Loss: 0.2879085780984565, accuracy: 0.95\n",
      "iteration no 1745: Loss: 0.2878667551362008, accuracy: 0.9466666666666667\n",
      "iteration no 1746: Loss: 0.28781926075015246, accuracy: 0.95\n",
      "iteration no 1747: Loss: 0.287775473831051, accuracy: 0.9466666666666667\n",
      "iteration no 1748: Loss: 0.28772812038595236, accuracy: 0.95\n",
      "iteration no 1749: Loss: 0.28768523671805596, accuracy: 0.9466666666666667\n",
      "iteration no 1750: Loss: 0.2876394941834615, accuracy: 0.95\n",
      "iteration no 1751: Loss: 0.2876021025887005, accuracy: 0.9466666666666667\n",
      "iteration no 1752: Loss: 0.28755541973939364, accuracy: 0.95\n",
      "iteration no 1753: Loss: 0.2875130274320412, accuracy: 0.9466666666666667\n",
      "iteration no 1754: Loss: 0.287470100408459, accuracy: 0.95\n",
      "iteration no 1755: Loss: 0.2874284592218485, accuracy: 0.9466666666666667\n",
      "iteration no 1756: Loss: 0.28738513354471734, accuracy: 0.95\n",
      "iteration no 1757: Loss: 0.2873461592971451, accuracy: 0.9466666666666667\n",
      "iteration no 1758: Loss: 0.2873005695454334, accuracy: 0.95\n",
      "iteration no 1759: Loss: 0.2872586258367326, accuracy: 0.9466666666666667\n",
      "iteration no 1760: Loss: 0.2872147323145402, accuracy: 0.95\n",
      "iteration no 1761: Loss: 0.28717528275431853, accuracy: 0.9466666666666667\n",
      "iteration no 1762: Loss: 0.28712768443607906, accuracy: 0.95\n",
      "iteration no 1763: Loss: 0.28708730518594466, accuracy: 0.9466666666666667\n",
      "iteration no 1764: Loss: 0.287044007770142, accuracy: 0.95\n",
      "iteration no 1765: Loss: 0.28700343015774926, accuracy: 0.9466666666666667\n",
      "iteration no 1766: Loss: 0.2869585581523573, accuracy: 0.95\n",
      "iteration no 1767: Loss: 0.2869208117851286, accuracy: 0.9466666666666667\n",
      "iteration no 1768: Loss: 0.28687671583380175, accuracy: 0.95\n",
      "iteration no 1769: Loss: 0.28683891223540525, accuracy: 0.9466666666666667\n",
      "iteration no 1770: Loss: 0.2867897906331379, accuracy: 0.95\n",
      "iteration no 1771: Loss: 0.28675088764775863, accuracy: 0.9466666666666667\n",
      "iteration no 1772: Loss: 0.28670185122621283, accuracy: 0.9533333333333334\n",
      "iteration no 1773: Loss: 0.2866606711406755, accuracy: 0.9466666666666667\n",
      "iteration no 1774: Loss: 0.2866125706944314, accuracy: 0.9533333333333334\n",
      "iteration no 1775: Loss: 0.2865671163724731, accuracy: 0.9466666666666667\n",
      "iteration no 1776: Loss: 0.28652673437491977, accuracy: 0.9533333333333334\n",
      "iteration no 1777: Loss: 0.2864835914724909, accuracy: 0.9466666666666667\n",
      "iteration no 1778: Loss: 0.28644092712935587, accuracy: 0.9533333333333334\n",
      "iteration no 1779: Loss: 0.28639668227807374, accuracy: 0.9466666666666667\n",
      "iteration no 1780: Loss: 0.2863522483450033, accuracy: 0.9533333333333334\n",
      "iteration no 1781: Loss: 0.28631510512897757, accuracy: 0.9466666666666667\n",
      "iteration no 1782: Loss: 0.2862685358824154, accuracy: 0.9533333333333334\n",
      "iteration no 1783: Loss: 0.28622945672646505, accuracy: 0.9466666666666667\n",
      "iteration no 1784: Loss: 0.28618604426256194, accuracy: 0.9533333333333334\n",
      "iteration no 1785: Loss: 0.28614355454761986, accuracy: 0.9466666666666667\n",
      "iteration no 1786: Loss: 0.28610416287514895, accuracy: 0.9533333333333334\n",
      "iteration no 1787: Loss: 0.286058240912883, accuracy: 0.95\n",
      "iteration no 1788: Loss: 0.2860191243820782, accuracy: 0.9533333333333334\n",
      "iteration no 1789: Loss: 0.28597500256678404, accuracy: 0.95\n",
      "iteration no 1790: Loss: 0.2859256545080019, accuracy: 0.9533333333333334\n",
      "iteration no 1791: Loss: 0.28588209547120863, accuracy: 0.95\n",
      "iteration no 1792: Loss: 0.28583432335344267, accuracy: 0.9533333333333334\n",
      "iteration no 1793: Loss: 0.28578977248957527, accuracy: 0.95\n",
      "iteration no 1794: Loss: 0.2857437638427146, accuracy: 0.9533333333333334\n",
      "iteration no 1795: Loss: 0.2856962218485956, accuracy: 0.95\n",
      "iteration no 1796: Loss: 0.28565061870902636, accuracy: 0.9533333333333334\n",
      "iteration no 1797: Loss: 0.285611550166004, accuracy: 0.95\n",
      "iteration no 1798: Loss: 0.2855668122071846, accuracy: 0.9533333333333334\n",
      "iteration no 1799: Loss: 0.28552186393405943, accuracy: 0.95\n",
      "iteration no 1800: Loss: 0.28547774217072347, accuracy: 0.9533333333333334\n",
      "iteration no 1801: Loss: 0.2854338672784876, accuracy: 0.95\n",
      "iteration no 1802: Loss: 0.285389577614147, accuracy: 0.9533333333333334\n",
      "iteration no 1803: Loss: 0.28534556829010715, accuracy: 0.95\n",
      "iteration no 1804: Loss: 0.2853064784628055, accuracy: 0.9533333333333334\n",
      "iteration no 1805: Loss: 0.2852627983655516, accuracy: 0.95\n",
      "iteration no 1806: Loss: 0.28521943241907827, accuracy: 0.9533333333333334\n",
      "iteration no 1807: Loss: 0.28517421894329253, accuracy: 0.95\n",
      "iteration no 1808: Loss: 0.2851336699767826, accuracy: 0.9533333333333334\n",
      "iteration no 1809: Loss: 0.28509100106363006, accuracy: 0.95\n",
      "iteration no 1810: Loss: 0.2850477653738804, accuracy: 0.9533333333333334\n",
      "iteration no 1811: Loss: 0.2850088880567801, accuracy: 0.95\n",
      "iteration no 1812: Loss: 0.2849638664629417, accuracy: 0.9533333333333334\n",
      "iteration no 1813: Loss: 0.28492147304554183, accuracy: 0.95\n",
      "iteration no 1814: Loss: 0.2848764957099196, accuracy: 0.9533333333333334\n",
      "iteration no 1815: Loss: 0.28483589521425373, accuracy: 0.95\n",
      "iteration no 1816: Loss: 0.2847909116313021, accuracy: 0.9533333333333334\n",
      "iteration no 1817: Loss: 0.2847464980893299, accuracy: 0.95\n",
      "iteration no 1818: Loss: 0.28470552205914856, accuracy: 0.9533333333333334\n",
      "iteration no 1819: Loss: 0.2846605548114742, accuracy: 0.95\n",
      "iteration no 1820: Loss: 0.2846177572967969, accuracy: 0.9533333333333334\n",
      "iteration no 1821: Loss: 0.28457597955650055, accuracy: 0.95\n",
      "iteration no 1822: Loss: 0.28453852643519917, accuracy: 0.9533333333333334\n",
      "iteration no 1823: Loss: 0.284496935006859, accuracy: 0.95\n",
      "iteration no 1824: Loss: 0.2844527046488002, accuracy: 0.9533333333333334\n",
      "iteration no 1825: Loss: 0.2844115011976078, accuracy: 0.95\n",
      "iteration no 1826: Loss: 0.2843705708607117, accuracy: 0.9533333333333334\n",
      "iteration no 1827: Loss: 0.2843261147687204, accuracy: 0.95\n",
      "iteration no 1828: Loss: 0.2842835118999071, accuracy: 0.9533333333333334\n",
      "iteration no 1829: Loss: 0.2842434062585564, accuracy: 0.95\n",
      "iteration no 1830: Loss: 0.2842015897414541, accuracy: 0.9533333333333334\n",
      "iteration no 1831: Loss: 0.28416125257949304, accuracy: 0.95\n",
      "iteration no 1832: Loss: 0.28411457604936696, accuracy: 0.9533333333333334\n",
      "iteration no 1833: Loss: 0.2840775010328924, accuracy: 0.95\n",
      "iteration no 1834: Loss: 0.28403439920400475, accuracy: 0.9533333333333334\n",
      "iteration no 1835: Loss: 0.28398955331931647, accuracy: 0.95\n",
      "iteration no 1836: Loss: 0.28394741645083654, accuracy: 0.9533333333333334\n",
      "iteration no 1837: Loss: 0.2839080676513275, accuracy: 0.95\n",
      "iteration no 1838: Loss: 0.28386445840267494, accuracy: 0.9533333333333334\n",
      "iteration no 1839: Loss: 0.283822078387514, accuracy: 0.95\n",
      "iteration no 1840: Loss: 0.2837738866745445, accuracy: 0.9533333333333334\n",
      "iteration no 1841: Loss: 0.2837376992594683, accuracy: 0.95\n",
      "iteration no 1842: Loss: 0.28369015382133617, accuracy: 0.9533333333333334\n",
      "iteration no 1843: Loss: 0.28365165999403197, accuracy: 0.95\n",
      "iteration no 1844: Loss: 0.28360326095048805, accuracy: 0.9533333333333334\n",
      "iteration no 1845: Loss: 0.2835609597338271, accuracy: 0.95\n",
      "iteration no 1846: Loss: 0.28351829985212096, accuracy: 0.9533333333333334\n",
      "iteration no 1847: Loss: 0.28347616600092773, accuracy: 0.95\n",
      "iteration no 1848: Loss: 0.2834338909372622, accuracy: 0.9533333333333334\n",
      "iteration no 1849: Loss: 0.2833940616959261, accuracy: 0.95\n",
      "iteration no 1850: Loss: 0.2833484400356901, accuracy: 0.9533333333333334\n",
      "iteration no 1851: Loss: 0.2833106864702226, accuracy: 0.95\n",
      "iteration no 1852: Loss: 0.28326952788237203, accuracy: 0.9533333333333334\n",
      "iteration no 1853: Loss: 0.28323151893263004, accuracy: 0.95\n",
      "iteration no 1854: Loss: 0.2831842760289787, accuracy: 0.9533333333333334\n",
      "iteration no 1855: Loss: 0.2831463742005429, accuracy: 0.95\n",
      "iteration no 1856: Loss: 0.283102870921236, accuracy: 0.9533333333333334\n",
      "iteration no 1857: Loss: 0.28305957535876675, accuracy: 0.95\n",
      "iteration no 1858: Loss: 0.28301651940428907, accuracy: 0.9533333333333334\n",
      "iteration no 1859: Loss: 0.28297696942388606, accuracy: 0.95\n",
      "iteration no 1860: Loss: 0.2829351258198273, accuracy: 0.9533333333333334\n",
      "iteration no 1861: Loss: 0.2828928716662164, accuracy: 0.95\n",
      "iteration no 1862: Loss: 0.2828515107469776, accuracy: 0.9533333333333334\n",
      "iteration no 1863: Loss: 0.2828135840877184, accuracy: 0.95\n",
      "iteration no 1864: Loss: 0.2827719822044208, accuracy: 0.9533333333333334\n",
      "iteration no 1865: Loss: 0.2827314749044005, accuracy: 0.95\n",
      "iteration no 1866: Loss: 0.2826908778040003, accuracy: 0.9533333333333334\n",
      "iteration no 1867: Loss: 0.2826542563978393, accuracy: 0.95\n",
      "iteration no 1868: Loss: 0.28261141320125666, accuracy: 0.9533333333333334\n",
      "iteration no 1869: Loss: 0.28257113793517097, accuracy: 0.95\n",
      "iteration no 1870: Loss: 0.28253159180612697, accuracy: 0.9533333333333334\n",
      "iteration no 1871: Loss: 0.28249459428222623, accuracy: 0.95\n",
      "iteration no 1872: Loss: 0.2824507372025348, accuracy: 0.9533333333333334\n",
      "iteration no 1873: Loss: 0.2824142128703843, accuracy: 0.95\n",
      "iteration no 1874: Loss: 0.2823744558092064, accuracy: 0.9533333333333334\n",
      "iteration no 1875: Loss: 0.2823378821491737, accuracy: 0.95\n",
      "iteration no 1876: Loss: 0.282294126957181, accuracy: 0.9533333333333334\n",
      "iteration no 1877: Loss: 0.28225843024323394, accuracy: 0.95\n",
      "iteration no 1878: Loss: 0.28222005598247546, accuracy: 0.9533333333333334\n",
      "iteration no 1879: Loss: 0.28217775735596307, accuracy: 0.95\n",
      "iteration no 1880: Loss: 0.2821390106206574, accuracy: 0.9533333333333334\n",
      "iteration no 1881: Loss: 0.2821046730654211, accuracy: 0.95\n",
      "iteration no 1882: Loss: 0.2820670340236366, accuracy: 0.9533333333333334\n",
      "iteration no 1883: Loss: 0.2820276401436677, accuracy: 0.95\n",
      "iteration no 1884: Loss: 0.2819890235274407, accuracy: 0.9533333333333334\n",
      "iteration no 1885: Loss: 0.2819553941281484, accuracy: 0.95\n",
      "iteration no 1886: Loss: 0.281917798206873, accuracy: 0.9533333333333334\n",
      "iteration no 1887: Loss: 0.28188051005733916, accuracy: 0.95\n",
      "iteration no 1888: Loss: 0.2818405304842231, accuracy: 0.9533333333333334\n",
      "iteration no 1889: Loss: 0.2818095015236832, accuracy: 0.95\n",
      "iteration no 1890: Loss: 0.2817690462986091, accuracy: 0.9533333333333334\n",
      "iteration no 1891: Loss: 0.28173678107575845, accuracy: 0.95\n",
      "iteration no 1892: Loss: 0.28169689194240405, accuracy: 0.9533333333333334\n",
      "iteration no 1893: Loss: 0.2816669887338412, accuracy: 0.95\n",
      "iteration no 1894: Loss: 0.28162535180353504, accuracy: 0.9533333333333334\n",
      "iteration no 1895: Loss: 0.2815856785843434, accuracy: 0.9533333333333334\n",
      "iteration no 1896: Loss: 0.28155324702710355, accuracy: 0.9533333333333334\n",
      "iteration no 1897: Loss: 0.2815170173565741, accuracy: 0.9533333333333334\n",
      "iteration no 1898: Loss: 0.28148015950590816, accuracy: 0.9533333333333334\n",
      "iteration no 1899: Loss: 0.28144568041619467, accuracy: 0.9533333333333334\n",
      "iteration no 1900: Loss: 0.28141122966931215, accuracy: 0.9533333333333334\n",
      "iteration no 1901: Loss: 0.2813761515300418, accuracy: 0.9533333333333334\n",
      "iteration no 1902: Loss: 0.28133830625318257, accuracy: 0.9533333333333334\n",
      "iteration no 1903: Loss: 0.28130671849473937, accuracy: 0.9533333333333334\n",
      "iteration no 1904: Loss: 0.2812709474574069, accuracy: 0.9533333333333334\n",
      "iteration no 1905: Loss: 0.28123804361199184, accuracy: 0.9533333333333334\n",
      "iteration no 1906: Loss: 0.2811983470104173, accuracy: 0.9533333333333334\n",
      "iteration no 1907: Loss: 0.2811676005111463, accuracy: 0.9533333333333334\n",
      "iteration no 1908: Loss: 0.2811311370265892, accuracy: 0.9533333333333334\n",
      "iteration no 1909: Loss: 0.2810966887393471, accuracy: 0.9533333333333334\n",
      "iteration no 1910: Loss: 0.28106095250038665, accuracy: 0.9533333333333334\n",
      "iteration no 1911: Loss: 0.2810279703331038, accuracy: 0.9533333333333334\n",
      "iteration no 1912: Loss: 0.2809888856830931, accuracy: 0.9533333333333334\n",
      "iteration no 1913: Loss: 0.2809487014323546, accuracy: 0.9533333333333334\n",
      "iteration no 1914: Loss: 0.280912673209595, accuracy: 0.9533333333333334\n",
      "iteration no 1915: Loss: 0.2808757729801582, accuracy: 0.9533333333333334\n",
      "iteration no 1916: Loss: 0.28083594432600156, accuracy: 0.9533333333333334\n",
      "iteration no 1917: Loss: 0.2807997674748214, accuracy: 0.9533333333333334\n",
      "iteration no 1918: Loss: 0.28076239314592044, accuracy: 0.9533333333333334\n",
      "iteration no 1919: Loss: 0.28072570820244275, accuracy: 0.9533333333333334\n",
      "iteration no 1920: Loss: 0.2806842659856687, accuracy: 0.9533333333333334\n",
      "iteration no 1921: Loss: 0.2806503451762385, accuracy: 0.9533333333333334\n",
      "iteration no 1922: Loss: 0.2806113415200007, accuracy: 0.9533333333333334\n",
      "iteration no 1923: Loss: 0.28057193606340647, accuracy: 0.9533333333333334\n",
      "iteration no 1924: Loss: 0.2805320296633664, accuracy: 0.9533333333333334\n",
      "iteration no 1925: Loss: 0.28049510392546634, accuracy: 0.9533333333333334\n",
      "iteration no 1926: Loss: 0.28046019933425104, accuracy: 0.9533333333333334\n",
      "iteration no 1927: Loss: 0.28042184129455866, accuracy: 0.9533333333333334\n",
      "iteration no 1928: Loss: 0.28038335775739714, accuracy: 0.9533333333333334\n",
      "iteration no 1929: Loss: 0.2803487306459886, accuracy: 0.9533333333333334\n",
      "iteration no 1930: Loss: 0.28031109957739453, accuracy: 0.9533333333333334\n",
      "iteration no 1931: Loss: 0.280273738311786, accuracy: 0.9533333333333334\n",
      "iteration no 1932: Loss: 0.2802349642543254, accuracy: 0.9533333333333334\n",
      "iteration no 1933: Loss: 0.280203208042791, accuracy: 0.9533333333333334\n",
      "iteration no 1934: Loss: 0.2801626400805974, accuracy: 0.9533333333333334\n",
      "iteration no 1935: Loss: 0.28012718469638426, accuracy: 0.9533333333333334\n",
      "iteration no 1936: Loss: 0.28008991333876954, accuracy: 0.9533333333333334\n",
      "iteration no 1937: Loss: 0.2800563038570216, accuracy: 0.9533333333333334\n",
      "iteration no 1938: Loss: 0.2800159273367404, accuracy: 0.9533333333333334\n",
      "iteration no 1939: Loss: 0.27998040954720327, accuracy: 0.9533333333333334\n",
      "iteration no 1940: Loss: 0.2799446802724485, accuracy: 0.9533333333333334\n",
      "iteration no 1941: Loss: 0.2799092228050295, accuracy: 0.9533333333333334\n",
      "iteration no 1942: Loss: 0.2798710559792753, accuracy: 0.9533333333333334\n",
      "iteration no 1943: Loss: 0.2798361681001507, accuracy: 0.9533333333333334\n",
      "iteration no 1944: Loss: 0.2798002770048384, accuracy: 0.9533333333333334\n",
      "iteration no 1945: Loss: 0.27976089996313735, accuracy: 0.9533333333333334\n",
      "iteration no 1946: Loss: 0.27972528707903677, accuracy: 0.9533333333333334\n",
      "iteration no 1947: Loss: 0.2796926912796577, accuracy: 0.9533333333333334\n",
      "iteration no 1948: Loss: 0.27965660023156724, accuracy: 0.9533333333333334\n",
      "iteration no 1949: Loss: 0.27962043074083115, accuracy: 0.9533333333333334\n",
      "iteration no 1950: Loss: 0.2795853183770203, accuracy: 0.9533333333333334\n",
      "iteration no 1951: Loss: 0.2795506435985116, accuracy: 0.9533333333333334\n",
      "iteration no 1952: Loss: 0.27951348965405304, accuracy: 0.9533333333333334\n",
      "iteration no 1953: Loss: 0.27947961379671993, accuracy: 0.9533333333333334\n",
      "iteration no 1954: Loss: 0.27944638379757275, accuracy: 0.9533333333333334\n",
      "iteration no 1955: Loss: 0.2794124064804845, accuracy: 0.9533333333333334\n",
      "iteration no 1956: Loss: 0.27937428791360014, accuracy: 0.9533333333333334\n",
      "iteration no 1957: Loss: 0.2793428741689777, accuracy: 0.9533333333333334\n",
      "iteration no 1958: Loss: 0.27930771192772164, accuracy: 0.9533333333333334\n",
      "iteration no 1959: Loss: 0.279270288361935, accuracy: 0.9533333333333334\n",
      "iteration no 1960: Loss: 0.27923465110659074, accuracy: 0.9533333333333334\n",
      "iteration no 1961: Loss: 0.27920492793771673, accuracy: 0.9533333333333334\n",
      "iteration no 1962: Loss: 0.27916822335195146, accuracy: 0.9533333333333334\n",
      "iteration no 1963: Loss: 0.2791347198348702, accuracy: 0.9566666666666667\n",
      "iteration no 1964: Loss: 0.2790988371704475, accuracy: 0.9533333333333334\n",
      "iteration no 1965: Loss: 0.27906656338713554, accuracy: 0.9533333333333334\n",
      "iteration no 1966: Loss: 0.2790284993027943, accuracy: 0.9533333333333334\n",
      "iteration no 1967: Loss: 0.27899126471856056, accuracy: 0.9533333333333334\n",
      "iteration no 1968: Loss: 0.27896184432470594, accuracy: 0.9533333333333334\n",
      "iteration no 1969: Loss: 0.2789268998532589, accuracy: 0.9566666666666667\n",
      "iteration no 1970: Loss: 0.27889209341063115, accuracy: 0.9533333333333334\n",
      "iteration no 1971: Loss: 0.2788599848460406, accuracy: 0.9566666666666667\n",
      "iteration no 1972: Loss: 0.2788270750731734, accuracy: 0.9533333333333334\n",
      "iteration no 1973: Loss: 0.27879451231776115, accuracy: 0.9566666666666667\n",
      "iteration no 1974: Loss: 0.2787570390510475, accuracy: 0.9533333333333334\n",
      "iteration no 1975: Loss: 0.278729395318776, accuracy: 0.9566666666666667\n",
      "iteration no 1976: Loss: 0.2786934280270034, accuracy: 0.9533333333333334\n",
      "iteration no 1977: Loss: 0.27865631516837486, accuracy: 0.9566666666666667\n",
      "iteration no 1978: Loss: 0.27862203023791854, accuracy: 0.9533333333333334\n",
      "iteration no 1979: Loss: 0.2785922957508237, accuracy: 0.96\n",
      "iteration no 1980: Loss: 0.2785584403520379, accuracy: 0.9533333333333334\n",
      "iteration no 1981: Loss: 0.2785265111494592, accuracy: 0.96\n",
      "iteration no 1982: Loss: 0.27849433126789475, accuracy: 0.9533333333333334\n",
      "iteration no 1983: Loss: 0.2784616449533752, accuracy: 0.9633333333333334\n",
      "iteration no 1984: Loss: 0.2784270005715957, accuracy: 0.9533333333333334\n",
      "iteration no 1985: Loss: 0.27839290149684304, accuracy: 0.9633333333333334\n",
      "iteration no 1986: Loss: 0.2783643525630776, accuracy: 0.9533333333333334\n",
      "iteration no 1987: Loss: 0.27833047612191286, accuracy: 0.9633333333333334\n",
      "iteration no 1988: Loss: 0.278297301134847, accuracy: 0.9533333333333334\n",
      "iteration no 1989: Loss: 0.2782646168360035, accuracy: 0.9633333333333334\n",
      "iteration no 1990: Loss: 0.27823478593465767, accuracy: 0.9533333333333334\n",
      "iteration no 1991: Loss: 0.2782032211545401, accuracy: 0.9633333333333334\n",
      "iteration no 1992: Loss: 0.2781688210137416, accuracy: 0.9533333333333334\n",
      "iteration no 1993: Loss: 0.278140231374786, accuracy: 0.9633333333333334\n",
      "iteration no 1994: Loss: 0.2781045445969531, accuracy: 0.9533333333333334\n",
      "iteration no 1995: Loss: 0.27807076226205246, accuracy: 0.9633333333333334\n",
      "iteration no 1996: Loss: 0.2780346522353256, accuracy: 0.9533333333333334\n",
      "iteration no 1997: Loss: 0.27800459783725334, accuracy: 0.9633333333333334\n",
      "iteration no 1998: Loss: 0.27796969586331166, accuracy: 0.9566666666666667\n",
      "iteration no 1999: Loss: 0.27793469058258996, accuracy: 0.9633333333333334\n",
      "iteration no 2000: Loss: 0.27790360840206035, accuracy: 0.96\n",
      "iteration no 2001: Loss: 0.2778715079891351, accuracy: 0.9633333333333334\n",
      "iteration no 2002: Loss: 0.2778429764558436, accuracy: 0.9566666666666667\n",
      "iteration no 2003: Loss: 0.27781301265393304, accuracy: 0.9633333333333334\n",
      "iteration no 2004: Loss: 0.27778618143936745, accuracy: 0.9566666666666667\n",
      "iteration no 2005: Loss: 0.2777558121587521, accuracy: 0.9633333333333334\n",
      "iteration no 2006: Loss: 0.27772529488990344, accuracy: 0.9566666666666667\n",
      "iteration no 2007: Loss: 0.2776983416081522, accuracy: 0.9666666666666667\n",
      "iteration no 2008: Loss: 0.2776670924046954, accuracy: 0.96\n",
      "iteration no 2009: Loss: 0.2776358624294886, accuracy: 0.9666666666666667\n",
      "iteration no 2010: Loss: 0.2776053680733174, accuracy: 0.96\n",
      "iteration no 2011: Loss: 0.277573536816682, accuracy: 0.9666666666666667\n",
      "iteration no 2012: Loss: 0.2775437710530923, accuracy: 0.96\n",
      "iteration no 2013: Loss: 0.2775140772606949, accuracy: 0.9666666666666667\n",
      "iteration no 2014: Loss: 0.2774878763585525, accuracy: 0.96\n",
      "iteration no 2015: Loss: 0.277455955375601, accuracy: 0.9666666666666667\n",
      "iteration no 2016: Loss: 0.2774245848283101, accuracy: 0.96\n",
      "iteration no 2017: Loss: 0.2773982355454047, accuracy: 0.9666666666666667\n",
      "iteration no 2018: Loss: 0.2773672685997932, accuracy: 0.96\n",
      "iteration no 2019: Loss: 0.2773451643187641, accuracy: 0.9666666666666667\n",
      "iteration no 2020: Loss: 0.27731085152570484, accuracy: 0.96\n",
      "iteration no 2021: Loss: 0.2772857008031494, accuracy: 0.9666666666666667\n",
      "iteration no 2022: Loss: 0.2772545047068715, accuracy: 0.96\n",
      "iteration no 2023: Loss: 0.2772283630314187, accuracy: 0.9666666666666667\n",
      "iteration no 2024: Loss: 0.27719789210385837, accuracy: 0.96\n",
      "iteration no 2025: Loss: 0.27716948015071163, accuracy: 0.9666666666666667\n",
      "iteration no 2026: Loss: 0.27714020378843773, accuracy: 0.96\n",
      "iteration no 2027: Loss: 0.27711406239617403, accuracy: 0.9666666666666667\n",
      "iteration no 2028: Loss: 0.2770835976363972, accuracy: 0.96\n",
      "iteration no 2029: Loss: 0.2770582107792215, accuracy: 0.9666666666666667\n",
      "iteration no 2030: Loss: 0.2770271189876543, accuracy: 0.96\n",
      "iteration no 2031: Loss: 0.2769989976000424, accuracy: 0.9666666666666667\n",
      "iteration no 2032: Loss: 0.2769694990105631, accuracy: 0.96\n",
      "iteration no 2033: Loss: 0.27694410070519204, accuracy: 0.9666666666666667\n",
      "iteration no 2034: Loss: 0.27691326969409125, accuracy: 0.96\n",
      "iteration no 2035: Loss: 0.2768824928283003, accuracy: 0.9666666666666667\n",
      "iteration no 2036: Loss: 0.2768559392730502, accuracy: 0.96\n",
      "iteration no 2037: Loss: 0.2768267487316102, accuracy: 0.9666666666666667\n",
      "iteration no 2038: Loss: 0.27679915147477907, accuracy: 0.96\n",
      "iteration no 2039: Loss: 0.2767731811593848, accuracy: 0.9666666666666667\n",
      "iteration no 2040: Loss: 0.2767432894178462, accuracy: 0.96\n",
      "iteration no 2041: Loss: 0.2767139283025626, accuracy: 0.9666666666666667\n",
      "iteration no 2042: Loss: 0.276686154818872, accuracy: 0.96\n",
      "iteration no 2043: Loss: 0.27665925895346344, accuracy: 0.9666666666666667\n",
      "iteration no 2044: Loss: 0.2766293776941426, accuracy: 0.96\n",
      "iteration no 2045: Loss: 0.2765999729173609, accuracy: 0.9666666666666667\n",
      "iteration no 2046: Loss: 0.2765734613548845, accuracy: 0.96\n",
      "iteration no 2047: Loss: 0.27654333447241175, accuracy: 0.9666666666666667\n",
      "iteration no 2048: Loss: 0.2765164617095191, accuracy: 0.96\n",
      "iteration no 2049: Loss: 0.2764867885067751, accuracy: 0.9666666666666667\n",
      "iteration no 2050: Loss: 0.2764609600834802, accuracy: 0.96\n",
      "iteration no 2051: Loss: 0.2764320715403457, accuracy: 0.9666666666666667\n",
      "iteration no 2052: Loss: 0.2764049193060045, accuracy: 0.96\n",
      "iteration no 2053: Loss: 0.27637943817166233, accuracy: 0.9666666666666667\n",
      "iteration no 2054: Loss: 0.2763503771481183, accuracy: 0.96\n",
      "iteration no 2055: Loss: 0.27631804118659153, accuracy: 0.9666666666666667\n",
      "iteration no 2056: Loss: 0.276294148904855, accuracy: 0.96\n",
      "iteration no 2057: Loss: 0.2762627751803436, accuracy: 0.9666666666666667\n",
      "iteration no 2058: Loss: 0.2762370734890271, accuracy: 0.96\n",
      "iteration no 2059: Loss: 0.27620651174627825, accuracy: 0.9666666666666667\n",
      "iteration no 2060: Loss: 0.27618197102522957, accuracy: 0.96\n",
      "iteration no 2061: Loss: 0.2761548626702736, accuracy: 0.9666666666666667\n",
      "iteration no 2062: Loss: 0.2761272611725269, accuracy: 0.96\n",
      "iteration no 2063: Loss: 0.27609838320577673, accuracy: 0.9666666666666667\n",
      "iteration no 2064: Loss: 0.2760729106000094, accuracy: 0.9633333333333334\n",
      "iteration no 2065: Loss: 0.2760404872157314, accuracy: 0.9666666666666667\n",
      "iteration no 2066: Loss: 0.27601613572690326, accuracy: 0.9633333333333334\n",
      "iteration no 2067: Loss: 0.27598569461733186, accuracy: 0.9666666666666667\n",
      "iteration no 2068: Loss: 0.2759609843076253, accuracy: 0.96\n",
      "iteration no 2069: Loss: 0.2759325853759843, accuracy: 0.9666666666666667\n",
      "iteration no 2070: Loss: 0.2759061977511811, accuracy: 0.96\n",
      "iteration no 2071: Loss: 0.2758792912225868, accuracy: 0.9666666666666667\n",
      "iteration no 2072: Loss: 0.27585125564354096, accuracy: 0.9633333333333334\n",
      "iteration no 2073: Loss: 0.2758213389466365, accuracy: 0.9666666666666667\n",
      "iteration no 2074: Loss: 0.2757948284195656, accuracy: 0.9666666666666667\n",
      "iteration no 2075: Loss: 0.27576342977816826, accuracy: 0.9666666666666667\n",
      "iteration no 2076: Loss: 0.2757407540712489, accuracy: 0.9633333333333334\n",
      "iteration no 2077: Loss: 0.27571272039820227, accuracy: 0.9666666666666667\n",
      "iteration no 2078: Loss: 0.27568767911618725, accuracy: 0.9633333333333334\n",
      "iteration no 2079: Loss: 0.27566091556407774, accuracy: 0.9666666666666667\n",
      "iteration no 2080: Loss: 0.2756351416301135, accuracy: 0.9666666666666667\n",
      "iteration no 2081: Loss: 0.27560344314062585, accuracy: 0.9666666666666667\n",
      "iteration no 2082: Loss: 0.27557975018601566, accuracy: 0.9666666666666667\n",
      "iteration no 2083: Loss: 0.27555233760786035, accuracy: 0.9666666666666667\n",
      "iteration no 2084: Loss: 0.27552761737198134, accuracy: 0.9666666666666667\n",
      "iteration no 2085: Loss: 0.27549563140596073, accuracy: 0.9666666666666667\n",
      "iteration no 2086: Loss: 0.27547416532420743, accuracy: 0.9666666666666667\n",
      "iteration no 2087: Loss: 0.275442265847595, accuracy: 0.9666666666666667\n",
      "iteration no 2088: Loss: 0.2754179897990989, accuracy: 0.9666666666666667\n",
      "iteration no 2089: Loss: 0.27538947925764123, accuracy: 0.9666666666666667\n",
      "iteration no 2090: Loss: 0.27536413289386946, accuracy: 0.9666666666666667\n",
      "iteration no 2091: Loss: 0.2753376431085187, accuracy: 0.9666666666666667\n",
      "iteration no 2092: Loss: 0.27530979026279856, accuracy: 0.9666666666666667\n",
      "iteration no 2093: Loss: 0.27528180461655655, accuracy: 0.9666666666666667\n",
      "iteration no 2094: Loss: 0.27525540875627863, accuracy: 0.9666666666666667\n",
      "iteration no 2095: Loss: 0.27522840590475994, accuracy: 0.9666666666666667\n",
      "iteration no 2096: Loss: 0.2752012567985573, accuracy: 0.9666666666666667\n",
      "iteration no 2097: Loss: 0.27517519952526726, accuracy: 0.9666666666666667\n",
      "iteration no 2098: Loss: 0.2751470682157107, accuracy: 0.9666666666666667\n",
      "iteration no 2099: Loss: 0.2751213605299675, accuracy: 0.9666666666666667\n",
      "iteration no 2100: Loss: 0.27509349285353374, accuracy: 0.9666666666666667\n",
      "iteration no 2101: Loss: 0.27506725191947745, accuracy: 0.9666666666666667\n",
      "iteration no 2102: Loss: 0.27503983724699366, accuracy: 0.9666666666666667\n",
      "iteration no 2103: Loss: 0.27501563522927974, accuracy: 0.9666666666666667\n",
      "iteration no 2104: Loss: 0.27498723178089, accuracy: 0.9666666666666667\n",
      "iteration no 2105: Loss: 0.27496106959934824, accuracy: 0.9666666666666667\n",
      "iteration no 2106: Loss: 0.2749323417625508, accuracy: 0.9666666666666667\n",
      "iteration no 2107: Loss: 0.274909079237584, accuracy: 0.9666666666666667\n",
      "iteration no 2108: Loss: 0.274879324197036, accuracy: 0.9666666666666667\n",
      "iteration no 2109: Loss: 0.2748542266306133, accuracy: 0.9666666666666667\n",
      "iteration no 2110: Loss: 0.2748257077408693, accuracy: 0.9666666666666667\n",
      "iteration no 2111: Loss: 0.2748038562265137, accuracy: 0.9666666666666667\n",
      "iteration no 2112: Loss: 0.27477435369872094, accuracy: 0.9666666666666667\n",
      "iteration no 2113: Loss: 0.2747511950195759, accuracy: 0.9666666666666667\n",
      "iteration no 2114: Loss: 0.2747211545439823, accuracy: 0.9666666666666667\n",
      "iteration no 2115: Loss: 0.27469911225829075, accuracy: 0.9666666666666667\n",
      "iteration no 2116: Loss: 0.2746696460967567, accuracy: 0.9666666666666667\n",
      "iteration no 2117: Loss: 0.2746474812258307, accuracy: 0.9666666666666667\n",
      "iteration no 2118: Loss: 0.2746183702822187, accuracy: 0.9666666666666667\n",
      "iteration no 2119: Loss: 0.27459539878096195, accuracy: 0.9666666666666667\n",
      "iteration no 2120: Loss: 0.2745702237965511, accuracy: 0.9666666666666667\n",
      "iteration no 2121: Loss: 0.2745441433923266, accuracy: 0.9666666666666667\n",
      "iteration no 2122: Loss: 0.27452030369163993, accuracy: 0.9666666666666667\n",
      "iteration no 2123: Loss: 0.27449120896709334, accuracy: 0.9666666666666667\n",
      "iteration no 2124: Loss: 0.27447022756218814, accuracy: 0.9666666666666667\n",
      "iteration no 2125: Loss: 0.27444064854746414, accuracy: 0.9666666666666667\n",
      "iteration no 2126: Loss: 0.2744162723384576, accuracy: 0.9666666666666667\n",
      "iteration no 2127: Loss: 0.27439082333885345, accuracy: 0.9666666666666667\n",
      "iteration no 2128: Loss: 0.2743633100557148, accuracy: 0.9666666666666667\n",
      "iteration no 2129: Loss: 0.27434048328107435, accuracy: 0.9666666666666667\n",
      "iteration no 2130: Loss: 0.2743112270201396, accuracy: 0.9666666666666667\n",
      "iteration no 2131: Loss: 0.2742899537967383, accuracy: 0.9666666666666667\n",
      "iteration no 2132: Loss: 0.27426255678602274, accuracy: 0.9666666666666667\n",
      "iteration no 2133: Loss: 0.2742391057390381, accuracy: 0.9666666666666667\n",
      "iteration no 2134: Loss: 0.27421175806558107, accuracy: 0.9666666666666667\n",
      "iteration no 2135: Loss: 0.27419004575437633, accuracy: 0.9666666666666667\n",
      "iteration no 2136: Loss: 0.27416363603924737, accuracy: 0.9666666666666667\n",
      "iteration no 2137: Loss: 0.2741378294018233, accuracy: 0.9666666666666667\n",
      "iteration no 2138: Loss: 0.27411474734458124, accuracy: 0.9666666666666667\n",
      "iteration no 2139: Loss: 0.2740903800743798, accuracy: 0.9666666666666667\n",
      "iteration no 2140: Loss: 0.2740661203695577, accuracy: 0.9666666666666667\n",
      "iteration no 2141: Loss: 0.2740391506879677, accuracy: 0.9666666666666667\n",
      "iteration no 2142: Loss: 0.2740156407941427, accuracy: 0.9666666666666667\n",
      "iteration no 2143: Loss: 0.27398956806513614, accuracy: 0.9666666666666667\n",
      "iteration no 2144: Loss: 0.27396486075143983, accuracy: 0.9666666666666667\n",
      "iteration no 2145: Loss: 0.27393875428843684, accuracy: 0.9666666666666667\n",
      "iteration no 2146: Loss: 0.2739150450158787, accuracy: 0.9666666666666667\n",
      "iteration no 2147: Loss: 0.2738892741760704, accuracy: 0.9666666666666667\n",
      "iteration no 2148: Loss: 0.2738630433227019, accuracy: 0.9666666666666667\n",
      "iteration no 2149: Loss: 0.27383797369848195, accuracy: 0.9666666666666667\n",
      "iteration no 2150: Loss: 0.2738119208201395, accuracy: 0.9666666666666667\n",
      "iteration no 2151: Loss: 0.27378655256075685, accuracy: 0.9666666666666667\n",
      "iteration no 2152: Loss: 0.2737605944201719, accuracy: 0.9666666666666667\n",
      "iteration no 2153: Loss: 0.27373633383668805, accuracy: 0.9666666666666667\n",
      "iteration no 2154: Loss: 0.2737087317420739, accuracy: 0.9666666666666667\n",
      "iteration no 2155: Loss: 0.27368614981436895, accuracy: 0.9666666666666667\n",
      "iteration no 2156: Loss: 0.2736567136849128, accuracy: 0.9666666666666667\n",
      "iteration no 2157: Loss: 0.2736343786722961, accuracy: 0.9666666666666667\n",
      "iteration no 2158: Loss: 0.2736056399866714, accuracy: 0.9666666666666667\n",
      "iteration no 2159: Loss: 0.27358419857516025, accuracy: 0.9666666666666667\n",
      "iteration no 2160: Loss: 0.27355589191640584, accuracy: 0.9666666666666667\n",
      "iteration no 2161: Loss: 0.27353394660698305, accuracy: 0.9666666666666667\n",
      "iteration no 2162: Loss: 0.27350511713673076, accuracy: 0.9666666666666667\n",
      "iteration no 2163: Loss: 0.27348246512316854, accuracy: 0.9666666666666667\n",
      "iteration no 2164: Loss: 0.2734563938179243, accuracy: 0.9666666666666667\n",
      "iteration no 2165: Loss: 0.27343034385826387, accuracy: 0.9666666666666667\n",
      "iteration no 2166: Loss: 0.27340522277079565, accuracy: 0.9666666666666667\n",
      "iteration no 2167: Loss: 0.27338063280500546, accuracy: 0.9666666666666667\n",
      "iteration no 2168: Loss: 0.27335590497415485, accuracy: 0.9666666666666667\n",
      "iteration no 2169: Loss: 0.2733290577072651, accuracy: 0.9666666666666667\n",
      "iteration no 2170: Loss: 0.273305921859747, accuracy: 0.9666666666666667\n",
      "iteration no 2171: Loss: 0.27327738483414676, accuracy: 0.9666666666666667\n",
      "iteration no 2172: Loss: 0.27325499473794523, accuracy: 0.9666666666666667\n",
      "iteration no 2173: Loss: 0.2732255202312105, accuracy: 0.9666666666666667\n",
      "iteration no 2174: Loss: 0.2732032005247745, accuracy: 0.9666666666666667\n",
      "iteration no 2175: Loss: 0.2731727659776153, accuracy: 0.9666666666666667\n",
      "iteration no 2176: Loss: 0.27315194084957845, accuracy: 0.9666666666666667\n",
      "iteration no 2177: Loss: 0.27312185379447607, accuracy: 0.9666666666666667\n",
      "iteration no 2178: Loss: 0.27309916212759866, accuracy: 0.9666666666666667\n",
      "iteration no 2179: Loss: 0.2730701197753074, accuracy: 0.9666666666666667\n",
      "iteration no 2180: Loss: 0.2730462262556917, accuracy: 0.9666666666666667\n",
      "iteration no 2181: Loss: 0.2730178754425465, accuracy: 0.9666666666666667\n",
      "iteration no 2182: Loss: 0.2729944316881992, accuracy: 0.9666666666666667\n",
      "iteration no 2183: Loss: 0.272966714257507, accuracy: 0.9666666666666667\n",
      "iteration no 2184: Loss: 0.2729433225525701, accuracy: 0.9666666666666667\n",
      "iteration no 2185: Loss: 0.27291754236833626, accuracy: 0.9666666666666667\n",
      "iteration no 2186: Loss: 0.27289304861769825, accuracy: 0.9666666666666667\n",
      "iteration no 2187: Loss: 0.27286549365723545, accuracy: 0.9666666666666667\n",
      "iteration no 2188: Loss: 0.2728421773687002, accuracy: 0.9666666666666667\n",
      "iteration no 2189: Loss: 0.2728141311112825, accuracy: 0.9666666666666667\n",
      "iteration no 2190: Loss: 0.2727899638838022, accuracy: 0.9666666666666667\n",
      "iteration no 2191: Loss: 0.27276489037322466, accuracy: 0.9666666666666667\n",
      "iteration no 2192: Loss: 0.27273850271125705, accuracy: 0.9666666666666667\n",
      "iteration no 2193: Loss: 0.2727132964576277, accuracy: 0.9666666666666667\n",
      "iteration no 2194: Loss: 0.27268756343256767, accuracy: 0.9666666666666667\n",
      "iteration no 2195: Loss: 0.27266149223558084, accuracy: 0.9666666666666667\n",
      "iteration no 2196: Loss: 0.27263679963513476, accuracy: 0.9666666666666667\n",
      "iteration no 2197: Loss: 0.27261305365562294, accuracy: 0.9666666666666667\n",
      "iteration no 2198: Loss: 0.27258601896032675, accuracy: 0.9666666666666667\n",
      "iteration no 2199: Loss: 0.27256243219158355, accuracy: 0.9666666666666667\n",
      "iteration no 2200: Loss: 0.2725357042146856, accuracy: 0.9666666666666667\n",
      "iteration no 2201: Loss: 0.272510192127577, accuracy: 0.9666666666666667\n",
      "iteration no 2202: Loss: 0.2724836215887728, accuracy: 0.9666666666666667\n",
      "iteration no 2203: Loss: 0.2724604379210324, accuracy: 0.9666666666666667\n",
      "iteration no 2204: Loss: 0.2724308871371807, accuracy: 0.9666666666666667\n",
      "iteration no 2205: Loss: 0.2724075170229153, accuracy: 0.9666666666666667\n",
      "iteration no 2206: Loss: 0.27237759918478305, accuracy: 0.9666666666666667\n",
      "iteration no 2207: Loss: 0.2723550196177152, accuracy: 0.9666666666666667\n",
      "iteration no 2208: Loss: 0.2723240332793701, accuracy: 0.9666666666666667\n",
      "iteration no 2209: Loss: 0.27230112439288096, accuracy: 0.9666666666666667\n",
      "iteration no 2210: Loss: 0.2722714404242807, accuracy: 0.9666666666666667\n",
      "iteration no 2211: Loss: 0.2722478955178445, accuracy: 0.9666666666666667\n",
      "iteration no 2212: Loss: 0.27221873809893804, accuracy: 0.9666666666666667\n",
      "iteration no 2213: Loss: 0.2721948040692137, accuracy: 0.9666666666666667\n",
      "iteration no 2214: Loss: 0.2721658398297926, accuracy: 0.9666666666666667\n",
      "iteration no 2215: Loss: 0.27214032462911164, accuracy: 0.9666666666666667\n",
      "iteration no 2216: Loss: 0.2721142827309196, accuracy: 0.9666666666666667\n",
      "iteration no 2217: Loss: 0.2720870750182139, accuracy: 0.9666666666666667\n",
      "iteration no 2218: Loss: 0.27206306768246724, accuracy: 0.9666666666666667\n",
      "iteration no 2219: Loss: 0.27203395888706766, accuracy: 0.9666666666666667\n",
      "iteration no 2220: Loss: 0.27200969777978856, accuracy: 0.9666666666666667\n",
      "iteration no 2221: Loss: 0.27198129703231094, accuracy: 0.9666666666666667\n",
      "iteration no 2222: Loss: 0.2719579262845186, accuracy: 0.9666666666666667\n",
      "iteration no 2223: Loss: 0.27192804595238423, accuracy: 0.9666666666666667\n",
      "iteration no 2224: Loss: 0.27190571200288677, accuracy: 0.9666666666666667\n",
      "iteration no 2225: Loss: 0.2718762039865591, accuracy: 0.9666666666666667\n",
      "iteration no 2226: Loss: 0.2718538616180777, accuracy: 0.9666666666666667\n",
      "iteration no 2227: Loss: 0.27182519018769175, accuracy: 0.9666666666666667\n",
      "iteration no 2228: Loss: 0.2718013250070129, accuracy: 0.9666666666666667\n",
      "iteration no 2229: Loss: 0.2717729899629503, accuracy: 0.9666666666666667\n",
      "iteration no 2230: Loss: 0.2717503135161665, accuracy: 0.9666666666666667\n",
      "iteration no 2231: Loss: 0.2717221355468459, accuracy: 0.9666666666666667\n",
      "iteration no 2232: Loss: 0.27169828012251496, accuracy: 0.9666666666666667\n",
      "iteration no 2233: Loss: 0.2716735232590521, accuracy: 0.9666666666666667\n",
      "iteration no 2234: Loss: 0.2716475730722078, accuracy: 0.9666666666666667\n",
      "iteration no 2235: Loss: 0.2716195307647648, accuracy: 0.9666666666666667\n",
      "iteration no 2236: Loss: 0.27159712462921626, accuracy: 0.9666666666666667\n",
      "iteration no 2237: Loss: 0.2715703967465128, accuracy: 0.9666666666666667\n",
      "iteration no 2238: Loss: 0.27154524440840644, accuracy: 0.9666666666666667\n",
      "iteration no 2239: Loss: 0.2715209433536698, accuracy: 0.9666666666666667\n",
      "iteration no 2240: Loss: 0.27149488010808104, accuracy: 0.9666666666666667\n",
      "iteration no 2241: Loss: 0.27146926764681256, accuracy: 0.9666666666666667\n",
      "iteration no 2242: Loss: 0.27144451455300284, accuracy: 0.9666666666666667\n",
      "iteration no 2243: Loss: 0.27141922912383004, accuracy: 0.9666666666666667\n",
      "iteration no 2244: Loss: 0.27139302231332696, accuracy: 0.9666666666666667\n",
      "iteration no 2245: Loss: 0.2713679304855333, accuracy: 0.9666666666666667\n",
      "iteration no 2246: Loss: 0.27134159438851296, accuracy: 0.9666666666666667\n",
      "iteration no 2247: Loss: 0.2713174069986992, accuracy: 0.9666666666666667\n",
      "iteration no 2248: Loss: 0.27128877547531804, accuracy: 0.9666666666666667\n",
      "iteration no 2249: Loss: 0.27126688185136527, accuracy: 0.9666666666666667\n",
      "iteration no 2250: Loss: 0.27123803693106385, accuracy: 0.9666666666666667\n",
      "iteration no 2251: Loss: 0.2712150337744386, accuracy: 0.9666666666666667\n",
      "iteration no 2252: Loss: 0.2711870910374441, accuracy: 0.9666666666666667\n",
      "iteration no 2253: Loss: 0.2711649950175419, accuracy: 0.9666666666666667\n",
      "iteration no 2254: Loss: 0.2711361443028748, accuracy: 0.9666666666666667\n",
      "iteration no 2255: Loss: 0.2711154685335582, accuracy: 0.9666666666666667\n",
      "iteration no 2256: Loss: 0.2710858239495439, accuracy: 0.9666666666666667\n",
      "iteration no 2257: Loss: 0.2710662309131156, accuracy: 0.9666666666666667\n",
      "iteration no 2258: Loss: 0.27103679313003093, accuracy: 0.9666666666666667\n",
      "iteration no 2259: Loss: 0.27101570013857657, accuracy: 0.9666666666666667\n",
      "iteration no 2260: Loss: 0.270986059198419, accuracy: 0.9666666666666667\n",
      "iteration no 2261: Loss: 0.2709624572585572, accuracy: 0.9666666666666667\n",
      "iteration no 2262: Loss: 0.2709359041264819, accuracy: 0.9666666666666667\n",
      "iteration no 2263: Loss: 0.2709117680599021, accuracy: 0.9666666666666667\n",
      "iteration no 2264: Loss: 0.2708846784838722, accuracy: 0.9666666666666667\n",
      "iteration no 2265: Loss: 0.27086242015077683, accuracy: 0.9666666666666667\n",
      "iteration no 2266: Loss: 0.27083487854842714, accuracy: 0.9666666666666667\n",
      "iteration no 2267: Loss: 0.27080946762013214, accuracy: 0.9666666666666667\n",
      "iteration no 2268: Loss: 0.2707857969900195, accuracy: 0.9666666666666667\n",
      "iteration no 2269: Loss: 0.27075936018215147, accuracy: 0.9666666666666667\n",
      "iteration no 2270: Loss: 0.27073573964471714, accuracy: 0.9666666666666667\n",
      "iteration no 2271: Loss: 0.2707082453977031, accuracy: 0.9666666666666667\n",
      "iteration no 2272: Loss: 0.270685119454197, accuracy: 0.9666666666666667\n",
      "iteration no 2273: Loss: 0.2706569255268477, accuracy: 0.9666666666666667\n",
      "iteration no 2274: Loss: 0.2706330702214687, accuracy: 0.9666666666666667\n",
      "iteration no 2275: Loss: 0.2706079514689314, accuracy: 0.9666666666666667\n",
      "iteration no 2276: Loss: 0.2705821133538407, accuracy: 0.9666666666666667\n",
      "iteration no 2277: Loss: 0.27055971151044744, accuracy: 0.9666666666666667\n",
      "iteration no 2278: Loss: 0.270531603328632, accuracy: 0.9666666666666667\n",
      "iteration no 2279: Loss: 0.27050754725403664, accuracy: 0.9666666666666667\n",
      "iteration no 2280: Loss: 0.27048113854549377, accuracy: 0.9666666666666667\n",
      "iteration no 2281: Loss: 0.2704550046120466, accuracy: 0.9666666666666667\n",
      "iteration no 2282: Loss: 0.2704308645629491, accuracy: 0.9666666666666667\n",
      "iteration no 2283: Loss: 0.2704046837288624, accuracy: 0.9666666666666667\n",
      "iteration no 2284: Loss: 0.2703826190448012, accuracy: 0.9666666666666667\n",
      "iteration no 2285: Loss: 0.27035534066831696, accuracy: 0.9666666666666667\n",
      "iteration no 2286: Loss: 0.27033057965423857, accuracy: 0.9666666666666667\n",
      "iteration no 2287: Loss: 0.2703084882454706, accuracy: 0.9666666666666667\n",
      "iteration no 2288: Loss: 0.2702801077778147, accuracy: 0.9666666666666667\n",
      "iteration no 2289: Loss: 0.2702572947283762, accuracy: 0.9666666666666667\n",
      "iteration no 2290: Loss: 0.2702312404456327, accuracy: 0.9666666666666667\n",
      "iteration no 2291: Loss: 0.2702046028054227, accuracy: 0.97\n",
      "iteration no 2292: Loss: 0.2701815941609471, accuracy: 0.9666666666666667\n",
      "iteration no 2293: Loss: 0.2701543354769613, accuracy: 0.9666666666666667\n",
      "iteration no 2294: Loss: 0.2701316131133818, accuracy: 0.9666666666666667\n",
      "iteration no 2295: Loss: 0.2701045334490836, accuracy: 0.9666666666666667\n",
      "iteration no 2296: Loss: 0.27007882581340137, accuracy: 0.9666666666666667\n",
      "iteration no 2297: Loss: 0.27005453717367134, accuracy: 0.9666666666666667\n",
      "iteration no 2298: Loss: 0.2700274853917036, accuracy: 0.9666666666666667\n",
      "iteration no 2299: Loss: 0.2700038948109014, accuracy: 0.97\n",
      "iteration no 2300: Loss: 0.2699783102830527, accuracy: 0.9666666666666667\n",
      "iteration no 2301: Loss: 0.26995066579943217, accuracy: 0.97\n",
      "iteration no 2302: Loss: 0.2699286825733975, accuracy: 0.9666666666666667\n",
      "iteration no 2303: Loss: 0.2699007758131099, accuracy: 0.97\n",
      "iteration no 2304: Loss: 0.2698767976987378, accuracy: 0.97\n",
      "iteration no 2305: Loss: 0.2698506202603109, accuracy: 0.97\n",
      "iteration no 2306: Loss: 0.26982475215653745, accuracy: 0.97\n",
      "iteration no 2307: Loss: 0.269801501189485, accuracy: 0.97\n",
      "iteration no 2308: Loss: 0.2697737300559544, accuracy: 0.97\n",
      "iteration no 2309: Loss: 0.26975041280408923, accuracy: 0.97\n",
      "iteration no 2310: Loss: 0.2697235791379973, accuracy: 0.97\n",
      "iteration no 2311: Loss: 0.2696961734491182, accuracy: 0.97\n",
      "iteration no 2312: Loss: 0.26967481555838474, accuracy: 0.97\n",
      "iteration no 2313: Loss: 0.26964596106349903, accuracy: 0.97\n",
      "iteration no 2314: Loss: 0.26962216009811724, accuracy: 0.97\n",
      "iteration no 2315: Loss: 0.2695989154046036, accuracy: 0.97\n",
      "iteration no 2316: Loss: 0.26957063333207065, accuracy: 0.97\n",
      "iteration no 2317: Loss: 0.2695472275622516, accuracy: 0.97\n",
      "iteration no 2318: Loss: 0.2695198271344896, accuracy: 0.97\n",
      "iteration no 2319: Loss: 0.2694940787270822, accuracy: 0.97\n",
      "iteration no 2320: Loss: 0.26946887532280395, accuracy: 0.97\n",
      "iteration no 2321: Loss: 0.2694421583884771, accuracy: 0.97\n",
      "iteration no 2322: Loss: 0.2694176044506622, accuracy: 0.97\n",
      "iteration no 2323: Loss: 0.26939005016476864, accuracy: 0.97\n",
      "iteration no 2324: Loss: 0.26936422922946945, accuracy: 0.97\n",
      "iteration no 2325: Loss: 0.26934051188391317, accuracy: 0.97\n",
      "iteration no 2326: Loss: 0.2693136055414087, accuracy: 0.97\n",
      "iteration no 2327: Loss: 0.2692902830960151, accuracy: 0.97\n",
      "iteration no 2328: Loss: 0.26926454869130734, accuracy: 0.97\n",
      "iteration no 2329: Loss: 0.2692394213229522, accuracy: 0.97\n",
      "iteration no 2330: Loss: 0.26921607179966633, accuracy: 0.97\n",
      "iteration no 2331: Loss: 0.2691912164402263, accuracy: 0.97\n",
      "iteration no 2332: Loss: 0.26916774453550457, accuracy: 0.97\n",
      "iteration no 2333: Loss: 0.2691423291255325, accuracy: 0.97\n",
      "iteration no 2334: Loss: 0.2691169606519213, accuracy: 0.97\n",
      "iteration no 2335: Loss: 0.26909417410566083, accuracy: 0.97\n",
      "iteration no 2336: Loss: 0.2690679369964278, accuracy: 0.97\n",
      "iteration no 2337: Loss: 0.26904687519862175, accuracy: 0.97\n",
      "iteration no 2338: Loss: 0.2690199018768635, accuracy: 0.97\n",
      "iteration no 2339: Loss: 0.2689982150473101, accuracy: 0.97\n",
      "iteration no 2340: Loss: 0.26897164523197586, accuracy: 0.97\n",
      "iteration no 2341: Loss: 0.2689481251775298, accuracy: 0.97\n",
      "iteration no 2342: Loss: 0.26892360076645283, accuracy: 0.97\n",
      "iteration no 2343: Loss: 0.2688988581394149, accuracy: 0.97\n",
      "iteration no 2344: Loss: 0.2688754786252293, accuracy: 0.97\n",
      "iteration no 2345: Loss: 0.2688498425351263, accuracy: 0.97\n",
      "iteration no 2346: Loss: 0.26882853997188005, accuracy: 0.97\n",
      "iteration no 2347: Loss: 0.26879983268148844, accuracy: 0.97\n",
      "iteration no 2348: Loss: 0.26877880195031584, accuracy: 0.97\n",
      "iteration no 2349: Loss: 0.268751415999733, accuracy: 0.97\n",
      "iteration no 2350: Loss: 0.2687295456722645, accuracy: 0.97\n",
      "iteration no 2351: Loss: 0.2687027078322119, accuracy: 0.97\n",
      "iteration no 2352: Loss: 0.268678553411252, accuracy: 0.97\n",
      "iteration no 2353: Loss: 0.26865638173538176, accuracy: 0.97\n",
      "iteration no 2354: Loss: 0.2686278888613617, accuracy: 0.97\n",
      "iteration no 2355: Loss: 0.2686101078238358, accuracy: 0.97\n",
      "iteration no 2356: Loss: 0.26858028025804875, accuracy: 0.97\n",
      "iteration no 2357: Loss: 0.26855964691308665, accuracy: 0.97\n",
      "iteration no 2358: Loss: 0.26853292769340165, accuracy: 0.97\n",
      "iteration no 2359: Loss: 0.2685104800117119, accuracy: 0.97\n",
      "iteration no 2360: Loss: 0.26848827539038445, accuracy: 0.97\n",
      "iteration no 2361: Loss: 0.26845998299349816, accuracy: 0.97\n",
      "iteration no 2362: Loss: 0.2684412446822323, accuracy: 0.97\n",
      "iteration no 2363: Loss: 0.26841300755906256, accuracy: 0.97\n",
      "iteration no 2364: Loss: 0.26839368885831194, accuracy: 0.97\n",
      "iteration no 2365: Loss: 0.2683672436257919, accuracy: 0.97\n",
      "iteration no 2366: Loss: 0.268343757511559, accuracy: 0.97\n",
      "iteration no 2367: Loss: 0.26831938396150123, accuracy: 0.97\n",
      "iteration no 2368: Loss: 0.2682940522752445, accuracy: 0.97\n",
      "iteration no 2369: Loss: 0.2682722098093298, accuracy: 0.97\n",
      "iteration no 2370: Loss: 0.26824744139877227, accuracy: 0.97\n",
      "iteration no 2371: Loss: 0.2682236637490976, accuracy: 0.97\n",
      "iteration no 2372: Loss: 0.26819902371111776, accuracy: 0.97\n",
      "iteration no 2373: Loss: 0.26817808781983454, accuracy: 0.97\n",
      "iteration no 2374: Loss: 0.268152435938263, accuracy: 0.97\n",
      "iteration no 2375: Loss: 0.26813135732438836, accuracy: 0.97\n",
      "iteration no 2376: Loss: 0.26810634656956156, accuracy: 0.97\n",
      "iteration no 2377: Loss: 0.26808288209983466, accuracy: 0.97\n",
      "iteration no 2378: Loss: 0.26805988990285073, accuracy: 0.97\n",
      "iteration no 2379: Loss: 0.26803614157319644, accuracy: 0.97\n",
      "iteration no 2380: Loss: 0.2680168789523006, accuracy: 0.97\n",
      "iteration no 2381: Loss: 0.2679871545850671, accuracy: 0.97\n",
      "iteration no 2382: Loss: 0.26796877687724674, accuracy: 0.97\n",
      "iteration no 2383: Loss: 0.2679420202297331, accuracy: 0.97\n",
      "iteration no 2384: Loss: 0.2679190458899715, accuracy: 0.97\n",
      "iteration no 2385: Loss: 0.2678967519145496, accuracy: 0.97\n",
      "iteration no 2386: Loss: 0.2678705975574815, accuracy: 0.97\n",
      "iteration no 2387: Loss: 0.26785017806069633, accuracy: 0.97\n",
      "iteration no 2388: Loss: 0.26782270835314337, accuracy: 0.97\n",
      "iteration no 2389: Loss: 0.267805444414574, accuracy: 0.97\n",
      "iteration no 2390: Loss: 0.2677781678817853, accuracy: 0.97\n",
      "iteration no 2391: Loss: 0.2677556461620034, accuracy: 0.97\n",
      "iteration no 2392: Loss: 0.26773415384572125, accuracy: 0.97\n",
      "iteration no 2393: Loss: 0.2677103080695477, accuracy: 0.97\n",
      "iteration no 2394: Loss: 0.2676900747212241, accuracy: 0.97\n",
      "iteration no 2395: Loss: 0.2676624609370416, accuracy: 0.97\n",
      "iteration no 2396: Loss: 0.26764455824324634, accuracy: 0.97\n",
      "iteration no 2397: Loss: 0.2676180232989413, accuracy: 0.97\n",
      "iteration no 2398: Loss: 0.26759635361653267, accuracy: 0.97\n",
      "iteration no 2399: Loss: 0.2675747294987219, accuracy: 0.97\n",
      "iteration no 2400: Loss: 0.26754919586084536, accuracy: 0.97\n",
      "iteration no 2401: Loss: 0.2675282837172805, accuracy: 0.97\n",
      "iteration no 2402: Loss: 0.26750227168211377, accuracy: 0.97\n",
      "iteration no 2403: Loss: 0.2674847179254839, accuracy: 0.97\n",
      "iteration no 2404: Loss: 0.26745923004156924, accuracy: 0.97\n",
      "iteration no 2405: Loss: 0.26743762713191177, accuracy: 0.97\n",
      "iteration no 2406: Loss: 0.26741592018014126, accuracy: 0.97\n",
      "iteration no 2407: Loss: 0.26739144037886386, accuracy: 0.97\n",
      "iteration no 2408: Loss: 0.2673706635983378, accuracy: 0.97\n",
      "iteration no 2409: Loss: 0.26734617867585325, accuracy: 0.97\n",
      "iteration no 2410: Loss: 0.26732674945993784, accuracy: 0.97\n",
      "iteration no 2411: Loss: 0.2673011943002943, accuracy: 0.97\n",
      "iteration no 2412: Loss: 0.2672799723484208, accuracy: 0.97\n",
      "iteration no 2413: Loss: 0.26725668448965967, accuracy: 0.97\n",
      "iteration no 2414: Loss: 0.26723395489133345, accuracy: 0.97\n",
      "iteration no 2415: Loss: 0.26721350235606744, accuracy: 0.97\n",
      "iteration no 2416: Loss: 0.26718846009082303, accuracy: 0.97\n",
      "iteration no 2417: Loss: 0.26717062482442244, accuracy: 0.9733333333333334\n",
      "iteration no 2418: Loss: 0.2671446890557318, accuracy: 0.97\n",
      "iteration no 2419: Loss: 0.2671243355369929, accuracy: 0.9733333333333334\n",
      "iteration no 2420: Loss: 0.26710393101615715, accuracy: 0.97\n",
      "iteration no 2421: Loss: 0.2670786575530996, accuracy: 0.9733333333333334\n",
      "iteration no 2422: Loss: 0.2670596671622155, accuracy: 0.97\n",
      "iteration no 2423: Loss: 0.26703218576303533, accuracy: 0.9733333333333334\n",
      "iteration no 2424: Loss: 0.2670148860068637, accuracy: 0.97\n",
      "iteration no 2425: Loss: 0.2669912242900281, accuracy: 0.9733333333333334\n",
      "iteration no 2426: Loss: 0.2669668113065178, accuracy: 0.97\n",
      "iteration no 2427: Loss: 0.266948053885842, accuracy: 0.97\n",
      "iteration no 2428: Loss: 0.26692186315699806, accuracy: 0.97\n",
      "iteration no 2429: Loss: 0.26690542311587523, accuracy: 0.9733333333333334\n",
      "iteration no 2430: Loss: 0.26688045822185824, accuracy: 0.97\n",
      "iteration no 2431: Loss: 0.26685908783715756, accuracy: 0.9733333333333334\n",
      "iteration no 2432: Loss: 0.26684113121491915, accuracy: 0.97\n",
      "iteration no 2433: Loss: 0.2668158208428566, accuracy: 0.9733333333333334\n",
      "iteration no 2434: Loss: 0.2667969389350143, accuracy: 0.97\n",
      "iteration no 2435: Loss: 0.2667760741459014, accuracy: 0.9733333333333334\n",
      "iteration no 2436: Loss: 0.2667525484666327, accuracy: 0.97\n",
      "iteration no 2437: Loss: 0.2667319007112365, accuracy: 0.9733333333333334\n",
      "iteration no 2438: Loss: 0.2667128960321699, accuracy: 0.97\n",
      "iteration no 2439: Loss: 0.2666910344387998, accuracy: 0.9733333333333334\n",
      "iteration no 2440: Loss: 0.2666694373444291, accuracy: 0.97\n",
      "iteration no 2441: Loss: 0.2666519896707206, accuracy: 0.9733333333333334\n",
      "iteration no 2442: Loss: 0.26662919153049397, accuracy: 0.97\n",
      "iteration no 2443: Loss: 0.2666086519280165, accuracy: 0.9733333333333334\n",
      "iteration no 2444: Loss: 0.2665912753612379, accuracy: 0.97\n",
      "iteration no 2445: Loss: 0.2665682022960518, accuracy: 0.9733333333333334\n",
      "iteration no 2446: Loss: 0.26654844882953543, accuracy: 0.97\n",
      "iteration no 2447: Loss: 0.2665300419124935, accuracy: 0.9733333333333334\n",
      "iteration no 2448: Loss: 0.2665070700794702, accuracy: 0.97\n",
      "iteration no 2449: Loss: 0.26648833736814886, accuracy: 0.9733333333333334\n",
      "iteration no 2450: Loss: 0.2664688220223409, accuracy: 0.97\n",
      "iteration no 2451: Loss: 0.2664478655393288, accuracy: 0.9733333333333334\n",
      "iteration no 2452: Loss: 0.26642622306362407, accuracy: 0.97\n",
      "iteration no 2453: Loss: 0.26640827466173683, accuracy: 0.9733333333333334\n",
      "iteration no 2454: Loss: 0.2663881470155815, accuracy: 0.97\n",
      "iteration no 2455: Loss: 0.26636449050134064, accuracy: 0.9733333333333334\n",
      "iteration no 2456: Loss: 0.26634883266875986, accuracy: 0.97\n",
      "iteration no 2457: Loss: 0.26632554381732243, accuracy: 0.9733333333333334\n",
      "iteration no 2458: Loss: 0.2663047580780595, accuracy: 0.9733333333333334\n",
      "iteration no 2459: Loss: 0.2662882561360919, accuracy: 0.9733333333333334\n",
      "iteration no 2460: Loss: 0.2662649646466302, accuracy: 0.9733333333333334\n",
      "iteration no 2461: Loss: 0.2662448193651846, accuracy: 0.9733333333333334\n",
      "iteration no 2462: Loss: 0.2662270833395405, accuracy: 0.9733333333333334\n",
      "iteration no 2463: Loss: 0.26620608372731075, accuracy: 0.9733333333333334\n",
      "iteration no 2464: Loss: 0.26618431177220814, accuracy: 0.9733333333333334\n",
      "iteration no 2465: Loss: 0.26616729898298697, accuracy: 0.9733333333333334\n",
      "iteration no 2466: Loss: 0.2661476208040566, accuracy: 0.9733333333333334\n",
      "iteration no 2467: Loss: 0.2661250215580423, accuracy: 0.9733333333333334\n",
      "iteration no 2468: Loss: 0.2661071912576545, accuracy: 0.9733333333333334\n",
      "iteration no 2469: Loss: 0.26608893452429627, accuracy: 0.9733333333333334\n",
      "iteration no 2470: Loss: 0.2660666349268306, accuracy: 0.9733333333333334\n",
      "iteration no 2471: Loss: 0.26604853186270855, accuracy: 0.9733333333333334\n",
      "iteration no 2472: Loss: 0.2660305136628695, accuracy: 0.9733333333333334\n",
      "iteration no 2473: Loss: 0.26601163107572473, accuracy: 0.9733333333333334\n",
      "iteration no 2474: Loss: 0.2659912089336676, accuracy: 0.97\n",
      "iteration no 2475: Loss: 0.2659716781545534, accuracy: 0.9733333333333334\n",
      "iteration no 2476: Loss: 0.26595239397510545, accuracy: 0.9733333333333334\n",
      "iteration no 2477: Loss: 0.26593504767827214, accuracy: 0.9733333333333334\n",
      "iteration no 2478: Loss: 0.26591759213798233, accuracy: 0.9733333333333334\n",
      "iteration no 2479: Loss: 0.2658966153195601, accuracy: 0.9733333333333334\n",
      "iteration no 2480: Loss: 0.26587724893356623, accuracy: 0.9733333333333334\n",
      "iteration no 2481: Loss: 0.2658581029835367, accuracy: 0.9733333333333334\n",
      "iteration no 2482: Loss: 0.2658401408793356, accuracy: 0.9733333333333334\n",
      "iteration no 2483: Loss: 0.26582333064639024, accuracy: 0.9733333333333334\n",
      "iteration no 2484: Loss: 0.2658019218646794, accuracy: 0.9733333333333334\n",
      "iteration no 2485: Loss: 0.2657844208769122, accuracy: 0.9733333333333334\n",
      "iteration no 2486: Loss: 0.2657655427692368, accuracy: 0.9733333333333334\n",
      "iteration no 2487: Loss: 0.2657450568470016, accuracy: 0.9733333333333334\n",
      "iteration no 2488: Loss: 0.26572897635723414, accuracy: 0.9733333333333334\n",
      "iteration no 2489: Loss: 0.26570956665489287, accuracy: 0.9733333333333334\n",
      "iteration no 2490: Loss: 0.2656930919480188, accuracy: 0.9733333333333334\n",
      "iteration no 2491: Loss: 0.2656720177684807, accuracy: 0.9733333333333334\n",
      "iteration no 2492: Loss: 0.2656535343098781, accuracy: 0.9733333333333334\n",
      "iteration no 2493: Loss: 0.26563545110169295, accuracy: 0.9733333333333334\n",
      "iteration no 2494: Loss: 0.26561740228117625, accuracy: 0.9733333333333334\n",
      "iteration no 2495: Loss: 0.26560020696901654, accuracy: 0.9733333333333334\n",
      "iteration no 2496: Loss: 0.26558033687963833, accuracy: 0.9733333333333334\n",
      "iteration no 2497: Loss: 0.2655622206043081, accuracy: 0.9733333333333334\n",
      "iteration no 2498: Loss: 0.26554261119239453, accuracy: 0.9733333333333334\n",
      "iteration no 2499: Loss: 0.26552427685603286, accuracy: 0.9733333333333334\n",
      "iteration no 2500: Loss: 0.26550942286420626, accuracy: 0.9733333333333334\n",
      "iteration no 2501: Loss: 0.26548917338796785, accuracy: 0.9733333333333334\n",
      "iteration no 2502: Loss: 0.2654711948966556, accuracy: 0.9733333333333334\n",
      "iteration no 2503: Loss: 0.2654517436205225, accuracy: 0.9733333333333334\n",
      "iteration no 2504: Loss: 0.2654331576199647, accuracy: 0.9733333333333334\n",
      "iteration no 2505: Loss: 0.2654180549813472, accuracy: 0.9733333333333334\n",
      "iteration no 2506: Loss: 0.26539797617155075, accuracy: 0.9733333333333334\n",
      "iteration no 2507: Loss: 0.2653812686175297, accuracy: 0.9733333333333334\n",
      "iteration no 2508: Loss: 0.2653612779397283, accuracy: 0.9733333333333334\n",
      "iteration no 2509: Loss: 0.2653439714251102, accuracy: 0.9733333333333334\n",
      "iteration no 2510: Loss: 0.26532529789979364, accuracy: 0.9733333333333334\n",
      "iteration no 2511: Loss: 0.26530916395836757, accuracy: 0.9733333333333334\n",
      "iteration no 2512: Loss: 0.26529308978273664, accuracy: 0.9733333333333334\n",
      "iteration no 2513: Loss: 0.2652723703285161, accuracy: 0.9733333333333334\n",
      "iteration no 2514: Loss: 0.26525472940448025, accuracy: 0.9733333333333334\n",
      "iteration no 2515: Loss: 0.26523686012125375, accuracy: 0.9733333333333334\n",
      "iteration no 2516: Loss: 0.26521826839800433, accuracy: 0.9733333333333334\n",
      "iteration no 2517: Loss: 0.2652037574844027, accuracy: 0.9733333333333334\n",
      "iteration no 2518: Loss: 0.2651838060772459, accuracy: 0.9733333333333334\n",
      "iteration no 2519: Loss: 0.265167551609245, accuracy: 0.9733333333333334\n",
      "iteration no 2520: Loss: 0.2651492789470252, accuracy: 0.9733333333333334\n",
      "iteration no 2521: Loss: 0.2651305797921298, accuracy: 0.9733333333333334\n",
      "iteration no 2522: Loss: 0.2651133379311672, accuracy: 0.9733333333333334\n",
      "iteration no 2523: Loss: 0.2650962955061074, accuracy: 0.9733333333333334\n",
      "iteration no 2524: Loss: 0.265081087406953, accuracy: 0.9733333333333334\n",
      "iteration no 2525: Loss: 0.2650611806794239, accuracy: 0.9733333333333334\n",
      "iteration no 2526: Loss: 0.2650435978174458, accuracy: 0.9733333333333334\n",
      "iteration no 2527: Loss: 0.2650264669568639, accuracy: 0.9733333333333334\n",
      "iteration no 2528: Loss: 0.26500696562681875, accuracy: 0.9733333333333334\n",
      "iteration no 2529: Loss: 0.2649926813282092, accuracy: 0.9733333333333334\n",
      "iteration no 2530: Loss: 0.2649748149492097, accuracy: 0.9733333333333334\n",
      "iteration no 2531: Loss: 0.26495782370710863, accuracy: 0.9733333333333334\n",
      "iteration no 2532: Loss: 0.2649398535096911, accuracy: 0.9733333333333334\n",
      "iteration no 2533: Loss: 0.2649206922564323, accuracy: 0.9733333333333334\n",
      "iteration no 2534: Loss: 0.26490521020903274, accuracy: 0.9733333333333334\n",
      "iteration no 2535: Loss: 0.26488712866477737, accuracy: 0.9733333333333334\n",
      "iteration no 2536: Loss: 0.2648730825328704, accuracy: 0.9733333333333334\n",
      "iteration no 2537: Loss: 0.2648549504493044, accuracy: 0.9733333333333334\n",
      "iteration no 2538: Loss: 0.264836708557316, accuracy: 0.9733333333333334\n",
      "iteration no 2539: Loss: 0.264819566213768, accuracy: 0.9733333333333334\n",
      "iteration no 2540: Loss: 0.2648013379328394, accuracy: 0.9733333333333334\n",
      "iteration no 2541: Loss: 0.26478554736758114, accuracy: 0.9733333333333334\n",
      "iteration no 2542: Loss: 0.2647682958742954, accuracy: 0.9733333333333334\n",
      "iteration no 2543: Loss: 0.26475024759924676, accuracy: 0.9733333333333334\n",
      "iteration no 2544: Loss: 0.26473446713167215, accuracy: 0.9733333333333334\n",
      "iteration no 2545: Loss: 0.2647189237253375, accuracy: 0.9733333333333334\n",
      "iteration no 2546: Loss: 0.2647029522132318, accuracy: 0.9733333333333334\n",
      "iteration no 2547: Loss: 0.2646837457155974, accuracy: 0.9733333333333334\n",
      "iteration no 2548: Loss: 0.26466722998227316, accuracy: 0.9733333333333334\n",
      "iteration no 2549: Loss: 0.264651526367593, accuracy: 0.9733333333333334\n",
      "iteration no 2550: Loss: 0.26463276621494813, accuracy: 0.9733333333333334\n",
      "iteration no 2551: Loss: 0.26461683759265753, accuracy: 0.9733333333333334\n",
      "iteration no 2552: Loss: 0.26459908548039757, accuracy: 0.9733333333333334\n",
      "iteration no 2553: Loss: 0.2645839915142644, accuracy: 0.9733333333333334\n",
      "iteration no 2554: Loss: 0.2645685127422317, accuracy: 0.9733333333333334\n",
      "iteration no 2555: Loss: 0.26454976074575276, accuracy: 0.9733333333333334\n",
      "iteration no 2556: Loss: 0.2645343884754817, accuracy: 0.9733333333333334\n",
      "iteration no 2557: Loss: 0.26451710787450067, accuracy: 0.9733333333333334\n",
      "iteration no 2558: Loss: 0.2645005745594978, accuracy: 0.9733333333333334\n",
      "iteration no 2559: Loss: 0.26448358757196455, accuracy: 0.9733333333333334\n",
      "iteration no 2560: Loss: 0.2644667966050033, accuracy: 0.9733333333333334\n",
      "iteration no 2561: Loss: 0.2644501028338398, accuracy: 0.9733333333333334\n",
      "iteration no 2562: Loss: 0.2644330830661845, accuracy: 0.9733333333333334\n",
      "iteration no 2563: Loss: 0.2644192028014914, accuracy: 0.9733333333333334\n",
      "iteration no 2564: Loss: 0.26440248030323577, accuracy: 0.9733333333333334\n",
      "iteration no 2565: Loss: 0.2643849354412507, accuracy: 0.9733333333333334\n",
      "iteration no 2566: Loss: 0.2643698708056369, accuracy: 0.9733333333333334\n",
      "iteration no 2567: Loss: 0.26435227154502167, accuracy: 0.9733333333333334\n",
      "iteration no 2568: Loss: 0.26433647078561173, accuracy: 0.9733333333333334\n",
      "iteration no 2569: Loss: 0.264319132749581, accuracy: 0.9733333333333334\n",
      "iteration no 2570: Loss: 0.2643038485936308, accuracy: 0.9733333333333334\n",
      "iteration no 2571: Loss: 0.264287871852278, accuracy: 0.9733333333333334\n",
      "iteration no 2572: Loss: 0.2642701775400182, accuracy: 0.9733333333333334\n",
      "iteration no 2573: Loss: 0.2642567347544623, accuracy: 0.9733333333333334\n",
      "iteration no 2574: Loss: 0.2642411736195002, accuracy: 0.9733333333333334\n",
      "iteration no 2575: Loss: 0.26422544180034163, accuracy: 0.9733333333333334\n",
      "iteration no 2576: Loss: 0.2642088223525104, accuracy: 0.9733333333333334\n",
      "iteration no 2577: Loss: 0.26419228926114446, accuracy: 0.9733333333333334\n",
      "iteration no 2578: Loss: 0.26417616214142636, accuracy: 0.9733333333333334\n",
      "iteration no 2579: Loss: 0.2641594672998853, accuracy: 0.9733333333333334\n",
      "iteration no 2580: Loss: 0.26414421898901363, accuracy: 0.9733333333333334\n",
      "iteration no 2581: Loss: 0.2641282387448877, accuracy: 0.9733333333333334\n",
      "iteration no 2582: Loss: 0.26411124017611726, accuracy: 0.9733333333333334\n",
      "iteration no 2583: Loss: 0.26409631431222114, accuracy: 0.9733333333333334\n",
      "iteration no 2584: Loss: 0.2640816264918879, accuracy: 0.9733333333333334\n",
      "iteration no 2585: Loss: 0.2640665846292674, accuracy: 0.9733333333333334\n",
      "iteration no 2586: Loss: 0.26404901153546545, accuracy: 0.9733333333333334\n",
      "iteration no 2587: Loss: 0.2640333066410775, accuracy: 0.9733333333333334\n",
      "iteration no 2588: Loss: 0.2640182904396221, accuracy: 0.9733333333333334\n",
      "iteration no 2589: Loss: 0.26400235335182637, accuracy: 0.9733333333333334\n",
      "iteration no 2590: Loss: 0.263986632434643, accuracy: 0.9733333333333334\n",
      "iteration no 2591: Loss: 0.26397075160166544, accuracy: 0.9733333333333334\n",
      "iteration no 2592: Loss: 0.26395571485125735, accuracy: 0.9733333333333334\n",
      "iteration no 2593: Loss: 0.2639389586152383, accuracy: 0.9733333333333334\n",
      "iteration no 2594: Loss: 0.2639244608224198, accuracy: 0.9733333333333334\n",
      "iteration no 2595: Loss: 0.2639111606175585, accuracy: 0.9733333333333334\n",
      "iteration no 2596: Loss: 0.26389286290348657, accuracy: 0.9733333333333334\n",
      "iteration no 2597: Loss: 0.2638784409634316, accuracy: 0.9733333333333334\n",
      "iteration no 2598: Loss: 0.26386245642403217, accuracy: 0.9733333333333334\n",
      "iteration no 2599: Loss: 0.26384850055274006, accuracy: 0.9733333333333334\n",
      "iteration no 2600: Loss: 0.2638310485902819, accuracy: 0.9733333333333334\n",
      "iteration no 2601: Loss: 0.2638152322592466, accuracy: 0.9733333333333334\n",
      "iteration no 2602: Loss: 0.2638007742301367, accuracy: 0.9733333333333334\n",
      "iteration no 2603: Loss: 0.26378515803747343, accuracy: 0.9733333333333334\n",
      "iteration no 2604: Loss: 0.26376955672604746, accuracy: 0.9733333333333334\n",
      "iteration no 2605: Loss: 0.26375372888084675, accuracy: 0.9733333333333334\n",
      "iteration no 2606: Loss: 0.2637408326234807, accuracy: 0.9733333333333334\n",
      "iteration no 2607: Loss: 0.2637243239741387, accuracy: 0.9733333333333334\n",
      "iteration no 2608: Loss: 0.26370881300133486, accuracy: 0.9733333333333334\n",
      "iteration no 2609: Loss: 0.26369399096132995, accuracy: 0.9733333333333334\n",
      "iteration no 2610: Loss: 0.26367676374927, accuracy: 0.9733333333333334\n",
      "iteration no 2611: Loss: 0.2636631096446901, accuracy: 0.9733333333333334\n",
      "iteration no 2612: Loss: 0.26364763070302955, accuracy: 0.9733333333333334\n",
      "iteration no 2613: Loss: 0.2636320375605478, accuracy: 0.9733333333333334\n",
      "iteration no 2614: Loss: 0.26361603926574906, accuracy: 0.9733333333333334\n",
      "iteration no 2615: Loss: 0.2636011582609109, accuracy: 0.9733333333333334\n",
      "iteration no 2616: Loss: 0.2635877066986308, accuracy: 0.9733333333333334\n",
      "iteration no 2617: Loss: 0.26357169800367053, accuracy: 0.9733333333333334\n",
      "iteration no 2618: Loss: 0.26355711288130046, accuracy: 0.9733333333333334\n",
      "iteration no 2619: Loss: 0.26354191707670727, accuracy: 0.9733333333333334\n",
      "iteration no 2620: Loss: 0.2635276832753822, accuracy: 0.9733333333333334\n",
      "iteration no 2621: Loss: 0.26351146493764355, accuracy: 0.9733333333333334\n",
      "iteration no 2622: Loss: 0.2634966335130777, accuracy: 0.9733333333333334\n",
      "iteration no 2623: Loss: 0.2634814881950914, accuracy: 0.9733333333333334\n",
      "iteration no 2624: Loss: 0.26346804061788465, accuracy: 0.9733333333333334\n",
      "iteration no 2625: Loss: 0.26345430231901273, accuracy: 0.9733333333333334\n",
      "iteration no 2626: Loss: 0.26343623819608186, accuracy: 0.9733333333333334\n",
      "iteration no 2627: Loss: 0.2634222919143451, accuracy: 0.9733333333333334\n",
      "iteration no 2628: Loss: 0.2634079569089318, accuracy: 0.9733333333333334\n",
      "iteration no 2629: Loss: 0.2633949525092102, accuracy: 0.9733333333333334\n",
      "iteration no 2630: Loss: 0.2633793861614252, accuracy: 0.9733333333333334\n",
      "iteration no 2631: Loss: 0.26336417180892946, accuracy: 0.9733333333333334\n",
      "iteration no 2632: Loss: 0.26335044286509174, accuracy: 0.9733333333333334\n",
      "iteration no 2633: Loss: 0.2633336957876285, accuracy: 0.9733333333333334\n",
      "iteration no 2634: Loss: 0.263319379151171, accuracy: 0.9733333333333334\n",
      "iteration no 2635: Loss: 0.2633060032815664, accuracy: 0.9733333333333334\n",
      "iteration no 2636: Loss: 0.26329157506825274, accuracy: 0.9733333333333334\n",
      "iteration no 2637: Loss: 0.2632761425406538, accuracy: 0.9733333333333334\n",
      "iteration no 2638: Loss: 0.26326250228037185, accuracy: 0.9733333333333334\n",
      "iteration no 2639: Loss: 0.2632449983049377, accuracy: 0.9733333333333334\n",
      "iteration no 2640: Loss: 0.2632329072467, accuracy: 0.9733333333333334\n",
      "iteration no 2641: Loss: 0.26321981334999406, accuracy: 0.9733333333333334\n",
      "iteration no 2642: Loss: 0.26320424248856555, accuracy: 0.9733333333333334\n",
      "iteration no 2643: Loss: 0.2631890905791926, accuracy: 0.9733333333333334\n",
      "iteration no 2644: Loss: 0.2631734951957847, accuracy: 0.9733333333333334\n",
      "iteration no 2645: Loss: 0.2631605588816689, accuracy: 0.9733333333333334\n",
      "iteration no 2646: Loss: 0.26314529980139933, accuracy: 0.9733333333333334\n",
      "iteration no 2647: Loss: 0.26313218297967267, accuracy: 0.9733333333333334\n",
      "iteration no 2648: Loss: 0.26311444213550766, accuracy: 0.9733333333333334\n",
      "iteration no 2649: Loss: 0.26310163669459774, accuracy: 0.9733333333333334\n",
      "iteration no 2650: Loss: 0.2630875473019702, accuracy: 0.9733333333333334\n",
      "iteration no 2651: Loss: 0.2630745517848958, accuracy: 0.9733333333333334\n",
      "iteration no 2652: Loss: 0.26305737457648454, accuracy: 0.9733333333333334\n",
      "iteration no 2653: Loss: 0.2630445564054019, accuracy: 0.9733333333333334\n",
      "iteration no 2654: Loss: 0.2630286896543201, accuracy: 0.9733333333333334\n",
      "iteration no 2655: Loss: 0.2630168655594407, accuracy: 0.9733333333333334\n",
      "iteration no 2656: Loss: 0.2630032530001107, accuracy: 0.9733333333333334\n",
      "iteration no 2657: Loss: 0.26298752873903025, accuracy: 0.9733333333333334\n",
      "iteration no 2658: Loss: 0.262972742109816, accuracy: 0.9733333333333334\n",
      "iteration no 2659: Loss: 0.2629580257450882, accuracy: 0.9733333333333334\n",
      "iteration no 2660: Loss: 0.26294607591387187, accuracy: 0.9733333333333334\n",
      "iteration no 2661: Loss: 0.2629299567862625, accuracy: 0.9733333333333334\n",
      "iteration no 2662: Loss: 0.2629176011456187, accuracy: 0.9733333333333334\n",
      "iteration no 2663: Loss: 0.26290069416336187, accuracy: 0.9766666666666667\n",
      "iteration no 2664: Loss: 0.2628876025474839, accuracy: 0.9733333333333334\n",
      "iteration no 2665: Loss: 0.262874191992275, accuracy: 0.9766666666666667\n",
      "iteration no 2666: Loss: 0.26286104957452094, accuracy: 0.9733333333333334\n",
      "iteration no 2667: Loss: 0.2628445199845293, accuracy: 0.9766666666666667\n",
      "iteration no 2668: Loss: 0.26283326506966753, accuracy: 0.9733333333333334\n",
      "iteration no 2669: Loss: 0.2628165242045217, accuracy: 0.9766666666666667\n",
      "iteration no 2670: Loss: 0.2628061622805976, accuracy: 0.9733333333333334\n",
      "iteration no 2671: Loss: 0.2627905768951715, accuracy: 0.9733333333333334\n",
      "iteration no 2672: Loss: 0.2627768711997017, accuracy: 0.9733333333333334\n",
      "iteration no 2673: Loss: 0.26276035515240537, accuracy: 0.9733333333333334\n",
      "iteration no 2674: Loss: 0.2627487578504169, accuracy: 0.9733333333333334\n",
      "iteration no 2675: Loss: 0.26273427891376366, accuracy: 0.9733333333333334\n",
      "iteration no 2676: Loss: 0.2627204138460845, accuracy: 0.9733333333333334\n",
      "iteration no 2677: Loss: 0.2627062148408735, accuracy: 0.9766666666666667\n",
      "iteration no 2678: Loss: 0.26269138033750145, accuracy: 0.9733333333333334\n",
      "iteration no 2679: Loss: 0.262678221720995, accuracy: 0.9766666666666667\n",
      "iteration no 2680: Loss: 0.2626660102918265, accuracy: 0.9733333333333334\n",
      "iteration no 2681: Loss: 0.2626508176302706, accuracy: 0.9766666666666667\n",
      "iteration no 2682: Loss: 0.2626363935750176, accuracy: 0.9733333333333334\n",
      "iteration no 2683: Loss: 0.26262439815253624, accuracy: 0.9766666666666667\n",
      "iteration no 2684: Loss: 0.26261069687160327, accuracy: 0.9733333333333334\n",
      "iteration no 2685: Loss: 0.26259768676934203, accuracy: 0.9733333333333334\n",
      "iteration no 2686: Loss: 0.2625831766701232, accuracy: 0.9733333333333334\n",
      "iteration no 2687: Loss: 0.2625688223056751, accuracy: 0.9733333333333334\n",
      "iteration no 2688: Loss: 0.26255503596664875, accuracy: 0.9733333333333334\n",
      "iteration no 2689: Loss: 0.2625436855499708, accuracy: 0.9733333333333334\n",
      "iteration no 2690: Loss: 0.26252788519364595, accuracy: 0.9733333333333334\n",
      "iteration no 2691: Loss: 0.26251559795534674, accuracy: 0.9733333333333334\n",
      "iteration no 2692: Loss: 0.26249868399851106, accuracy: 0.9733333333333334\n",
      "iteration no 2693: Loss: 0.26248526514939957, accuracy: 0.9733333333333334\n",
      "iteration no 2694: Loss: 0.26246954575275494, accuracy: 0.9733333333333334\n",
      "iteration no 2695: Loss: 0.2624543634325247, accuracy: 0.9733333333333334\n",
      "iteration no 2696: Loss: 0.2624386275397425, accuracy: 0.9733333333333334\n",
      "iteration no 2697: Loss: 0.26242585758241593, accuracy: 0.9733333333333334\n",
      "iteration no 2698: Loss: 0.2624099859740784, accuracy: 0.9766666666666667\n",
      "iteration no 2699: Loss: 0.2623969327250729, accuracy: 0.9766666666666667\n",
      "iteration no 2700: Loss: 0.26238024474780736, accuracy: 0.9766666666666667\n",
      "iteration no 2701: Loss: 0.26236701098747617, accuracy: 0.9766666666666667\n",
      "iteration no 2702: Loss: 0.26235294492793243, accuracy: 0.9766666666666667\n",
      "iteration no 2703: Loss: 0.26234020641838174, accuracy: 0.9766666666666667\n",
      "iteration no 2704: Loss: 0.26232273786426985, accuracy: 0.9766666666666667\n",
      "iteration no 2705: Loss: 0.26231026810626373, accuracy: 0.9766666666666667\n",
      "iteration no 2706: Loss: 0.26229411646565376, accuracy: 0.9766666666666667\n",
      "iteration no 2707: Loss: 0.2622826133970106, accuracy: 0.9733333333333334\n",
      "iteration no 2708: Loss: 0.26226648127368923, accuracy: 0.9766666666666667\n",
      "iteration no 2709: Loss: 0.2622523248230418, accuracy: 0.9733333333333334\n",
      "iteration no 2710: Loss: 0.26223769947133674, accuracy: 0.9766666666666667\n",
      "iteration no 2711: Loss: 0.26222718573762793, accuracy: 0.9733333333333334\n",
      "iteration no 2712: Loss: 0.2622104478390147, accuracy: 0.9766666666666667\n",
      "iteration no 2713: Loss: 0.2621968040514492, accuracy: 0.9733333333333334\n",
      "iteration no 2714: Loss: 0.262181390989061, accuracy: 0.9766666666666667\n",
      "iteration no 2715: Loss: 0.2621690229787245, accuracy: 0.9733333333333334\n",
      "iteration no 2716: Loss: 0.2621549383368495, accuracy: 0.9766666666666667\n",
      "iteration no 2717: Loss: 0.2621420484015266, accuracy: 0.9733333333333334\n",
      "iteration no 2718: Loss: 0.2621258022335971, accuracy: 0.9766666666666667\n",
      "iteration no 2719: Loss: 0.26211415789969583, accuracy: 0.9733333333333334\n",
      "iteration no 2720: Loss: 0.26210025966472505, accuracy: 0.9766666666666667\n",
      "iteration no 2721: Loss: 0.262086584154757, accuracy: 0.9733333333333334\n",
      "iteration no 2722: Loss: 0.2620716823080338, accuracy: 0.9766666666666667\n",
      "iteration no 2723: Loss: 0.2620581718461865, accuracy: 0.9733333333333334\n",
      "iteration no 2724: Loss: 0.26204480884556147, accuracy: 0.9766666666666667\n",
      "iteration no 2725: Loss: 0.262032095134719, accuracy: 0.9733333333333334\n",
      "iteration no 2726: Loss: 0.2620161222237585, accuracy: 0.9766666666666667\n",
      "iteration no 2727: Loss: 0.2620039215798789, accuracy: 0.9733333333333334\n",
      "iteration no 2728: Loss: 0.26199094796429556, accuracy: 0.9766666666666667\n",
      "iteration no 2729: Loss: 0.26197700832373977, accuracy: 0.9733333333333334\n",
      "iteration no 2730: Loss: 0.26196296898267923, accuracy: 0.9766666666666667\n",
      "iteration no 2731: Loss: 0.26194890011881505, accuracy: 0.9733333333333334\n",
      "iteration no 2732: Loss: 0.2619362412279961, accuracy: 0.9766666666666667\n",
      "iteration no 2733: Loss: 0.2619236242418613, accuracy: 0.9733333333333334\n",
      "iteration no 2734: Loss: 0.26190866975427746, accuracy: 0.9766666666666667\n",
      "iteration no 2735: Loss: 0.2618965458220517, accuracy: 0.9733333333333334\n",
      "iteration no 2736: Loss: 0.2618833318548618, accuracy: 0.9766666666666667\n",
      "iteration no 2737: Loss: 0.2618698974229234, accuracy: 0.9733333333333334\n",
      "iteration no 2738: Loss: 0.2618552282585658, accuracy: 0.9766666666666667\n",
      "iteration no 2739: Loss: 0.26184173703711694, accuracy: 0.9733333333333334\n",
      "iteration no 2740: Loss: 0.26182903902287424, accuracy: 0.9766666666666667\n",
      "iteration no 2741: Loss: 0.2618168372167827, accuracy: 0.9733333333333334\n",
      "iteration no 2742: Loss: 0.26180141655217287, accuracy: 0.9766666666666667\n",
      "iteration no 2743: Loss: 0.2617886096937245, accuracy: 0.9733333333333334\n",
      "iteration no 2744: Loss: 0.2617756676123485, accuracy: 0.9766666666666667\n",
      "iteration no 2745: Loss: 0.26176264646153574, accuracy: 0.9733333333333334\n",
      "iteration no 2746: Loss: 0.26174828595185257, accuracy: 0.9766666666666667\n",
      "iteration no 2747: Loss: 0.26173351290353314, accuracy: 0.9733333333333334\n",
      "iteration no 2748: Loss: 0.26172232246553806, accuracy: 0.9766666666666667\n",
      "iteration no 2749: Loss: 0.2617112341951362, accuracy: 0.9733333333333334\n",
      "iteration no 2750: Loss: 0.26169641565343793, accuracy: 0.9766666666666667\n",
      "iteration no 2751: Loss: 0.261682475780671, accuracy: 0.9766666666666667\n",
      "iteration no 2752: Loss: 0.26167047999209847, accuracy: 0.9766666666666667\n",
      "iteration no 2753: Loss: 0.2616571442734979, accuracy: 0.9733333333333334\n",
      "iteration no 2754: Loss: 0.26164394751287867, accuracy: 0.9766666666666667\n",
      "iteration no 2755: Loss: 0.2616283767969354, accuracy: 0.9733333333333334\n",
      "iteration no 2756: Loss: 0.2616191695031981, accuracy: 0.9766666666666667\n",
      "iteration no 2757: Loss: 0.261605136886375, accuracy: 0.9733333333333334\n",
      "iteration no 2758: Loss: 0.2615922380274858, accuracy: 0.9766666666666667\n",
      "iteration no 2759: Loss: 0.26157777354549694, accuracy: 0.9733333333333334\n",
      "iteration no 2760: Loss: 0.2615668583713905, accuracy: 0.9766666666666667\n",
      "iteration no 2761: Loss: 0.26155235971366525, accuracy: 0.9733333333333334\n",
      "iteration no 2762: Loss: 0.26154038194414053, accuracy: 0.9766666666666667\n",
      "iteration no 2763: Loss: 0.2615250041048573, accuracy: 0.9733333333333334\n",
      "iteration no 2764: Loss: 0.2615167444258225, accuracy: 0.9733333333333334\n",
      "iteration no 2765: Loss: 0.2614998563706402, accuracy: 0.9766666666666667\n",
      "iteration no 2766: Loss: 0.261488798401599, accuracy: 0.9733333333333334\n",
      "iteration no 2767: Loss: 0.2614739263079082, accuracy: 0.9766666666666667\n",
      "iteration no 2768: Loss: 0.26146450606891536, accuracy: 0.9733333333333334\n",
      "iteration no 2769: Loss: 0.261449905977173, accuracy: 0.9766666666666667\n",
      "iteration no 2770: Loss: 0.2614373699012459, accuracy: 0.9733333333333334\n",
      "iteration no 2771: Loss: 0.26142354178245075, accuracy: 0.9766666666666667\n",
      "iteration no 2772: Loss: 0.26141361912700983, accuracy: 0.9733333333333334\n",
      "iteration no 2773: Loss: 0.2613976368527449, accuracy: 0.9766666666666667\n",
      "iteration no 2774: Loss: 0.2613858111987814, accuracy: 0.9733333333333334\n",
      "iteration no 2775: Loss: 0.2613726873257758, accuracy: 0.9766666666666667\n",
      "iteration no 2776: Loss: 0.26136224102934996, accuracy: 0.9733333333333334\n",
      "iteration no 2777: Loss: 0.26134822740340763, accuracy: 0.9766666666666667\n",
      "iteration no 2778: Loss: 0.2613340631705561, accuracy: 0.9733333333333334\n",
      "iteration no 2779: Loss: 0.2613238874721129, accuracy: 0.9766666666666667\n",
      "iteration no 2780: Loss: 0.26130944728447425, accuracy: 0.9733333333333334\n",
      "iteration no 2781: Loss: 0.2612963809730547, accuracy: 0.9766666666666667\n",
      "iteration no 2782: Loss: 0.2612833824737309, accuracy: 0.9733333333333334\n",
      "iteration no 2783: Loss: 0.261273375428243, accuracy: 0.9766666666666667\n",
      "iteration no 2784: Loss: 0.2612586315209492, accuracy: 0.9733333333333334\n",
      "iteration no 2785: Loss: 0.26124550633787824, accuracy: 0.9766666666666667\n",
      "iteration no 2786: Loss: 0.2612323184820763, accuracy: 0.9733333333333334\n",
      "iteration no 2787: Loss: 0.261221531895937, accuracy: 0.9766666666666667\n",
      "iteration no 2788: Loss: 0.26120676406361154, accuracy: 0.9766666666666667\n",
      "iteration no 2789: Loss: 0.26119648612808294, accuracy: 0.9733333333333334\n",
      "iteration no 2790: Loss: 0.261183256648388, accuracy: 0.9766666666666667\n",
      "iteration no 2791: Loss: 0.26117161756160856, accuracy: 0.9733333333333334\n",
      "iteration no 2792: Loss: 0.26115692338138763, accuracy: 0.9766666666666667\n",
      "iteration no 2793: Loss: 0.26114570322918834, accuracy: 0.9733333333333334\n",
      "iteration no 2794: Loss: 0.261134118011792, accuracy: 0.9766666666666667\n",
      "iteration no 2795: Loss: 0.2611208154679062, accuracy: 0.9733333333333334\n",
      "iteration no 2796: Loss: 0.26110749891349094, accuracy: 0.9766666666666667\n",
      "iteration no 2797: Loss: 0.2610962622249498, accuracy: 0.9733333333333334\n",
      "iteration no 2798: Loss: 0.261083848224683, accuracy: 0.9766666666666667\n",
      "iteration no 2799: Loss: 0.26107013900880444, accuracy: 0.9733333333333334\n",
      "iteration no 2800: Loss: 0.2610585438328499, accuracy: 0.9766666666666667\n",
      "iteration no 2801: Loss: 0.26104832241918857, accuracy: 0.9733333333333334\n",
      "iteration no 2802: Loss: 0.2610354024907118, accuracy: 0.9766666666666667\n",
      "iteration no 2803: Loss: 0.2610210602591233, accuracy: 0.9733333333333334\n",
      "iteration no 2804: Loss: 0.26101164008750005, accuracy: 0.9766666666666667\n",
      "iteration no 2805: Loss: 0.26099652191019257, accuracy: 0.9733333333333334\n",
      "iteration no 2806: Loss: 0.26098687793694764, accuracy: 0.9766666666666667\n",
      "iteration no 2807: Loss: 0.2609718856962192, accuracy: 0.9733333333333334\n",
      "iteration no 2808: Loss: 0.26096270930250465, accuracy: 0.9766666666666667\n",
      "iteration no 2809: Loss: 0.2609489369211947, accuracy: 0.9733333333333334\n",
      "iteration no 2810: Loss: 0.26093567722383204, accuracy: 0.9766666666666667\n",
      "iteration no 2811: Loss: 0.26092491306428117, accuracy: 0.9733333333333334\n",
      "iteration no 2812: Loss: 0.260913716306394, accuracy: 0.9733333333333334\n",
      "iteration no 2813: Loss: 0.2609000798697033, accuracy: 0.9733333333333334\n",
      "iteration no 2814: Loss: 0.26088854299336445, accuracy: 0.9766666666666667\n",
      "iteration no 2815: Loss: 0.26087766795474804, accuracy: 0.9733333333333334\n",
      "iteration no 2816: Loss: 0.26086327853389923, accuracy: 0.9766666666666667\n",
      "iteration no 2817: Loss: 0.26085272514673363, accuracy: 0.9733333333333334\n",
      "iteration no 2818: Loss: 0.2608409588350219, accuracy: 0.9766666666666667\n",
      "iteration no 2819: Loss: 0.26083026420793276, accuracy: 0.9733333333333334\n",
      "iteration no 2820: Loss: 0.26081531679502556, accuracy: 0.9733333333333334\n",
      "iteration no 2821: Loss: 0.2608052007269115, accuracy: 0.9766666666666667\n",
      "iteration no 2822: Loss: 0.2607933942081534, accuracy: 0.9733333333333334\n",
      "iteration no 2823: Loss: 0.2607815390469898, accuracy: 0.9766666666666667\n",
      "iteration no 2824: Loss: 0.26076854673470173, accuracy: 0.9733333333333334\n",
      "iteration no 2825: Loss: 0.26075820282161766, accuracy: 0.9733333333333334\n",
      "iteration no 2826: Loss: 0.260746517248401, accuracy: 0.9733333333333334\n",
      "iteration no 2827: Loss: 0.26073167939895064, accuracy: 0.9733333333333334\n",
      "iteration no 2828: Loss: 0.26072409516487083, accuracy: 0.9733333333333334\n",
      "iteration no 2829: Loss: 0.2607106942087424, accuracy: 0.9733333333333334\n",
      "iteration no 2830: Loss: 0.2606991251201717, accuracy: 0.9733333333333334\n",
      "iteration no 2831: Loss: 0.26068620151829464, accuracy: 0.9733333333333334\n",
      "iteration no 2832: Loss: 0.2606766662108355, accuracy: 0.9733333333333334\n",
      "iteration no 2833: Loss: 0.2606641799179234, accuracy: 0.9733333333333334\n",
      "iteration no 2834: Loss: 0.26065135544112805, accuracy: 0.9733333333333334\n",
      "iteration no 2835: Loss: 0.26064277258736485, accuracy: 0.9766666666666667\n",
      "iteration no 2836: Loss: 0.2606290983720545, accuracy: 0.9733333333333334\n",
      "iteration no 2837: Loss: 0.26061750216258744, accuracy: 0.9766666666666667\n",
      "iteration no 2838: Loss: 0.26060536848227556, accuracy: 0.9733333333333334\n",
      "iteration no 2839: Loss: 0.26059681921854305, accuracy: 0.9733333333333334\n",
      "iteration no 2840: Loss: 0.26058309489658327, accuracy: 0.9733333333333334\n",
      "iteration no 2841: Loss: 0.26057131595372124, accuracy: 0.9766666666666667\n",
      "iteration no 2842: Loss: 0.26056214732285043, accuracy: 0.9766666666666667\n",
      "iteration no 2843: Loss: 0.260548952684845, accuracy: 0.9766666666666667\n",
      "iteration no 2844: Loss: 0.2605382735778528, accuracy: 0.9766666666666667\n",
      "iteration no 2845: Loss: 0.2605278538653655, accuracy: 0.9766666666666667\n",
      "iteration no 2846: Loss: 0.2605157273751218, accuracy: 0.9766666666666667\n",
      "iteration no 2847: Loss: 0.2605024607166244, accuracy: 0.9766666666666667\n",
      "iteration no 2848: Loss: 0.2604936032397514, accuracy: 0.9766666666666667\n",
      "iteration no 2849: Loss: 0.2604815313952835, accuracy: 0.9733333333333334\n",
      "iteration no 2850: Loss: 0.26047069089177566, accuracy: 0.9766666666666667\n",
      "iteration no 2851: Loss: 0.26045744013877004, accuracy: 0.9733333333333334\n",
      "iteration no 2852: Loss: 0.2604480786716756, accuracy: 0.9733333333333334\n",
      "iteration no 2853: Loss: 0.2604345150175934, accuracy: 0.9733333333333334\n",
      "iteration no 2854: Loss: 0.2604230689263005, accuracy: 0.9766666666666667\n",
      "iteration no 2855: Loss: 0.2604140583292949, accuracy: 0.9733333333333334\n",
      "iteration no 2856: Loss: 0.26039929391514716, accuracy: 0.9733333333333334\n",
      "iteration no 2857: Loss: 0.2603894420115704, accuracy: 0.9733333333333334\n",
      "iteration no 2858: Loss: 0.2603777684119434, accuracy: 0.9766666666666667\n",
      "iteration no 2859: Loss: 0.26036779383770575, accuracy: 0.9733333333333334\n",
      "iteration no 2860: Loss: 0.2603550182645894, accuracy: 0.9766666666666667\n",
      "iteration no 2861: Loss: 0.2603439672457595, accuracy: 0.9733333333333334\n",
      "iteration no 2862: Loss: 0.2603339294219768, accuracy: 0.9766666666666667\n",
      "iteration no 2863: Loss: 0.2603197962740161, accuracy: 0.9766666666666667\n",
      "iteration no 2864: Loss: 0.2603121255246835, accuracy: 0.9766666666666667\n",
      "iteration no 2865: Loss: 0.26029946471645143, accuracy: 0.9766666666666667\n",
      "iteration no 2866: Loss: 0.260288295745303, accuracy: 0.9766666666666667\n",
      "iteration no 2867: Loss: 0.2602758692530786, accuracy: 0.9766666666666667\n",
      "iteration no 2868: Loss: 0.2602660879814832, accuracy: 0.9766666666666667\n",
      "iteration no 2869: Loss: 0.2602552523538473, accuracy: 0.9766666666666667\n",
      "iteration no 2870: Loss: 0.2602418681927061, accuracy: 0.9766666666666667\n",
      "iteration no 2871: Loss: 0.2602345713644157, accuracy: 0.9733333333333334\n",
      "iteration no 2872: Loss: 0.2602212847335176, accuracy: 0.9766666666666667\n",
      "iteration no 2873: Loss: 0.2602088854047889, accuracy: 0.9766666666666667\n",
      "iteration no 2874: Loss: 0.26020091015942604, accuracy: 0.9766666666666667\n",
      "iteration no 2875: Loss: 0.26018878168397097, accuracy: 0.9733333333333334\n",
      "iteration no 2876: Loss: 0.26017733820689964, accuracy: 0.9766666666666667\n",
      "iteration no 2877: Loss: 0.2601659364010025, accuracy: 0.9766666666666667\n",
      "iteration no 2878: Loss: 0.26015511906959315, accuracy: 0.9766666666666667\n",
      "iteration no 2879: Loss: 0.2601442531173659, accuracy: 0.9766666666666667\n",
      "iteration no 2880: Loss: 0.26013239753871087, accuracy: 0.9766666666666667\n",
      "iteration no 2881: Loss: 0.26012321864106364, accuracy: 0.9766666666666667\n",
      "iteration no 2882: Loss: 0.2601097230489711, accuracy: 0.9766666666666667\n",
      "iteration no 2883: Loss: 0.2601004247806585, accuracy: 0.9766666666666667\n",
      "iteration no 2884: Loss: 0.2600906601412912, accuracy: 0.9766666666666667\n",
      "iteration no 2885: Loss: 0.26007761731216805, accuracy: 0.9766666666666667\n",
      "iteration no 2886: Loss: 0.26006765993702796, accuracy: 0.9766666666666667\n",
      "iteration no 2887: Loss: 0.260055701934989, accuracy: 0.9766666666666667\n",
      "iteration no 2888: Loss: 0.2600471234594114, accuracy: 0.9766666666666667\n",
      "iteration no 2889: Loss: 0.26003440733735916, accuracy: 0.9766666666666667\n",
      "iteration no 2890: Loss: 0.2600251551101074, accuracy: 0.9766666666666667\n",
      "iteration no 2891: Loss: 0.26001460340810656, accuracy: 0.9766666666666667\n",
      "iteration no 2892: Loss: 0.2599994477387879, accuracy: 0.9766666666666667\n",
      "iteration no 2893: Loss: 0.2599954285910199, accuracy: 0.9766666666666667\n",
      "iteration no 2894: Loss: 0.2599815987336839, accuracy: 0.9766666666666667\n",
      "iteration no 2895: Loss: 0.25997012888443183, accuracy: 0.9766666666666667\n",
      "iteration no 2896: Loss: 0.25996060245949043, accuracy: 0.9766666666666667\n",
      "iteration no 2897: Loss: 0.25994896315077143, accuracy: 0.9766666666666667\n",
      "iteration no 2898: Loss: 0.25993809756676733, accuracy: 0.9766666666666667\n",
      "iteration no 2899: Loss: 0.25992921785002443, accuracy: 0.9766666666666667\n",
      "iteration no 2900: Loss: 0.259917024342963, accuracy: 0.9766666666666667\n",
      "iteration no 2901: Loss: 0.259906363785012, accuracy: 0.9766666666666667\n",
      "iteration no 2902: Loss: 0.25989576663127123, accuracy: 0.9766666666666667\n",
      "iteration no 2903: Loss: 0.2598865208170224, accuracy: 0.9766666666666667\n",
      "iteration no 2904: Loss: 0.259874900027273, accuracy: 0.9766666666666667\n",
      "iteration no 2905: Loss: 0.25986336790164505, accuracy: 0.9766666666666667\n",
      "iteration no 2906: Loss: 0.25985486378320644, accuracy: 0.9766666666666667\n",
      "iteration no 2907: Loss: 0.25984299211294004, accuracy: 0.9766666666666667\n",
      "iteration no 2908: Loss: 0.25983262516030975, accuracy: 0.9766666666666667\n",
      "iteration no 2909: Loss: 0.25982345285586, accuracy: 0.9766666666666667\n",
      "iteration no 2910: Loss: 0.259809709422049, accuracy: 0.9766666666666667\n",
      "iteration no 2911: Loss: 0.2598013680253712, accuracy: 0.9766666666666667\n",
      "iteration no 2912: Loss: 0.259791605628935, accuracy: 0.9766666666666667\n",
      "iteration no 2913: Loss: 0.25977847414325683, accuracy: 0.9766666666666667\n",
      "iteration no 2914: Loss: 0.2597684725444412, accuracy: 0.9766666666666667\n",
      "iteration no 2915: Loss: 0.25975803045847096, accuracy: 0.9766666666666667\n",
      "iteration no 2916: Loss: 0.25974720066216694, accuracy: 0.9766666666666667\n",
      "iteration no 2917: Loss: 0.2597370055764113, accuracy: 0.9766666666666667\n",
      "iteration no 2918: Loss: 0.25972609703204114, accuracy: 0.9766666666666667\n",
      "iteration no 2919: Loss: 0.25971687764460294, accuracy: 0.9766666666666667\n",
      "iteration no 2920: Loss: 0.2597050163781341, accuracy: 0.9766666666666667\n",
      "iteration no 2921: Loss: 0.2596969311126567, accuracy: 0.9766666666666667\n",
      "iteration no 2922: Loss: 0.25968531116851695, accuracy: 0.9766666666666667\n",
      "iteration no 2923: Loss: 0.2596731609919125, accuracy: 0.9766666666666667\n",
      "iteration no 2924: Loss: 0.25966622947622947, accuracy: 0.9766666666666667\n",
      "iteration no 2925: Loss: 0.2596546834727467, accuracy: 0.9766666666666667\n",
      "iteration no 2926: Loss: 0.2596424332875604, accuracy: 0.9766666666666667\n",
      "iteration no 2927: Loss: 0.2596359817662825, accuracy: 0.9766666666666667\n",
      "iteration no 2928: Loss: 0.2596234447339179, accuracy: 0.9766666666666667\n",
      "iteration no 2929: Loss: 0.25961326206649643, accuracy: 0.9766666666666667\n",
      "iteration no 2930: Loss: 0.25960513314537026, accuracy: 0.9766666666666667\n",
      "iteration no 2931: Loss: 0.25959164532406787, accuracy: 0.9766666666666667\n",
      "iteration no 2932: Loss: 0.2595825674812773, accuracy: 0.9766666666666667\n",
      "iteration no 2933: Loss: 0.2595747461913447, accuracy: 0.9766666666666667\n",
      "iteration no 2934: Loss: 0.259563050796566, accuracy: 0.9766666666666667\n",
      "iteration no 2935: Loss: 0.2595523812923613, accuracy: 0.9766666666666667\n",
      "iteration no 2936: Loss: 0.2595433005103043, accuracy: 0.9766666666666667\n",
      "iteration no 2937: Loss: 0.2595339493959308, accuracy: 0.9766666666666667\n",
      "iteration no 2938: Loss: 0.2595219805717529, accuracy: 0.9766666666666667\n",
      "iteration no 2939: Loss: 0.2595130452413881, accuracy: 0.9766666666666667\n",
      "iteration no 2940: Loss: 0.25950368711922656, accuracy: 0.9766666666666667\n",
      "iteration no 2941: Loss: 0.2594916078476248, accuracy: 0.9766666666666667\n",
      "iteration no 2942: Loss: 0.2594848395572012, accuracy: 0.9766666666666667\n",
      "iteration no 2943: Loss: 0.2594730648999627, accuracy: 0.9766666666666667\n",
      "iteration no 2944: Loss: 0.2594616578154845, accuracy: 0.9766666666666667\n",
      "iteration no 2945: Loss: 0.2594546218150637, accuracy: 0.9766666666666667\n",
      "iteration no 2946: Loss: 0.25944320444682323, accuracy: 0.9766666666666667\n",
      "iteration no 2947: Loss: 0.2594314963807366, accuracy: 0.9766666666666667\n",
      "iteration no 2948: Loss: 0.25942536605798505, accuracy: 0.9766666666666667\n",
      "iteration no 2949: Loss: 0.25941417972020103, accuracy: 0.9766666666666667\n",
      "iteration no 2950: Loss: 0.2594031946045496, accuracy: 0.9766666666666667\n",
      "iteration no 2951: Loss: 0.2593939780427508, accuracy: 0.9766666666666667\n",
      "iteration no 2952: Loss: 0.2593828486094304, accuracy: 0.9766666666666667\n",
      "iteration no 2953: Loss: 0.25937442128852095, accuracy: 0.9766666666666667\n",
      "iteration no 2954: Loss: 0.2593655450203993, accuracy: 0.9766666666666667\n",
      "iteration no 2955: Loss: 0.2593538909643994, accuracy: 0.9766666666666667\n",
      "iteration no 2956: Loss: 0.25934306017939923, accuracy: 0.9766666666666667\n",
      "iteration no 2957: Loss: 0.2593356175745925, accuracy: 0.9766666666666667\n",
      "iteration no 2958: Loss: 0.25932583181354624, accuracy: 0.9766666666666667\n",
      "iteration no 2959: Loss: 0.2593146125637566, accuracy: 0.9766666666666667\n",
      "iteration no 2960: Loss: 0.25930518174049033, accuracy: 0.9766666666666667\n",
      "iteration no 2961: Loss: 0.2592960925854267, accuracy: 0.9766666666666667\n",
      "iteration no 2962: Loss: 0.2592855218017707, accuracy: 0.9766666666666667\n",
      "iteration no 2963: Loss: 0.25927704938037865, accuracy: 0.9766666666666667\n",
      "iteration no 2964: Loss: 0.2592661981287813, accuracy: 0.9766666666666667\n",
      "iteration no 2965: Loss: 0.2592554781011508, accuracy: 0.9766666666666667\n",
      "iteration no 2966: Loss: 0.2592489196829115, accuracy: 0.9766666666666667\n",
      "iteration no 2967: Loss: 0.2592383320135979, accuracy: 0.9766666666666667\n",
      "iteration no 2968: Loss: 0.2592261115375065, accuracy: 0.9766666666666667\n",
      "iteration no 2969: Loss: 0.25921965277762515, accuracy: 0.9766666666666667\n",
      "iteration no 2970: Loss: 0.25920886691769573, accuracy: 0.9766666666666667\n",
      "iteration no 2971: Loss: 0.25919838996096783, accuracy: 0.9766666666666667\n",
      "iteration no 2972: Loss: 0.2591911563247037, accuracy: 0.9766666666666667\n",
      "iteration no 2973: Loss: 0.2591796069851981, accuracy: 0.9766666666666667\n",
      "iteration no 2974: Loss: 0.25917063016855607, accuracy: 0.9766666666666667\n",
      "iteration no 2975: Loss: 0.2591621615161189, accuracy: 0.9766666666666667\n",
      "iteration no 2976: Loss: 0.2591498044511269, accuracy: 0.9766666666666667\n",
      "iteration no 2977: Loss: 0.2591420074491095, accuracy: 0.9766666666666667\n",
      "iteration no 2978: Loss: 0.25913345917839714, accuracy: 0.9766666666666667\n",
      "iteration no 2979: Loss: 0.2591223982722779, accuracy: 0.9766666666666667\n",
      "iteration no 2980: Loss: 0.2591131693109812, accuracy: 0.9766666666666667\n",
      "iteration no 2981: Loss: 0.2591041495560619, accuracy: 0.9766666666666667\n",
      "iteration no 2982: Loss: 0.25909513068122797, accuracy: 0.9766666666666667\n",
      "iteration no 2983: Loss: 0.25908519903695576, accuracy: 0.9766666666666667\n",
      "iteration no 2984: Loss: 0.25907500688901575, accuracy: 0.9766666666666667\n",
      "iteration no 2985: Loss: 0.2590665608375457, accuracy: 0.9766666666666667\n",
      "iteration no 2986: Loss: 0.2590562558275904, accuracy: 0.9766666666666667\n",
      "iteration no 2987: Loss: 0.2590491511156881, accuracy: 0.9766666666666667\n",
      "iteration no 2988: Loss: 0.2590373257880775, accuracy: 0.9766666666666667\n",
      "iteration no 2989: Loss: 0.25902785752108837, accuracy: 0.9766666666666667\n",
      "iteration no 2990: Loss: 0.2590215189002432, accuracy: 0.9766666666666667\n",
      "iteration no 2991: Loss: 0.25900866288724417, accuracy: 0.9766666666666667\n",
      "iteration no 2992: Loss: 0.2590004928453226, accuracy: 0.9766666666666667\n",
      "iteration no 2993: Loss: 0.25899262877933493, accuracy: 0.9766666666666667\n",
      "iteration no 2994: Loss: 0.25898089791820966, accuracy: 0.9766666666666667\n",
      "iteration no 2995: Loss: 0.25897401542608645, accuracy: 0.9766666666666667\n",
      "iteration no 2996: Loss: 0.2589636103805749, accuracy: 0.9766666666666667\n",
      "iteration no 2997: Loss: 0.2589527847071322, accuracy: 0.9766666666666667\n",
      "iteration no 2998: Loss: 0.258946918262887, accuracy: 0.9766666666666667\n",
      "iteration no 2999: Loss: 0.2589361117567264, accuracy: 0.9766666666666667\n",
      "iteration no 3000: Loss: 0.2589242352490235, accuracy: 0.9766666666666667\n",
      "iteration no 3001: Loss: 0.2589190257650149, accuracy: 0.9766666666666667\n",
      "iteration no 3002: Loss: 0.2589080136192124, accuracy: 0.9766666666666667\n",
      "iteration no 3003: Loss: 0.2588973479302621, accuracy: 0.9766666666666667\n",
      "iteration no 3004: Loss: 0.25888963397879894, accuracy: 0.9766666666666667\n",
      "iteration no 3005: Loss: 0.2588794060415307, accuracy: 0.9766666666666667\n",
      "iteration no 3006: Loss: 0.2588704268223967, accuracy: 0.9766666666666667\n",
      "iteration no 3007: Loss: 0.2588621456527471, accuracy: 0.9766666666666667\n",
      "iteration no 3008: Loss: 0.25884899315092813, accuracy: 0.9766666666666667\n",
      "iteration no 3009: Loss: 0.2588429043819537, accuracy: 0.9766666666666667\n",
      "iteration no 3010: Loss: 0.2588333963041028, accuracy: 0.9766666666666667\n",
      "iteration no 3011: Loss: 0.2588231223222866, accuracy: 0.9766666666666667\n",
      "iteration no 3012: Loss: 0.2588141470323152, accuracy: 0.9766666666666667\n",
      "iteration no 3013: Loss: 0.2588048422970917, accuracy: 0.9766666666666667\n",
      "iteration no 3014: Loss: 0.25879514157970096, accuracy: 0.9766666666666667\n",
      "iteration no 3015: Loss: 0.25878684183811723, accuracy: 0.9766666666666667\n",
      "iteration no 3016: Loss: 0.25877622638341413, accuracy: 0.9766666666666667\n",
      "iteration no 3017: Loss: 0.2587683873372979, accuracy: 0.9766666666666667\n",
      "iteration no 3018: Loss: 0.2587595313940518, accuracy: 0.9766666666666667\n",
      "iteration no 3019: Loss: 0.25875041034113705, accuracy: 0.9766666666666667\n",
      "iteration no 3020: Loss: 0.25874016847079706, accuracy: 0.9766666666666667\n",
      "iteration no 3021: Loss: 0.2587316667293247, accuracy: 0.9766666666666667\n",
      "iteration no 3022: Loss: 0.25872478672553306, accuracy: 0.9766666666666667\n",
      "iteration no 3023: Loss: 0.25871353735697783, accuracy: 0.9766666666666667\n",
      "iteration no 3024: Loss: 0.258703623675575, accuracy: 0.9766666666666667\n",
      "iteration no 3025: Loss: 0.2586967823933141, accuracy: 0.9766666666666667\n",
      "iteration no 3026: Loss: 0.25868672585948765, accuracy: 0.9766666666666667\n",
      "iteration no 3027: Loss: 0.2586778587494869, accuracy: 0.9766666666666667\n",
      "iteration no 3028: Loss: 0.2586700443310581, accuracy: 0.9766666666666667\n",
      "iteration no 3029: Loss: 0.25865846633031186, accuracy: 0.9766666666666667\n",
      "iteration no 3030: Loss: 0.2586504363740403, accuracy: 0.9766666666666667\n",
      "iteration no 3031: Loss: 0.2586442126520357, accuracy: 0.9766666666666667\n",
      "iteration no 3032: Loss: 0.258632055095391, accuracy: 0.9766666666666667\n",
      "iteration no 3033: Loss: 0.25862362043402903, accuracy: 0.9766666666666667\n",
      "iteration no 3034: Loss: 0.25861753199903814, accuracy: 0.9766666666666667\n",
      "iteration no 3035: Loss: 0.25860485741150296, accuracy: 0.9766666666666667\n",
      "iteration no 3036: Loss: 0.25859688631740263, accuracy: 0.9766666666666667\n",
      "iteration no 3037: Loss: 0.25858885542574545, accuracy: 0.9766666666666667\n",
      "iteration no 3038: Loss: 0.2585789647291177, accuracy: 0.9766666666666667\n",
      "iteration no 3039: Loss: 0.25857134688415584, accuracy: 0.9766666666666667\n",
      "iteration no 3040: Loss: 0.258562226893745, accuracy: 0.9766666666666667\n",
      "iteration no 3041: Loss: 0.25855215121670927, accuracy: 0.9766666666666667\n",
      "iteration no 3042: Loss: 0.2585451020911413, accuracy: 0.9766666666666667\n",
      "iteration no 3043: Loss: 0.2585358422704519, accuracy: 0.9766666666666667\n",
      "iteration no 3044: Loss: 0.2585259794510623, accuracy: 0.9766666666666667\n",
      "iteration no 3045: Loss: 0.25851724452218816, accuracy: 0.9766666666666667\n",
      "iteration no 3046: Loss: 0.25851028552514677, accuracy: 0.9766666666666667\n",
      "iteration no 3047: Loss: 0.25850070777438405, accuracy: 0.9766666666666667\n",
      "iteration no 3048: Loss: 0.2584897591184887, accuracy: 0.9766666666666667\n",
      "iteration no 3049: Loss: 0.25848298277028525, accuracy: 0.9766666666666667\n",
      "iteration no 3050: Loss: 0.25847495654217867, accuracy: 0.9766666666666667\n",
      "iteration no 3051: Loss: 0.2584634527068384, accuracy: 0.9766666666666667\n",
      "iteration no 3052: Loss: 0.2584568559009721, accuracy: 0.9766666666666667\n",
      "iteration no 3053: Loss: 0.25844876330373256, accuracy: 0.9766666666666667\n",
      "iteration no 3054: Loss: 0.25843808676118374, accuracy: 0.9766666666666667\n",
      "iteration no 3055: Loss: 0.2584318500357092, accuracy: 0.9766666666666667\n",
      "iteration no 3056: Loss: 0.2584214048104363, accuracy: 0.9766666666666667\n",
      "iteration no 3057: Loss: 0.2584120462060079, accuracy: 0.9766666666666667\n",
      "iteration no 3058: Loss: 0.25840755012074534, accuracy: 0.9766666666666667\n",
      "iteration no 3059: Loss: 0.25839509312759695, accuracy: 0.9766666666666667\n",
      "iteration no 3060: Loss: 0.25838496358368596, accuracy: 0.9766666666666667\n",
      "iteration no 3061: Loss: 0.2583812885156188, accuracy: 0.9766666666666667\n",
      "iteration no 3062: Loss: 0.25837062829065194, accuracy: 0.9766666666666667\n",
      "iteration no 3063: Loss: 0.2583593005688368, accuracy: 0.9766666666666667\n",
      "iteration no 3064: Loss: 0.25835424681210956, accuracy: 0.9766666666666667\n",
      "iteration no 3065: Loss: 0.2583448323017793, accuracy: 0.9766666666666667\n",
      "iteration no 3066: Loss: 0.25833621744697455, accuracy: 0.9766666666666667\n",
      "iteration no 3067: Loss: 0.25832733704705035, accuracy: 0.9766666666666667\n",
      "iteration no 3068: Loss: 0.2583189524709475, accuracy: 0.9766666666666667\n",
      "iteration no 3069: Loss: 0.2583107502642855, accuracy: 0.9766666666666667\n",
      "iteration no 3070: Loss: 0.2583021872068535, accuracy: 0.9766666666666667\n",
      "iteration no 3071: Loss: 0.25829330494426495, accuracy: 0.9766666666666667\n",
      "iteration no 3072: Loss: 0.25828545766975014, accuracy: 0.9766666666666667\n",
      "iteration no 3073: Loss: 0.2582761404680556, accuracy: 0.9766666666666667\n",
      "iteration no 3074: Loss: 0.2582694983731819, accuracy: 0.9766666666666667\n",
      "iteration no 3075: Loss: 0.25826005115222983, accuracy: 0.9766666666666667\n",
      "iteration no 3076: Loss: 0.2582511335038273, accuracy: 0.9766666666666667\n",
      "iteration no 3077: Loss: 0.25824492919638287, accuracy: 0.9766666666666667\n",
      "iteration no 3078: Loss: 0.25823458467606447, accuracy: 0.9766666666666667\n",
      "iteration no 3079: Loss: 0.2582243556426529, accuracy: 0.9766666666666667\n",
      "iteration no 3080: Loss: 0.25821964786736357, accuracy: 0.9766666666666667\n",
      "iteration no 3081: Loss: 0.2582101038173647, accuracy: 0.9766666666666667\n",
      "iteration no 3082: Loss: 0.25819988107286473, accuracy: 0.9766666666666667\n",
      "iteration no 3083: Loss: 0.2581951333858229, accuracy: 0.9766666666666667\n",
      "iteration no 3084: Loss: 0.2581848364097131, accuracy: 0.9766666666666667\n",
      "iteration no 3085: Loss: 0.25817548668625045, accuracy: 0.9766666666666667\n",
      "iteration no 3086: Loss: 0.2581692563961563, accuracy: 0.9766666666666667\n",
      "iteration no 3087: Loss: 0.2581601244257211, accuracy: 0.9766666666666667\n",
      "iteration no 3088: Loss: 0.2581503253897305, accuracy: 0.9766666666666667\n",
      "iteration no 3089: Loss: 0.25814654690395755, accuracy: 0.9766666666666667\n",
      "iteration no 3090: Loss: 0.25813386296278756, accuracy: 0.9766666666666667\n",
      "iteration no 3091: Loss: 0.2581259932166179, accuracy: 0.9766666666666667\n",
      "iteration no 3092: Loss: 0.2581209707126402, accuracy: 0.9766666666666667\n",
      "iteration no 3093: Loss: 0.25811038530170555, accuracy: 0.9766666666666667\n",
      "iteration no 3094: Loss: 0.25810057574875045, accuracy: 0.9766666666666667\n",
      "iteration no 3095: Loss: 0.25809713230609577, accuracy: 0.9766666666666667\n",
      "iteration no 3096: Loss: 0.2580856929307995, accuracy: 0.9766666666666667\n",
      "iteration no 3097: Loss: 0.25807714158320183, accuracy: 0.9766666666666667\n",
      "iteration no 3098: Loss: 0.25807023839482746, accuracy: 0.9766666666666667\n",
      "iteration no 3099: Loss: 0.25806248213866917, accuracy: 0.9766666666666667\n",
      "iteration no 3100: Loss: 0.2580534256737408, accuracy: 0.9766666666666667\n",
      "iteration no 3101: Loss: 0.25804487886323463, accuracy: 0.9766666666666667\n",
      "iteration no 3102: Loss: 0.2580377325076415, accuracy: 0.9766666666666667\n",
      "iteration no 3103: Loss: 0.25802944580423853, accuracy: 0.9766666666666667\n",
      "iteration no 3104: Loss: 0.2580201666862001, accuracy: 0.9766666666666667\n",
      "iteration no 3105: Loss: 0.2580123633177056, accuracy: 0.9766666666666667\n",
      "iteration no 3106: Loss: 0.2580066438216384, accuracy: 0.9766666666666667\n",
      "iteration no 3107: Loss: 0.2579960839825477, accuracy: 0.9766666666666667\n",
      "iteration no 3108: Loss: 0.2579881840190387, accuracy: 0.9766666666666667\n",
      "iteration no 3109: Loss: 0.25798143310598465, accuracy: 0.9766666666666667\n",
      "iteration no 3110: Loss: 0.25797395648628607, accuracy: 0.9766666666666667\n",
      "iteration no 3111: Loss: 0.2579634411010219, accuracy: 0.9766666666666667\n",
      "iteration no 3112: Loss: 0.25795869972288155, accuracy: 0.9766666666666667\n",
      "iteration no 3113: Loss: 0.2579492317688023, accuracy: 0.9766666666666667\n",
      "iteration no 3114: Loss: 0.25794022020766294, accuracy: 0.9766666666666667\n",
      "iteration no 3115: Loss: 0.25793386787941874, accuracy: 0.9766666666666667\n",
      "iteration no 3116: Loss: 0.25792470281909485, accuracy: 0.9766666666666667\n",
      "iteration no 3117: Loss: 0.25791680111353477, accuracy: 0.9766666666666667\n",
      "iteration no 3118: Loss: 0.2579094760535868, accuracy: 0.9766666666666667\n",
      "iteration no 3119: Loss: 0.25790206403810934, accuracy: 0.9766666666666667\n",
      "iteration no 3120: Loss: 0.25789165734672004, accuracy: 0.9766666666666667\n",
      "iteration no 3121: Loss: 0.2578854671733764, accuracy: 0.9766666666666667\n",
      "iteration no 3122: Loss: 0.2578774216769194, accuracy: 0.9766666666666667\n",
      "iteration no 3123: Loss: 0.25787040018625845, accuracy: 0.9766666666666667\n",
      "iteration no 3124: Loss: 0.2578609047782846, accuracy: 0.9766666666666667\n",
      "iteration no 3125: Loss: 0.2578541912169144, accuracy: 0.9766666666666667\n",
      "iteration no 3126: Loss: 0.2578471471134621, accuracy: 0.9766666666666667\n",
      "iteration no 3127: Loss: 0.2578375642577115, accuracy: 0.9766666666666667\n",
      "iteration no 3128: Loss: 0.25782869096586714, accuracy: 0.9766666666666667\n",
      "iteration no 3129: Loss: 0.2578260143079121, accuracy: 0.9766666666666667\n",
      "iteration no 3130: Loss: 0.25781474260499343, accuracy: 0.9766666666666667\n",
      "iteration no 3131: Loss: 0.2578060824621059, accuracy: 0.9766666666666667\n",
      "iteration no 3132: Loss: 0.2578024186100084, accuracy: 0.9766666666666667\n",
      "iteration no 3133: Loss: 0.25779131233959973, accuracy: 0.9766666666666667\n",
      "iteration no 3134: Loss: 0.2577820764149665, accuracy: 0.9766666666666667\n",
      "iteration no 3135: Loss: 0.25777679115899477, accuracy: 0.9766666666666667\n",
      "iteration no 3136: Loss: 0.25777086509880615, accuracy: 0.9766666666666667\n",
      "iteration no 3137: Loss: 0.25775958803547583, accuracy: 0.9766666666666667\n",
      "iteration no 3138: Loss: 0.2577536852910881, accuracy: 0.9766666666666667\n",
      "iteration no 3139: Loss: 0.25774735882071886, accuracy: 0.9766666666666667\n",
      "iteration no 3140: Loss: 0.257737816155543, accuracy: 0.9766666666666667\n",
      "iteration no 3141: Loss: 0.2577292675998441, accuracy: 0.9766666666666667\n",
      "iteration no 3142: Loss: 0.2577238576352096, accuracy: 0.9766666666666667\n",
      "iteration no 3143: Loss: 0.2577151046236458, accuracy: 0.9766666666666667\n",
      "iteration no 3144: Loss: 0.2577069578489496, accuracy: 0.9766666666666667\n",
      "iteration no 3145: Loss: 0.25770084848766917, accuracy: 0.9766666666666667\n",
      "iteration no 3146: Loss: 0.25769156054874703, accuracy: 0.9766666666666667\n",
      "iteration no 3147: Loss: 0.2576841432343283, accuracy: 0.9766666666666667\n",
      "iteration no 3148: Loss: 0.25767704468582253, accuracy: 0.9766666666666667\n",
      "iteration no 3149: Loss: 0.2576695938579818, accuracy: 0.9766666666666667\n",
      "iteration no 3150: Loss: 0.257662105756561, accuracy: 0.9766666666666667\n",
      "iteration no 3151: Loss: 0.25765536986038207, accuracy: 0.9766666666666667\n",
      "iteration no 3152: Loss: 0.2576463230555166, accuracy: 0.9766666666666667\n",
      "iteration no 3153: Loss: 0.2576397110166343, accuracy: 0.9766666666666667\n",
      "iteration no 3154: Loss: 0.25763142940464856, accuracy: 0.9766666666666667\n",
      "iteration no 3155: Loss: 0.2576247271796392, accuracy: 0.9766666666666667\n",
      "iteration no 3156: Loss: 0.2576170609610157, accuracy: 0.9766666666666667\n",
      "iteration no 3157: Loss: 0.25760923860018803, accuracy: 0.9766666666666667\n",
      "iteration no 3158: Loss: 0.25760217597897184, accuracy: 0.9766666666666667\n",
      "iteration no 3159: Loss: 0.25759446794722995, accuracy: 0.9766666666666667\n",
      "iteration no 3160: Loss: 0.257587212551149, accuracy: 0.9766666666666667\n",
      "iteration no 3161: Loss: 0.25757899843020626, accuracy: 0.9766666666666667\n",
      "iteration no 3162: Loss: 0.2575731823118387, accuracy: 0.9766666666666667\n",
      "iteration no 3163: Loss: 0.2575633710988929, accuracy: 0.9766666666666667\n",
      "iteration no 3164: Loss: 0.25755669807039433, accuracy: 0.9766666666666667\n",
      "iteration no 3165: Loss: 0.2575505472969838, accuracy: 0.9766666666666667\n",
      "iteration no 3166: Loss: 0.2575418570296013, accuracy: 0.9766666666666667\n",
      "iteration no 3167: Loss: 0.25753350472528813, accuracy: 0.9766666666666667\n",
      "iteration no 3168: Loss: 0.25752865226977145, accuracy: 0.9766666666666667\n",
      "iteration no 3169: Loss: 0.25752089335649225, accuracy: 0.9766666666666667\n",
      "iteration no 3170: Loss: 0.2575104897304663, accuracy: 0.9766666666666667\n",
      "iteration no 3171: Loss: 0.2575067429011516, accuracy: 0.9766666666666667\n",
      "iteration no 3172: Loss: 0.2574986052275979, accuracy: 0.9766666666666667\n",
      "iteration no 3173: Loss: 0.25748903597813205, accuracy: 0.9766666666666667\n",
      "iteration no 3174: Loss: 0.2574838778849131, accuracy: 0.9766666666666667\n",
      "iteration no 3175: Loss: 0.2574772792744934, accuracy: 0.9766666666666667\n",
      "iteration no 3176: Loss: 0.25746711433735764, accuracy: 0.9766666666666667\n",
      "iteration no 3177: Loss: 0.2574606002133899, accuracy: 0.9766666666666667\n",
      "iteration no 3178: Loss: 0.2574565917673257, accuracy: 0.9766666666666667\n",
      "iteration no 3179: Loss: 0.2574452826664792, accuracy: 0.9766666666666667\n",
      "iteration no 3180: Loss: 0.25743836899291406, accuracy: 0.9766666666666667\n",
      "iteration no 3181: Loss: 0.2574343906545884, accuracy: 0.9766666666666667\n",
      "iteration no 3182: Loss: 0.2574240665621873, accuracy: 0.9766666666666667\n",
      "iteration no 3183: Loss: 0.25741536492971656, accuracy: 0.9766666666666667\n",
      "iteration no 3184: Loss: 0.2574114970974724, accuracy: 0.9766666666666667\n",
      "iteration no 3185: Loss: 0.25740425346022355, accuracy: 0.9766666666666667\n",
      "iteration no 3186: Loss: 0.2573941778229488, accuracy: 0.9766666666666667\n",
      "iteration no 3187: Loss: 0.25738946085921444, accuracy: 0.9766666666666667\n",
      "iteration no 3188: Loss: 0.25738152640200906, accuracy: 0.9766666666666667\n",
      "iteration no 3189: Loss: 0.25737290294885445, accuracy: 0.9766666666666667\n",
      "iteration no 3190: Loss: 0.25736568745032984, accuracy: 0.9766666666666667\n",
      "iteration no 3191: Loss: 0.2573615303869435, accuracy: 0.9766666666666667\n",
      "iteration no 3192: Loss: 0.2573512816817984, accuracy: 0.9766666666666667\n",
      "iteration no 3193: Loss: 0.2573450026980546, accuracy: 0.9766666666666667\n",
      "iteration no 3194: Loss: 0.25733926986524935, accuracy: 0.9766666666666667\n",
      "iteration no 3195: Loss: 0.25732951845317265, accuracy: 0.9766666666666667\n",
      "iteration no 3196: Loss: 0.2573235939103534, accuracy: 0.9766666666666667\n",
      "iteration no 3197: Loss: 0.25731687787554147, accuracy: 0.9766666666666667\n",
      "iteration no 3198: Loss: 0.2573085165059588, accuracy: 0.9766666666666667\n",
      "iteration no 3199: Loss: 0.25730230495038475, accuracy: 0.9766666666666667\n",
      "iteration no 3200: Loss: 0.2572953145259171, accuracy: 0.9766666666666667\n",
      "iteration no 3201: Loss: 0.2572876472381079, accuracy: 0.9766666666666667\n",
      "iteration no 3202: Loss: 0.25728023671806843, accuracy: 0.9766666666666667\n",
      "iteration no 3203: Loss: 0.2572739193301446, accuracy: 0.9766666666666667\n",
      "iteration no 3204: Loss: 0.2572654457323928, accuracy: 0.9766666666666667\n",
      "iteration no 3205: Loss: 0.2572601234463117, accuracy: 0.9766666666666667\n",
      "iteration no 3206: Loss: 0.2572517959068372, accuracy: 0.9766666666666667\n",
      "iteration no 3207: Loss: 0.25724490353239937, accuracy: 0.9766666666666667\n",
      "iteration no 3208: Loss: 0.25723824758399483, accuracy: 0.9766666666666667\n",
      "iteration no 3209: Loss: 0.2572323452119823, accuracy: 0.9766666666666667\n",
      "iteration no 3210: Loss: 0.2572233008034999, accuracy: 0.9766666666666667\n",
      "iteration no 3211: Loss: 0.25721730120823666, accuracy: 0.9766666666666667\n",
      "iteration no 3212: Loss: 0.2572111754292655, accuracy: 0.9766666666666667\n",
      "iteration no 3213: Loss: 0.2572020357387868, accuracy: 0.9766666666666667\n",
      "iteration no 3214: Loss: 0.25719608761512414, accuracy: 0.9766666666666667\n",
      "iteration no 3215: Loss: 0.2571891540200275, accuracy: 0.9766666666666667\n",
      "iteration no 3216: Loss: 0.25718259726587545, accuracy: 0.9766666666666667\n",
      "iteration no 3217: Loss: 0.2571736832435586, accuracy: 0.9766666666666667\n",
      "iteration no 3218: Loss: 0.257169607064753, accuracy: 0.9766666666666667\n",
      "iteration no 3219: Loss: 0.2571613842054751, accuracy: 0.9766666666666667\n",
      "iteration no 3220: Loss: 0.25715265948809296, accuracy: 0.9766666666666667\n",
      "iteration no 3221: Loss: 0.25714875918582386, accuracy: 0.9766666666666667\n",
      "iteration no 3222: Loss: 0.2571402280899451, accuracy: 0.9766666666666667\n",
      "iteration no 3223: Loss: 0.25713141524172123, accuracy: 0.9766666666666667\n",
      "iteration no 3224: Loss: 0.25712789154029814, accuracy: 0.9766666666666667\n",
      "iteration no 3225: Loss: 0.2571211140251247, accuracy: 0.9766666666666667\n",
      "iteration no 3226: Loss: 0.25711170457092847, accuracy: 0.9766666666666667\n",
      "iteration no 3227: Loss: 0.25710563178974866, accuracy: 0.9766666666666667\n",
      "iteration no 3228: Loss: 0.2571010642688935, accuracy: 0.9766666666666667\n",
      "iteration no 3229: Loss: 0.25708997575540504, accuracy: 0.9766666666666667\n",
      "iteration no 3230: Loss: 0.2570847209740651, accuracy: 0.9766666666666667\n",
      "iteration no 3231: Loss: 0.257080670813019, accuracy: 0.9766666666666667\n",
      "iteration no 3232: Loss: 0.2570703376307115, accuracy: 0.9766666666666667\n",
      "iteration no 3233: Loss: 0.2570634593177473, accuracy: 0.9766666666666667\n",
      "iteration no 3234: Loss: 0.2570601703416762, accuracy: 0.9766666666666667\n",
      "iteration no 3235: Loss: 0.2570504756727105, accuracy: 0.9766666666666667\n",
      "iteration no 3236: Loss: 0.25704127935113397, accuracy: 0.9766666666666667\n",
      "iteration no 3237: Loss: 0.25704076247410856, accuracy: 0.9766666666666667\n",
      "iteration no 3238: Loss: 0.2570293753878303, accuracy: 0.9766666666666667\n",
      "iteration no 3239: Loss: 0.2570223608452426, accuracy: 0.9766666666666667\n",
      "iteration no 3240: Loss: 0.2570191801962931, accuracy: 0.9766666666666667\n",
      "iteration no 3241: Loss: 0.2570093960235762, accuracy: 0.9766666666666667\n",
      "iteration no 3242: Loss: 0.2570011367809034, accuracy: 0.9766666666666667\n",
      "iteration no 3243: Loss: 0.25699767896508324, accuracy: 0.9766666666666667\n",
      "iteration no 3244: Loss: 0.25698997605078155, accuracy: 0.9766666666666667\n",
      "iteration no 3245: Loss: 0.2569808008405703, accuracy: 0.9766666666666667\n",
      "iteration no 3246: Loss: 0.2569764589264416, accuracy: 0.9766666666666667\n",
      "iteration no 3247: Loss: 0.25697006133441236, accuracy: 0.9766666666666667\n",
      "iteration no 3248: Loss: 0.2569606376802604, accuracy: 0.9766666666666667\n",
      "iteration no 3249: Loss: 0.2569559977845804, accuracy: 0.9766666666666667\n",
      "iteration no 3250: Loss: 0.25694914208404174, accuracy: 0.9766666666666667\n",
      "iteration no 3251: Loss: 0.2569412403965072, accuracy: 0.9766666666666667\n",
      "iteration no 3252: Loss: 0.2569333208580857, accuracy: 0.9766666666666667\n",
      "iteration no 3253: Loss: 0.2569298815335042, accuracy: 0.9766666666666667\n",
      "iteration no 3254: Loss: 0.2569215042889234, accuracy: 0.9766666666666667\n",
      "iteration no 3255: Loss: 0.25691216318036647, accuracy: 0.9766666666666667\n",
      "iteration no 3256: Loss: 0.25691056413135654, accuracy: 0.9766666666666667\n",
      "iteration no 3257: Loss: 0.2569010697215616, accuracy: 0.9766666666666667\n",
      "iteration no 3258: Loss: 0.25689268684745, accuracy: 0.9766666666666667\n",
      "iteration no 3259: Loss: 0.25688982835610535, accuracy: 0.9766666666666667\n",
      "iteration no 3260: Loss: 0.2568809884323078, accuracy: 0.9766666666666667\n",
      "iteration no 3261: Loss: 0.2568732784015198, accuracy: 0.9766666666666667\n",
      "iteration no 3262: Loss: 0.25686874943638005, accuracy: 0.9766666666666667\n",
      "iteration no 3263: Loss: 0.25686225808946084, accuracy: 0.9766666666666667\n",
      "iteration no 3264: Loss: 0.25685305930845237, accuracy: 0.9766666666666667\n",
      "iteration no 3265: Loss: 0.2568482050095501, accuracy: 0.9766666666666667\n",
      "iteration no 3266: Loss: 0.25684317438071175, accuracy: 0.9766666666666667\n",
      "iteration no 3267: Loss: 0.25683325852406536, accuracy: 0.9766666666666667\n",
      "iteration no 3268: Loss: 0.2568279972367976, accuracy: 0.9766666666666667\n",
      "iteration no 3269: Loss: 0.256823749222712, accuracy: 0.9766666666666667\n",
      "iteration no 3270: Loss: 0.25681406622789593, accuracy: 0.9766666666666667\n",
      "iteration no 3271: Loss: 0.2568068087156966, accuracy: 0.9766666666666667\n",
      "iteration no 3272: Loss: 0.25680402540696556, accuracy: 0.9766666666666667\n",
      "iteration no 3273: Loss: 0.2567949292702671, accuracy: 0.9766666666666667\n",
      "iteration no 3274: Loss: 0.2567867054388348, accuracy: 0.9766666666666667\n",
      "iteration no 3275: Loss: 0.25678513680416515, accuracy: 0.98\n",
      "iteration no 3276: Loss: 0.2567754630322933, accuracy: 0.9766666666666667\n",
      "iteration no 3277: Loss: 0.2567666201679264, accuracy: 0.9766666666666667\n",
      "iteration no 3278: Loss: 0.2567646639355935, accuracy: 0.9766666666666667\n",
      "iteration no 3279: Loss: 0.2567572853864031, accuracy: 0.98\n",
      "iteration no 3280: Loss: 0.2567483552172619, accuracy: 0.9766666666666667\n",
      "iteration no 3281: Loss: 0.2567439266167338, accuracy: 0.98\n",
      "iteration no 3282: Loss: 0.2567383954377033, accuracy: 0.9766666666666667\n",
      "iteration no 3283: Loss: 0.25672904633917737, accuracy: 0.98\n",
      "iteration no 3284: Loss: 0.2567225313798263, accuracy: 0.9766666666666667\n",
      "iteration no 3285: Loss: 0.2567202105712737, accuracy: 0.98\n",
      "iteration no 3286: Loss: 0.25670967191564775, accuracy: 0.9766666666666667\n",
      "iteration no 3287: Loss: 0.25670286745377646, accuracy: 0.9766666666666667\n",
      "iteration no 3288: Loss: 0.2567001219316688, accuracy: 0.9766666666666667\n",
      "iteration no 3289: Loss: 0.25669128422642906, accuracy: 0.9766666666666667\n",
      "iteration no 3290: Loss: 0.25668445936490436, accuracy: 0.9766666666666667\n",
      "iteration no 3291: Loss: 0.25667997783202384, accuracy: 0.98\n",
      "iteration no 3292: Loss: 0.2566730561553737, accuracy: 0.98\n",
      "iteration no 3293: Loss: 0.25666430083380676, accuracy: 0.98\n",
      "iteration no 3294: Loss: 0.2566600600657498, accuracy: 0.9766666666666667\n",
      "iteration no 3295: Loss: 0.2566537859755297, accuracy: 0.98\n",
      "iteration no 3296: Loss: 0.2566461489364701, accuracy: 0.9766666666666667\n",
      "iteration no 3297: Loss: 0.2566418493014172, accuracy: 0.98\n",
      "iteration no 3298: Loss: 0.2566346990140257, accuracy: 0.9766666666666667\n",
      "iteration no 3299: Loss: 0.25662764717639436, accuracy: 0.98\n",
      "iteration no 3300: Loss: 0.2566217462323749, accuracy: 0.98\n",
      "iteration no 3301: Loss: 0.25661479579180463, accuracy: 0.98\n",
      "iteration no 3302: Loss: 0.25660925294559966, accuracy: 0.98\n",
      "iteration no 3303: Loss: 0.2566027695037247, accuracy: 0.9766666666666667\n",
      "iteration no 3304: Loss: 0.25659750735204967, accuracy: 0.98\n",
      "iteration no 3305: Loss: 0.2565899755270067, accuracy: 0.98\n",
      "iteration no 3306: Loss: 0.25658377433624274, accuracy: 0.98\n",
      "iteration no 3307: Loss: 0.25657869926049526, accuracy: 0.98\n",
      "iteration no 3308: Loss: 0.2565703333175329, accuracy: 0.98\n",
      "iteration no 3309: Loss: 0.25656524829864796, accuracy: 0.98\n",
      "iteration no 3310: Loss: 0.2565593638230503, accuracy: 0.98\n",
      "iteration no 3311: Loss: 0.25655128574654035, accuracy: 0.98\n",
      "iteration no 3312: Loss: 0.25654546457961314, accuracy: 0.9766666666666667\n",
      "iteration no 3313: Loss: 0.25653905837129687, accuracy: 0.98\n",
      "iteration no 3314: Loss: 0.25653188074414623, accuracy: 0.98\n",
      "iteration no 3315: Loss: 0.2565256417485746, accuracy: 0.98\n",
      "iteration no 3316: Loss: 0.2565176091648034, accuracy: 0.98\n",
      "iteration no 3317: Loss: 0.2565136100649109, accuracy: 0.98\n",
      "iteration no 3318: Loss: 0.25650602783423043, accuracy: 0.98\n",
      "iteration no 3319: Loss: 0.2564979085989158, accuracy: 0.98\n",
      "iteration no 3320: Loss: 0.25649431280932544, accuracy: 0.98\n",
      "iteration no 3321: Loss: 0.25648649561057324, accuracy: 0.98\n",
      "iteration no 3322: Loss: 0.25647910919492267, accuracy: 0.98\n",
      "iteration no 3323: Loss: 0.2564725600718204, accuracy: 0.98\n",
      "iteration no 3324: Loss: 0.2564682756183715, accuracy: 0.98\n",
      "iteration no 3325: Loss: 0.25646099300812997, accuracy: 0.98\n",
      "iteration no 3326: Loss: 0.2564526116976276, accuracy: 0.98\n",
      "iteration no 3327: Loss: 0.2564490881930033, accuracy: 0.98\n",
      "iteration no 3328: Loss: 0.25644247979282286, accuracy: 0.98\n",
      "iteration no 3329: Loss: 0.25643410962107593, accuracy: 0.98\n",
      "iteration no 3330: Loss: 0.25642993701120137, accuracy: 0.98\n",
      "iteration no 3331: Loss: 0.2564238956883934, accuracy: 0.98\n",
      "iteration no 3332: Loss: 0.25641722286659974, accuracy: 0.98\n",
      "iteration no 3333: Loss: 0.2564114613256111, accuracy: 0.98\n",
      "iteration no 3334: Loss: 0.2564054557965737, accuracy: 0.98\n",
      "iteration no 3335: Loss: 0.25639941337065436, accuracy: 0.98\n",
      "iteration no 3336: Loss: 0.25639261031567295, accuracy: 0.98\n",
      "iteration no 3337: Loss: 0.25638755402851787, accuracy: 0.98\n",
      "iteration no 3338: Loss: 0.25638111969558547, accuracy: 0.98\n",
      "iteration no 3339: Loss: 0.25637495612802286, accuracy: 0.98\n",
      "iteration no 3340: Loss: 0.256368785026521, accuracy: 0.98\n",
      "iteration no 3341: Loss: 0.25636367258681364, accuracy: 0.98\n",
      "iteration no 3342: Loss: 0.25635781512242417, accuracy: 0.98\n",
      "iteration no 3343: Loss: 0.25635082012658117, accuracy: 0.98\n",
      "iteration no 3344: Loss: 0.2563459287708716, accuracy: 0.98\n",
      "iteration no 3345: Loss: 0.2563388230246064, accuracy: 0.98\n",
      "iteration no 3346: Loss: 0.2563322350616629, accuracy: 0.98\n",
      "iteration no 3347: Loss: 0.2563279519641859, accuracy: 0.98\n",
      "iteration no 3348: Loss: 0.25632138436103835, accuracy: 0.98\n",
      "iteration no 3349: Loss: 0.256315804551408, accuracy: 0.98\n",
      "iteration no 3350: Loss: 0.25630930154036363, accuracy: 0.98\n",
      "iteration no 3351: Loss: 0.25630315183974534, accuracy: 0.98\n",
      "iteration no 3352: Loss: 0.2562986579472864, accuracy: 0.98\n",
      "iteration no 3353: Loss: 0.2562911187010016, accuracy: 0.98\n",
      "iteration no 3354: Loss: 0.25628550400106787, accuracy: 0.98\n",
      "iteration no 3355: Loss: 0.2562803367090568, accuracy: 0.98\n",
      "iteration no 3356: Loss: 0.2562739698998806, accuracy: 0.98\n",
      "iteration no 3357: Loss: 0.25626771062863896, accuracy: 0.98\n",
      "iteration no 3358: Loss: 0.25626263876086924, accuracy: 0.98\n",
      "iteration no 3359: Loss: 0.2562575830242189, accuracy: 0.98\n",
      "iteration no 3360: Loss: 0.25624921541116524, accuracy: 0.98\n",
      "iteration no 3361: Loss: 0.2562442753993566, accuracy: 0.98\n",
      "iteration no 3362: Loss: 0.25624066376273724, accuracy: 0.98\n",
      "iteration no 3363: Loss: 0.2562314889104771, accuracy: 0.98\n",
      "iteration no 3364: Loss: 0.25622670293900385, accuracy: 0.98\n",
      "iteration no 3365: Loss: 0.2562228303798858, accuracy: 0.98\n",
      "iteration no 3366: Loss: 0.25621491505882876, accuracy: 0.98\n",
      "iteration no 3367: Loss: 0.25620843387850806, accuracy: 0.98\n",
      "iteration no 3368: Loss: 0.2562056679028226, accuracy: 0.98\n",
      "iteration no 3369: Loss: 0.2561982151611116, accuracy: 0.98\n",
      "iteration no 3370: Loss: 0.2561900279584721, accuracy: 0.98\n",
      "iteration no 3371: Loss: 0.25618833102616057, accuracy: 0.98\n",
      "iteration no 3372: Loss: 0.2561813946506711, accuracy: 0.98\n",
      "iteration no 3373: Loss: 0.25617285932613054, accuracy: 0.98\n",
      "iteration no 3374: Loss: 0.25617058335323223, accuracy: 0.98\n",
      "iteration no 3375: Loss: 0.25616384760006994, accuracy: 0.98\n",
      "iteration no 3376: Loss: 0.25615625567321026, accuracy: 0.98\n",
      "iteration no 3377: Loss: 0.2561518163882928, accuracy: 0.98\n",
      "iteration no 3378: Loss: 0.25614789272209015, accuracy: 0.98\n",
      "iteration no 3379: Loss: 0.2561391677852133, accuracy: 0.98\n",
      "iteration no 3380: Loss: 0.25613345246442104, accuracy: 0.98\n",
      "iteration no 3381: Loss: 0.2561315911732272, accuracy: 0.98\n",
      "iteration no 3382: Loss: 0.25612277298269415, accuracy: 0.98\n",
      "iteration no 3383: Loss: 0.2561163111509331, accuracy: 0.98\n",
      "iteration no 3384: Loss: 0.25611364147218924, accuracy: 0.98\n",
      "iteration no 3385: Loss: 0.256105502597026, accuracy: 0.98\n",
      "iteration no 3386: Loss: 0.25610001241623903, accuracy: 0.98\n",
      "iteration no 3387: Loss: 0.25609602041742285, accuracy: 0.98\n",
      "iteration no 3388: Loss: 0.2560891402649888, accuracy: 0.98\n",
      "iteration no 3389: Loss: 0.25608233201019304, accuracy: 0.98\n",
      "iteration no 3390: Loss: 0.2560772103928318, accuracy: 0.98\n",
      "iteration no 3391: Loss: 0.25607370783634953, accuracy: 0.98\n",
      "iteration no 3392: Loss: 0.2560658939835315, accuracy: 0.98\n",
      "iteration no 3393: Loss: 0.2560598031965463, accuracy: 0.98\n",
      "iteration no 3394: Loss: 0.2560566861136681, accuracy: 0.98\n",
      "iteration no 3395: Loss: 0.25604961403869086, accuracy: 0.98\n",
      "iteration no 3396: Loss: 0.2560430524296658, accuracy: 0.98\n",
      "iteration no 3397: Loss: 0.2560392607179856, accuracy: 0.98\n",
      "iteration no 3398: Loss: 0.2560331765453625, accuracy: 0.98\n",
      "iteration no 3399: Loss: 0.2560251773490827, accuracy: 0.98\n",
      "iteration no 3400: Loss: 0.25602219249786307, accuracy: 0.98\n",
      "iteration no 3401: Loss: 0.256017236114467, accuracy: 0.98\n",
      "iteration no 3402: Loss: 0.25600890772898754, accuracy: 0.98\n",
      "iteration no 3403: Loss: 0.25600479524734815, accuracy: 0.98\n",
      "iteration no 3404: Loss: 0.25600091533970304, accuracy: 0.98\n",
      "iteration no 3405: Loss: 0.2559928221175113, accuracy: 0.98\n",
      "iteration no 3406: Loss: 0.25598722773546984, accuracy: 0.98\n",
      "iteration no 3407: Loss: 0.2559842211238993, accuracy: 0.98\n",
      "iteration no 3408: Loss: 0.2559766202819698, accuracy: 0.98\n",
      "iteration no 3409: Loss: 0.2559693325742353, accuracy: 0.98\n",
      "iteration no 3410: Loss: 0.25596836565251674, accuracy: 0.98\n",
      "iteration no 3411: Loss: 0.2559610229462793, accuracy: 0.98\n",
      "iteration no 3412: Loss: 0.2559519484478484, accuracy: 0.98\n",
      "iteration no 3413: Loss: 0.2559521494399986, accuracy: 0.98\n",
      "iteration no 3414: Loss: 0.2559441079603677, accuracy: 0.98\n",
      "iteration no 3415: Loss: 0.25593621412669504, accuracy: 0.98\n",
      "iteration no 3416: Loss: 0.25593468993330365, accuracy: 0.98\n",
      "iteration no 3417: Loss: 0.25592801178862035, accuracy: 0.98\n",
      "iteration no 3418: Loss: 0.25592048366042824, accuracy: 0.98\n",
      "iteration no 3419: Loss: 0.2559168687021804, accuracy: 0.98\n",
      "iteration no 3420: Loss: 0.2559122367970307, accuracy: 0.98\n",
      "iteration no 3421: Loss: 0.2559053928300236, accuracy: 0.98\n",
      "iteration no 3422: Loss: 0.2558996291414328, accuracy: 0.98\n",
      "iteration no 3423: Loss: 0.25589570633051617, accuracy: 0.98\n",
      "iteration no 3424: Loss: 0.2558892885010668, accuracy: 0.98\n",
      "iteration no 3425: Loss: 0.25588381971948987, accuracy: 0.98\n",
      "iteration no 3426: Loss: 0.2558795731860186, accuracy: 0.98\n",
      "iteration no 3427: Loss: 0.2558725314276172, accuracy: 0.98\n",
      "iteration no 3428: Loss: 0.25586780390092095, accuracy: 0.98\n",
      "iteration no 3429: Loss: 0.2558622822151827, accuracy: 0.98\n",
      "iteration no 3430: Loss: 0.2558573222521248, accuracy: 0.98\n",
      "iteration no 3431: Loss: 0.2558511216924425, accuracy: 0.98\n",
      "iteration no 3432: Loss: 0.25584533317547226, accuracy: 0.98\n",
      "iteration no 3433: Loss: 0.2558408645384461, accuracy: 0.98\n",
      "iteration no 3434: Loss: 0.25583549095406016, accuracy: 0.98\n",
      "iteration no 3435: Loss: 0.2558306402827051, accuracy: 0.98\n",
      "iteration no 3436: Loss: 0.25582329807228155, accuracy: 0.98\n",
      "iteration no 3437: Loss: 0.2558195998974815, accuracy: 0.98\n",
      "iteration no 3438: Loss: 0.25581469570567333, accuracy: 0.98\n",
      "iteration no 3439: Loss: 0.2558069716027508, accuracy: 0.98\n",
      "iteration no 3440: Loss: 0.2558036258377266, accuracy: 0.98\n",
      "iteration no 3441: Loss: 0.2557985536861202, accuracy: 0.98\n",
      "iteration no 3442: Loss: 0.2557909447622084, accuracy: 0.98\n",
      "iteration no 3443: Loss: 0.2557875763786168, accuracy: 0.98\n",
      "iteration no 3444: Loss: 0.25578252958331293, accuracy: 0.98\n",
      "iteration no 3445: Loss: 0.25577671100571997, accuracy: 0.98\n",
      "iteration no 3446: Loss: 0.2557704849258612, accuracy: 0.98\n",
      "iteration no 3447: Loss: 0.2557656111608385, accuracy: 0.98\n",
      "iteration no 3448: Loss: 0.2557617767765039, accuracy: 0.98\n",
      "iteration no 3449: Loss: 0.25575385733538497, accuracy: 0.98\n",
      "iteration no 3450: Loss: 0.2557504847553019, accuracy: 0.98\n",
      "iteration no 3451: Loss: 0.25574567388094943, accuracy: 0.98\n",
      "iteration no 3452: Loss: 0.255739173827116, accuracy: 0.98\n",
      "iteration no 3453: Loss: 0.2557339322451402, accuracy: 0.98\n",
      "iteration no 3454: Loss: 0.25573008085889565, accuracy: 0.98\n",
      "iteration no 3455: Loss: 0.2557237226273947, accuracy: 0.98\n",
      "iteration no 3456: Loss: 0.25571739129356824, accuracy: 0.98\n",
      "iteration no 3457: Loss: 0.2557151227169165, accuracy: 0.98\n",
      "iteration no 3458: Loss: 0.25570773627891163, accuracy: 0.98\n",
      "iteration no 3459: Loss: 0.25570019774189834, accuracy: 0.98\n",
      "iteration no 3460: Loss: 0.25570055856980173, accuracy: 0.98\n",
      "iteration no 3461: Loss: 0.25569297392244267, accuracy: 0.98\n",
      "iteration no 3462: Loss: 0.25568431982049333, accuracy: 0.98\n",
      "iteration no 3463: Loss: 0.2556851161465013, accuracy: 0.98\n",
      "iteration no 3464: Loss: 0.2556770514418184, accuracy: 0.98\n",
      "iteration no 3465: Loss: 0.25566951309619396, accuracy: 0.98\n",
      "iteration no 3466: Loss: 0.255668176823881, accuracy: 0.98\n",
      "iteration no 3467: Loss: 0.2556617029452892, accuracy: 0.98\n",
      "iteration no 3468: Loss: 0.2556546492894858, accuracy: 0.98\n",
      "iteration no 3469: Loss: 0.2556517990080562, accuracy: 0.98\n",
      "iteration no 3470: Loss: 0.25564650078369344, accuracy: 0.98\n",
      "iteration no 3471: Loss: 0.2556400200941092, accuracy: 0.98\n",
      "iteration no 3472: Loss: 0.2556358287876212, accuracy: 0.98\n",
      "iteration no 3473: Loss: 0.2556307172003612, accuracy: 0.98\n",
      "iteration no 3474: Loss: 0.2556247381917716, accuracy: 0.98\n",
      "iteration no 3475: Loss: 0.2556206979124621, accuracy: 0.98\n",
      "iteration no 3476: Loss: 0.2556146232566401, accuracy: 0.98\n",
      "iteration no 3477: Loss: 0.2556094728007128, accuracy: 0.98\n",
      "iteration no 3478: Loss: 0.2556057548750603, accuracy: 0.98\n",
      "iteration no 3479: Loss: 0.25559930649289625, accuracy: 0.98\n",
      "iteration no 3480: Loss: 0.2555944269484461, accuracy: 0.98\n",
      "iteration no 3481: Loss: 0.25558995227613523, accuracy: 0.98\n",
      "iteration no 3482: Loss: 0.25558365434146035, accuracy: 0.98\n",
      "iteration no 3483: Loss: 0.2555794896868747, accuracy: 0.98\n",
      "iteration no 3484: Loss: 0.2555746152246271, accuracy: 0.98\n",
      "iteration no 3485: Loss: 0.2555691948114655, accuracy: 0.98\n",
      "iteration no 3486: Loss: 0.25556427507534674, accuracy: 0.98\n",
      "iteration no 3487: Loss: 0.25555915921016753, accuracy: 0.98\n",
      "iteration no 3488: Loss: 0.2555542614874526, accuracy: 0.98\n",
      "iteration no 3489: Loss: 0.25554862100282916, accuracy: 0.98\n",
      "iteration no 3490: Loss: 0.25554363847857686, accuracy: 0.98\n",
      "iteration no 3491: Loss: 0.255539579399676, accuracy: 0.98\n",
      "iteration no 3492: Loss: 0.25553320253391365, accuracy: 0.98\n",
      "iteration no 3493: Loss: 0.2555283071720085, accuracy: 0.98\n",
      "iteration no 3494: Loss: 0.25552498092245407, accuracy: 0.98\n",
      "iteration no 3495: Loss: 0.25551829094005646, accuracy: 0.98\n",
      "iteration no 3496: Loss: 0.25551298259721017, accuracy: 0.98\n",
      "iteration no 3497: Loss: 0.25550988128769003, accuracy: 0.98\n",
      "iteration no 3498: Loss: 0.25550357477056274, accuracy: 0.98\n",
      "iteration no 3499: Loss: 0.25549772903608703, accuracy: 0.98\n",
      "iteration no 3500: Loss: 0.2554951260395127, accuracy: 0.98\n",
      "iteration no 3501: Loss: 0.2554883512904913, accuracy: 0.98\n",
      "iteration no 3502: Loss: 0.25548226092007287, accuracy: 0.98\n",
      "iteration no 3503: Loss: 0.25548019649321907, accuracy: 0.98\n",
      "iteration no 3504: Loss: 0.25547301374144327, accuracy: 0.98\n",
      "iteration no 3505: Loss: 0.2554684100725636, accuracy: 0.98\n",
      "iteration no 3506: Loss: 0.2554657614608141, accuracy: 0.98\n",
      "iteration no 3507: Loss: 0.25545719640380316, accuracy: 0.98\n",
      "iteration no 3508: Loss: 0.25545409354673393, accuracy: 0.98\n",
      "iteration no 3509: Loss: 0.255450003597274, accuracy: 0.98\n",
      "iteration no 3510: Loss: 0.25544235707941004, accuracy: 0.98\n",
      "iteration no 3511: Loss: 0.25543949769416485, accuracy: 0.98\n",
      "iteration no 3512: Loss: 0.25543472334072875, accuracy: 0.98\n",
      "iteration no 3513: Loss: 0.2554284602087257, accuracy: 0.98\n",
      "iteration no 3514: Loss: 0.2554242016661722, accuracy: 0.98\n",
      "iteration no 3515: Loss: 0.2554197958098779, accuracy: 0.98\n",
      "iteration no 3516: Loss: 0.2554136927891122, accuracy: 0.98\n",
      "iteration no 3517: Loss: 0.2554085601550614, accuracy: 0.98\n",
      "iteration no 3518: Loss: 0.2554050592145767, accuracy: 0.98\n",
      "iteration no 3519: Loss: 0.25539824556722435, accuracy: 0.98\n",
      "iteration no 3520: Loss: 0.2553942142984934, accuracy: 0.98\n",
      "iteration no 3521: Loss: 0.2553907222345087, accuracy: 0.98\n",
      "iteration no 3522: Loss: 0.255383728776196, accuracy: 0.98\n",
      "iteration no 3523: Loss: 0.2553797590456328, accuracy: 0.98\n",
      "iteration no 3524: Loss: 0.2553755684811151, accuracy: 0.98\n",
      "iteration no 3525: Loss: 0.2553692428417985, accuracy: 0.98\n",
      "iteration no 3526: Loss: 0.2553648834102973, accuracy: 0.98\n",
      "iteration no 3527: Loss: 0.25535984757208635, accuracy: 0.98\n",
      "iteration no 3528: Loss: 0.2553559690903663, accuracy: 0.98\n",
      "iteration no 3529: Loss: 0.25535023947623336, accuracy: 0.98\n",
      "iteration no 3530: Loss: 0.25534474767464926, accuracy: 0.98\n",
      "iteration no 3531: Loss: 0.2553417291555599, accuracy: 0.98\n",
      "iteration no 3532: Loss: 0.2553353643618779, accuracy: 0.98\n",
      "iteration no 3533: Loss: 0.25533077778337254, accuracy: 0.98\n",
      "iteration no 3534: Loss: 0.25532683241764986, accuracy: 0.98\n",
      "iteration no 3535: Loss: 0.25532095134883437, accuracy: 0.98\n",
      "iteration no 3536: Loss: 0.25531721217237996, accuracy: 0.98\n",
      "iteration no 3537: Loss: 0.255311848914861, accuracy: 0.98\n",
      "iteration no 3538: Loss: 0.2553069223140459, accuracy: 0.98\n",
      "iteration no 3539: Loss: 0.25530336462758774, accuracy: 0.98\n",
      "iteration no 3540: Loss: 0.2552962159544584, accuracy: 0.98\n",
      "iteration no 3541: Loss: 0.2552930771118284, accuracy: 0.98\n",
      "iteration no 3542: Loss: 0.2552890791448834, accuracy: 0.98\n",
      "iteration no 3543: Loss: 0.25528108056985527, accuracy: 0.98\n",
      "iteration no 3544: Loss: 0.25527993932641346, accuracy: 0.98\n",
      "iteration no 3545: Loss: 0.2552753120647889, accuracy: 0.98\n",
      "iteration no 3546: Loss: 0.2552672981788654, accuracy: 0.98\n",
      "iteration no 3547: Loss: 0.2552653671092322, accuracy: 0.98\n",
      "iteration no 3548: Loss: 0.2552605249306667, accuracy: 0.98\n",
      "iteration no 3549: Loss: 0.2552532014542743, accuracy: 0.98\n",
      "iteration no 3550: Loss: 0.2552507489107974, accuracy: 0.98\n",
      "iteration no 3551: Loss: 0.2552467299157668, accuracy: 0.98\n",
      "iteration no 3552: Loss: 0.25523980120746376, accuracy: 0.98\n",
      "iteration no 3553: Loss: 0.2552367545332651, accuracy: 0.98\n",
      "iteration no 3554: Loss: 0.2552327534312262, accuracy: 0.98\n",
      "iteration no 3555: Loss: 0.25522591210555995, accuracy: 0.98\n",
      "iteration no 3556: Loss: 0.25522227684407345, accuracy: 0.98\n",
      "iteration no 3557: Loss: 0.2552185245421144, accuracy: 0.98\n",
      "iteration no 3558: Loss: 0.2552127101589518, accuracy: 0.98\n",
      "iteration no 3559: Loss: 0.25520944400416834, accuracy: 0.98\n",
      "iteration no 3560: Loss: 0.2552033029112651, accuracy: 0.98\n",
      "iteration no 3561: Loss: 0.2551990098564953, accuracy: 0.98\n",
      "iteration no 3562: Loss: 0.2551959907686856, accuracy: 0.98\n",
      "iteration no 3563: Loss: 0.2551885441927597, accuracy: 0.98\n",
      "iteration no 3564: Loss: 0.25518481814701566, accuracy: 0.98\n",
      "iteration no 3565: Loss: 0.2551826899831735, accuracy: 0.98\n",
      "iteration no 3566: Loss: 0.25517492535730535, accuracy: 0.98\n",
      "iteration no 3567: Loss: 0.2551714863241679, accuracy: 0.98\n",
      "iteration no 3568: Loss: 0.25516933670341857, accuracy: 0.98\n",
      "iteration no 3569: Loss: 0.25516198024851305, accuracy: 0.98\n",
      "iteration no 3570: Loss: 0.25515703844232795, accuracy: 0.98\n",
      "iteration no 3571: Loss: 0.25515423476872146, accuracy: 0.98\n",
      "iteration no 3572: Loss: 0.2551489083950856, accuracy: 0.98\n",
      "iteration no 3573: Loss: 0.2551437355719243, accuracy: 0.98\n",
      "iteration no 3574: Loss: 0.255139977424419, accuracy: 0.98\n",
      "iteration no 3575: Loss: 0.2551358544945217, accuracy: 0.98\n",
      "iteration no 3576: Loss: 0.2551304481609823, accuracy: 0.98\n",
      "iteration no 3577: Loss: 0.2551267435237963, accuracy: 0.98\n",
      "iteration no 3578: Loss: 0.2551221373785557, accuracy: 0.98\n",
      "iteration no 3579: Loss: 0.2551171476628323, accuracy: 0.98\n",
      "iteration no 3580: Loss: 0.2551124081803723, accuracy: 0.98\n",
      "iteration no 3581: Loss: 0.2551085165955026, accuracy: 0.98\n",
      "iteration no 3582: Loss: 0.255103530561571, accuracy: 0.98\n",
      "iteration no 3583: Loss: 0.2550994461208934, accuracy: 0.98\n",
      "iteration no 3584: Loss: 0.2550949030397984, accuracy: 0.98\n",
      "iteration no 3585: Loss: 0.25509004458866863, accuracy: 0.98\n",
      "iteration no 3586: Loss: 0.25508719331331275, accuracy: 0.98\n",
      "iteration no 3587: Loss: 0.25507991904610705, accuracy: 0.98\n",
      "iteration no 3588: Loss: 0.25507689262679334, accuracy: 0.98\n",
      "iteration no 3589: Loss: 0.255074206707608, accuracy: 0.98\n",
      "iteration no 3590: Loss: 0.25506594772439567, accuracy: 0.98\n",
      "iteration no 3591: Loss: 0.2550642650851508, accuracy: 0.98\n",
      "iteration no 3592: Loss: 0.25506119766877045, accuracy: 0.98\n",
      "iteration no 3593: Loss: 0.2550528723783101, accuracy: 0.98\n",
      "iteration no 3594: Loss: 0.25505048102151406, accuracy: 0.98\n",
      "iteration no 3595: Loss: 0.25504712910574756, accuracy: 0.98\n",
      "iteration no 3596: Loss: 0.25503977188484017, accuracy: 0.98\n",
      "iteration no 3597: Loss: 0.25503773221839243, accuracy: 0.98\n",
      "iteration no 3598: Loss: 0.25503266825743676, accuracy: 0.98\n",
      "iteration no 3599: Loss: 0.2550271123903047, accuracy: 0.98\n",
      "iteration no 3600: Loss: 0.255025709134914, accuracy: 0.98\n",
      "iteration no 3601: Loss: 0.255018072909658, accuracy: 0.98\n",
      "iteration no 3602: Loss: 0.2550142511298996, accuracy: 0.98\n",
      "iteration no 3603: Loss: 0.25501284421378, accuracy: 0.98\n",
      "iteration no 3604: Loss: 0.25500426682056954, accuracy: 0.98\n",
      "iteration no 3605: Loss: 0.2550021708836784, accuracy: 0.98\n",
      "iteration no 3606: Loss: 0.2549988536480175, accuracy: 0.98\n",
      "iteration no 3607: Loss: 0.25499214037483353, accuracy: 0.98\n",
      "iteration no 3608: Loss: 0.25498856549422977, accuracy: 0.98\n",
      "iteration no 3609: Loss: 0.2549853110627569, accuracy: 0.98\n",
      "iteration no 3610: Loss: 0.2549796860063255, accuracy: 0.98\n",
      "iteration no 3611: Loss: 0.2549754247945076, accuracy: 0.98\n",
      "iteration no 3612: Loss: 0.2549717535052495, accuracy: 0.98\n",
      "iteration no 3613: Loss: 0.2549671024877259, accuracy: 0.98\n",
      "iteration no 3614: Loss: 0.25496269994509413, accuracy: 0.98\n",
      "iteration no 3615: Loss: 0.25495793681470647, accuracy: 0.98\n",
      "iteration no 3616: Loss: 0.25495449212761845, accuracy: 0.98\n",
      "iteration no 3617: Loss: 0.25494977534387037, accuracy: 0.98\n",
      "iteration no 3618: Loss: 0.2549447717814762, accuracy: 0.98\n",
      "iteration no 3619: Loss: 0.2549419597166812, accuracy: 0.98\n",
      "iteration no 3620: Loss: 0.25493667204852855, accuracy: 0.98\n",
      "iteration no 3621: Loss: 0.25493269237546656, accuracy: 0.98\n",
      "iteration no 3622: Loss: 0.2549282734047068, accuracy: 0.98\n",
      "iteration no 3623: Loss: 0.25492392388496143, accuracy: 0.98\n",
      "iteration no 3624: Loss: 0.254920562902744, accuracy: 0.98\n",
      "iteration no 3625: Loss: 0.2549146305015486, accuracy: 0.98\n",
      "iteration no 3626: Loss: 0.2549119551113239, accuracy: 0.98\n",
      "iteration no 3627: Loss: 0.2549079871301783, accuracy: 0.98\n",
      "iteration no 3628: Loss: 0.25490145567378864, accuracy: 0.98\n",
      "iteration no 3629: Loss: 0.25489965507822365, accuracy: 0.98\n",
      "iteration no 3630: Loss: 0.2548952113220643, accuracy: 0.98\n",
      "iteration no 3631: Loss: 0.2548881724930415, accuracy: 0.98\n",
      "iteration no 3632: Loss: 0.25488636079704874, accuracy: 0.98\n",
      "iteration no 3633: Loss: 0.25488317124703413, accuracy: 0.98\n",
      "iteration no 3634: Loss: 0.25487572675147724, accuracy: 0.98\n",
      "iteration no 3635: Loss: 0.25487512171793253, accuracy: 0.98\n",
      "iteration no 3636: Loss: 0.25486867543821884, accuracy: 0.98\n",
      "iteration no 3637: Loss: 0.2548641180038641, accuracy: 0.98\n",
      "iteration no 3638: Loss: 0.25486189832513406, accuracy: 0.98\n",
      "iteration no 3639: Loss: 0.2548554431788307, accuracy: 0.98\n",
      "iteration no 3640: Loss: 0.2548522159086544, accuracy: 0.98\n",
      "iteration no 3641: Loss: 0.2548489937358358, accuracy: 0.98\n",
      "iteration no 3642: Loss: 0.2548422965890388, accuracy: 0.98\n",
      "iteration no 3643: Loss: 0.2548397079622075, accuracy: 0.98\n",
      "iteration no 3644: Loss: 0.2548372180401208, accuracy: 0.98\n",
      "iteration no 3645: Loss: 0.25482983684118166, accuracy: 0.98\n",
      "iteration no 3646: Loss: 0.2548263366838298, accuracy: 0.98\n",
      "iteration no 3647: Loss: 0.2548245815148263, accuracy: 0.98\n",
      "iteration no 3648: Loss: 0.25481844546052257, accuracy: 0.98\n",
      "iteration no 3649: Loss: 0.2548139183252742, accuracy: 0.98\n",
      "iteration no 3650: Loss: 0.25481051540852867, accuracy: 0.98\n",
      "iteration no 3651: Loss: 0.25480663009669047, accuracy: 0.98\n",
      "iteration no 3652: Loss: 0.25480220277987786, accuracy: 0.98\n",
      "iteration no 3653: Loss: 0.25479764576444697, accuracy: 0.98\n",
      "iteration no 3654: Loss: 0.2547942547854162, accuracy: 0.98\n",
      "iteration no 3655: Loss: 0.2547906771070406, accuracy: 0.98\n",
      "iteration no 3656: Loss: 0.25478604990035447, accuracy: 0.98\n",
      "iteration no 3657: Loss: 0.25478033182161, accuracy: 0.98\n",
      "iteration no 3658: Loss: 0.25477866799428767, accuracy: 0.98\n",
      "iteration no 3659: Loss: 0.2547736207856877, accuracy: 0.98\n",
      "iteration no 3660: Loss: 0.2547675712249149, accuracy: 0.98\n",
      "iteration no 3661: Loss: 0.2547659501528527, accuracy: 0.98\n",
      "iteration no 3662: Loss: 0.2547623532477169, accuracy: 0.98\n",
      "iteration no 3663: Loss: 0.25475501044744336, accuracy: 0.98\n",
      "iteration no 3664: Loss: 0.2547530372679007, accuracy: 0.98\n",
      "iteration no 3665: Loss: 0.2547504666717847, accuracy: 0.98\n",
      "iteration no 3666: Loss: 0.25474337397613583, accuracy: 0.98\n",
      "iteration no 3667: Loss: 0.2547411325155996, accuracy: 0.98\n",
      "iteration no 3668: Loss: 0.2547366169614873, accuracy: 0.98\n",
      "iteration no 3669: Loss: 0.2547316661760005, accuracy: 0.98\n",
      "iteration no 3670: Loss: 0.25472943306020557, accuracy: 0.98\n",
      "iteration no 3671: Loss: 0.2547230979368965, accuracy: 0.98\n",
      "iteration no 3672: Loss: 0.25472031941900625, accuracy: 0.98\n",
      "iteration no 3673: Loss: 0.25471822086616736, accuracy: 0.98\n",
      "iteration no 3674: Loss: 0.2547101663778603, accuracy: 0.98\n",
      "iteration no 3675: Loss: 0.2547076368560494, accuracy: 0.98\n",
      "iteration no 3676: Loss: 0.2547063387145106, accuracy: 0.98\n",
      "iteration no 3677: Loss: 0.25469823125443386, accuracy: 0.98\n",
      "iteration no 3678: Loss: 0.2546955421760972, accuracy: 0.98\n",
      "iteration no 3679: Loss: 0.25469339424841964, accuracy: 0.98\n",
      "iteration no 3680: Loss: 0.2546866881452371, accuracy: 0.98\n",
      "iteration no 3681: Loss: 0.25468454069382584, accuracy: 0.98\n",
      "iteration no 3682: Loss: 0.25467919608554074, accuracy: 0.98\n",
      "iteration no 3683: Loss: 0.25467558389892286, accuracy: 0.98\n",
      "iteration no 3684: Loss: 0.2546728607155028, accuracy: 0.98\n",
      "iteration no 3685: Loss: 0.2546662582134336, accuracy: 0.98\n",
      "iteration no 3686: Loss: 0.25466325980081095, accuracy: 0.98\n",
      "iteration no 3687: Loss: 0.25466156466051665, accuracy: 0.98\n",
      "iteration no 3688: Loss: 0.25465461383381294, accuracy: 0.98\n",
      "iteration no 3689: Loss: 0.2546501993716903, accuracy: 0.98\n",
      "iteration no 3690: Loss: 0.25464958314446734, accuracy: 0.98\n",
      "iteration no 3691: Loss: 0.2546433817309805, accuracy: 0.98\n",
      "iteration no 3692: Loss: 0.2546385440295532, accuracy: 0.98\n",
      "iteration no 3693: Loss: 0.2546354638606859, accuracy: 0.98\n",
      "iteration no 3694: Loss: 0.25463202594396595, accuracy: 0.98\n",
      "iteration no 3695: Loss: 0.25462710938828004, accuracy: 0.98\n",
      "iteration no 3696: Loss: 0.2546233331978101, accuracy: 0.98\n",
      "iteration no 3697: Loss: 0.25461948174428795, accuracy: 0.98\n",
      "iteration no 3698: Loss: 0.25461545908116345, accuracy: 0.98\n",
      "iteration no 3699: Loss: 0.2546113808718595, accuracy: 0.98\n",
      "iteration no 3700: Loss: 0.2546070533620147, accuracy: 0.98\n",
      "iteration no 3701: Loss: 0.25460340080171917, accuracy: 0.98\n",
      "iteration no 3702: Loss: 0.25460037699786753, accuracy: 0.98\n",
      "iteration no 3703: Loss: 0.25459464271359017, accuracy: 0.98\n",
      "iteration no 3704: Loss: 0.25459008380595083, accuracy: 0.98\n",
      "iteration no 3705: Loss: 0.2545896178329643, accuracy: 0.98\n",
      "iteration no 3706: Loss: 0.25458315069896764, accuracy: 0.98\n",
      "iteration no 3707: Loss: 0.25457798251787944, accuracy: 0.98\n",
      "iteration no 3708: Loss: 0.25457725653507807, accuracy: 0.98\n",
      "iteration no 3709: Loss: 0.25457169998261486, accuracy: 0.98\n",
      "iteration no 3710: Loss: 0.25456690962070583, accuracy: 0.98\n",
      "iteration no 3711: Loss: 0.2545638115399514, accuracy: 0.98\n",
      "iteration no 3712: Loss: 0.254559670583441, accuracy: 0.98\n",
      "iteration no 3713: Loss: 0.2545567902720543, accuracy: 0.98\n",
      "iteration no 3714: Loss: 0.2545520394539397, accuracy: 0.98\n",
      "iteration no 3715: Loss: 0.25454645575485574, accuracy: 0.98\n",
      "iteration no 3716: Loss: 0.25454606362697785, accuracy: 0.98\n",
      "iteration no 3717: Loss: 0.2545404014912391, accuracy: 0.98\n",
      "iteration no 3718: Loss: 0.2545339217089143, accuracy: 0.98\n",
      "iteration no 3719: Loss: 0.25453375071922885, accuracy: 0.98\n",
      "iteration no 3720: Loss: 0.2545289151153331, accuracy: 0.98\n",
      "iteration no 3721: Loss: 0.2545234312644572, accuracy: 0.98\n",
      "iteration no 3722: Loss: 0.2545211845624975, accuracy: 0.98\n",
      "iteration no 3723: Loss: 0.25451691911613933, accuracy: 0.98\n",
      "iteration no 3724: Loss: 0.2545127271974713, accuracy: 0.98\n",
      "iteration no 3725: Loss: 0.2545093443548346, accuracy: 0.98\n",
      "iteration no 3726: Loss: 0.25450334835327393, accuracy: 0.98\n",
      "iteration no 3727: Loss: 0.2545019150061248, accuracy: 0.98\n",
      "iteration no 3728: Loss: 0.2544983113021297, accuracy: 0.98\n",
      "iteration no 3729: Loss: 0.25449184351072135, accuracy: 0.98\n",
      "iteration no 3730: Loss: 0.2544896653732054, accuracy: 0.98\n",
      "iteration no 3731: Loss: 0.25448746424026136, accuracy: 0.98\n",
      "iteration no 3732: Loss: 0.25448136666700494, accuracy: 0.98\n",
      "iteration no 3733: Loss: 0.25447661912075825, accuracy: 0.98\n",
      "iteration no 3734: Loss: 0.2544751104387294, accuracy: 0.98\n",
      "iteration no 3735: Loss: 0.25447079584215404, accuracy: 0.98\n",
      "iteration no 3736: Loss: 0.2544656441041247, accuracy: 0.98\n",
      "iteration no 3737: Loss: 0.254462464796489, accuracy: 0.98\n",
      "iteration no 3738: Loss: 0.2544598016692362, accuracy: 0.98\n",
      "iteration no 3739: Loss: 0.2544548139899014, accuracy: 0.98\n",
      "iteration no 3740: Loss: 0.25445094792958084, accuracy: 0.98\n",
      "iteration no 3741: Loss: 0.254446589648945, accuracy: 0.98\n",
      "iteration no 3742: Loss: 0.25444359974162933, accuracy: 0.98\n",
      "iteration no 3743: Loss: 0.25444063436445363, accuracy: 0.98\n",
      "iteration no 3744: Loss: 0.25443447194006, accuracy: 0.98\n",
      "iteration no 3745: Loss: 0.25443196355795067, accuracy: 0.98\n",
      "iteration no 3746: Loss: 0.25443016678328273, accuracy: 0.98\n",
      "iteration no 3747: Loss: 0.2544233336079279, accuracy: 0.98\n",
      "iteration no 3748: Loss: 0.25441961880561, accuracy: 0.98\n",
      "iteration no 3749: Loss: 0.25441812491277016, accuracy: 0.98\n",
      "iteration no 3750: Loss: 0.25441152838251707, accuracy: 0.98\n",
      "iteration no 3751: Loss: 0.254409003145242, accuracy: 0.98\n",
      "iteration no 3752: Loss: 0.2544046138734625, accuracy: 0.98\n",
      "iteration no 3753: Loss: 0.2543998629282279, accuracy: 0.98\n",
      "iteration no 3754: Loss: 0.25439847386595427, accuracy: 0.98\n",
      "iteration no 3755: Loss: 0.2543922261640568, accuracy: 0.98\n",
      "iteration no 3756: Loss: 0.2543876544252665, accuracy: 0.98\n",
      "iteration no 3757: Loss: 0.2543869136248514, accuracy: 0.98\n",
      "iteration no 3758: Loss: 0.25438074044795694, accuracy: 0.98\n",
      "iteration no 3759: Loss: 0.2543753876242869, accuracy: 0.98\n",
      "iteration no 3760: Loss: 0.254374540106529, accuracy: 0.98\n",
      "iteration no 3761: Loss: 0.254369342964179, accuracy: 0.98\n",
      "iteration no 3762: Loss: 0.2543650086260816, accuracy: 0.98\n",
      "iteration no 3763: Loss: 0.2543618046440254, accuracy: 0.98\n",
      "iteration no 3764: Loss: 0.25435730371594373, accuracy: 0.98\n",
      "iteration no 3765: Loss: 0.25435362015807017, accuracy: 0.98\n",
      "iteration no 3766: Loss: 0.2543510540007803, accuracy: 0.98\n",
      "iteration no 3767: Loss: 0.2543449403831702, accuracy: 0.98\n",
      "iteration no 3768: Loss: 0.25434216652386965, accuracy: 0.98\n",
      "iteration no 3769: Loss: 0.25433936864234724, accuracy: 0.98\n",
      "iteration no 3770: Loss: 0.25433419934572665, accuracy: 0.98\n",
      "iteration no 3771: Loss: 0.2543290038139532, accuracy: 0.98\n",
      "iteration no 3772: Loss: 0.2543285492846563, accuracy: 0.98\n",
      "iteration no 3773: Loss: 0.25432241190672195, accuracy: 0.98\n",
      "iteration no 3774: Loss: 0.25431815845260575, accuracy: 0.98\n",
      "iteration no 3775: Loss: 0.25431589333179977, accuracy: 0.98\n",
      "iteration no 3776: Loss: 0.2543115421822192, accuracy: 0.98\n",
      "iteration no 3777: Loss: 0.2543075804715794, accuracy: 0.98\n",
      "iteration no 3778: Loss: 0.2543040157518971, accuracy: 0.98\n",
      "iteration no 3779: Loss: 0.25429952569967285, accuracy: 0.98\n",
      "iteration no 3780: Loss: 0.25429617140937333, accuracy: 0.98\n",
      "iteration no 3781: Loss: 0.25429424743223084, accuracy: 0.98\n",
      "iteration no 3782: Loss: 0.2542870710769292, accuracy: 0.98\n",
      "iteration no 3783: Loss: 0.25428502552971866, accuracy: 0.98\n",
      "iteration no 3784: Loss: 0.25428177591472884, accuracy: 0.98\n",
      "iteration no 3785: Loss: 0.25427676602903315, accuracy: 0.98\n",
      "iteration no 3786: Loss: 0.2542729591704411, accuracy: 0.98\n",
      "iteration no 3787: Loss: 0.2542712783616239, accuracy: 0.98\n",
      "iteration no 3788: Loss: 0.2542648452635621, accuracy: 0.98\n",
      "iteration no 3789: Loss: 0.254262830476153, accuracy: 0.98\n",
      "iteration no 3790: Loss: 0.25425860623996455, accuracy: 0.98\n",
      "iteration no 3791: Loss: 0.2542543122235768, accuracy: 0.98\n",
      "iteration no 3792: Loss: 0.2542511256440731, accuracy: 0.98\n",
      "iteration no 3793: Loss: 0.25424769738221903, accuracy: 0.98\n",
      "iteration no 3794: Loss: 0.254242573855707, accuracy: 0.98\n",
      "iteration no 3795: Loss: 0.2542395896542361, accuracy: 0.98\n",
      "iteration no 3796: Loss: 0.25423686166448256, accuracy: 0.98\n",
      "iteration no 3797: Loss: 0.2542301588156781, accuracy: 0.98\n",
      "iteration no 3798: Loss: 0.2542293981595717, accuracy: 0.98\n",
      "iteration no 3799: Loss: 0.25422458015464794, accuracy: 0.98\n",
      "iteration no 3800: Loss: 0.25421973184566915, accuracy: 0.98\n",
      "iteration no 3801: Loss: 0.25421755445195093, accuracy: 0.98\n",
      "iteration no 3802: Loss: 0.25421340760369504, accuracy: 0.98\n",
      "iteration no 3803: Loss: 0.25420879823778997, accuracy: 0.98\n",
      "iteration no 3804: Loss: 0.2542055819848734, accuracy: 0.98\n",
      "iteration no 3805: Loss: 0.2542029769436919, accuracy: 0.98\n",
      "iteration no 3806: Loss: 0.2541965440093809, accuracy: 0.98\n",
      "iteration no 3807: Loss: 0.2541959014840457, accuracy: 0.98\n",
      "iteration no 3808: Loss: 0.2541905884513902, accuracy: 0.98\n",
      "iteration no 3809: Loss: 0.2541857151802597, accuracy: 0.98\n",
      "iteration no 3810: Loss: 0.2541838832936901, accuracy: 0.98\n",
      "iteration no 3811: Loss: 0.25417988117218693, accuracy: 0.98\n",
      "iteration no 3812: Loss: 0.25417559737986983, accuracy: 0.98\n",
      "iteration no 3813: Loss: 0.2541725185477567, accuracy: 0.98\n",
      "iteration no 3814: Loss: 0.25416936881562413, accuracy: 0.98\n",
      "iteration no 3815: Loss: 0.2541635085719747, accuracy: 0.98\n",
      "iteration no 3816: Loss: 0.254161834617428, accuracy: 0.98\n",
      "iteration no 3817: Loss: 0.25415696918234143, accuracy: 0.98\n",
      "iteration no 3818: Loss: 0.2541519541272601, accuracy: 0.98\n",
      "iteration no 3819: Loss: 0.2541505310964214, accuracy: 0.98\n",
      "iteration no 3820: Loss: 0.2541448935548178, accuracy: 0.98\n",
      "iteration no 3821: Loss: 0.2541422185727607, accuracy: 0.98\n",
      "iteration no 3822: Loss: 0.25413765322219084, accuracy: 0.98\n",
      "iteration no 3823: Loss: 0.2541344679650043, accuracy: 0.98\n",
      "iteration no 3824: Loss: 0.2541300476156405, accuracy: 0.98\n",
      "iteration no 3825: Loss: 0.25412701741572713, accuracy: 0.98\n",
      "iteration no 3826: Loss: 0.25412216794205916, accuracy: 0.98\n",
      "iteration no 3827: Loss: 0.25411847341447463, accuracy: 0.98\n",
      "iteration no 3828: Loss: 0.25411670841576006, accuracy: 0.98\n",
      "iteration no 3829: Loss: 0.2541100624617126, accuracy: 0.98\n",
      "iteration no 3830: Loss: 0.2541076160416018, accuracy: 0.98\n",
      "iteration no 3831: Loss: 0.25410402350000777, accuracy: 0.98\n",
      "iteration no 3832: Loss: 0.254099796506533, accuracy: 0.98\n",
      "iteration no 3833: Loss: 0.2540963371021644, accuracy: 0.98\n",
      "iteration no 3834: Loss: 0.25409190883801874, accuracy: 0.98\n",
      "iteration no 3835: Loss: 0.25408872998148796, accuracy: 0.98\n",
      "iteration no 3836: Loss: 0.2540849917265454, accuracy: 0.98\n",
      "iteration no 3837: Loss: 0.2540820467071826, accuracy: 0.98\n",
      "iteration no 3838: Loss: 0.25407539465218076, accuracy: 0.98\n",
      "iteration no 3839: Loss: 0.2540749178031891, accuracy: 0.98\n",
      "iteration no 3840: Loss: 0.2540706195676986, accuracy: 0.98\n",
      "iteration no 3841: Loss: 0.2540642429067176, accuracy: 0.98\n",
      "iteration no 3842: Loss: 0.25406275916497845, accuracy: 0.98\n",
      "iteration no 3843: Loss: 0.25405890618270704, accuracy: 0.98\n",
      "iteration no 3844: Loss: 0.25405492628328885, accuracy: 0.98\n",
      "iteration no 3845: Loss: 0.25405065687325945, accuracy: 0.98\n",
      "iteration no 3846: Loss: 0.2540473863744338, accuracy: 0.98\n",
      "iteration no 3847: Loss: 0.2540428112419389, accuracy: 0.98\n",
      "iteration no 3848: Loss: 0.2540408050046601, accuracy: 0.98\n",
      "iteration no 3849: Loss: 0.25403623413634613, accuracy: 0.98\n",
      "iteration no 3850: Loss: 0.25403011762884575, accuracy: 0.98\n",
      "iteration no 3851: Loss: 0.2540311554083844, accuracy: 0.98\n",
      "iteration no 3852: Loss: 0.25402470568592744, accuracy: 0.98\n",
      "iteration no 3853: Loss: 0.2540198109796187, accuracy: 0.98\n",
      "iteration no 3854: Loss: 0.2540178040877466, accuracy: 0.98\n",
      "iteration no 3855: Loss: 0.254014699932253, accuracy: 0.98\n",
      "iteration no 3856: Loss: 0.2540089408933509, accuracy: 0.98\n",
      "iteration no 3857: Loss: 0.25400606259457936, accuracy: 0.98\n",
      "iteration no 3858: Loss: 0.2540031951417357, accuracy: 0.98\n",
      "iteration no 3859: Loss: 0.2539977107865164, accuracy: 0.98\n",
      "iteration no 3860: Loss: 0.2539968926023587, accuracy: 0.98\n",
      "iteration no 3861: Loss: 0.25399045885118143, accuracy: 0.98\n",
      "iteration no 3862: Loss: 0.25398631687884626, accuracy: 0.98\n",
      "iteration no 3863: Loss: 0.2539860494541417, accuracy: 0.98\n",
      "iteration no 3864: Loss: 0.25397977116828774, accuracy: 0.98\n",
      "iteration no 3865: Loss: 0.25397487202351626, accuracy: 0.98\n",
      "iteration no 3866: Loss: 0.2539738506027913, accuracy: 0.98\n",
      "iteration no 3867: Loss: 0.2539707296881355, accuracy: 0.98\n",
      "iteration no 3868: Loss: 0.25396348592311757, accuracy: 0.98\n",
      "iteration no 3869: Loss: 0.2539625476509525, accuracy: 0.98\n",
      "iteration no 3870: Loss: 0.2539586206473007, accuracy: 0.98\n",
      "iteration no 3871: Loss: 0.2539532847326543, accuracy: 0.98\n",
      "iteration no 3872: Loss: 0.2539521537438453, accuracy: 0.98\n",
      "iteration no 3873: Loss: 0.2539455160109591, accuracy: 0.98\n",
      "iteration no 3874: Loss: 0.25394360564665425, accuracy: 0.98\n",
      "iteration no 3875: Loss: 0.25394073456551874, accuracy: 0.98\n",
      "iteration no 3876: Loss: 0.25393527893853285, accuracy: 0.98\n",
      "iteration no 3877: Loss: 0.25393176650260807, accuracy: 0.98\n",
      "iteration no 3878: Loss: 0.2539296894872479, accuracy: 0.98\n",
      "iteration no 3879: Loss: 0.2539256065170982, accuracy: 0.98\n",
      "iteration no 3880: Loss: 0.2539194880877477, accuracy: 0.98\n",
      "iteration no 3881: Loss: 0.2539195180155974, accuracy: 0.98\n",
      "iteration no 3882: Loss: 0.2539140229049108, accuracy: 0.98\n",
      "iteration no 3883: Loss: 0.25391039509147295, accuracy: 0.98\n",
      "iteration no 3884: Loss: 0.25390726479613135, accuracy: 0.98\n",
      "iteration no 3885: Loss: 0.2539023253034257, accuracy: 0.98\n",
      "iteration no 3886: Loss: 0.25390032990344535, accuracy: 0.98\n",
      "iteration no 3887: Loss: 0.2538959397166507, accuracy: 0.98\n",
      "iteration no 3888: Loss: 0.25389267918824143, accuracy: 0.98\n",
      "iteration no 3889: Loss: 0.2538881241629932, accuracy: 0.98\n",
      "iteration no 3890: Loss: 0.25388632892008073, accuracy: 0.98\n",
      "iteration no 3891: Loss: 0.253881370084359, accuracy: 0.98\n",
      "iteration no 3892: Loss: 0.2538776007625712, accuracy: 0.98\n",
      "iteration no 3893: Loss: 0.25387491986745536, accuracy: 0.98\n",
      "iteration no 3894: Loss: 0.2538697643617899, accuracy: 0.98\n",
      "iteration no 3895: Loss: 0.25386783455655104, accuracy: 0.98\n",
      "iteration no 3896: Loss: 0.2538635944856614, accuracy: 0.98\n",
      "iteration no 3897: Loss: 0.2538595305162714, accuracy: 0.98\n",
      "iteration no 3898: Loss: 0.25385702300132706, accuracy: 0.98\n",
      "iteration no 3899: Loss: 0.25385287724871486, accuracy: 0.98\n",
      "iteration no 3900: Loss: 0.25384941758793467, accuracy: 0.98\n",
      "iteration no 3901: Loss: 0.25384495213777786, accuracy: 0.98\n",
      "iteration no 3902: Loss: 0.25384324250347196, accuracy: 0.98\n",
      "iteration no 3903: Loss: 0.2538378159060992, accuracy: 0.98\n",
      "iteration no 3904: Loss: 0.2538358269827089, accuracy: 0.98\n",
      "iteration no 3905: Loss: 0.25383097401891447, accuracy: 0.98\n",
      "iteration no 3906: Loss: 0.25382757133303957, accuracy: 0.98\n",
      "iteration no 3907: Loss: 0.2538257747492112, accuracy: 0.98\n",
      "iteration no 3908: Loss: 0.25382025877243647, accuracy: 0.98\n",
      "iteration no 3909: Loss: 0.25381727859211795, accuracy: 0.98\n",
      "iteration no 3910: Loss: 0.2538140774902677, accuracy: 0.98\n",
      "iteration no 3911: Loss: 0.25381141223703874, accuracy: 0.98\n",
      "iteration no 3912: Loss: 0.25380605659688316, accuracy: 0.98\n",
      "iteration no 3913: Loss: 0.2538035223731369, accuracy: 0.98\n",
      "iteration no 3914: Loss: 0.25380003684484276, accuracy: 0.98\n",
      "iteration no 3915: Loss: 0.25379587180465824, accuracy: 0.98\n",
      "iteration no 3916: Loss: 0.2537942136436789, accuracy: 0.98\n",
      "iteration no 3917: Loss: 0.25378810631231474, accuracy: 0.98\n",
      "iteration no 3918: Loss: 0.25378611690791053, accuracy: 0.98\n",
      "iteration no 3919: Loss: 0.25378404753192196, accuracy: 0.98\n",
      "iteration no 3920: Loss: 0.2537790145311541, accuracy: 0.98\n",
      "iteration no 3921: Loss: 0.25377445039591773, accuracy: 0.98\n",
      "iteration no 3922: Loss: 0.25377298516908087, accuracy: 0.98\n",
      "iteration no 3923: Loss: 0.2537695683809353, accuracy: 0.98\n",
      "iteration no 3924: Loss: 0.25376314862592675, accuracy: 0.98\n",
      "iteration no 3925: Loss: 0.2537625953488049, accuracy: 0.98\n",
      "iteration no 3926: Loss: 0.25375847321467904, accuracy: 0.98\n",
      "iteration no 3927: Loss: 0.25375457063286516, accuracy: 0.98\n",
      "iteration no 3928: Loss: 0.25375228499983116, accuracy: 0.98\n",
      "iteration no 3929: Loss: 0.2537468458965079, accuracy: 0.98\n",
      "iteration no 3930: Loss: 0.25374474735875463, accuracy: 0.98\n",
      "iteration no 3931: Loss: 0.2537418023275385, accuracy: 0.98\n",
      "iteration no 3932: Loss: 0.2537379871513431, accuracy: 0.98\n",
      "iteration no 3933: Loss: 0.2537319123093086, accuracy: 0.98\n",
      "iteration no 3934: Loss: 0.25373272335119684, accuracy: 0.98\n",
      "iteration no 3935: Loss: 0.2537275154395166, accuracy: 0.98\n",
      "iteration no 3936: Loss: 0.2537222624808246, accuracy: 0.98\n",
      "iteration no 3937: Loss: 0.25372192697445817, accuracy: 0.98\n",
      "iteration no 3938: Loss: 0.2537173853622903, accuracy: 0.98\n",
      "iteration no 3939: Loss: 0.2537136287963788, accuracy: 0.98\n",
      "iteration no 3940: Loss: 0.25371042054575915, accuracy: 0.98\n",
      "iteration no 3941: Loss: 0.25370763766780846, accuracy: 0.98\n",
      "iteration no 3942: Loss: 0.25370229506465036, accuracy: 0.98\n",
      "iteration no 3943: Loss: 0.2537021453685801, accuracy: 0.98\n",
      "iteration no 3944: Loss: 0.2536963267497518, accuracy: 0.98\n",
      "iteration no 3945: Loss: 0.2536921811093825, accuracy: 0.98\n",
      "iteration no 3946: Loss: 0.253692429460793, accuracy: 0.98\n",
      "iteration no 3947: Loss: 0.2536870364321078, accuracy: 0.98\n",
      "iteration no 3948: Loss: 0.2536826212955655, accuracy: 0.98\n",
      "iteration no 3949: Loss: 0.25368058977391894, accuracy: 0.98\n",
      "iteration no 3950: Loss: 0.25367815678356914, accuracy: 0.98\n",
      "iteration no 3951: Loss: 0.2536723966966805, accuracy: 0.98\n",
      "iteration no 3952: Loss: 0.2536713147152946, accuracy: 0.98\n",
      "iteration no 3953: Loss: 0.2536667159587313, accuracy: 0.98\n",
      "iteration no 3954: Loss: 0.2536636546403775, accuracy: 0.98\n",
      "iteration no 3955: Loss: 0.2536614966836942, accuracy: 0.98\n",
      "iteration no 3956: Loss: 0.2536563785747479, accuracy: 0.98\n",
      "iteration no 3957: Loss: 0.25365410878016037, accuracy: 0.98\n",
      "iteration no 3958: Loss: 0.2536513033837598, accuracy: 0.98\n",
      "iteration no 3959: Loss: 0.25364759359257627, accuracy: 0.98\n",
      "iteration no 3960: Loss: 0.2536424589345789, accuracy: 0.98\n",
      "iteration no 3961: Loss: 0.2536422544482, accuracy: 0.98\n",
      "iteration no 3962: Loss: 0.25363638089391777, accuracy: 0.98\n",
      "iteration no 3963: Loss: 0.2536336109207715, accuracy: 0.98\n",
      "iteration no 3964: Loss: 0.25363093731565833, accuracy: 0.98\n",
      "iteration no 3965: Loss: 0.2536271602673892, accuracy: 0.98\n",
      "iteration no 3966: Loss: 0.25362407274014354, accuracy: 0.98\n",
      "iteration no 3967: Loss: 0.2536207893766192, accuracy: 0.98\n",
      "iteration no 3968: Loss: 0.25361746994869344, accuracy: 0.98\n",
      "iteration no 3969: Loss: 0.2536134578334294, accuracy: 0.98\n",
      "iteration no 3970: Loss: 0.25361202045465286, accuracy: 0.98\n",
      "iteration no 3971: Loss: 0.2536062705568809, accuracy: 0.98\n",
      "iteration no 3972: Loss: 0.2536045249647512, accuracy: 0.98\n",
      "iteration no 3973: Loss: 0.25360070847469807, accuracy: 0.98\n",
      "iteration no 3974: Loss: 0.25359706372950225, accuracy: 0.98\n",
      "iteration no 3975: Loss: 0.2535951974143051, accuracy: 0.98\n",
      "iteration no 3976: Loss: 0.25359162088765674, accuracy: 0.98\n",
      "iteration no 3977: Loss: 0.2535873391658477, accuracy: 0.98\n",
      "iteration no 3978: Loss: 0.2535839120219084, accuracy: 0.98\n",
      "iteration no 3979: Loss: 0.25358318995466383, accuracy: 0.98\n",
      "iteration no 3980: Loss: 0.2535767554063618, accuracy: 0.98\n",
      "iteration no 3981: Loss: 0.2535754294400931, accuracy: 0.98\n",
      "iteration no 3982: Loss: 0.2535710537514216, accuracy: 0.98\n",
      "iteration no 3983: Loss: 0.25356853671969976, accuracy: 0.98\n",
      "iteration no 3984: Loss: 0.25356591438781045, accuracy: 0.98\n",
      "iteration no 3985: Loss: 0.2535620594672028, accuracy: 0.98\n",
      "iteration no 3986: Loss: 0.2535577417621836, accuracy: 0.98\n",
      "iteration no 3987: Loss: 0.253556183754193, accuracy: 0.98\n",
      "iteration no 3988: Loss: 0.2535527783212315, accuracy: 0.98\n",
      "iteration no 3989: Loss: 0.2535470854448062, accuracy: 0.98\n",
      "iteration no 3990: Loss: 0.2535470186675394, accuracy: 0.98\n",
      "iteration no 3991: Loss: 0.25354326801359955, accuracy: 0.98\n",
      "iteration no 3992: Loss: 0.2535389455744812, accuracy: 0.98\n",
      "iteration no 3993: Loss: 0.25353597361059327, accuracy: 0.98\n",
      "iteration no 3994: Loss: 0.25353092396538746, accuracy: 0.98\n",
      "iteration no 3995: Loss: 0.2535259350865732, accuracy: 0.98\n",
      "iteration no 3996: Loss: 0.25352182634880727, accuracy: 0.98\n",
      "iteration no 3997: Loss: 0.25351575235167306, accuracy: 0.98\n",
      "iteration no 3998: Loss: 0.25351112302360534, accuracy: 0.98\n",
      "iteration no 3999: Loss: 0.2535083246239818, accuracy: 0.98\n",
      "iteration no 4000: Loss: 0.253504147420487, accuracy: 0.98\n",
      "iteration no 4001: Loss: 0.2534968391771618, accuracy: 0.98\n",
      "iteration no 4002: Loss: 0.25349363327060326, accuracy: 0.98\n",
      "iteration no 4003: Loss: 0.2534901939189043, accuracy: 0.98\n",
      "iteration no 4004: Loss: 0.2534862501022923, accuracy: 0.98\n",
      "iteration no 4005: Loss: 0.25347924141343586, accuracy: 0.98\n",
      "iteration no 4006: Loss: 0.25347612037278255, accuracy: 0.98\n",
      "iteration no 4007: Loss: 0.25347348733559455, accuracy: 0.98\n",
      "iteration no 4008: Loss: 0.253466701657633, accuracy: 0.98\n",
      "iteration no 4009: Loss: 0.25346290363776136, accuracy: 0.98\n",
      "iteration no 4010: Loss: 0.25345866049635835, accuracy: 0.98\n",
      "iteration no 4011: Loss: 0.25345639751530313, accuracy: 0.98\n",
      "iteration no 4012: Loss: 0.2534502718795659, accuracy: 0.98\n",
      "iteration no 4013: Loss: 0.2534443727714743, accuracy: 0.98\n",
      "iteration no 4014: Loss: 0.2534434111002394, accuracy: 0.98\n",
      "iteration no 4015: Loss: 0.25343830926882194, accuracy: 0.98\n",
      "iteration no 4016: Loss: 0.25343408346672813, accuracy: 0.9833333333333333\n",
      "iteration no 4017: Loss: 0.2534290760927813, accuracy: 0.98\n",
      "iteration no 4018: Loss: 0.253426259269916, accuracy: 0.98\n",
      "iteration no 4019: Loss: 0.2534241955424623, accuracy: 0.98\n",
      "iteration no 4020: Loss: 0.25341751470900936, accuracy: 0.9833333333333333\n",
      "iteration no 4021: Loss: 0.25341392777536786, accuracy: 0.98\n",
      "iteration no 4022: Loss: 0.2534115659795702, accuracy: 0.98\n",
      "iteration no 4023: Loss: 0.2534067424277368, accuracy: 0.98\n",
      "iteration no 4024: Loss: 0.25340312803361087, accuracy: 0.98\n",
      "iteration no 4025: Loss: 0.2533980493313913, accuracy: 0.98\n",
      "iteration no 4026: Loss: 0.2533961721744386, accuracy: 0.98\n",
      "iteration no 4027: Loss: 0.2533922700263336, accuracy: 0.98\n",
      "iteration no 4028: Loss: 0.25338650812171065, accuracy: 0.98\n",
      "iteration no 4029: Loss: 0.2533844180555436, accuracy: 0.98\n",
      "iteration no 4030: Loss: 0.2533803748717884, accuracy: 0.98\n",
      "iteration no 4031: Loss: 0.2533771784433065, accuracy: 0.98\n",
      "iteration no 4032: Loss: 0.2533724219728656, accuracy: 0.98\n",
      "iteration no 4033: Loss: 0.2533675907582075, accuracy: 0.98\n",
      "iteration no 4034: Loss: 0.2533661195587777, accuracy: 0.98\n",
      "iteration no 4035: Loss: 0.25336243070925013, accuracy: 0.98\n",
      "iteration no 4036: Loss: 0.25335706014932413, accuracy: 0.98\n",
      "iteration no 4037: Loss: 0.25335367287097765, accuracy: 0.98\n",
      "iteration no 4038: Loss: 0.25335105733700014, accuracy: 0.98\n",
      "iteration no 4039: Loss: 0.2533468276510278, accuracy: 0.98\n",
      "iteration no 4040: Loss: 0.2533433584989609, accuracy: 0.98\n",
      "iteration no 4041: Loss: 0.25333846166991364, accuracy: 0.98\n",
      "iteration no 4042: Loss: 0.2533355054483112, accuracy: 0.98\n",
      "iteration no 4043: Loss: 0.2533333759784515, accuracy: 0.98\n",
      "iteration no 4044: Loss: 0.25332841651473725, accuracy: 0.98\n",
      "iteration no 4045: Loss: 0.2533228164782011, accuracy: 0.98\n",
      "iteration no 4046: Loss: 0.2533222618054287, accuracy: 0.98\n",
      "iteration no 4047: Loss: 0.2533183840733321, accuracy: 0.98\n",
      "iteration no 4048: Loss: 0.2533124266333113, accuracy: 0.98\n",
      "iteration no 4049: Loss: 0.2533093532429457, accuracy: 0.98\n",
      "iteration no 4050: Loss: 0.2533080507175166, accuracy: 0.98\n",
      "iteration no 4051: Loss: 0.25330302143500594, accuracy: 0.9833333333333333\n",
      "iteration no 4052: Loss: 0.25329908149136826, accuracy: 0.98\n",
      "iteration no 4053: Loss: 0.25329413321531025, accuracy: 0.98\n",
      "iteration no 4054: Loss: 0.2532928893944514, accuracy: 0.98\n",
      "iteration no 4055: Loss: 0.25328993023735924, accuracy: 0.98\n",
      "iteration no 4056: Loss: 0.2532840722064379, accuracy: 0.98\n",
      "iteration no 4057: Loss: 0.2532788888257194, accuracy: 0.98\n",
      "iteration no 4058: Loss: 0.2532800001237238, accuracy: 0.9833333333333333\n",
      "iteration no 4059: Loss: 0.2532751351894249, accuracy: 0.98\n",
      "iteration no 4060: Loss: 0.2532690427899272, accuracy: 0.98\n",
      "iteration no 4061: Loss: 0.25326737489648343, accuracy: 0.98\n",
      "iteration no 4062: Loss: 0.2532639229629957, accuracy: 0.98\n",
      "iteration no 4063: Loss: 0.25326043154612665, accuracy: 0.98\n",
      "iteration no 4064: Loss: 0.2532564638236914, accuracy: 0.98\n",
      "iteration no 4065: Loss: 0.25325236899415016, accuracy: 0.98\n",
      "iteration no 4066: Loss: 0.2532496813856303, accuracy: 0.9833333333333333\n",
      "iteration no 4067: Loss: 0.25324676324878465, accuracy: 0.98\n",
      "iteration no 4068: Loss: 0.25324235967837305, accuracy: 0.9833333333333333\n",
      "iteration no 4069: Loss: 0.2532393578513603, accuracy: 0.98\n",
      "iteration no 4070: Loss: 0.2532357252465168, accuracy: 0.98\n",
      "iteration no 4071: Loss: 0.25323164677703625, accuracy: 0.98\n",
      "iteration no 4072: Loss: 0.2532287619174793, accuracy: 0.98\n",
      "iteration no 4073: Loss: 0.25322589436643395, accuracy: 0.98\n",
      "iteration no 4074: Loss: 0.2532205966220329, accuracy: 0.98\n",
      "iteration no 4075: Loss: 0.2532193163741806, accuracy: 0.98\n",
      "iteration no 4076: Loss: 0.25321529894786055, accuracy: 0.9833333333333333\n",
      "iteration no 4077: Loss: 0.25321188281571133, accuracy: 0.98\n",
      "iteration no 4078: Loss: 0.2532080168122779, accuracy: 0.9833333333333333\n",
      "iteration no 4079: Loss: 0.2532059454551929, accuracy: 0.98\n",
      "iteration no 4080: Loss: 0.2532016357324607, accuracy: 0.98\n",
      "iteration no 4081: Loss: 0.2531983758589944, accuracy: 0.98\n",
      "iteration no 4082: Loss: 0.25319489621069613, accuracy: 0.98\n",
      "iteration no 4083: Loss: 0.25319137407953995, accuracy: 0.9833333333333333\n",
      "iteration no 4084: Loss: 0.25318937077114917, accuracy: 0.98\n",
      "iteration no 4085: Loss: 0.2531849662061831, accuracy: 0.98\n",
      "iteration no 4086: Loss: 0.2531802664276349, accuracy: 0.98\n",
      "iteration no 4087: Loss: 0.25317827038485397, accuracy: 0.98\n",
      "iteration no 4088: Loss: 0.2531763849029547, accuracy: 0.98\n",
      "iteration no 4089: Loss: 0.25317186229941735, accuracy: 0.98\n",
      "iteration no 4090: Loss: 0.25316808324022944, accuracy: 0.98\n",
      "iteration no 4091: Loss: 0.2531642756974897, accuracy: 0.98\n",
      "iteration no 4092: Loss: 0.2531628817409989, accuracy: 0.98\n",
      "iteration no 4093: Loss: 0.25315974540948105, accuracy: 0.98\n",
      "iteration no 4094: Loss: 0.25315453202585203, accuracy: 0.98\n",
      "iteration no 4095: Loss: 0.253150069758663, accuracy: 0.98\n",
      "iteration no 4096: Loss: 0.25315066377817674, accuracy: 0.98\n",
      "iteration no 4097: Loss: 0.2531464598306925, accuracy: 0.98\n",
      "iteration no 4098: Loss: 0.25314098776630467, accuracy: 0.98\n",
      "iteration no 4099: Loss: 0.25313881335936284, accuracy: 0.98\n",
      "iteration no 4100: Loss: 0.2531366298777712, accuracy: 0.98\n",
      "iteration no 4101: Loss: 0.25313249327496673, accuracy: 0.98\n",
      "iteration no 4102: Loss: 0.25312932381870346, accuracy: 0.98\n",
      "iteration no 4103: Loss: 0.2531264639250116, accuracy: 0.98\n",
      "iteration no 4104: Loss: 0.2531223366820026, accuracy: 0.98\n",
      "iteration no 4105: Loss: 0.25312019539910063, accuracy: 0.98\n",
      "iteration no 4106: Loss: 0.2531160726151079, accuracy: 0.98\n",
      "iteration no 4107: Loss: 0.25311336960219283, accuracy: 0.98\n",
      "iteration no 4108: Loss: 0.25311014831271716, accuracy: 0.98\n",
      "iteration no 4109: Loss: 0.2531066829780533, accuracy: 0.98\n",
      "iteration no 4110: Loss: 0.2531035651472931, accuracy: 0.98\n",
      "iteration no 4111: Loss: 0.25310193768847866, accuracy: 0.98\n",
      "iteration no 4112: Loss: 0.25309610646155234, accuracy: 0.98\n",
      "iteration no 4113: Loss: 0.2530938148080208, accuracy: 0.98\n",
      "iteration no 4114: Loss: 0.2530924359810588, accuracy: 0.98\n",
      "iteration no 4115: Loss: 0.2530878810239888, accuracy: 0.98\n",
      "iteration no 4116: Loss: 0.25308319681879105, accuracy: 0.98\n",
      "iteration no 4117: Loss: 0.2530821124940026, accuracy: 0.98\n",
      "iteration no 4118: Loss: 0.25307931974478737, accuracy: 0.98\n",
      "iteration no 4119: Loss: 0.25307493212667487, accuracy: 0.98\n",
      "iteration no 4120: Loss: 0.25307157633863575, accuracy: 0.98\n",
      "iteration no 4121: Loss: 0.2530683809471334, accuracy: 0.98\n",
      "iteration no 4122: Loss: 0.25306692801998465, accuracy: 0.98\n",
      "iteration no 4123: Loss: 0.2530630858883556, accuracy: 0.98\n",
      "iteration no 4124: Loss: 0.25305858585113, accuracy: 0.98\n",
      "iteration no 4125: Loss: 0.25305566521391887, accuracy: 0.98\n",
      "iteration no 4126: Loss: 0.2530553639931362, accuracy: 0.98\n",
      "iteration no 4127: Loss: 0.25305005479731774, accuracy: 0.9833333333333333\n",
      "iteration no 4128: Loss: 0.2530465827328149, accuracy: 0.98\n",
      "iteration no 4129: Loss: 0.2530433605431797, accuracy: 0.98\n",
      "iteration no 4130: Loss: 0.25304159917926783, accuracy: 0.98\n",
      "iteration no 4131: Loss: 0.2530386577160655, accuracy: 0.98\n",
      "iteration no 4132: Loss: 0.25303460379155995, accuracy: 0.98\n",
      "iteration no 4133: Loss: 0.2530306172185155, accuracy: 0.98\n",
      "iteration no 4134: Loss: 0.2530287881110932, accuracy: 0.98\n",
      "iteration no 4135: Loss: 0.25302678227947667, accuracy: 0.98\n",
      "iteration no 4136: Loss: 0.2530212860192553, accuracy: 0.98\n",
      "iteration no 4137: Loss: 0.253019406827669, accuracy: 0.98\n",
      "iteration no 4138: Loss: 0.2530165209850827, accuracy: 0.98\n",
      "iteration no 4139: Loss: 0.25301307141385576, accuracy: 0.98\n",
      "iteration no 4140: Loss: 0.25301035438613995, accuracy: 0.98\n",
      "iteration no 4141: Loss: 0.2530075119641677, accuracy: 0.98\n",
      "iteration no 4142: Loss: 0.2530031393071396, accuracy: 0.98\n",
      "iteration no 4143: Loss: 0.25300200881245066, accuracy: 0.98\n",
      "iteration no 4144: Loss: 0.25299754337597935, accuracy: 0.98\n",
      "iteration no 4145: Loss: 0.25299551517597346, accuracy: 0.98\n",
      "iteration no 4146: Loss: 0.25299207079412106, accuracy: 0.98\n",
      "iteration no 4147: Loss: 0.2529885531181546, accuracy: 0.98\n",
      "iteration no 4148: Loss: 0.2529859731382812, accuracy: 0.98\n",
      "iteration no 4149: Loss: 0.25298460813896145, accuracy: 0.98\n",
      "iteration no 4150: Loss: 0.25297927489203986, accuracy: 0.98\n",
      "iteration no 4151: Loss: 0.2529759125154959, accuracy: 0.98\n",
      "iteration no 4152: Loss: 0.25297584763598274, accuracy: 0.98\n",
      "iteration no 4153: Loss: 0.25297142843392995, accuracy: 0.98\n",
      "iteration no 4154: Loss: 0.25296730222211256, accuracy: 0.98\n",
      "iteration no 4155: Loss: 0.2529645842979874, accuracy: 0.98\n",
      "iteration no 4156: Loss: 0.252963059365046, accuracy: 0.98\n",
      "iteration no 4157: Loss: 0.2529595020685649, accuracy: 0.98\n",
      "iteration no 4158: Loss: 0.25295603160457825, accuracy: 0.98\n",
      "iteration no 4159: Loss: 0.2529516264945342, accuracy: 0.98\n",
      "iteration no 4160: Loss: 0.25295197496980226, accuracy: 0.98\n",
      "iteration no 4161: Loss: 0.2529481623176476, accuracy: 0.98\n",
      "iteration no 4162: Loss: 0.2529429346943767, accuracy: 0.98\n",
      "iteration no 4163: Loss: 0.25294085987717563, accuracy: 0.98\n",
      "iteration no 4164: Loss: 0.25293982133454795, accuracy: 0.98\n",
      "iteration no 4165: Loss: 0.2529352142875942, accuracy: 0.98\n",
      "iteration no 4166: Loss: 0.2529325239651417, accuracy: 0.98\n",
      "iteration no 4167: Loss: 0.2529286625182128, accuracy: 0.98\n",
      "iteration no 4168: Loss: 0.2529268218167079, accuracy: 0.98\n",
      "iteration no 4169: Loss: 0.2529247919488921, accuracy: 0.98\n",
      "iteration no 4170: Loss: 0.25292063502585255, accuracy: 0.98\n",
      "iteration no 4171: Loss: 0.2529162484914227, accuracy: 0.98\n",
      "iteration no 4172: Loss: 0.2529158632158511, accuracy: 0.98\n",
      "iteration no 4173: Loss: 0.2529132292184644, accuracy: 0.98\n",
      "iteration no 4174: Loss: 0.2529085191296576, accuracy: 0.98\n",
      "iteration no 4175: Loss: 0.2529059601748253, accuracy: 0.98\n",
      "iteration no 4176: Loss: 0.2529025343975057, accuracy: 0.98\n",
      "iteration no 4177: Loss: 0.25290123190072733, accuracy: 0.98\n",
      "iteration no 4178: Loss: 0.2528978120525416, accuracy: 0.98\n",
      "iteration no 4179: Loss: 0.25289360132360444, accuracy: 0.98\n",
      "iteration no 4180: Loss: 0.2528915336555933, accuracy: 0.98\n",
      "iteration no 4181: Loss: 0.25288993384296143, accuracy: 0.98\n",
      "iteration no 4182: Loss: 0.2528854189499977, accuracy: 0.98\n",
      "iteration no 4183: Loss: 0.25288304582384535, accuracy: 0.98\n",
      "iteration no 4184: Loss: 0.25288041499367137, accuracy: 0.98\n",
      "iteration no 4185: Loss: 0.2528763284396307, accuracy: 0.98\n",
      "iteration no 4186: Loss: 0.2528744927563046, accuracy: 0.98\n",
      "iteration no 4187: Loss: 0.25287210365834933, accuracy: 0.98\n",
      "iteration no 4188: Loss: 0.2528680196233444, accuracy: 0.98\n",
      "iteration no 4189: Loss: 0.2528650908095215, accuracy: 0.98\n",
      "iteration no 4190: Loss: 0.25286295227558464, accuracy: 0.98\n",
      "iteration no 4191: Loss: 0.252860083878015, accuracy: 0.98\n",
      "iteration no 4192: Loss: 0.25285784205530676, accuracy: 0.98\n",
      "iteration no 4193: Loss: 0.25285245143397417, accuracy: 0.98\n",
      "iteration no 4194: Loss: 0.2528506934900368, accuracy: 0.98\n",
      "iteration no 4195: Loss: 0.25284999892607224, accuracy: 0.98\n",
      "iteration no 4196: Loss: 0.252845385167684, accuracy: 0.98\n",
      "iteration no 4197: Loss: 0.25284085626700525, accuracy: 0.98\n",
      "iteration no 4198: Loss: 0.252839535373817, accuracy: 0.98\n",
      "iteration no 4199: Loss: 0.25283738295880986, accuracy: 0.98\n",
      "iteration no 4200: Loss: 0.2528343439022075, accuracy: 0.98\n",
      "iteration no 4201: Loss: 0.2528305986721501, accuracy: 0.98\n",
      "iteration no 4202: Loss: 0.25282647287221693, accuracy: 0.98\n",
      "iteration no 4203: Loss: 0.25282568838673236, accuracy: 0.98\n",
      "iteration no 4204: Loss: 0.2528222039303617, accuracy: 0.98\n",
      "iteration no 4205: Loss: 0.25281785573987836, accuracy: 0.98\n",
      "iteration no 4206: Loss: 0.2528146496833359, accuracy: 0.98\n",
      "iteration no 4207: Loss: 0.25281387592152854, accuracy: 0.98\n",
      "iteration no 4208: Loss: 0.25280939244100503, accuracy: 0.9833333333333333\n",
      "iteration no 4209: Loss: 0.2528072464505776, accuracy: 0.9833333333333333\n",
      "iteration no 4210: Loss: 0.2528026445963012, accuracy: 0.98\n",
      "iteration no 4211: Loss: 0.25280073780238743, accuracy: 0.9833333333333333\n",
      "iteration no 4212: Loss: 0.2527996167236023, accuracy: 0.98\n",
      "iteration no 4213: Loss: 0.2527944403578368, accuracy: 0.9833333333333333\n",
      "iteration no 4214: Loss: 0.2527900882021038, accuracy: 0.9833333333333333\n",
      "iteration no 4215: Loss: 0.2527901974951896, accuracy: 0.9833333333333333\n",
      "iteration no 4216: Loss: 0.25278728331031464, accuracy: 0.98\n",
      "iteration no 4217: Loss: 0.252782459637964, accuracy: 0.9833333333333333\n",
      "iteration no 4218: Loss: 0.25277981588054327, accuracy: 0.98\n",
      "iteration no 4219: Loss: 0.25277764010723547, accuracy: 0.9833333333333333\n",
      "iteration no 4220: Loss: 0.25277602338344285, accuracy: 0.98\n",
      "iteration no 4221: Loss: 0.2527721544005479, accuracy: 0.9833333333333333\n",
      "iteration no 4222: Loss: 0.25276769984553493, accuracy: 0.9833333333333333\n",
      "iteration no 4223: Loss: 0.2527660606684979, accuracy: 0.9833333333333333\n",
      "iteration no 4224: Loss: 0.2527648492731613, accuracy: 0.98\n",
      "iteration no 4225: Loss: 0.2527606139442767, accuracy: 0.98\n",
      "iteration no 4226: Loss: 0.25275719557623433, accuracy: 0.98\n",
      "iteration no 4227: Loss: 0.2527544845461061, accuracy: 0.98\n",
      "iteration no 4228: Loss: 0.2527514200177254, accuracy: 0.9833333333333333\n",
      "iteration no 4229: Loss: 0.2527509727715045, accuracy: 0.98\n",
      "iteration no 4230: Loss: 0.252746095206353, accuracy: 0.98\n",
      "iteration no 4231: Loss: 0.25274258437372926, accuracy: 0.9833333333333333\n",
      "iteration no 4232: Loss: 0.25274135231682787, accuracy: 0.98\n",
      "iteration no 4233: Loss: 0.25273881227622275, accuracy: 0.98\n",
      "iteration no 4234: Loss: 0.25273451124674956, accuracy: 0.98\n",
      "iteration no 4235: Loss: 0.25273289899760887, accuracy: 0.98\n",
      "iteration no 4236: Loss: 0.2527290993686848, accuracy: 0.9833333333333333\n",
      "iteration no 4237: Loss: 0.25272692146644626, accuracy: 0.98\n",
      "iteration no 4238: Loss: 0.2527241485719286, accuracy: 0.98\n",
      "iteration no 4239: Loss: 0.2527213149545975, accuracy: 0.98\n",
      "iteration no 4240: Loss: 0.25271857291710476, accuracy: 0.9833333333333333\n",
      "iteration no 4241: Loss: 0.2527160458316841, accuracy: 0.98\n",
      "iteration no 4242: Loss: 0.2527116666691386, accuracy: 0.98\n",
      "iteration no 4243: Loss: 0.2527112552802091, accuracy: 0.98\n",
      "iteration no 4244: Loss: 0.25270741356160864, accuracy: 0.9833333333333333\n",
      "iteration no 4245: Loss: 0.25270350615933407, accuracy: 0.9833333333333333\n",
      "iteration no 4246: Loss: 0.2527023706220592, accuracy: 0.98\n",
      "iteration no 4247: Loss: 0.25269937511097673, accuracy: 0.98\n",
      "iteration no 4248: Loss: 0.2526957180763601, accuracy: 0.98\n",
      "iteration no 4249: Loss: 0.25269395219247215, accuracy: 0.9833333333333333\n",
      "iteration no 4250: Loss: 0.2526906706281708, accuracy: 0.98\n",
      "iteration no 4251: Loss: 0.2526883736800557, accuracy: 0.9833333333333333\n",
      "iteration no 4252: Loss: 0.2526852203530332, accuracy: 0.98\n",
      "iteration no 4253: Loss: 0.2526820553388259, accuracy: 0.9833333333333333\n",
      "iteration no 4254: Loss: 0.25268016318286046, accuracy: 0.98\n",
      "iteration no 4255: Loss: 0.2526775159667597, accuracy: 0.98\n",
      "iteration no 4256: Loss: 0.25267370186231985, accuracy: 0.98\n",
      "iteration no 4257: Loss: 0.2526712161027095, accuracy: 0.9833333333333333\n",
      "iteration no 4258: Loss: 0.25266918528599364, accuracy: 0.98\n",
      "iteration no 4259: Loss: 0.252666104377003, accuracy: 0.9833333333333333\n",
      "iteration no 4260: Loss: 0.2526648797714891, accuracy: 0.98\n",
      "iteration no 4261: Loss: 0.2526593511043385, accuracy: 0.9833333333333333\n",
      "iteration no 4262: Loss: 0.2526574034355656, accuracy: 0.98\n",
      "iteration no 4263: Loss: 0.25265652802091254, accuracy: 0.98\n",
      "iteration no 4264: Loss: 0.2526529161724676, accuracy: 0.98\n",
      "iteration no 4265: Loss: 0.2526490022543586, accuracy: 0.9833333333333333\n",
      "iteration no 4266: Loss: 0.2526466510954482, accuracy: 0.98\n",
      "iteration no 4267: Loss: 0.25264437174551424, accuracy: 0.98\n",
      "iteration no 4268: Loss: 0.2526430465629307, accuracy: 0.98\n",
      "iteration no 4269: Loss: 0.25263830774601015, accuracy: 0.98\n",
      "iteration no 4270: Loss: 0.25263532223697893, accuracy: 0.98\n",
      "iteration no 4271: Loss: 0.25263492070102195, accuracy: 0.98\n",
      "iteration no 4272: Loss: 0.2526308122209471, accuracy: 0.98\n",
      "iteration no 4273: Loss: 0.25262733416263605, accuracy: 0.98\n",
      "iteration no 4274: Loss: 0.2526254462321158, accuracy: 0.98\n",
      "iteration no 4275: Loss: 0.2526229642938126, accuracy: 0.98\n",
      "iteration no 4276: Loss: 0.2526204044103098, accuracy: 0.98\n",
      "iteration no 4277: Loss: 0.25261704284468145, accuracy: 0.98\n",
      "iteration no 4278: Loss: 0.2526137636442447, accuracy: 0.9833333333333333\n",
      "iteration no 4279: Loss: 0.25261327766183117, accuracy: 0.98\n",
      "iteration no 4280: Loss: 0.25260939194329046, accuracy: 0.98\n",
      "iteration no 4281: Loss: 0.25260539972842744, accuracy: 0.98\n",
      "iteration no 4282: Loss: 0.2526034799887287, accuracy: 0.98\n",
      "iteration no 4283: Loss: 0.2526014603035607, accuracy: 0.98\n",
      "iteration no 4284: Loss: 0.25259859427971704, accuracy: 0.9833333333333333\n",
      "iteration no 4285: Loss: 0.25259594314815265, accuracy: 0.98\n",
      "iteration no 4286: Loss: 0.252591591122469, accuracy: 0.98\n",
      "iteration no 4287: Loss: 0.2525906269430694, accuracy: 0.98\n",
      "iteration no 4288: Loss: 0.2525883822077087, accuracy: 0.98\n",
      "iteration no 4289: Loss: 0.25258417163057606, accuracy: 0.9833333333333333\n",
      "iteration no 4290: Loss: 0.2525817896472279, accuracy: 0.98\n",
      "iteration no 4291: Loss: 0.2525792557198349, accuracy: 0.98\n",
      "iteration no 4292: Loss: 0.25257706884172487, accuracy: 0.98\n",
      "iteration no 4293: Loss: 0.2525750120304194, accuracy: 0.98\n",
      "iteration no 4294: Loss: 0.2525697889619657, accuracy: 0.98\n",
      "iteration no 4295: Loss: 0.25256938117451566, accuracy: 0.98\n",
      "iteration no 4296: Loss: 0.2525669949177703, accuracy: 0.98\n",
      "iteration no 4297: Loss: 0.2525625128131949, accuracy: 0.98\n",
      "iteration no 4298: Loss: 0.2525602674146842, accuracy: 0.98\n",
      "iteration no 4299: Loss: 0.25255794501061135, accuracy: 0.9833333333333333\n",
      "iteration no 4300: Loss: 0.25255559168121033, accuracy: 0.98\n",
      "iteration no 4301: Loss: 0.25255343116116513, accuracy: 0.98\n",
      "iteration no 4302: Loss: 0.2525488989971319, accuracy: 0.98\n",
      "iteration no 4303: Loss: 0.25254743936456997, accuracy: 0.9833333333333333\n",
      "iteration no 4304: Loss: 0.25254629909556814, accuracy: 0.98\n",
      "iteration no 4305: Loss: 0.2525415377020275, accuracy: 0.9833333333333333\n",
      "iteration no 4306: Loss: 0.2525390270672373, accuracy: 0.98\n",
      "iteration no 4307: Loss: 0.2525363993562909, accuracy: 0.9833333333333333\n",
      "iteration no 4308: Loss: 0.2525341249340073, accuracy: 0.9833333333333333\n",
      "iteration no 4309: Loss: 0.2525327854066542, accuracy: 0.9833333333333333\n",
      "iteration no 4310: Loss: 0.2525283019216914, accuracy: 0.98\n",
      "iteration no 4311: Loss: 0.25252552131578965, accuracy: 0.98\n",
      "iteration no 4312: Loss: 0.25252408819289457, accuracy: 0.9833333333333333\n",
      "iteration no 4313: Loss: 0.2525211595728393, accuracy: 0.9833333333333333\n",
      "iteration no 4314: Loss: 0.2525185739933675, accuracy: 0.98\n",
      "iteration no 4315: Loss: 0.2525150788306305, accuracy: 0.98\n",
      "iteration no 4316: Loss: 0.25251252634024524, accuracy: 0.9833333333333333\n",
      "iteration no 4317: Loss: 0.2525117988970719, accuracy: 0.98\n",
      "iteration no 4318: Loss: 0.25250728532700184, accuracy: 0.98\n",
      "iteration no 4319: Loss: 0.2525039488227663, accuracy: 0.9833333333333333\n",
      "iteration no 4320: Loss: 0.2525044548112456, accuracy: 0.9833333333333333\n",
      "iteration no 4321: Loss: 0.25249984757720423, accuracy: 0.9833333333333333\n",
      "iteration no 4322: Loss: 0.25249651567969167, accuracy: 0.9833333333333333\n",
      "iteration no 4323: Loss: 0.2524948193861192, accuracy: 0.9833333333333333\n",
      "iteration no 4324: Loss: 0.25249200498698665, accuracy: 0.98\n",
      "iteration no 4325: Loss: 0.2524908590933601, accuracy: 0.98\n",
      "iteration no 4326: Loss: 0.2524865819836485, accuracy: 0.98\n",
      "iteration no 4327: Loss: 0.25248282302748204, accuracy: 0.98\n",
      "iteration no 4328: Loss: 0.2524828997356286, accuracy: 0.9833333333333333\n",
      "iteration no 4329: Loss: 0.2524795540692606, accuracy: 0.9833333333333333\n",
      "iteration no 4330: Loss: 0.2524760077944149, accuracy: 0.98\n",
      "iteration no 4331: Loss: 0.2524737859751792, accuracy: 0.98\n",
      "iteration no 4332: Loss: 0.25247071725833237, accuracy: 0.9866666666666667\n",
      "iteration no 4333: Loss: 0.25246949883899, accuracy: 0.9833333333333333\n",
      "iteration no 4334: Loss: 0.2524668813075484, accuracy: 0.98\n",
      "iteration no 4335: Loss: 0.25246222546525904, accuracy: 0.9833333333333333\n",
      "iteration no 4336: Loss: 0.2524622835507633, accuracy: 0.9833333333333333\n",
      "iteration no 4337: Loss: 0.25245822365165366, accuracy: 0.98\n",
      "iteration no 4338: Loss: 0.25245547279825364, accuracy: 0.98\n",
      "iteration no 4339: Loss: 0.2524533553796653, accuracy: 0.98\n",
      "iteration no 4340: Loss: 0.25245038819379095, accuracy: 0.9833333333333333\n",
      "iteration no 4341: Loss: 0.2524492300507237, accuracy: 0.9833333333333333\n",
      "iteration no 4342: Loss: 0.25244594950343024, accuracy: 0.98\n",
      "iteration no 4343: Loss: 0.2524411668644002, accuracy: 0.9833333333333333\n",
      "iteration no 4344: Loss: 0.25244214590794894, accuracy: 0.9833333333333333\n",
      "iteration no 4345: Loss: 0.25243801963572965, accuracy: 0.9833333333333333\n",
      "iteration no 4346: Loss: 0.25243549573443996, accuracy: 0.98\n",
      "iteration no 4347: Loss: 0.25243240789611227, accuracy: 0.9833333333333333\n",
      "iteration no 4348: Loss: 0.25243005101448807, accuracy: 0.9833333333333333\n",
      "iteration no 4349: Loss: 0.25242857317248557, accuracy: 0.9833333333333333\n",
      "iteration no 4350: Loss: 0.2524256558563178, accuracy: 0.9833333333333333\n",
      "iteration no 4351: Loss: 0.252421830592667, accuracy: 0.9833333333333333\n",
      "iteration no 4352: Loss: 0.25242146394344456, accuracy: 0.9833333333333333\n",
      "iteration no 4353: Loss: 0.25241730248848865, accuracy: 0.9833333333333333\n",
      "iteration no 4354: Loss: 0.252415141646653, accuracy: 0.9833333333333333\n",
      "iteration no 4355: Loss: 0.25241269446723386, accuracy: 0.9833333333333333\n",
      "iteration no 4356: Loss: 0.2524102890899529, accuracy: 0.9866666666666667\n",
      "iteration no 4357: Loss: 0.25240788422062804, accuracy: 0.9833333333333333\n",
      "iteration no 4358: Loss: 0.25240477349523754, accuracy: 0.9833333333333333\n",
      "iteration no 4359: Loss: 0.2524015916183236, accuracy: 0.9866666666666667\n",
      "iteration no 4360: Loss: 0.2524010193844786, accuracy: 0.9833333333333333\n",
      "iteration no 4361: Loss: 0.2523978796604819, accuracy: 0.9833333333333333\n",
      "iteration no 4362: Loss: 0.25239485627015223, accuracy: 0.9833333333333333\n",
      "iteration no 4363: Loss: 0.2523926169937983, accuracy: 0.9866666666666667\n",
      "iteration no 4364: Loss: 0.2523894070264959, accuracy: 0.9833333333333333\n",
      "iteration no 4365: Loss: 0.2523879682621745, accuracy: 0.9833333333333333\n",
      "iteration no 4366: Loss: 0.2523854997165513, accuracy: 0.9833333333333333\n",
      "iteration no 4367: Loss: 0.25238151029942535, accuracy: 0.9833333333333333\n",
      "iteration no 4368: Loss: 0.2523802800431413, accuracy: 0.9833333333333333\n",
      "iteration no 4369: Loss: 0.2523777242516913, accuracy: 0.9833333333333333\n",
      "iteration no 4370: Loss: 0.2523742708411164, accuracy: 0.9866666666666667\n",
      "iteration no 4371: Loss: 0.2523734289272136, accuracy: 0.9833333333333333\n",
      "iteration no 4372: Loss: 0.2523692737356289, accuracy: 0.9866666666666667\n",
      "iteration no 4373: Loss: 0.2523681861062432, accuracy: 0.9833333333333333\n",
      "iteration no 4374: Loss: 0.2523649135352688, accuracy: 0.9833333333333333\n",
      "iteration no 4375: Loss: 0.25236121365460745, accuracy: 0.9833333333333333\n",
      "iteration no 4376: Loss: 0.25236018639030244, accuracy: 0.9866666666666667\n",
      "iteration no 4377: Loss: 0.25235867706094584, accuracy: 0.9833333333333333\n",
      "iteration no 4378: Loss: 0.2523539815896182, accuracy: 0.9866666666666667\n",
      "iteration no 4379: Loss: 0.2523514765439867, accuracy: 0.9866666666666667\n",
      "iteration no 4380: Loss: 0.25235005147755096, accuracy: 0.9866666666666667\n",
      "iteration no 4381: Loss: 0.2523480494897413, accuracy: 0.9833333333333333\n",
      "iteration no 4382: Loss: 0.2523441184029762, accuracy: 0.9866666666666667\n",
      "iteration no 4383: Loss: 0.2523427914078756, accuracy: 0.9833333333333333\n",
      "iteration no 4384: Loss: 0.2523395321464275, accuracy: 0.9866666666666667\n",
      "iteration no 4385: Loss: 0.25233731622679395, accuracy: 0.9833333333333333\n",
      "iteration no 4386: Loss: 0.2523352338683202, accuracy: 0.9833333333333333\n",
      "iteration no 4387: Loss: 0.25233129868676013, accuracy: 0.9866666666666667\n",
      "iteration no 4388: Loss: 0.2523295006021887, accuracy: 0.9866666666666667\n",
      "iteration no 4389: Loss: 0.25232800539640793, accuracy: 0.9866666666666667\n",
      "iteration no 4390: Loss: 0.25232543278293995, accuracy: 0.9833333333333333\n",
      "iteration no 4391: Loss: 0.2523209906034437, accuracy: 0.9866666666666667\n",
      "iteration no 4392: Loss: 0.25232009612569806, accuracy: 0.9866666666666667\n",
      "iteration no 4393: Loss: 0.2523189094335313, accuracy: 0.9833333333333333\n",
      "iteration no 4394: Loss: 0.25231470438271086, accuracy: 0.9866666666666667\n",
      "iteration no 4395: Loss: 0.25231096324294455, accuracy: 0.9866666666666667\n",
      "iteration no 4396: Loss: 0.252309665359674, accuracy: 0.9866666666666667\n",
      "iteration no 4397: Loss: 0.25230814557234527, accuracy: 0.9866666666666667\n",
      "iteration no 4398: Loss: 0.25230460252435216, accuracy: 0.9866666666666667\n",
      "iteration no 4399: Loss: 0.2523016338595504, accuracy: 0.9866666666666667\n",
      "iteration no 4400: Loss: 0.2522991525259293, accuracy: 0.9833333333333333\n",
      "iteration no 4401: Loss: 0.2522972906818813, accuracy: 0.9866666666666667\n",
      "iteration no 4402: Loss: 0.25229381818109575, accuracy: 0.9866666666666667\n",
      "iteration no 4403: Loss: 0.2522913237511995, accuracy: 0.9866666666666667\n",
      "iteration no 4404: Loss: 0.25228865251886257, accuracy: 0.9866666666666667\n",
      "iteration no 4405: Loss: 0.2522869950546784, accuracy: 0.9833333333333333\n",
      "iteration no 4406: Loss: 0.2522839645574177, accuracy: 0.9866666666666667\n",
      "iteration no 4407: Loss: 0.25228059958518795, accuracy: 0.9833333333333333\n",
      "iteration no 4408: Loss: 0.2522781606669581, accuracy: 0.9866666666666667\n",
      "iteration no 4409: Loss: 0.2522769460873044, accuracy: 0.9833333333333333\n",
      "iteration no 4410: Loss: 0.2522735553779722, accuracy: 0.9866666666666667\n",
      "iteration no 4411: Loss: 0.25227007165720305, accuracy: 0.9833333333333333\n",
      "iteration no 4412: Loss: 0.2522677393443533, accuracy: 0.9866666666666667\n",
      "iteration no 4413: Loss: 0.25226672136353967, accuracy: 0.9833333333333333\n",
      "iteration no 4414: Loss: 0.2522632948440482, accuracy: 0.9833333333333333\n",
      "iteration no 4415: Loss: 0.2522595468639225, accuracy: 0.9866666666666667\n",
      "iteration no 4416: Loss: 0.2522573119166646, accuracy: 0.9833333333333333\n",
      "iteration no 4417: Loss: 0.25225604144481095, accuracy: 0.9866666666666667\n",
      "iteration no 4418: Loss: 0.25225333425514285, accuracy: 0.9833333333333333\n",
      "iteration no 4419: Loss: 0.2522490206983912, accuracy: 0.9866666666666667\n",
      "iteration no 4420: Loss: 0.2522473305717646, accuracy: 0.9866666666666667\n",
      "iteration no 4421: Loss: 0.2522459835317623, accuracy: 0.9833333333333333\n",
      "iteration no 4422: Loss: 0.25224291692184453, accuracy: 0.9866666666666667\n",
      "iteration no 4423: Loss: 0.25223860909154006, accuracy: 0.9833333333333333\n",
      "iteration no 4424: Loss: 0.25223737886130265, accuracy: 0.9866666666666667\n",
      "iteration no 4425: Loss: 0.25223523436417844, accuracy: 0.9833333333333333\n",
      "iteration no 4426: Loss: 0.2522325547964402, accuracy: 0.9866666666666667\n",
      "iteration no 4427: Loss: 0.25222840990714235, accuracy: 0.9866666666666667\n",
      "iteration no 4428: Loss: 0.25222796944278475, accuracy: 0.9833333333333333\n",
      "iteration no 4429: Loss: 0.25222445518675507, accuracy: 0.9833333333333333\n",
      "iteration no 4430: Loss: 0.25222188967194326, accuracy: 0.9866666666666667\n",
      "iteration no 4431: Loss: 0.2522185825798998, accuracy: 0.9866666666666667\n",
      "iteration no 4432: Loss: 0.2522180076274911, accuracy: 0.9833333333333333\n",
      "iteration no 4433: Loss: 0.252213980286481, accuracy: 0.9866666666666667\n",
      "iteration no 4434: Loss: 0.25221120638471084, accuracy: 0.9833333333333333\n",
      "iteration no 4435: Loss: 0.2522094685245855, accuracy: 0.9866666666666667\n",
      "iteration no 4436: Loss: 0.25220801391868986, accuracy: 0.9833333333333333\n",
      "iteration no 4437: Loss: 0.2522035837161263, accuracy: 0.9866666666666667\n",
      "iteration no 4438: Loss: 0.2522002904441253, accuracy: 0.9866666666666667\n",
      "iteration no 4439: Loss: 0.2521998389874878, accuracy: 0.9833333333333333\n",
      "iteration no 4440: Loss: 0.2521974306431439, accuracy: 0.9866666666666667\n",
      "iteration no 4441: Loss: 0.25219341890399377, accuracy: 0.9833333333333333\n",
      "iteration no 4442: Loss: 0.25219020973157297, accuracy: 0.9866666666666667\n",
      "iteration no 4443: Loss: 0.2521903635443092, accuracy: 0.9833333333333333\n",
      "iteration no 4444: Loss: 0.2521871056326657, accuracy: 0.9866666666666667\n",
      "iteration no 4445: Loss: 0.2521831597973936, accuracy: 0.9833333333333333\n",
      "iteration no 4446: Loss: 0.2521806523859553, accuracy: 0.9833333333333333\n",
      "iteration no 4447: Loss: 0.252180221647926, accuracy: 0.9866666666666667\n",
      "iteration no 4448: Loss: 0.25217699834836965, accuracy: 0.9833333333333333\n",
      "iteration no 4449: Loss: 0.2521735905939481, accuracy: 0.9866666666666667\n",
      "iteration no 4450: Loss: 0.25217079732591213, accuracy: 0.9833333333333333\n",
      "iteration no 4451: Loss: 0.2521702119282804, accuracy: 0.9866666666666667\n",
      "iteration no 4452: Loss: 0.2521664444010203, accuracy: 0.9866666666666667\n",
      "iteration no 4453: Loss: 0.25216433826930323, accuracy: 0.9833333333333333\n",
      "iteration no 4454: Loss: 0.2521612123951292, accuracy: 0.9866666666666667\n",
      "iteration no 4455: Loss: 0.25215982266657333, accuracy: 0.9833333333333333\n",
      "iteration no 4456: Loss: 0.2521565787488473, accuracy: 0.9866666666666667\n",
      "iteration no 4457: Loss: 0.2521542654784418, accuracy: 0.9833333333333333\n",
      "iteration no 4458: Loss: 0.2521517458752057, accuracy: 0.9866666666666667\n",
      "iteration no 4459: Loss: 0.2521495430389632, accuracy: 0.9833333333333333\n",
      "iteration no 4460: Loss: 0.252147433912703, accuracy: 0.9866666666666667\n",
      "iteration no 4461: Loss: 0.25214384969908066, accuracy: 0.9833333333333333\n",
      "iteration no 4462: Loss: 0.25214231590995095, accuracy: 0.9866666666666667\n",
      "iteration no 4463: Loss: 0.2521393943723267, accuracy: 0.9833333333333333\n",
      "iteration no 4464: Loss: 0.25213754227167595, accuracy: 0.9833333333333333\n",
      "iteration no 4465: Loss: 0.25213352865841354, accuracy: 0.9866666666666667\n",
      "iteration no 4466: Loss: 0.25213267275154605, accuracy: 0.9866666666666667\n",
      "iteration no 4467: Loss: 0.2521301359993827, accuracy: 0.9833333333333333\n",
      "iteration no 4468: Loss: 0.25212745014954463, accuracy: 0.9866666666666667\n",
      "iteration no 4469: Loss: 0.2521236350102841, accuracy: 0.9866666666666667\n",
      "iteration no 4470: Loss: 0.25212308489924484, accuracy: 0.9833333333333333\n",
      "iteration no 4471: Loss: 0.25212029835645244, accuracy: 0.9833333333333333\n",
      "iteration no 4472: Loss: 0.2521173458862234, accuracy: 0.9866666666666667\n",
      "iteration no 4473: Loss: 0.25211421947688695, accuracy: 0.9866666666666667\n",
      "iteration no 4474: Loss: 0.25211392824592543, accuracy: 0.9833333333333333\n",
      "iteration no 4475: Loss: 0.2521104050642051, accuracy: 0.9866666666666667\n",
      "iteration no 4476: Loss: 0.25210730974134354, accuracy: 0.9833333333333333\n",
      "iteration no 4477: Loss: 0.2521050805584455, accuracy: 0.9866666666666667\n",
      "iteration no 4478: Loss: 0.2521040555502616, accuracy: 0.9833333333333333\n",
      "iteration no 4479: Loss: 0.2521005894364677, accuracy: 0.9866666666666667\n",
      "iteration no 4480: Loss: 0.2520970369499779, accuracy: 0.9866666666666667\n",
      "iteration no 4481: Loss: 0.25209680466924533, accuracy: 0.9866666666666667\n",
      "iteration no 4482: Loss: 0.2520935924375028, accuracy: 0.9833333333333333\n",
      "iteration no 4483: Loss: 0.2520909004667794, accuracy: 0.9833333333333333\n",
      "iteration no 4484: Loss: 0.2520870781836957, accuracy: 0.9866666666666667\n",
      "iteration no 4485: Loss: 0.25208756402321, accuracy: 0.9866666666666667\n",
      "iteration no 4486: Loss: 0.2520837702386674, accuracy: 0.9833333333333333\n",
      "iteration no 4487: Loss: 0.2520805895006873, accuracy: 0.9866666666666667\n",
      "iteration no 4488: Loss: 0.2520786861355381, accuracy: 0.9866666666666667\n",
      "iteration no 4489: Loss: 0.25207808621686867, accuracy: 0.9833333333333333\n",
      "iteration no 4490: Loss: 0.2520743717580506, accuracy: 0.9833333333333333\n",
      "iteration no 4491: Loss: 0.2520712414180635, accuracy: 0.9866666666666667\n",
      "iteration no 4492: Loss: 0.25206938296429493, accuracy: 0.9833333333333333\n",
      "iteration no 4493: Loss: 0.25206790406719487, accuracy: 0.9866666666666667\n",
      "iteration no 4494: Loss: 0.2520645474816409, accuracy: 0.9866666666666667\n",
      "iteration no 4495: Loss: 0.2520623954234985, accuracy: 0.9866666666666667\n",
      "iteration no 4496: Loss: 0.25206004085573364, accuracy: 0.9866666666666667\n",
      "iteration no 4497: Loss: 0.2520583172351232, accuracy: 0.9866666666666667\n",
      "iteration no 4498: Loss: 0.25205560504764785, accuracy: 0.9833333333333333\n",
      "iteration no 4499: Loss: 0.25205261093645814, accuracy: 0.9866666666666667\n",
      "iteration no 4500: Loss: 0.25205017378725647, accuracy: 0.9833333333333333\n",
      "iteration no 4501: Loss: 0.252048375494247, accuracy: 0.9866666666666667\n",
      "iteration no 4502: Loss: 0.25204647444452266, accuracy: 0.9866666666666667\n",
      "iteration no 4503: Loss: 0.2520428283275671, accuracy: 0.9866666666666667\n",
      "iteration no 4504: Loss: 0.2520407537834507, accuracy: 0.9833333333333333\n",
      "iteration no 4505: Loss: 0.25203958610768235, accuracy: 0.9866666666666667\n",
      "iteration no 4506: Loss: 0.25203670206037243, accuracy: 0.9833333333333333\n",
      "iteration no 4507: Loss: 0.2520329129918004, accuracy: 0.9866666666666667\n",
      "iteration no 4508: Loss: 0.25203203178676326, accuracy: 0.9833333333333333\n",
      "iteration no 4509: Loss: 0.2520303371714864, accuracy: 0.9833333333333333\n",
      "iteration no 4510: Loss: 0.2520262800192055, accuracy: 0.9866666666666667\n",
      "iteration no 4511: Loss: 0.2520241384702186, accuracy: 0.9866666666666667\n",
      "iteration no 4512: Loss: 0.2520229679908188, accuracy: 0.9833333333333333\n",
      "iteration no 4513: Loss: 0.2520202324691739, accuracy: 0.9866666666666667\n",
      "iteration no 4514: Loss: 0.25201655569334497, accuracy: 0.9866666666666667\n",
      "iteration no 4515: Loss: 0.25201542570805385, accuracy: 0.9866666666666667\n",
      "iteration no 4516: Loss: 0.2520146291345353, accuracy: 0.9833333333333333\n",
      "iteration no 4517: Loss: 0.25201030277570124, accuracy: 0.9866666666666667\n",
      "iteration no 4518: Loss: 0.2520064085683573, accuracy: 0.9866666666666667\n",
      "iteration no 4519: Loss: 0.2520076113121156, accuracy: 0.9866666666666667\n",
      "iteration no 4520: Loss: 0.2520045938339845, accuracy: 0.9833333333333333\n",
      "iteration no 4521: Loss: 0.2520000625456227, accuracy: 0.9833333333333333\n",
      "iteration no 4522: Loss: 0.2519979004245876, accuracy: 0.9866666666666667\n",
      "iteration no 4523: Loss: 0.25199864841746, accuracy: 0.9833333333333333\n",
      "iteration no 4524: Loss: 0.25199505286655616, accuracy: 0.9866666666666667\n",
      "iteration no 4525: Loss: 0.2519907307222506, accuracy: 0.9866666666666667\n",
      "iteration no 4526: Loss: 0.2519896383188929, accuracy: 0.9866666666666667\n",
      "iteration no 4527: Loss: 0.25198876507538936, accuracy: 0.9833333333333333\n",
      "iteration no 4528: Loss: 0.25198543334687284, accuracy: 0.9866666666666667\n",
      "iteration no 4529: Loss: 0.2519815359795141, accuracy: 0.9866666666666667\n",
      "iteration no 4530: Loss: 0.25198105321755504, accuracy: 0.9833333333333333\n",
      "iteration no 4531: Loss: 0.2519786070996348, accuracy: 0.9866666666666667\n",
      "iteration no 4532: Loss: 0.25197514057109105, accuracy: 0.9866666666666667\n",
      "iteration no 4533: Loss: 0.25197359786219325, accuracy: 0.9866666666666667\n",
      "iteration no 4534: Loss: 0.25197202098821164, accuracy: 0.9866666666666667\n",
      "iteration no 4535: Loss: 0.25196899699238234, accuracy: 0.9833333333333333\n",
      "iteration no 4536: Loss: 0.25196684651193885, accuracy: 0.9866666666666667\n",
      "iteration no 4537: Loss: 0.25196390548861725, accuracy: 0.9833333333333333\n",
      "iteration no 4538: Loss: 0.2519630316354234, accuracy: 0.9833333333333333\n",
      "iteration no 4539: Loss: 0.2519596167063144, accuracy: 0.9866666666666667\n",
      "iteration no 4540: Loss: 0.25195688383448567, accuracy: 0.9833333333333333\n",
      "iteration no 4541: Loss: 0.25195553592829806, accuracy: 0.9866666666666667\n",
      "iteration no 4542: Loss: 0.25195337110117055, accuracy: 0.9866666666666667\n",
      "iteration no 4543: Loss: 0.251951075706445, accuracy: 0.9866666666666667\n",
      "iteration no 4544: Loss: 0.2519477073095908, accuracy: 0.9866666666666667\n",
      "iteration no 4545: Loss: 0.25194647944099974, accuracy: 0.9866666666666667\n",
      "iteration no 4546: Loss: 0.25194440761882914, accuracy: 0.9833333333333333\n",
      "iteration no 4547: Loss: 0.25194206417024245, accuracy: 0.9866666666666667\n",
      "iteration no 4548: Loss: 0.2519376583124009, accuracy: 0.9866666666666667\n",
      "iteration no 4549: Loss: 0.251937175691613, accuracy: 0.9866666666666667\n",
      "iteration no 4550: Loss: 0.2519359429065714, accuracy: 0.9866666666666667\n",
      "iteration no 4551: Loss: 0.2519320223932866, accuracy: 0.9866666666666667\n",
      "iteration no 4552: Loss: 0.2519293190870673, accuracy: 0.9866666666666667\n",
      "iteration no 4553: Loss: 0.25192870145647805, accuracy: 0.9833333333333333\n",
      "iteration no 4554: Loss: 0.2519264625517844, accuracy: 0.9866666666666667\n",
      "iteration no 4555: Loss: 0.25192268494667297, accuracy: 0.9866666666666667\n",
      "iteration no 4556: Loss: 0.2519206517701162, accuracy: 0.9866666666666667\n",
      "iteration no 4557: Loss: 0.25192000526341807, accuracy: 0.9833333333333333\n",
      "iteration no 4558: Loss: 0.2519166554025992, accuracy: 0.9866666666666667\n",
      "iteration no 4559: Loss: 0.25191283177064683, accuracy: 0.9866666666666667\n",
      "iteration no 4560: Loss: 0.25191340098446674, accuracy: 0.9866666666666667\n",
      "iteration no 4561: Loss: 0.25191037000138006, accuracy: 0.9833333333333333\n",
      "iteration no 4562: Loss: 0.2519064645775151, accuracy: 0.9866666666666667\n",
      "iteration no 4563: Loss: 0.25190492094771355, accuracy: 0.9866666666666667\n",
      "iteration no 4564: Loss: 0.2519045080078617, accuracy: 0.9866666666666667\n",
      "iteration no 4565: Loss: 0.2519001705031857, accuracy: 0.9833333333333333\n",
      "iteration no 4566: Loss: 0.2518961660592805, accuracy: 0.9866666666666667\n",
      "iteration no 4567: Loss: 0.2518975786850488, accuracy: 0.9833333333333333\n",
      "iteration no 4568: Loss: 0.25189464223313446, accuracy: 0.9866666666666667\n",
      "iteration no 4569: Loss: 0.25189012019575396, accuracy: 0.9866666666666667\n",
      "iteration no 4570: Loss: 0.25188883467714573, accuracy: 0.9833333333333333\n",
      "iteration no 4571: Loss: 0.25188764393513075, accuracy: 0.9866666666666667\n",
      "iteration no 4572: Loss: 0.25188419864449874, accuracy: 0.9833333333333333\n",
      "iteration no 4573: Loss: 0.25188264656100223, accuracy: 0.9866666666666667\n",
      "iteration no 4574: Loss: 0.2518802444472079, accuracy: 0.9833333333333333\n",
      "iteration no 4575: Loss: 0.2518764685118092, accuracy: 0.9866666666666667\n",
      "iteration no 4576: Loss: 0.25187591345772636, accuracy: 0.9833333333333333\n",
      "iteration no 4577: Loss: 0.2518736568597474, accuracy: 0.9866666666666667\n",
      "iteration no 4578: Loss: 0.25187127511341517, accuracy: 0.9866666666666667\n",
      "iteration no 4579: Loss: 0.2518673831561118, accuracy: 0.9866666666666667\n",
      "iteration no 4580: Loss: 0.2518664998771095, accuracy: 0.9866666666666667\n",
      "iteration no 4581: Loss: 0.2518645801499053, accuracy: 0.9833333333333333\n",
      "iteration no 4582: Loss: 0.2518621310971338, accuracy: 0.9866666666666667\n",
      "iteration no 4583: Loss: 0.2518573846805633, accuracy: 0.9833333333333333\n",
      "iteration no 4584: Loss: 0.25185734785255404, accuracy: 0.9866666666666667\n",
      "iteration no 4585: Loss: 0.25185488363450553, accuracy: 0.9833333333333333\n",
      "iteration no 4586: Loss: 0.25185180241719224, accuracy: 0.9866666666666667\n",
      "iteration no 4587: Loss: 0.25184907192950556, accuracy: 0.9833333333333333\n",
      "iteration no 4588: Loss: 0.25184632888210856, accuracy: 0.9866666666666667\n",
      "iteration no 4589: Loss: 0.2518449959347541, accuracy: 0.9833333333333333\n",
      "iteration no 4590: Loss: 0.25184299059091275, accuracy: 0.9866666666666667\n",
      "iteration no 4591: Loss: 0.2518405444441557, accuracy: 0.9833333333333333\n",
      "iteration no 4592: Loss: 0.25183699960970946, accuracy: 0.9866666666666667\n",
      "iteration no 4593: Loss: 0.25183443346184353, accuracy: 0.9833333333333333\n",
      "iteration no 4594: Loss: 0.2518339755579074, accuracy: 0.9866666666666667\n",
      "iteration no 4595: Loss: 0.25183109222720373, accuracy: 0.9833333333333333\n",
      "iteration no 4596: Loss: 0.25182683891211766, accuracy: 0.9866666666666667\n",
      "iteration no 4597: Loss: 0.25182489642137584, accuracy: 0.9833333333333333\n",
      "iteration no 4598: Loss: 0.2518247994016385, accuracy: 0.9866666666666667\n",
      "iteration no 4599: Loss: 0.25182097856117824, accuracy: 0.9833333333333333\n",
      "iteration no 4600: Loss: 0.2518175500680488, accuracy: 0.9866666666666667\n",
      "iteration no 4601: Loss: 0.25181644905317874, accuracy: 0.9833333333333333\n",
      "iteration no 4602: Loss: 0.2518139757644548, accuracy: 0.9866666666666667\n",
      "iteration no 4603: Loss: 0.25181173089585296, accuracy: 0.9866666666666667\n",
      "iteration no 4604: Loss: 0.2518085764933572, accuracy: 0.9866666666666667\n",
      "iteration no 4605: Loss: 0.25180731564732317, accuracy: 0.9833333333333333\n",
      "iteration no 4606: Loss: 0.2518039441616179, accuracy: 0.9866666666666667\n",
      "iteration no 4607: Loss: 0.2518016151019711, accuracy: 0.9833333333333333\n",
      "iteration no 4608: Loss: 0.2518005146102283, accuracy: 0.9866666666666667\n",
      "iteration no 4609: Loss: 0.2517974126622152, accuracy: 0.9833333333333333\n",
      "iteration no 4610: Loss: 0.2517933011796618, accuracy: 0.9866666666666667\n",
      "iteration no 4611: Loss: 0.25179359027592957, accuracy: 0.9833333333333333\n",
      "iteration no 4612: Loss: 0.25179068079170636, accuracy: 0.9866666666666667\n",
      "iteration no 4613: Loss: 0.2517879664861837, accuracy: 0.9833333333333333\n",
      "iteration no 4614: Loss: 0.2517840292224316, accuracy: 0.9866666666666667\n",
      "iteration no 4615: Loss: 0.2517840512569173, accuracy: 0.9833333333333333\n",
      "iteration no 4616: Loss: 0.2517811234171661, accuracy: 0.9866666666666667\n",
      "iteration no 4617: Loss: 0.2517781444750299, accuracy: 0.9866666666666667\n",
      "iteration no 4618: Loss: 0.2517750543426358, accuracy: 0.9866666666666667\n",
      "iteration no 4619: Loss: 0.25177463663291466, accuracy: 0.9866666666666667\n",
      "iteration no 4620: Loss: 0.25177083134721767, accuracy: 0.9866666666666667\n",
      "iteration no 4621: Loss: 0.25176949143954075, accuracy: 0.9866666666666667\n",
      "iteration no 4622: Loss: 0.2517661351745193, accuracy: 0.9866666666666667\n",
      "iteration no 4623: Loss: 0.2517645922062477, accuracy: 0.9866666666666667\n",
      "iteration no 4624: Loss: 0.2517607653546707, accuracy: 0.9866666666666667\n",
      "iteration no 4625: Loss: 0.2517608523803707, accuracy: 0.9866666666666667\n",
      "iteration no 4626: Loss: 0.25175640054729664, accuracy: 0.9866666666666667\n",
      "iteration no 4627: Loss: 0.2517556499869658, accuracy: 0.9866666666666667\n",
      "iteration no 4628: Loss: 0.25175129449001316, accuracy: 0.9866666666666667\n",
      "iteration no 4629: Loss: 0.2517514992822276, accuracy: 0.9833333333333333\n",
      "iteration no 4630: Loss: 0.25174684852186285, accuracy: 0.9866666666666667\n",
      "iteration no 4631: Loss: 0.2517455212164639, accuracy: 0.9866666666666667\n",
      "iteration no 4632: Loss: 0.25174273933482955, accuracy: 0.9866666666666667\n",
      "iteration no 4633: Loss: 0.2517415652971982, accuracy: 0.9866666666666667\n",
      "iteration no 4634: Loss: 0.2517373721058173, accuracy: 0.9866666666666667\n",
      "iteration no 4635: Loss: 0.2517375594333745, accuracy: 0.9866666666666667\n",
      "iteration no 4636: Loss: 0.25173249037173695, accuracy: 0.9866666666666667\n",
      "iteration no 4637: Loss: 0.25173183834997914, accuracy: 0.9866666666666667\n",
      "iteration no 4638: Loss: 0.25172806114884805, accuracy: 0.9866666666666667\n",
      "iteration no 4639: Loss: 0.2517281475448333, accuracy: 0.9833333333333333\n",
      "iteration no 4640: Loss: 0.25172343002030234, accuracy: 0.9866666666666667\n",
      "iteration no 4641: Loss: 0.25172200025917185, accuracy: 0.9866666666666667\n",
      "iteration no 4642: Loss: 0.2517196002195797, accuracy: 0.9866666666666667\n",
      "iteration no 4643: Loss: 0.2517189495014021, accuracy: 0.9833333333333333\n",
      "iteration no 4644: Loss: 0.2517131298984939, accuracy: 0.9866666666666667\n",
      "iteration no 4645: Loss: 0.2517135767446243, accuracy: 0.9833333333333333\n",
      "iteration no 4646: Loss: 0.25170943256959244, accuracy: 0.9866666666666667\n",
      "iteration no 4647: Loss: 0.25170896404972765, accuracy: 0.9833333333333333\n",
      "iteration no 4648: Loss: 0.25170427086687797, accuracy: 0.9866666666666667\n",
      "iteration no 4649: Loss: 0.2517049601644823, accuracy: 0.9833333333333333\n",
      "iteration no 4650: Loss: 0.25169939364373317, accuracy: 0.9866666666666667\n",
      "iteration no 4651: Loss: 0.2516988743461188, accuracy: 0.9866666666666667\n",
      "iteration no 4652: Loss: 0.25169529501735444, accuracy: 0.9866666666666667\n",
      "iteration no 4653: Loss: 0.2516956296340432, accuracy: 0.9866666666666667\n",
      "iteration no 4654: Loss: 0.2516891076303083, accuracy: 0.9866666666666667\n",
      "iteration no 4655: Loss: 0.2516895202061617, accuracy: 0.9866666666666667\n",
      "iteration no 4656: Loss: 0.2516871950875826, accuracy: 0.9866666666666667\n",
      "iteration no 4657: Loss: 0.2516854081209915, accuracy: 0.9866666666666667\n",
      "iteration no 4658: Loss: 0.2516800605719758, accuracy: 0.9866666666666667\n",
      "iteration no 4659: Loss: 0.2516803354717492, accuracy: 0.9866666666666667\n",
      "iteration no 4660: Loss: 0.25167759346753216, accuracy: 0.9866666666666667\n",
      "iteration no 4661: Loss: 0.2516760029483094, accuracy: 0.9866666666666667\n",
      "iteration no 4662: Loss: 0.2516706985144245, accuracy: 0.9833333333333333\n",
      "iteration no 4663: Loss: 0.2516713278790753, accuracy: 0.9866666666666667\n",
      "iteration no 4664: Loss: 0.25166843584829757, accuracy: 0.9833333333333333\n",
      "iteration no 4665: Loss: 0.25166603651101344, accuracy: 0.9866666666666667\n",
      "iteration no 4666: Loss: 0.2516619250152301, accuracy: 0.9866666666666667\n",
      "iteration no 4667: Loss: 0.25166136265252637, accuracy: 0.9866666666666667\n",
      "iteration no 4668: Loss: 0.25165864099844487, accuracy: 0.9866666666666667\n",
      "iteration no 4669: Loss: 0.2516569369476357, accuracy: 0.9866666666666667\n",
      "iteration no 4670: Loss: 0.25165317054512704, accuracy: 0.9866666666666667\n",
      "iteration no 4671: Loss: 0.25165247209890546, accuracy: 0.9866666666666667\n",
      "iteration no 4672: Loss: 0.2516490945431641, accuracy: 0.9833333333333333\n",
      "iteration no 4673: Loss: 0.2516472453405887, accuracy: 0.9866666666666667\n",
      "iteration no 4674: Loss: 0.25164510194767387, accuracy: 0.9866666666666667\n",
      "iteration no 4675: Loss: 0.25164246010359037, accuracy: 0.9866666666666667\n",
      "iteration no 4676: Loss: 0.25163961290094644, accuracy: 0.9866666666666667\n",
      "iteration no 4677: Loss: 0.2516383107938041, accuracy: 0.9866666666666667\n",
      "iteration no 4678: Loss: 0.25163586676639343, accuracy: 0.9833333333333333\n",
      "iteration no 4679: Loss: 0.2516330690552226, accuracy: 0.9866666666666667\n",
      "iteration no 4680: Loss: 0.25162865873308043, accuracy: 0.9833333333333333\n",
      "iteration no 4681: Loss: 0.25162777757477717, accuracy: 0.9866666666666667\n",
      "iteration no 4682: Loss: 0.2516257856416597, accuracy: 0.9866666666666667\n",
      "iteration no 4683: Loss: 0.2516218251225887, accuracy: 0.9866666666666667\n",
      "iteration no 4684: Loss: 0.25161910911625, accuracy: 0.9866666666666667\n",
      "iteration no 4685: Loss: 0.25161651200382007, accuracy: 0.9866666666666667\n",
      "iteration no 4686: Loss: 0.25161621813127466, accuracy: 0.9866666666666667\n",
      "iteration no 4687: Loss: 0.25161113278339114, accuracy: 0.9866666666666667\n",
      "iteration no 4688: Loss: 0.25160921720224116, accuracy: 0.9866666666666667\n",
      "iteration no 4689: Loss: 0.25160607838248383, accuracy: 0.9866666666666667\n",
      "iteration no 4690: Loss: 0.2516062715205063, accuracy: 0.9866666666666667\n",
      "iteration no 4691: Loss: 0.25160082923426297, accuracy: 0.9866666666666667\n",
      "iteration no 4692: Loss: 0.2515995849630796, accuracy: 0.9866666666666667\n",
      "iteration no 4693: Loss: 0.2515958671840221, accuracy: 0.9866666666666667\n",
      "iteration no 4694: Loss: 0.25159582021410926, accuracy: 0.9866666666666667\n",
      "iteration no 4695: Loss: 0.2515911907207778, accuracy: 0.9866666666666667\n",
      "iteration no 4696: Loss: 0.2515894497479204, accuracy: 0.9866666666666667\n",
      "iteration no 4697: Loss: 0.25158638947760353, accuracy: 0.9866666666666667\n",
      "iteration no 4698: Loss: 0.2515848987391832, accuracy: 0.9866666666666667\n",
      "iteration no 4699: Loss: 0.2515823345219661, accuracy: 0.9866666666666667\n",
      "iteration no 4700: Loss: 0.25157859791113657, accuracy: 0.9866666666666667\n",
      "iteration no 4701: Loss: 0.25157743829076445, accuracy: 0.9866666666666667\n",
      "iteration no 4702: Loss: 0.25157323475402243, accuracy: 0.9866666666666667\n",
      "iteration no 4703: Loss: 0.25157353187294296, accuracy: 0.9866666666666667\n",
      "iteration no 4704: Loss: 0.2515683577582417, accuracy: 0.9866666666666667\n",
      "iteration no 4705: Loss: 0.251567842379226, accuracy: 0.9866666666666667\n",
      "iteration no 4706: Loss: 0.25156388019057385, accuracy: 0.9866666666666667\n",
      "iteration no 4707: Loss: 0.2515630382350885, accuracy: 0.9866666666666667\n",
      "iteration no 4708: Loss: 0.2515601410315892, accuracy: 0.9866666666666667\n",
      "iteration no 4709: Loss: 0.2515567036830618, accuracy: 0.9866666666666667\n",
      "iteration no 4710: Loss: 0.25155484505453013, accuracy: 0.9866666666666667\n",
      "iteration no 4711: Loss: 0.2515522782770722, accuracy: 0.9866666666666667\n",
      "iteration no 4712: Loss: 0.2515512137412338, accuracy: 0.9866666666666667\n",
      "iteration no 4713: Loss: 0.251545054713497, accuracy: 0.9866666666666667\n",
      "iteration no 4714: Loss: 0.2515430488774093, accuracy: 0.9866666666666667\n",
      "iteration no 4715: Loss: 0.25153975041746, accuracy: 0.9866666666666667\n",
      "iteration no 4716: Loss: 0.2515368597585241, accuracy: 0.9866666666666667\n",
      "iteration no 4717: Loss: 0.2515331459099904, accuracy: 0.9866666666666667\n",
      "iteration no 4718: Loss: 0.2515284347737442, accuracy: 0.9866666666666667\n",
      "iteration no 4719: Loss: 0.2515267029846428, accuracy: 0.9866666666666667\n",
      "iteration no 4720: Loss: 0.2515227177375664, accuracy: 0.9866666666666667\n",
      "iteration no 4721: Loss: 0.2515212076393856, accuracy: 0.9866666666666667\n",
      "iteration no 4722: Loss: 0.2515167057504383, accuracy: 0.9866666666666667\n",
      "iteration no 4723: Loss: 0.2515127264913305, accuracy: 0.9866666666666667\n",
      "iteration no 4724: Loss: 0.2515107728992427, accuracy: 0.9866666666666667\n",
      "iteration no 4725: Loss: 0.25150692185973517, accuracy: 0.9866666666666667\n",
      "iteration no 4726: Loss: 0.25150576824910503, accuracy: 0.9866666666666667\n",
      "iteration no 4727: Loss: 0.25150012828238055, accuracy: 0.9866666666666667\n",
      "iteration no 4728: Loss: 0.2514981339047877, accuracy: 0.9866666666666667\n",
      "iteration no 4729: Loss: 0.25149546078660523, accuracy: 0.9866666666666667\n",
      "iteration no 4730: Loss: 0.2514911714866205, accuracy: 0.9866666666666667\n",
      "iteration no 4731: Loss: 0.25148914444763415, accuracy: 0.9866666666666667\n",
      "iteration no 4732: Loss: 0.25148625585268836, accuracy: 0.9866666666666667\n",
      "iteration no 4733: Loss: 0.2514831344059711, accuracy: 0.9866666666666667\n",
      "iteration no 4734: Loss: 0.2514804538947996, accuracy: 0.9866666666666667\n",
      "iteration no 4735: Loss: 0.251474915789373, accuracy: 0.9866666666666667\n",
      "iteration no 4736: Loss: 0.25147529385527545, accuracy: 0.9866666666666667\n",
      "iteration no 4737: Loss: 0.2514721094045771, accuracy: 0.9866666666666667\n",
      "iteration no 4738: Loss: 0.25146701818794, accuracy: 0.9866666666666667\n",
      "iteration no 4739: Loss: 0.25146566294972844, accuracy: 0.9866666666666667\n",
      "iteration no 4740: Loss: 0.2514650664791551, accuracy: 0.9866666666666667\n",
      "iteration no 4741: Loss: 0.25146335424308003, accuracy: 0.9866666666666667\n",
      "iteration no 4742: Loss: 0.2514601137569854, accuracy: 0.9866666666666667\n",
      "iteration no 4743: Loss: 0.2514557222016456, accuracy: 0.9866666666666667\n",
      "iteration no 4744: Loss: 0.25145825327138704, accuracy: 0.9866666666666667\n",
      "iteration no 4745: Loss: 0.25145430587143, accuracy: 0.9866666666666667\n",
      "iteration no 4746: Loss: 0.25145061861548185, accuracy: 0.9866666666666667\n",
      "iteration no 4747: Loss: 0.2514492166273469, accuracy: 0.9866666666666667\n",
      "iteration no 4748: Loss: 0.25144812642001474, accuracy: 0.9866666666666667\n",
      "iteration no 4749: Loss: 0.2514478453510902, accuracy: 0.9866666666666667\n",
      "iteration no 4750: Loss: 0.25144244289592765, accuracy: 0.9866666666666667\n",
      "iteration no 4751: Loss: 0.25143991317059716, accuracy: 0.9866666666666667\n",
      "iteration no 4752: Loss: 0.25144279712317075, accuracy: 0.9866666666666667\n",
      "iteration no 4753: Loss: 0.25143712198091245, accuracy: 0.9866666666666667\n",
      "iteration no 4754: Loss: 0.25143587677339846, accuracy: 0.9866666666666667\n",
      "iteration no 4755: Loss: 0.25143231069885297, accuracy: 0.9866666666666667\n",
      "iteration no 4756: Loss: 0.25143356820086044, accuracy: 0.9866666666666667\n",
      "iteration no 4757: Loss: 0.2514310868462994, accuracy: 0.9866666666666667\n",
      "iteration no 4758: Loss: 0.25142554138031453, accuracy: 0.9866666666666667\n",
      "iteration no 4759: Loss: 0.25142763202190876, accuracy: 0.9866666666666667\n",
      "iteration no 4760: Loss: 0.2514237348767714, accuracy: 0.9866666666666667\n",
      "iteration no 4761: Loss: 0.25142310815583296, accuracy: 0.9866666666666667\n",
      "iteration no 4762: Loss: 0.2514188653433517, accuracy: 0.9866666666666667\n",
      "iteration no 4763: Loss: 0.2514185388618661, accuracy: 0.9866666666666667\n",
      "iteration no 4764: Loss: 0.2514170799712824, accuracy: 0.9866666666666667\n",
      "iteration no 4765: Loss: 0.25141288006738843, accuracy: 0.9866666666666667\n",
      "iteration no 4766: Loss: 0.2514141749164481, accuracy: 0.9866666666666667\n",
      "iteration no 4767: Loss: 0.2514110739707179, accuracy: 0.9866666666666667\n",
      "iteration no 4768: Loss: 0.2514080059233711, accuracy: 0.9866666666666667\n",
      "iteration no 4769: Loss: 0.251405961682697, accuracy: 0.9866666666666667\n",
      "iteration no 4770: Loss: 0.251405207570474, accuracy: 0.9866666666666667\n",
      "iteration no 4771: Loss: 0.2514042442036257, accuracy: 0.9866666666666667\n",
      "iteration no 4772: Loss: 0.2513984652535618, accuracy: 0.9866666666666667\n",
      "iteration no 4773: Loss: 0.2514002152939581, accuracy: 0.9866666666666667\n",
      "iteration no 4774: Loss: 0.2513988576654711, accuracy: 0.9866666666666667\n",
      "iteration no 4775: Loss: 0.2513942155554991, accuracy: 0.9866666666666667\n",
      "iteration no 4776: Loss: 0.251393458047141, accuracy: 0.9866666666666667\n",
      "iteration no 4777: Loss: 0.2513905068412487, accuracy: 0.9866666666666667\n",
      "iteration no 4778: Loss: 0.2513915010960832, accuracy: 0.9866666666666667\n",
      "iteration no 4779: Loss: 0.25138568463833944, accuracy: 0.9866666666666667\n",
      "iteration no 4780: Loss: 0.2513871441062614, accuracy: 0.9866666666666667\n",
      "iteration no 4781: Loss: 0.25138445183247443, accuracy: 0.9866666666666667\n",
      "iteration no 4782: Loss: 0.2513804903186889, accuracy: 0.9866666666666667\n",
      "iteration no 4783: Loss: 0.2513813720589267, accuracy: 0.9866666666666667\n",
      "iteration no 4784: Loss: 0.2513775727397558, accuracy: 0.9866666666666667\n",
      "iteration no 4785: Loss: 0.25137674477213057, accuracy: 0.9866666666666667\n",
      "iteration no 4786: Loss: 0.2513736671814516, accuracy: 0.9866666666666667\n",
      "iteration no 4787: Loss: 0.2513737480170723, accuracy: 0.9866666666666667\n",
      "iteration no 4788: Loss: 0.2513703656194072, accuracy: 0.9866666666666667\n",
      "iteration no 4789: Loss: 0.2513673909396012, accuracy: 0.9866666666666667\n",
      "iteration no 4790: Loss: 0.2513691085819218, accuracy: 0.9866666666666667\n",
      "iteration no 4791: Loss: 0.25136395578405735, accuracy: 0.9866666666666667\n",
      "iteration no 4792: Loss: 0.25136259513832215, accuracy: 0.9866666666666667\n",
      "iteration no 4793: Loss: 0.25136230206220633, accuracy: 0.9866666666666667\n",
      "iteration no 4794: Loss: 0.251359890514407, accuracy: 0.9866666666666667\n",
      "iteration no 4795: Loss: 0.2513567283867421, accuracy: 0.9866666666666667\n",
      "iteration no 4796: Loss: 0.25135542239004477, accuracy: 0.9866666666666667\n",
      "iteration no 4797: Loss: 0.25135585020388246, accuracy: 0.9866666666666667\n",
      "iteration no 4798: Loss: 0.2513500831697262, accuracy: 0.9866666666666667\n",
      "iteration no 4799: Loss: 0.25135112530952025, accuracy: 0.9866666666666667\n",
      "iteration no 4800: Loss: 0.25134777709331274, accuracy: 0.9866666666666667\n",
      "iteration no 4801: Loss: 0.25134735620047327, accuracy: 0.9866666666666667\n",
      "iteration no 4802: Loss: 0.2513426047966378, accuracy: 0.9866666666666667\n",
      "iteration no 4803: Loss: 0.25134473573348964, accuracy: 0.9866666666666667\n",
      "iteration no 4804: Loss: 0.2513405691216154, accuracy: 0.9866666666666667\n",
      "iteration no 4805: Loss: 0.2513368660409784, accuracy: 0.9866666666666667\n",
      "iteration no 4806: Loss: 0.25133981395677724, accuracy: 0.9866666666666667\n",
      "iteration no 4807: Loss: 0.2513344497556694, accuracy: 0.9866666666666667\n",
      "iteration no 4808: Loss: 0.2513327101319903, accuracy: 0.9866666666666667\n",
      "iteration no 4809: Loss: 0.2513324942182837, accuracy: 0.9866666666666667\n",
      "iteration no 4810: Loss: 0.25133067418761507, accuracy: 0.9866666666666667\n",
      "iteration no 4811: Loss: 0.2513271966208743, accuracy: 0.9866666666666667\n",
      "iteration no 4812: Loss: 0.2513263131526768, accuracy: 0.9866666666666667\n",
      "iteration no 4813: Loss: 0.25132557145108725, accuracy: 0.9866666666666667\n",
      "iteration no 4814: Loss: 0.25132124519184973, accuracy: 0.9866666666666667\n",
      "iteration no 4815: Loss: 0.2513217157378519, accuracy: 0.9866666666666667\n",
      "iteration no 4816: Loss: 0.2513186212374765, accuracy: 0.9866666666666667\n",
      "iteration no 4817: Loss: 0.2513174972851309, accuracy: 0.9866666666666667\n",
      "iteration no 4818: Loss: 0.2513150982107, accuracy: 0.9866666666666667\n",
      "iteration no 4819: Loss: 0.25131459116610866, accuracy: 0.9866666666666667\n",
      "iteration no 4820: Loss: 0.2513118100167237, accuracy: 0.9866666666666667\n",
      "iteration no 4821: Loss: 0.2513084808295314, accuracy: 0.9866666666666667\n",
      "iteration no 4822: Loss: 0.25131016968801934, accuracy: 0.9866666666666667\n",
      "iteration no 4823: Loss: 0.25130658143619217, accuracy: 0.9866666666666667\n",
      "iteration no 4824: Loss: 0.2513038596720607, accuracy: 0.9866666666666667\n",
      "iteration no 4825: Loss: 0.25130479036564113, accuracy: 0.9833333333333333\n",
      "iteration no 4826: Loss: 0.2513010053293687, accuracy: 0.9866666666666667\n",
      "iteration no 4827: Loss: 0.2512991799520656, accuracy: 0.9866666666666667\n",
      "iteration no 4828: Loss: 0.25130023126980344, accuracy: 0.9866666666666667\n",
      "iteration no 4829: Loss: 0.2512935456310956, accuracy: 0.9866666666666667\n",
      "iteration no 4830: Loss: 0.251294776947502, accuracy: 0.9866666666666667\n",
      "iteration no 4831: Loss: 0.2512952901819805, accuracy: 0.9833333333333333\n",
      "iteration no 4832: Loss: 0.2512901414250299, accuracy: 0.9866666666666667\n",
      "iteration no 4833: Loss: 0.2512881234916954, accuracy: 0.9866666666666667\n",
      "iteration no 4834: Loss: 0.25128866727624494, accuracy: 0.9866666666666667\n",
      "iteration no 4835: Loss: 0.2512860331150244, accuracy: 0.9866666666666667\n",
      "iteration no 4836: Loss: 0.251283809917431, accuracy: 0.9866666666666667\n",
      "iteration no 4837: Loss: 0.25128167854222405, accuracy: 0.9866666666666667\n",
      "iteration no 4838: Loss: 0.25128066853581377, accuracy: 0.9866666666666667\n",
      "iteration no 4839: Loss: 0.251279509086825, accuracy: 0.9866666666666667\n",
      "iteration no 4840: Loss: 0.25127733859774953, accuracy: 0.9866666666666667\n",
      "iteration no 4841: Loss: 0.25127445495355616, accuracy: 0.9866666666666667\n",
      "iteration no 4842: Loss: 0.25127391421258205, accuracy: 0.9866666666666667\n",
      "iteration no 4843: Loss: 0.25127262360765423, accuracy: 0.9866666666666667\n",
      "iteration no 4844: Loss: 0.25126930079465803, accuracy: 0.9866666666666667\n",
      "iteration no 4845: Loss: 0.25126779357040485, accuracy: 0.9866666666666667\n",
      "iteration no 4846: Loss: 0.2512665104931626, accuracy: 0.9866666666666667\n",
      "iteration no 4847: Loss: 0.2512643947167357, accuracy: 0.9866666666666667\n",
      "iteration no 4848: Loss: 0.25126257985276773, accuracy: 0.9866666666666667\n",
      "iteration no 4849: Loss: 0.2512612724300926, accuracy: 0.9866666666666667\n",
      "iteration no 4850: Loss: 0.2512582466874086, accuracy: 0.9866666666666667\n",
      "iteration no 4851: Loss: 0.25125773245178273, accuracy: 0.9866666666666667\n",
      "iteration no 4852: Loss: 0.25125535271401356, accuracy: 0.9866666666666667\n",
      "iteration no 4853: Loss: 0.25125397533120253, accuracy: 0.9866666666666667\n",
      "iteration no 4854: Loss: 0.2512518296674891, accuracy: 0.9866666666666667\n",
      "iteration no 4855: Loss: 0.2512494527762332, accuracy: 0.9866666666666667\n",
      "iteration no 4856: Loss: 0.2512478913490739, accuracy: 0.9866666666666667\n",
      "iteration no 4857: Loss: 0.2512477660391942, accuracy: 0.9866666666666667\n",
      "iteration no 4858: Loss: 0.25124397181027885, accuracy: 0.9866666666666667\n",
      "iteration no 4859: Loss: 0.2512416677584933, accuracy: 0.9866666666666667\n",
      "iteration no 4860: Loss: 0.25124242998928836, accuracy: 0.9866666666666667\n",
      "iteration no 4861: Loss: 0.2512398091071185, accuracy: 0.9866666666666667\n",
      "iteration no 4862: Loss: 0.25123639030812567, accuracy: 0.9866666666666667\n",
      "iteration no 4863: Loss: 0.25123585925812514, accuracy: 0.9866666666666667\n",
      "iteration no 4864: Loss: 0.2512345292572042, accuracy: 0.9866666666666667\n",
      "iteration no 4865: Loss: 0.2512321921027466, accuracy: 0.9866666666666667\n",
      "iteration no 4866: Loss: 0.2512309878923521, accuracy: 0.9866666666666667\n",
      "iteration no 4867: Loss: 0.25122676724043946, accuracy: 0.9866666666666667\n",
      "iteration no 4868: Loss: 0.2512271912870794, accuracy: 0.9866666666666667\n",
      "iteration no 4869: Loss: 0.25122710873764326, accuracy: 0.9866666666666667\n",
      "iteration no 4870: Loss: 0.25122183989086, accuracy: 0.9866666666666667\n",
      "iteration no 4871: Loss: 0.25122099630635336, accuracy: 0.9866666666666667\n",
      "iteration no 4872: Loss: 0.251222666332972, accuracy: 0.9866666666666667\n",
      "iteration no 4873: Loss: 0.25121721172031297, accuracy: 0.9866666666666667\n",
      "iteration no 4874: Loss: 0.25121568677997863, accuracy: 0.9866666666666667\n",
      "iteration no 4875: Loss: 0.2512160883877072, accuracy: 0.9866666666666667\n",
      "iteration no 4876: Loss: 0.25121161351375826, accuracy: 0.9866666666666667\n",
      "iteration no 4877: Loss: 0.25121162673886877, accuracy: 0.9866666666666667\n",
      "iteration no 4878: Loss: 0.2512095925227702, accuracy: 0.9866666666666667\n",
      "iteration no 4879: Loss: 0.2512072677596241, accuracy: 0.9866666666666667\n",
      "iteration no 4880: Loss: 0.2512068795945552, accuracy: 0.9866666666666667\n",
      "iteration no 4881: Loss: 0.2512037489500702, accuracy: 0.9866666666666667\n",
      "iteration no 4882: Loss: 0.2512014955047841, accuracy: 0.9866666666666667\n",
      "iteration no 4883: Loss: 0.25120334873207056, accuracy: 0.9866666666666667\n",
      "iteration no 4884: Loss: 0.25119833075477294, accuracy: 0.9866666666666667\n",
      "iteration no 4885: Loss: 0.25119475328877966, accuracy: 0.9866666666666667\n",
      "iteration no 4886: Loss: 0.2511982833266065, accuracy: 0.9866666666666667\n",
      "iteration no 4887: Loss: 0.2511938932101693, accuracy: 0.9866666666666667\n",
      "iteration no 4888: Loss: 0.25119068269727113, accuracy: 0.9866666666666667\n",
      "iteration no 4889: Loss: 0.25119131121510485, accuracy: 0.9866666666666667\n",
      "iteration no 4890: Loss: 0.2511883440853173, accuracy: 0.9866666666666667\n",
      "iteration no 4891: Loss: 0.25118798869585723, accuracy: 0.9866666666666667\n",
      "iteration no 4892: Loss: 0.25118593544964685, accuracy: 0.9866666666666667\n",
      "iteration no 4893: Loss: 0.25118124699271216, accuracy: 0.9866666666666667\n",
      "iteration no 4894: Loss: 0.25118379224088466, accuracy: 0.9866666666666667\n",
      "iteration no 4895: Loss: 0.2511805792189583, accuracy: 0.9866666666666667\n",
      "iteration no 4896: Loss: 0.2511758222017274, accuracy: 0.9866666666666667\n",
      "iteration no 4897: Loss: 0.25117856698617497, accuracy: 0.9866666666666667\n",
      "iteration no 4898: Loss: 0.25117487890577095, accuracy: 0.9866666666666667\n",
      "iteration no 4899: Loss: 0.25117147835382964, accuracy: 0.9866666666666667\n",
      "iteration no 4900: Loss: 0.25117409392408896, accuracy: 0.9866666666666667\n",
      "iteration no 4901: Loss: 0.2511685716790257, accuracy: 0.9866666666666667\n",
      "iteration no 4902: Loss: 0.25116750063606774, accuracy: 0.9866666666666667\n",
      "iteration no 4903: Loss: 0.2511677625806954, accuracy: 0.9866666666666667\n",
      "iteration no 4904: Loss: 0.2511634586510747, accuracy: 0.9866666666666667\n",
      "iteration no 4905: Loss: 0.25116427702439714, accuracy: 0.9866666666666667\n",
      "iteration no 4906: Loss: 0.2511613939533629, accuracy: 0.9866666666666667\n",
      "iteration no 4907: Loss: 0.25115714633511765, accuracy: 0.9866666666666667\n",
      "iteration no 4908: Loss: 0.25116089131366826, accuracy: 0.9866666666666667\n",
      "iteration no 4909: Loss: 0.25115605717069134, accuracy: 0.9866666666666667\n",
      "iteration no 4910: Loss: 0.2511520539376239, accuracy: 0.9866666666666667\n",
      "iteration no 4911: Loss: 0.25115670066198376, accuracy: 0.9866666666666667\n",
      "iteration no 4912: Loss: 0.25115056700174637, accuracy: 0.9866666666666667\n",
      "iteration no 4913: Loss: 0.25114838689977503, accuracy: 0.9866666666666667\n",
      "iteration no 4914: Loss: 0.2511498869105805, accuracy: 0.9866666666666667\n",
      "iteration no 4915: Loss: 0.25114446984864025, accuracy: 0.9866666666666667\n",
      "iteration no 4916: Loss: 0.251145016337154, accuracy: 0.9866666666666667\n",
      "iteration no 4917: Loss: 0.25114318635372634, accuracy: 0.9866666666666667\n",
      "iteration no 4918: Loss: 0.2511399560898181, accuracy: 0.9866666666666667\n",
      "iteration no 4919: Loss: 0.2511416752193316, accuracy: 0.9866666666666667\n",
      "iteration no 4920: Loss: 0.2511362347549378, accuracy: 0.9866666666666667\n",
      "iteration no 4921: Loss: 0.25113579300604527, accuracy: 0.9866666666666667\n",
      "iteration no 4922: Loss: 0.2511370112726634, accuracy: 0.9866666666666667\n",
      "iteration no 4923: Loss: 0.25113083074698805, accuracy: 0.9866666666666667\n",
      "iteration no 4924: Loss: 0.25113174265721905, accuracy: 0.9866666666666667\n",
      "iteration no 4925: Loss: 0.2511309670842104, accuracy: 0.9866666666666667\n",
      "iteration no 4926: Loss: 0.2511259065404776, accuracy: 0.9866666666666667\n",
      "iteration no 4927: Loss: 0.25112741441167574, accuracy: 0.9866666666666667\n",
      "iteration no 4928: Loss: 0.2511248886885817, accuracy: 0.9866666666666667\n",
      "iteration no 4929: Loss: 0.25112112528631236, accuracy: 0.9866666666666667\n",
      "iteration no 4930: Loss: 0.2511229561209939, accuracy: 0.9866666666666667\n",
      "iteration no 4931: Loss: 0.25111908058396437, accuracy: 0.9866666666666667\n",
      "iteration no 4932: Loss: 0.25111786442885986, accuracy: 0.9866666666666667\n",
      "iteration no 4933: Loss: 0.25111700168854256, accuracy: 0.9866666666666667\n",
      "iteration no 4934: Loss: 0.25111284369128495, accuracy: 0.9866666666666667\n",
      "iteration no 4935: Loss: 0.25111518797374754, accuracy: 0.9866666666666667\n",
      "iteration no 4936: Loss: 0.2511113338289587, accuracy: 0.9866666666666667\n",
      "iteration no 4937: Loss: 0.2511087422888302, accuracy: 0.9866666666666667\n",
      "iteration no 4938: Loss: 0.2511089654567986, accuracy: 0.9866666666666667\n",
      "iteration no 4939: Loss: 0.2511056566458573, accuracy: 0.9866666666666667\n",
      "iteration no 4940: Loss: 0.2511055060485327, accuracy: 0.9866666666666667\n",
      "iteration no 4941: Loss: 0.25110338396814513, accuracy: 0.9866666666666667\n",
      "iteration no 4942: Loss: 0.25110114953861645, accuracy: 0.9866666666666667\n",
      "iteration no 4943: Loss: 0.25110023208095056, accuracy: 0.9866666666666667\n",
      "iteration no 4944: Loss: 0.2510978461062794, accuracy: 0.9866666666666667\n",
      "iteration no 4945: Loss: 0.251097019290877, accuracy: 0.9866666666666667\n",
      "iteration no 4946: Loss: 0.25109570946652593, accuracy: 0.9866666666666667\n",
      "iteration no 4947: Loss: 0.25109185521570804, accuracy: 0.9866666666666667\n",
      "iteration no 4948: Loss: 0.251092912729924, accuracy: 0.9866666666666667\n",
      "iteration no 4949: Loss: 0.25109096349811555, accuracy: 0.9866666666666667\n",
      "iteration no 4950: Loss: 0.2510882372768902, accuracy: 0.9866666666666667\n",
      "iteration no 4951: Loss: 0.25108613575071864, accuracy: 0.9866666666666667\n",
      "iteration no 4952: Loss: 0.2510852749660709, accuracy: 0.9866666666666667\n",
      "iteration no 4953: Loss: 0.2510852106950166, accuracy: 0.9866666666666667\n",
      "iteration no 4954: Loss: 0.25108097553062403, accuracy: 0.9866666666666667\n",
      "iteration no 4955: Loss: 0.25108079836670305, accuracy: 0.9866666666666667\n",
      "iteration no 4956: Loss: 0.25108003178155913, accuracy: 0.9866666666666667\n",
      "iteration no 4957: Loss: 0.2510756055597382, accuracy: 0.9866666666666667\n",
      "iteration no 4958: Loss: 0.25107710036333397, accuracy: 0.9866666666666667\n",
      "iteration no 4959: Loss: 0.251074529911435, accuracy: 0.9866666666666667\n",
      "iteration no 4960: Loss: 0.2510714669534809, accuracy: 0.9866666666666667\n",
      "iteration no 4961: Loss: 0.25107189859682033, accuracy: 0.9866666666666667\n",
      "iteration no 4962: Loss: 0.25106884842592186, accuracy: 0.9866666666666667\n",
      "iteration no 4963: Loss: 0.2510686234583861, accuracy: 0.9866666666666667\n",
      "iteration no 4964: Loss: 0.25106513982993195, accuracy: 0.9866666666666667\n",
      "iteration no 4965: Loss: 0.25106385793476793, accuracy: 0.9866666666666667\n",
      "iteration no 4966: Loss: 0.25106473958143033, accuracy: 0.9866666666666667\n",
      "iteration no 4967: Loss: 0.2510600139152829, accuracy: 0.9866666666666667\n",
      "iteration no 4968: Loss: 0.2510606567920728, accuracy: 0.9866666666666667\n",
      "iteration no 4969: Loss: 0.2510596109968636, accuracy: 0.9866666666666667\n",
      "iteration no 4970: Loss: 0.25105407749366276, accuracy: 0.9866666666666667\n",
      "iteration no 4971: Loss: 0.25105653365591163, accuracy: 0.9866666666666667\n",
      "iteration no 4972: Loss: 0.25105405296420763, accuracy: 0.9866666666666667\n",
      "iteration no 4973: Loss: 0.25105038137715974, accuracy: 0.9866666666666667\n",
      "iteration no 4974: Loss: 0.2510514231312465, accuracy: 0.9866666666666667\n",
      "iteration no 4975: Loss: 0.25104746476793177, accuracy: 0.9866666666666667\n",
      "iteration no 4976: Loss: 0.2510482024553604, accuracy: 0.9866666666666667\n",
      "iteration no 4977: Loss: 0.2510455292761168, accuracy: 0.9866666666666667\n",
      "iteration no 4978: Loss: 0.25104221260452325, accuracy: 0.9866666666666667\n",
      "iteration no 4979: Loss: 0.25104533139435226, accuracy: 0.9866666666666667\n",
      "iteration no 4980: Loss: 0.25103851652199405, accuracy: 0.9866666666666667\n",
      "iteration no 4981: Loss: 0.25103990240126134, accuracy: 0.9866666666666667\n",
      "iteration no 4982: Loss: 0.251039059524582, accuracy: 0.9866666666666667\n",
      "iteration no 4983: Loss: 0.25103322894875985, accuracy: 0.9866666666666667\n",
      "iteration no 4984: Loss: 0.25103676215759874, accuracy: 0.9866666666666667\n",
      "iteration no 4985: Loss: 0.2510320568797442, accuracy: 0.9866666666666667\n",
      "iteration no 4986: Loss: 0.2510309379482796, accuracy: 0.9866666666666667\n",
      "iteration no 4987: Loss: 0.25103132438663733, accuracy: 0.9866666666666667\n",
      "iteration no 4988: Loss: 0.2510262017991927, accuracy: 0.9866666666666667\n",
      "iteration no 4989: Loss: 0.2510276777539386, accuracy: 0.9866666666666667\n",
      "iteration no 4990: Loss: 0.25102380217309506, accuracy: 0.9866666666666667\n",
      "iteration no 4991: Loss: 0.25102366348588634, accuracy: 0.9866666666666667\n",
      "iteration no 4992: Loss: 0.2510235254526277, accuracy: 0.9866666666666667\n",
      "iteration no 4993: Loss: 0.2510177283287863, accuracy: 0.9866666666666667\n",
      "iteration no 4994: Loss: 0.2510197855360279, accuracy: 0.9866666666666667\n",
      "iteration no 4995: Loss: 0.25101777546095266, accuracy: 0.9866666666666667\n",
      "iteration no 4996: Loss: 0.25101478888500844, accuracy: 0.9866666666666667\n",
      "iteration no 4997: Loss: 0.25101519272816764, accuracy: 0.9866666666666667\n",
      "iteration no 4998: Loss: 0.2510105651806304, accuracy: 0.9866666666666667\n",
      "iteration no 4999: Loss: 0.25101054410445955, accuracy: 0.9866666666666667\n",
      "iteration no 5000: Loss: 0.2510107894570036, accuracy: 0.9866666666666667\n",
      "iteration no 5001: Loss: 0.2510068426673412, accuracy: 0.9866666666666667\n",
      "iteration no 5002: Loss: 0.2510057050320973, accuracy: 0.9866666666666667\n",
      "iteration no 5003: Loss: 0.25100448681076337, accuracy: 0.9866666666666667\n",
      "iteration no 5004: Loss: 0.25100326946789026, accuracy: 0.9866666666666667\n",
      "iteration no 5005: Loss: 0.2510012470786583, accuracy: 0.9866666666666667\n",
      "iteration no 5006: Loss: 0.2510006647334633, accuracy: 0.9866666666666667\n",
      "iteration no 5007: Loss: 0.2509978868809054, accuracy: 0.9866666666666667\n",
      "iteration no 5008: Loss: 0.2509958963791744, accuracy: 0.9866666666666667\n",
      "iteration no 5009: Loss: 0.250996267833317, accuracy: 0.9866666666666667\n",
      "iteration no 5010: Loss: 0.25099316553239387, accuracy: 0.9866666666666667\n",
      "iteration no 5011: Loss: 0.25099174467931884, accuracy: 0.9866666666666667\n",
      "iteration no 5012: Loss: 0.25099144017088615, accuracy: 0.9866666666666667\n",
      "iteration no 5013: Loss: 0.2509873123647421, accuracy: 0.9866666666666667\n",
      "iteration no 5014: Loss: 0.2509883834189657, accuracy: 0.9866666666666667\n",
      "iteration no 5015: Loss: 0.25098561820332527, accuracy: 0.9866666666666667\n",
      "iteration no 5016: Loss: 0.25098403170250394, accuracy: 0.9866666666666667\n",
      "iteration no 5017: Loss: 0.25098367958167583, accuracy: 0.9866666666666667\n",
      "iteration no 5018: Loss: 0.25097920270272706, accuracy: 0.9866666666666667\n",
      "iteration no 5019: Loss: 0.2509809241444556, accuracy: 0.9866666666666667\n",
      "iteration no 5020: Loss: 0.2509774961911971, accuracy: 0.9866666666666667\n",
      "iteration no 5021: Loss: 0.2509767019042928, accuracy: 0.9866666666666667\n",
      "iteration no 5022: Loss: 0.25097692279130074, accuracy: 0.9866666666666667\n",
      "iteration no 5023: Loss: 0.2509705616576756, accuracy: 0.9866666666666667\n",
      "iteration no 5024: Loss: 0.2509736941480354, accuracy: 0.9866666666666667\n",
      "iteration no 5025: Loss: 0.25097032975235756, accuracy: 0.9866666666666667\n",
      "iteration no 5026: Loss: 0.2509677013079631, accuracy: 0.9866666666666667\n",
      "iteration no 5027: Loss: 0.2509687122576257, accuracy: 0.9866666666666667\n",
      "iteration no 5028: Loss: 0.2509627212772959, accuracy: 0.9866666666666667\n",
      "iteration no 5029: Loss: 0.2509662979736495, accuracy: 0.9866666666666667\n",
      "iteration no 5030: Loss: 0.25096217703813406, accuracy: 0.9866666666666667\n",
      "iteration no 5031: Loss: 0.25095993415479634, accuracy: 0.9866666666666667\n",
      "iteration no 5032: Loss: 0.250961027224322, accuracy: 0.9866666666666667\n",
      "iteration no 5033: Loss: 0.25095405312268837, accuracy: 0.9866666666666667\n",
      "iteration no 5034: Loss: 0.2509589042164122, accuracy: 0.9866666666666667\n",
      "iteration no 5035: Loss: 0.250954724741683, accuracy: 0.9866666666666667\n",
      "iteration no 5036: Loss: 0.25095131110293023, accuracy: 0.9866666666666667\n",
      "iteration no 5037: Loss: 0.25095443948630375, accuracy: 0.9866666666666667\n",
      "iteration no 5038: Loss: 0.2509464889110492, accuracy: 0.9866666666666667\n",
      "iteration no 5039: Loss: 0.2509497986257889, accuracy: 0.9866666666666667\n",
      "iteration no 5040: Loss: 0.2509477707601597, accuracy: 0.9866666666666667\n",
      "iteration no 5041: Loss: 0.2509425199178336, accuracy: 0.9866666666666667\n",
      "iteration no 5042: Loss: 0.2509458522779716, accuracy: 0.9866666666666667\n",
      "iteration no 5043: Loss: 0.25094013650304353, accuracy: 0.9866666666666667\n",
      "iteration no 5044: Loss: 0.25094060595694223, accuracy: 0.9866666666666667\n",
      "iteration no 5045: Loss: 0.25094006215509274, accuracy: 0.9866666666666667\n",
      "iteration no 5046: Loss: 0.25093600453034565, accuracy: 0.9866666666666667\n",
      "iteration no 5047: Loss: 0.25093608887482644, accuracy: 0.9866666666666667\n",
      "iteration no 5048: Loss: 0.25093272317136, accuracy: 0.9866666666666667\n",
      "iteration no 5049: Loss: 0.2509324096787981, accuracy: 0.9866666666666667\n",
      "iteration no 5050: Loss: 0.25093016658836487, accuracy: 0.9866666666666667\n",
      "iteration no 5051: Loss: 0.2509305244232421, accuracy: 0.9866666666666667\n",
      "iteration no 5052: Loss: 0.2509267172511467, accuracy: 0.9866666666666667\n",
      "iteration no 5053: Loss: 0.250924221973555, accuracy: 0.9866666666666667\n",
      "iteration no 5054: Loss: 0.2509262655076438, accuracy: 0.9866666666666667\n",
      "iteration no 5055: Loss: 0.2509225906836463, accuracy: 0.9866666666666667\n",
      "iteration no 5056: Loss: 0.250920311180417, accuracy: 0.9866666666666667\n",
      "iteration no 5057: Loss: 0.2509194864750387, accuracy: 0.9866666666666667\n",
      "iteration no 5058: Loss: 0.2509172518022725, accuracy: 0.9866666666666667\n",
      "iteration no 5059: Loss: 0.2509148126949273, accuracy: 0.9866666666666667\n",
      "iteration no 5060: Loss: 0.2509151103491099, accuracy: 0.9866666666666667\n",
      "iteration no 5061: Loss: 0.25091376647225244, accuracy: 0.9866666666666667\n",
      "iteration no 5062: Loss: 0.25091025528033006, accuracy: 0.9866666666666667\n",
      "iteration no 5063: Loss: 0.25090864281835207, accuracy: 0.9866666666666667\n",
      "iteration no 5064: Loss: 0.25090750937440776, accuracy: 0.9866666666666667\n",
      "iteration no 5065: Loss: 0.2509078770797373, accuracy: 0.9866666666666667\n",
      "iteration no 5066: Loss: 0.2509048018399038, accuracy: 0.9866666666666667\n",
      "iteration no 5067: Loss: 0.25090016372688817, accuracy: 0.9866666666666667\n",
      "iteration no 5068: Loss: 0.2509025983352098, accuracy: 0.9866666666666667\n",
      "iteration no 5069: Loss: 0.2508991963944466, accuracy: 0.9866666666666667\n",
      "iteration no 5070: Loss: 0.2508990629409066, accuracy: 0.9866666666666667\n",
      "iteration no 5071: Loss: 0.25089588933083007, accuracy: 0.9866666666666667\n",
      "iteration no 5072: Loss: 0.25089378979412535, accuracy: 0.9866666666666667\n",
      "iteration no 5073: Loss: 0.2508954242696469, accuracy: 0.9866666666666667\n",
      "iteration no 5074: Loss: 0.2508907236453835, accuracy: 0.9866666666666667\n",
      "iteration no 5075: Loss: 0.25089017302795436, accuracy: 0.9866666666666667\n",
      "iteration no 5076: Loss: 0.2508874288539932, accuracy: 0.9866666666666667\n",
      "iteration no 5077: Loss: 0.2508868300602227, accuracy: 0.9866666666666667\n",
      "iteration no 5078: Loss: 0.25088531758267674, accuracy: 0.9866666666666667\n",
      "iteration no 5079: Loss: 0.2508819825900159, accuracy: 0.9866666666666667\n",
      "iteration no 5080: Loss: 0.2508828230516828, accuracy: 0.9866666666666667\n",
      "iteration no 5081: Loss: 0.2508810234503925, accuracy: 0.9866666666666667\n",
      "iteration no 5082: Loss: 0.2508764846111898, accuracy: 0.9866666666666667\n",
      "iteration no 5083: Loss: 0.25087632698634216, accuracy: 0.9866666666666667\n",
      "iteration no 5084: Loss: 0.2508768671108448, accuracy: 0.9866666666666667\n",
      "iteration no 5085: Loss: 0.25087372467455543, accuracy: 0.9866666666666667\n",
      "iteration no 5086: Loss: 0.2508721970615932, accuracy: 0.9866666666666667\n",
      "iteration no 5087: Loss: 0.2508675270306716, accuracy: 0.9866666666666667\n",
      "iteration no 5088: Loss: 0.2508715039330727, accuracy: 0.9866666666666667\n",
      "iteration no 5089: Loss: 0.2508666732207343, accuracy: 0.9866666666666667\n",
      "iteration no 5090: Loss: 0.2508659588182245, accuracy: 0.9866666666666667\n",
      "iteration no 5091: Loss: 0.25086313083490575, accuracy: 0.9866666666666667\n",
      "iteration no 5092: Loss: 0.2508631584946087, accuracy: 0.9866666666666667\n",
      "iteration no 5093: Loss: 0.25086083343583465, accuracy: 0.9866666666666667\n",
      "iteration no 5094: Loss: 0.2508587711122058, accuracy: 0.9866666666666667\n",
      "iteration no 5095: Loss: 0.25085624827936936, accuracy: 0.9866666666666667\n",
      "iteration no 5096: Loss: 0.25085919553268476, accuracy: 0.9866666666666667\n",
      "iteration no 5097: Loss: 0.2508523507739075, accuracy: 0.9866666666666667\n",
      "iteration no 5098: Loss: 0.25085225372885755, accuracy: 0.9866666666666667\n",
      "iteration no 5099: Loss: 0.25085126192659346, accuracy: 0.9866666666666667\n",
      "iteration no 5100: Loss: 0.25085031189237184, accuracy: 0.9866666666666667\n",
      "iteration no 5101: Loss: 0.2508477821267727, accuracy: 0.9866666666666667\n",
      "iteration no 5102: Loss: 0.25084519258953425, accuracy: 0.9866666666666667\n",
      "iteration no 5103: Loss: 0.2508445044703097, accuracy: 0.9866666666666667\n",
      "iteration no 5104: Loss: 0.25084539068561285, accuracy: 0.9866666666666667\n",
      "iteration no 5105: Loss: 0.250839342037952, accuracy: 0.9866666666666667\n",
      "iteration no 5106: Loss: 0.25084037958858757, accuracy: 0.9866666666666667\n",
      "iteration no 5107: Loss: 0.2508381356290342, accuracy: 0.9866666666666667\n",
      "iteration no 5108: Loss: 0.2508370429327599, accuracy: 0.9866666666666667\n",
      "iteration no 5109: Loss: 0.2508347962085831, accuracy: 0.9866666666666667\n",
      "iteration no 5110: Loss: 0.2508321026618976, accuracy: 0.9866666666666667\n",
      "iteration no 5111: Loss: 0.2508331934890833, accuracy: 0.9866666666666667\n",
      "iteration no 5112: Loss: 0.250830272867711, accuracy: 0.9866666666666667\n",
      "iteration no 5113: Loss: 0.2508269711410733, accuracy: 0.9866666666666667\n",
      "iteration no 5114: Loss: 0.2508280703046109, accuracy: 0.9866666666666667\n",
      "iteration no 5115: Loss: 0.2508252670086482, accuracy: 0.9866666666666667\n",
      "iteration no 5116: Loss: 0.25082413725463143, accuracy: 0.9866666666666667\n",
      "iteration no 5117: Loss: 0.25082045876693404, accuracy: 0.9866666666666667\n",
      "iteration no 5118: Loss: 0.25082073966914537, accuracy: 0.9866666666666667\n",
      "iteration no 5119: Loss: 0.25082075713856533, accuracy: 0.9866666666666667\n",
      "iteration no 5120: Loss: 0.2508154363180975, accuracy: 0.9866666666666667\n",
      "iteration no 5121: Loss: 0.25081600286230865, accuracy: 0.9866666666666667\n",
      "iteration no 5122: Loss: 0.25081550153425947, accuracy: 0.9866666666666667\n",
      "iteration no 5123: Loss: 0.25081181474190883, accuracy: 0.9866666666666667\n",
      "iteration no 5124: Loss: 0.2508112004741343, accuracy: 0.9866666666666667\n",
      "iteration no 5125: Loss: 0.2508095713131394, accuracy: 0.9866666666666667\n",
      "iteration no 5126: Loss: 0.25080833035272004, accuracy: 0.9866666666666667\n",
      "iteration no 5127: Loss: 0.2508068249603764, accuracy: 0.9866666666666667\n",
      "iteration no 5128: Loss: 0.2508010167243635, accuracy: 0.9866666666666667\n",
      "iteration no 5129: Loss: 0.25080540919432953, accuracy: 0.9866666666666667\n",
      "iteration no 5130: Loss: 0.2508010606349145, accuracy: 0.9866666666666667\n",
      "iteration no 5131: Loss: 0.25080046489629987, accuracy: 0.9866666666666667\n",
      "iteration no 5132: Loss: 0.2507960298734354, accuracy: 0.9866666666666667\n",
      "iteration no 5133: Loss: 0.2507973505307161, accuracy: 0.9866666666666667\n",
      "iteration no 5134: Loss: 0.25079609634592803, accuracy: 0.9866666666666667\n",
      "iteration no 5135: Loss: 0.2507935420021296, accuracy: 0.9866666666666667\n",
      "iteration no 5136: Loss: 0.25078957206651054, accuracy: 0.9866666666666667\n",
      "iteration no 5137: Loss: 0.2507929841782302, accuracy: 0.9866666666666667\n",
      "iteration no 5138: Loss: 0.2507867639451567, accuracy: 0.9866666666666667\n",
      "iteration no 5139: Loss: 0.2507868256588398, accuracy: 0.9866666666666667\n",
      "iteration no 5140: Loss: 0.2507854169511839, accuracy: 0.9866666666666667\n",
      "iteration no 5141: Loss: 0.250784364400495, accuracy: 0.9866666666666667\n",
      "iteration no 5142: Loss: 0.2507817497709693, accuracy: 0.9866666666666667\n",
      "iteration no 5143: Loss: 0.2507789502784469, accuracy: 0.9866666666666667\n",
      "iteration no 5144: Loss: 0.250780436100177, accuracy: 0.9866666666666667\n",
      "iteration no 5145: Loss: 0.2507782187103059, accuracy: 0.9866666666666667\n",
      "iteration no 5146: Loss: 0.25077344005507995, accuracy: 0.9866666666666667\n",
      "iteration no 5147: Loss: 0.250776196866641, accuracy: 0.9866666666666667\n",
      "iteration no 5148: Loss: 0.25077168916514325, accuracy: 0.9866666666666667\n",
      "iteration no 5149: Loss: 0.25077200647447007, accuracy: 0.9866666666666667\n",
      "iteration no 5150: Loss: 0.2507677446079789, accuracy: 0.9866666666666667\n",
      "iteration no 5151: Loss: 0.25076925637337055, accuracy: 0.9866666666666667\n",
      "iteration no 5152: Loss: 0.25076840344484097, accuracy: 0.9866666666666667\n",
      "iteration no 5153: Loss: 0.2507639531999288, accuracy: 0.9866666666666667\n",
      "iteration no 5154: Loss: 0.2507625223765182, accuracy: 0.9866666666666667\n",
      "iteration no 5155: Loss: 0.2507633072682218, accuracy: 0.9866666666666667\n",
      "iteration no 5156: Loss: 0.25075983480922703, accuracy: 0.9866666666666667\n",
      "iteration no 5157: Loss: 0.25075793511027816, accuracy: 0.9866666666666667\n",
      "iteration no 5158: Loss: 0.25075752747278357, accuracy: 0.9866666666666667\n",
      "iteration no 5159: Loss: 0.25075557913911145, accuracy: 0.9866666666666667\n",
      "iteration no 5160: Loss: 0.25075502552361756, accuracy: 0.9866666666666667\n",
      "iteration no 5161: Loss: 0.25074978614188903, accuracy: 0.9866666666666667\n",
      "iteration no 5162: Loss: 0.2507542831991314, accuracy: 0.9866666666666667\n",
      "iteration no 5163: Loss: 0.25074830126831277, accuracy: 0.9866666666666667\n",
      "iteration no 5164: Loss: 0.25074675543391634, accuracy: 0.9866666666666667\n",
      "iteration no 5165: Loss: 0.2507481923309521, accuracy: 0.9866666666666667\n",
      "iteration no 5166: Loss: 0.25074390212100506, accuracy: 0.9866666666666667\n",
      "iteration no 5167: Loss: 0.25074423073768104, accuracy: 0.9866666666666667\n",
      "iteration no 5168: Loss: 0.25074035714250414, accuracy: 0.9866666666666667\n",
      "iteration no 5169: Loss: 0.250741444517173, accuracy: 0.9866666666666667\n",
      "iteration no 5170: Loss: 0.25073903588113483, accuracy: 0.9866666666666667\n",
      "iteration no 5171: Loss: 0.2507361787773839, accuracy: 0.9866666666666667\n",
      "iteration no 5172: Loss: 0.25073673684556363, accuracy: 0.9866666666666667\n",
      "iteration no 5173: Loss: 0.2507361555236142, accuracy: 0.9866666666666667\n",
      "iteration no 5174: Loss: 0.25073098207136946, accuracy: 0.9866666666666667\n",
      "iteration no 5175: Loss: 0.2507318061860425, accuracy: 0.9866666666666667\n",
      "iteration no 5176: Loss: 0.2507317722398654, accuracy: 0.9866666666666667\n",
      "iteration no 5177: Loss: 0.2507270203304158, accuracy: 0.9866666666666667\n",
      "iteration no 5178: Loss: 0.2507280541093732, accuracy: 0.9866666666666667\n",
      "iteration no 5179: Loss: 0.2507249950321103, accuracy: 0.9866666666666667\n",
      "iteration no 5180: Loss: 0.25072465416892786, accuracy: 0.9866666666666667\n",
      "iteration no 5181: Loss: 0.2507224750163283, accuracy: 0.9866666666666667\n",
      "iteration no 5182: Loss: 0.2507206476715239, accuracy: 0.9866666666666667\n",
      "iteration no 5183: Loss: 0.2507200359279349, accuracy: 0.9866666666666667\n",
      "iteration no 5184: Loss: 0.2507187180913154, accuracy: 0.9866666666666667\n",
      "iteration no 5185: Loss: 0.2507138752409528, accuracy: 0.9866666666666667\n",
      "iteration no 5186: Loss: 0.2507161869560459, accuracy: 0.9866666666666667\n",
      "iteration no 5187: Loss: 0.25071617515540995, accuracy: 0.9866666666666667\n",
      "iteration no 5188: Loss: 0.2507101802995633, accuracy: 0.9866666666666667\n",
      "iteration no 5189: Loss: 0.2507098510750827, accuracy: 0.9866666666666667\n",
      "iteration no 5190: Loss: 0.25071090661363554, accuracy: 0.9866666666666667\n",
      "iteration no 5191: Loss: 0.25070743182189964, accuracy: 0.9866666666666667\n",
      "iteration no 5192: Loss: 0.2507034869366813, accuracy: 0.9866666666666667\n",
      "iteration no 5193: Loss: 0.2507080471313327, accuracy: 0.9866666666666667\n",
      "iteration no 5194: Loss: 0.25070189594828257, accuracy: 0.9866666666666667\n",
      "iteration no 5195: Loss: 0.25070008691715395, accuracy: 0.9866666666666667\n",
      "iteration no 5196: Loss: 0.25070216927296995, accuracy: 0.9866666666666667\n",
      "iteration no 5197: Loss: 0.25069914887581846, accuracy: 0.9866666666666667\n",
      "iteration no 5198: Loss: 0.2506953039732202, accuracy: 0.9866666666666667\n",
      "iteration no 5199: Loss: 0.25069621111427093, accuracy: 0.9866666666666667\n",
      "iteration no 5200: Loss: 0.2506956313284393, accuracy: 0.9866666666666667\n",
      "iteration no 5201: Loss: 0.2506912926135151, accuracy: 0.9866666666666667\n",
      "iteration no 5202: Loss: 0.2506919476214743, accuracy: 0.9866666666666667\n",
      "iteration no 5203: Loss: 0.25069063132800123, accuracy: 0.9866666666666667\n",
      "iteration no 5204: Loss: 0.2506880364981693, accuracy: 0.9866666666666667\n",
      "iteration no 5205: Loss: 0.25068596347784966, accuracy: 0.9866666666666667\n",
      "iteration no 5206: Loss: 0.2506876366221375, accuracy: 0.9866666666666667\n",
      "iteration no 5207: Loss: 0.25068367657723767, accuracy: 0.9866666666666667\n",
      "iteration no 5208: Loss: 0.25068330377276454, accuracy: 0.9866666666666667\n",
      "iteration no 5209: Loss: 0.25068106642717913, accuracy: 0.9866666666666667\n",
      "iteration no 5210: Loss: 0.2506793246666979, accuracy: 0.9866666666666667\n",
      "iteration no 5211: Loss: 0.25067863134031276, accuracy: 0.9866666666666667\n",
      "iteration no 5212: Loss: 0.25067640793621737, accuracy: 0.9866666666666667\n",
      "iteration no 5213: Loss: 0.2506769409156549, accuracy: 0.9866666666666667\n",
      "iteration no 5214: Loss: 0.2506741949836299, accuracy: 0.9866666666666667\n",
      "iteration no 5215: Loss: 0.2506727760875152, accuracy: 0.9866666666666667\n",
      "iteration no 5216: Loss: 0.2506692727820535, accuracy: 0.9866666666666667\n",
      "iteration no 5217: Loss: 0.2506715553453544, accuracy: 0.9866666666666667\n",
      "iteration no 5218: Loss: 0.2506695216686278, accuracy: 0.9866666666666667\n",
      "iteration no 5219: Loss: 0.25066590051904525, accuracy: 0.9866666666666667\n",
      "iteration no 5220: Loss: 0.2506639956015718, accuracy: 0.9866666666666667\n",
      "iteration no 5221: Loss: 0.25066582963853146, accuracy: 0.9866666666666667\n",
      "iteration no 5222: Loss: 0.2506627913216556, accuracy: 0.9866666666666667\n",
      "iteration no 5223: Loss: 0.25065919418433147, accuracy: 0.9866666666666667\n",
      "iteration no 5224: Loss: 0.25066150459112335, accuracy: 0.9866666666666667\n",
      "iteration no 5225: Loss: 0.2506580198942546, accuracy: 0.9866666666666667\n",
      "iteration no 5226: Loss: 0.2506556739448621, accuracy: 0.9866666666666667\n",
      "iteration no 5227: Loss: 0.2506547442642427, accuracy: 0.9866666666666667\n",
      "iteration no 5228: Loss: 0.2506559567186973, accuracy: 0.9866666666666667\n",
      "iteration no 5229: Loss: 0.2506513265566513, accuracy: 0.9866666666666667\n",
      "iteration no 5230: Loss: 0.25065251803279004, accuracy: 0.9866666666666667\n",
      "iteration no 5231: Loss: 0.25064812140814335, accuracy: 0.9866666666666667\n",
      "iteration no 5232: Loss: 0.2506491569361871, accuracy: 0.9866666666666667\n",
      "iteration no 5233: Loss: 0.25064712717453974, accuracy: 0.9866666666666667\n",
      "iteration no 5234: Loss: 0.25064673521597625, accuracy: 0.9866666666666667\n",
      "iteration no 5235: Loss: 0.2506426463169582, accuracy: 0.9866666666666667\n",
      "iteration no 5236: Loss: 0.25064253720821605, accuracy: 0.9866666666666667\n",
      "iteration no 5237: Loss: 0.2506433180879218, accuracy: 0.9866666666666667\n",
      "iteration no 5238: Loss: 0.2506397397912188, accuracy: 0.9866666666666667\n",
      "iteration no 5239: Loss: 0.2506376682021241, accuracy: 0.9866666666666667\n",
      "iteration no 5240: Loss: 0.25063814052217737, accuracy: 0.9866666666666667\n",
      "iteration no 5241: Loss: 0.250637450654305, accuracy: 0.9866666666666667\n",
      "iteration no 5242: Loss: 0.2506325973159747, accuracy: 0.9866666666666667\n",
      "iteration no 5243: Loss: 0.25063356739421694, accuracy: 0.9866666666666667\n",
      "iteration no 5244: Loss: 0.25063103759369554, accuracy: 0.9866666666666667\n",
      "iteration no 5245: Loss: 0.25063113467211806, accuracy: 0.9866666666666667\n",
      "iteration no 5246: Loss: 0.2506277773195159, accuracy: 0.9866666666666667\n",
      "iteration no 5247: Loss: 0.25062802735157663, accuracy: 0.9866666666666667\n",
      "iteration no 5248: Loss: 0.25062586008669735, accuracy: 0.9866666666666667\n",
      "iteration no 5249: Loss: 0.25062439259813046, accuracy: 0.9866666666666667\n",
      "iteration no 5250: Loss: 0.25062352209430916, accuracy: 0.9866666666666667\n",
      "iteration no 5251: Loss: 0.2506217977632454, accuracy: 0.9866666666666667\n",
      "iteration no 5252: Loss: 0.2506202482911991, accuracy: 0.9866666666666667\n",
      "iteration no 5253: Loss: 0.25061716898583836, accuracy: 0.9866666666666667\n",
      "iteration no 5254: Loss: 0.2506195611250871, accuracy: 0.9866666666666667\n",
      "iteration no 5255: Loss: 0.25061549247411186, accuracy: 0.9866666666666667\n",
      "iteration no 5256: Loss: 0.2506143575741143, accuracy: 0.9866666666666667\n",
      "iteration no 5257: Loss: 0.250612865674525, accuracy: 0.9866666666666667\n",
      "iteration no 5258: Loss: 0.25061238826498927, accuracy: 0.9866666666666667\n",
      "iteration no 5259: Loss: 0.25061092492578646, accuracy: 0.9866666666666667\n",
      "iteration no 5260: Loss: 0.2506102708140547, accuracy: 0.9866666666666667\n",
      "iteration no 5261: Loss: 0.25060802928078835, accuracy: 0.9866666666666667\n",
      "iteration no 5262: Loss: 0.2506059537777077, accuracy: 0.9866666666666667\n",
      "iteration no 5263: Loss: 0.25060327637599156, accuracy: 0.9866666666666667\n",
      "iteration no 5264: Loss: 0.25060469590965484, accuracy: 0.9866666666666667\n",
      "iteration no 5265: Loss: 0.2506036425629243, accuracy: 0.9866666666666667\n",
      "iteration no 5266: Loss: 0.25059965500894454, accuracy: 0.9866666666666667\n",
      "iteration no 5267: Loss: 0.250599795719063, accuracy: 0.9866666666666667\n",
      "iteration no 5268: Loss: 0.25059702900139724, accuracy: 0.9866666666666667\n",
      "iteration no 5269: Loss: 0.25059621028699486, accuracy: 0.9866666666666667\n",
      "iteration no 5270: Loss: 0.25059533119171795, accuracy: 0.9866666666666667\n",
      "iteration no 5271: Loss: 0.2505957299861984, accuracy: 0.9866666666666667\n",
      "iteration no 5272: Loss: 0.2505903900966137, accuracy: 0.9866666666666667\n",
      "iteration no 5273: Loss: 0.25059006135907946, accuracy: 0.9866666666666667\n",
      "iteration no 5274: Loss: 0.2505891510860898, accuracy: 0.9866666666666667\n",
      "iteration no 5275: Loss: 0.25059025485170816, accuracy: 0.9866666666666667\n",
      "iteration no 5276: Loss: 0.2505857766845695, accuracy: 0.9866666666666667\n",
      "iteration no 5277: Loss: 0.2505861712577983, accuracy: 0.9866666666666667\n",
      "iteration no 5278: Loss: 0.2505828488389876, accuracy: 0.9866666666666667\n",
      "iteration no 5279: Loss: 0.25058174104861886, accuracy: 0.9866666666666667\n",
      "iteration no 5280: Loss: 0.2505803344414841, accuracy: 0.9866666666666667\n",
      "iteration no 5281: Loss: 0.2505825017992056, accuracy: 0.9866666666666667\n",
      "iteration no 5282: Loss: 0.25057720216376955, accuracy: 0.9866666666666667\n",
      "iteration no 5283: Loss: 0.2505759493755982, accuracy: 0.9866666666666667\n",
      "iteration no 5284: Loss: 0.2505746335993757, accuracy: 0.9866666666666667\n",
      "iteration no 5285: Loss: 0.2505747234628245, accuracy: 0.9866666666666667\n",
      "iteration no 5286: Loss: 0.25057260568918527, accuracy: 0.9866666666666667\n",
      "iteration no 5287: Loss: 0.2505714331023369, accuracy: 0.9866666666666667\n",
      "iteration no 5288: Loss: 0.2505693953059804, accuracy: 0.9866666666666667\n",
      "iteration no 5289: Loss: 0.2505673454517381, accuracy: 0.9866666666666667\n",
      "iteration no 5290: Loss: 0.25056525596044743, accuracy: 0.9866666666666667\n",
      "iteration no 5291: Loss: 0.2505668875900135, accuracy: 0.9866666666666667\n",
      "iteration no 5292: Loss: 0.25056495056664535, accuracy: 0.9866666666666667\n",
      "iteration no 5293: Loss: 0.2505603240597284, accuracy: 0.9866666666666667\n",
      "iteration no 5294: Loss: 0.2505613627408429, accuracy: 0.9866666666666667\n",
      "iteration no 5295: Loss: 0.2505584564268687, accuracy: 0.9866666666666667\n",
      "iteration no 5296: Loss: 0.2505584412784349, accuracy: 0.9866666666666667\n",
      "iteration no 5297: Loss: 0.25055694987834354, accuracy: 0.9866666666666667\n",
      "iteration no 5298: Loss: 0.2505556784466712, accuracy: 0.9866666666666667\n",
      "iteration no 5299: Loss: 0.2505529420169447, accuracy: 0.9866666666666667\n",
      "iteration no 5300: Loss: 0.25054862669099, accuracy: 0.9866666666666667\n",
      "iteration no 5301: Loss: 0.25055321080825715, accuracy: 0.9866666666666667\n",
      "iteration no 5302: Loss: 0.250549699370443, accuracy: 0.9866666666666667\n",
      "iteration no 5303: Loss: 0.25054701383177935, accuracy: 0.9866666666666667\n",
      "iteration no 5304: Loss: 0.25054651883375906, accuracy: 0.9866666666666667\n",
      "iteration no 5305: Loss: 0.25054327767905205, accuracy: 0.9866666666666667\n",
      "iteration no 5306: Loss: 0.2505434093467541, accuracy: 0.9866666666666667\n",
      "iteration no 5307: Loss: 0.25054199532603577, accuracy: 0.9866666666666667\n",
      "iteration no 5308: Loss: 0.25054202758423444, accuracy: 0.9866666666666667\n",
      "iteration no 5309: Loss: 0.25053769742485205, accuracy: 0.9866666666666667\n",
      "iteration no 5310: Loss: 0.25053569709915346, accuracy: 0.9866666666666667\n",
      "iteration no 5311: Loss: 0.2505367481355523, accuracy: 0.9866666666666667\n",
      "iteration no 5312: Loss: 0.25053526188940406, accuracy: 0.9866666666666667\n",
      "iteration no 5313: Loss: 0.25053333328436267, accuracy: 0.9866666666666667\n",
      "iteration no 5314: Loss: 0.25053083780725427, accuracy: 0.9866666666666667\n",
      "iteration no 5315: Loss: 0.2505303080311095, accuracy: 0.9866666666666667\n",
      "iteration no 5316: Loss: 0.2505275186498972, accuracy: 0.9866666666666667\n",
      "iteration no 5317: Loss: 0.25052848250412807, accuracy: 0.9866666666666667\n",
      "iteration no 5318: Loss: 0.2505266601026901, accuracy: 0.9866666666666667\n",
      "iteration no 5319: Loss: 0.25052347064057134, accuracy: 0.9866666666666667\n",
      "iteration no 5320: Loss: 0.2505220511705229, accuracy: 0.9866666666666667\n",
      "iteration no 5321: Loss: 0.25052180942961844, accuracy: 0.9866666666666667\n",
      "iteration no 5322: Loss: 0.25052246761849023, accuracy: 0.9866666666666667\n",
      "iteration no 5323: Loss: 0.25051775532983755, accuracy: 0.9866666666666667\n",
      "iteration no 5324: Loss: 0.25051820381616785, accuracy: 0.9866666666666667\n",
      "iteration no 5325: Loss: 0.25051510392545406, accuracy: 0.9866666666666667\n",
      "iteration no 5326: Loss: 0.2505168751215898, accuracy: 0.9866666666666667\n",
      "iteration no 5327: Loss: 0.2505126868806413, accuracy: 0.9866666666666667\n",
      "iteration no 5328: Loss: 0.25051408928837626, accuracy: 0.9866666666666667\n",
      "iteration no 5329: Loss: 0.2505098925131317, accuracy: 0.9866666666666667\n",
      "iteration no 5330: Loss: 0.25051020470630225, accuracy: 0.9866666666666667\n",
      "iteration no 5331: Loss: 0.2505105164901158, accuracy: 0.9866666666666667\n",
      "iteration no 5332: Loss: 0.250507513866394, accuracy: 0.9866666666666667\n",
      "iteration no 5333: Loss: 0.2505041420868359, accuracy: 0.9866666666666667\n",
      "iteration no 5334: Loss: 0.2505057678435871, accuracy: 0.9866666666666667\n",
      "iteration no 5335: Loss: 0.250506776271873, accuracy: 0.9866666666666667\n",
      "iteration no 5336: Loss: 0.250501772567539, accuracy: 0.9866666666666667\n",
      "iteration no 5337: Loss: 0.25049894561529273, accuracy: 0.9866666666666667\n",
      "iteration no 5338: Loss: 0.2505018658433437, accuracy: 0.9866666666666667\n",
      "iteration no 5339: Loss: 0.25050035856048425, accuracy: 0.9866666666666667\n",
      "iteration no 5340: Loss: 0.2504954878246689, accuracy: 0.9866666666666667\n",
      "iteration no 5341: Loss: 0.25049649721332007, accuracy: 0.9866666666666667\n",
      "iteration no 5342: Loss: 0.25049581397076826, accuracy: 0.9866666666666667\n",
      "iteration no 5343: Loss: 0.2504931429362842, accuracy: 0.9866666666666667\n",
      "iteration no 5344: Loss: 0.2504910518102933, accuracy: 0.9866666666666667\n",
      "iteration no 5345: Loss: 0.2504931898930244, accuracy: 0.9866666666666667\n",
      "iteration no 5346: Loss: 0.2504900184017708, accuracy: 0.9866666666666667\n",
      "iteration no 5347: Loss: 0.25048891317501953, accuracy: 0.9866666666666667\n",
      "iteration no 5348: Loss: 0.2504855376405976, accuracy: 0.9866666666666667\n",
      "iteration no 5349: Loss: 0.250488445371436, accuracy: 0.9866666666666667\n",
      "iteration no 5350: Loss: 0.2504850376000812, accuracy: 0.9866666666666667\n",
      "iteration no 5351: Loss: 0.25048495363273326, accuracy: 0.9866666666666667\n",
      "iteration no 5352: Loss: 0.25048185034262427, accuracy: 0.9866666666666667\n",
      "iteration no 5353: Loss: 0.25048155330929217, accuracy: 0.9866666666666667\n",
      "iteration no 5354: Loss: 0.250480352866943, accuracy: 0.9866666666666667\n",
      "iteration no 5355: Loss: 0.2504779610053286, accuracy: 0.9866666666666667\n",
      "iteration no 5356: Loss: 0.2504791935832502, accuracy: 0.9866666666666667\n",
      "iteration no 5357: Loss: 0.2504757481926715, accuracy: 0.9866666666666667\n",
      "iteration no 5358: Loss: 0.2504757996726887, accuracy: 0.9866666666666667\n",
      "iteration no 5359: Loss: 0.2504726462977093, accuracy: 0.9866666666666667\n",
      "iteration no 5360: Loss: 0.2504738190167591, accuracy: 0.9866666666666667\n",
      "iteration no 5361: Loss: 0.250472658784758, accuracy: 0.9866666666666667\n",
      "iteration no 5362: Loss: 0.2504693479638007, accuracy: 0.9866666666666667\n",
      "iteration no 5363: Loss: 0.2504692658704869, accuracy: 0.9866666666666667\n",
      "iteration no 5364: Loss: 0.250468562384252, accuracy: 0.9866666666666667\n",
      "iteration no 5365: Loss: 0.2504673996172106, accuracy: 0.9866666666666667\n",
      "iteration no 5366: Loss: 0.25046455609451895, accuracy: 0.9866666666666667\n",
      "iteration no 5367: Loss: 0.250466089047918, accuracy: 0.9866666666666667\n",
      "iteration no 5368: Loss: 0.2504626013562098, accuracy: 0.9866666666666667\n",
      "iteration no 5369: Loss: 0.25046132745697214, accuracy: 0.9866666666666667\n",
      "iteration no 5370: Loss: 0.2504615297010031, accuracy: 0.9866666666666667\n",
      "iteration no 5371: Loss: 0.2504611959086577, accuracy: 0.9866666666666667\n",
      "iteration no 5372: Loss: 0.25045641715604844, accuracy: 0.9866666666666667\n",
      "iteration no 5373: Loss: 0.2504572715051092, accuracy: 0.9866666666666667\n",
      "iteration no 5374: Loss: 0.2504579388885028, accuracy: 0.9866666666666667\n",
      "iteration no 5375: Loss: 0.25045443565323405, accuracy: 0.9866666666666667\n",
      "iteration no 5376: Loss: 0.2504527631174907, accuracy: 0.9866666666666667\n",
      "iteration no 5377: Loss: 0.25045353384066377, accuracy: 0.9866666666666667\n",
      "iteration no 5378: Loss: 0.25045225762161394, accuracy: 0.9866666666666667\n",
      "iteration no 5379: Loss: 0.25044711403145137, accuracy: 0.9866666666666667\n",
      "iteration no 5380: Loss: 0.2504514802643526, accuracy: 0.9866666666666667\n",
      "iteration no 5381: Loss: 0.25044861865143103, accuracy: 0.9866666666666667\n",
      "iteration no 5382: Loss: 0.25044624734103293, accuracy: 0.9866666666666667\n",
      "iteration no 5383: Loss: 0.25044440046205924, accuracy: 0.9866666666666667\n",
      "iteration no 5384: Loss: 0.2504457458346527, accuracy: 0.9866666666666667\n",
      "iteration no 5385: Loss: 0.25044282715898397, accuracy: 0.9866666666666667\n",
      "iteration no 5386: Loss: 0.2504398838945463, accuracy: 0.9866666666666667\n",
      "iteration no 5387: Loss: 0.25044344694965887, accuracy: 0.9866666666666667\n",
      "iteration no 5388: Loss: 0.2504393941844024, accuracy: 0.9866666666666667\n",
      "iteration no 5389: Loss: 0.25043661373574505, accuracy: 0.9866666666666667\n",
      "iteration no 5390: Loss: 0.2504371863247023, accuracy: 0.9866666666666667\n",
      "iteration no 5391: Loss: 0.25043931976102923, accuracy: 0.9866666666666667\n",
      "iteration no 5392: Loss: 0.2504335903063279, accuracy: 0.9866666666666667\n",
      "iteration no 5393: Loss: 0.25043324995642313, accuracy: 0.9866666666666667\n",
      "iteration no 5394: Loss: 0.2504340497081214, accuracy: 0.9866666666666667\n",
      "iteration no 5395: Loss: 0.250431078014631, accuracy: 0.9866666666666667\n",
      "iteration no 5396: Loss: 0.2504284557467925, accuracy: 0.9866666666666667\n",
      "iteration no 5397: Loss: 0.2504314574637976, accuracy: 0.9866666666666667\n",
      "iteration no 5398: Loss: 0.25042847635051035, accuracy: 0.9866666666666667\n",
      "iteration no 5399: Loss: 0.25042383273804325, accuracy: 0.9866666666666667\n",
      "iteration no 5400: Loss: 0.25042741907697547, accuracy: 0.9866666666666667\n",
      "iteration no 5401: Loss: 0.25042541790388956, accuracy: 0.9866666666666667\n",
      "iteration no 5402: Loss: 0.25042255244116435, accuracy: 0.9866666666666667\n",
      "iteration no 5403: Loss: 0.2504211279981069, accuracy: 0.9866666666666667\n",
      "iteration no 5404: Loss: 0.25042314367270646, accuracy: 0.9866666666666667\n",
      "iteration no 5405: Loss: 0.25041836845129295, accuracy: 0.9866666666666667\n",
      "iteration no 5406: Loss: 0.25041875873294583, accuracy: 0.9866666666666667\n",
      "iteration no 5407: Loss: 0.2504185510868884, accuracy: 0.9866666666666667\n",
      "iteration no 5408: Loss: 0.2504163256276146, accuracy: 0.9866666666666667\n",
      "iteration no 5409: Loss: 0.25041422557310433, accuracy: 0.9866666666666667\n",
      "iteration no 5410: Loss: 0.25041377919587204, accuracy: 0.9866666666666667\n",
      "iteration no 5411: Loss: 0.2504145655619881, accuracy: 0.9866666666666667\n",
      "iteration no 5412: Loss: 0.25041050788651575, accuracy: 0.9866666666666667\n",
      "iteration no 5413: Loss: 0.25041034195758005, accuracy: 0.9866666666666667\n",
      "iteration no 5414: Loss: 0.2504101413341714, accuracy: 0.9866666666666667\n",
      "iteration no 5415: Loss: 0.25040779434965754, accuracy: 0.9866666666666667\n",
      "iteration no 5416: Loss: 0.2504062835806972, accuracy: 0.9866666666666667\n",
      "iteration no 5417: Loss: 0.2504080357885051, accuracy: 0.9866666666666667\n",
      "iteration no 5418: Loss: 0.25040454140316715, accuracy: 0.9866666666666667\n",
      "iteration no 5419: Loss: 0.25040400433481763, accuracy: 0.9866666666666667\n",
      "iteration no 5420: Loss: 0.2504020326694944, accuracy: 0.9866666666666667\n",
      "iteration no 5421: Loss: 0.2504023340141238, accuracy: 0.9866666666666667\n",
      "iteration no 5422: Loss: 0.25040036862491644, accuracy: 0.9866666666666667\n",
      "iteration no 5423: Loss: 0.2503986015230308, accuracy: 0.9866666666666667\n",
      "iteration no 5424: Loss: 0.25039920260598936, accuracy: 0.9866666666666667\n",
      "iteration no 5425: Loss: 0.2503956840895448, accuracy: 0.9866666666666667\n",
      "iteration no 5426: Loss: 0.250396371721992, accuracy: 0.9866666666666667\n",
      "iteration no 5427: Loss: 0.25039437935186154, accuracy: 0.9866666666666667\n",
      "iteration no 5428: Loss: 0.25039476726901944, accuracy: 0.9866666666666667\n",
      "iteration no 5429: Loss: 0.250392198438783, accuracy: 0.9866666666666667\n",
      "iteration no 5430: Loss: 0.25039066345296057, accuracy: 0.9866666666666667\n",
      "iteration no 5431: Loss: 0.25038967094867026, accuracy: 0.9866666666666667\n",
      "iteration no 5432: Loss: 0.2503913596448503, accuracy: 0.9866666666666667\n",
      "iteration no 5433: Loss: 0.2503869320002517, accuracy: 0.9866666666666667\n",
      "iteration no 5434: Loss: 0.25038694987432486, accuracy: 0.9866666666666667\n",
      "iteration no 5435: Loss: 0.250386795513152, accuracy: 0.9866666666666667\n",
      "iteration no 5436: Loss: 0.25038366774376936, accuracy: 0.9866666666666667\n",
      "iteration no 5437: Loss: 0.25038338006206173, accuracy: 0.9866666666666667\n",
      "iteration no 5438: Loss: 0.2503831699095158, accuracy: 0.9866666666666667\n",
      "iteration no 5439: Loss: 0.2503817666543911, accuracy: 0.9866666666666667\n",
      "iteration no 5440: Loss: 0.2503775365145001, accuracy: 0.9866666666666667\n",
      "iteration no 5441: Loss: 0.25038116312009956, accuracy: 0.9866666666666667\n",
      "iteration no 5442: Loss: 0.25037787278723855, accuracy: 0.9866666666666667\n",
      "iteration no 5443: Loss: 0.25037608168055747, accuracy: 0.9866666666666667\n",
      "iteration no 5444: Loss: 0.2503754597870133, accuracy: 0.9866666666666667\n",
      "iteration no 5445: Loss: 0.25037615361072657, accuracy: 0.9866666666666667\n",
      "iteration no 5446: Loss: 0.2503723819134901, accuracy: 0.9866666666666667\n",
      "iteration no 5447: Loss: 0.2503717278904857, accuracy: 0.9866666666666667\n",
      "iteration no 5448: Loss: 0.2503736167356318, accuracy: 0.9866666666666667\n",
      "iteration no 5449: Loss: 0.25036958055067016, accuracy: 0.9866666666666667\n",
      "iteration no 5450: Loss: 0.2503672119206653, accuracy: 0.9866666666666667\n",
      "iteration no 5451: Loss: 0.2503694434129263, accuracy: 0.9866666666666667\n",
      "iteration no 5452: Loss: 0.25036777845170316, accuracy: 0.9866666666666667\n",
      "iteration no 5453: Loss: 0.25036254889481396, accuracy: 0.9866666666666667\n",
      "iteration no 5454: Loss: 0.25036733967963587, accuracy: 0.9866666666666667\n",
      "iteration no 5455: Loss: 0.2503637904008263, accuracy: 0.9866666666666667\n",
      "iteration no 5456: Loss: 0.2503610247484316, accuracy: 0.9866666666666667\n",
      "iteration no 5457: Loss: 0.25036147196289193, accuracy: 0.9866666666666667\n",
      "iteration no 5458: Loss: 0.2503623569696725, accuracy: 0.9866666666666667\n",
      "iteration no 5459: Loss: 0.2503578073838941, accuracy: 0.9866666666666667\n",
      "iteration no 5460: Loss: 0.250357342304548, accuracy: 0.9866666666666667\n",
      "iteration no 5461: Loss: 0.2503589627120598, accuracy: 0.9866666666666667\n",
      "iteration no 5462: Loss: 0.2503550126283989, accuracy: 0.9866666666666667\n",
      "iteration no 5463: Loss: 0.2503534282378858, accuracy: 0.9866666666666667\n",
      "iteration no 5464: Loss: 0.25035599728148555, accuracy: 0.9866666666666667\n",
      "iteration no 5465: Loss: 0.250352411302268, accuracy: 0.9866666666666667\n",
      "iteration no 5466: Loss: 0.25034835044139137, accuracy: 0.9866666666666667\n",
      "iteration no 5467: Loss: 0.25035277616985885, accuracy: 0.9866666666666667\n",
      "iteration no 5468: Loss: 0.250349071990843, accuracy: 0.9866666666666667\n",
      "iteration no 5469: Loss: 0.2503474247090981, accuracy: 0.9866666666666667\n",
      "iteration no 5470: Loss: 0.2503472610770892, accuracy: 0.9866666666666667\n",
      "iteration no 5471: Loss: 0.2503466915228688, accuracy: 0.9866666666666667\n",
      "iteration no 5472: Loss: 0.250342982585296, accuracy: 0.9866666666666667\n",
      "iteration no 5473: Loss: 0.25034432989525535, accuracy: 0.9866666666666667\n",
      "iteration no 5474: Loss: 0.2503447999196443, accuracy: 0.9866666666666667\n",
      "iteration no 5475: Loss: 0.25034039971622296, accuracy: 0.9866666666666667\n",
      "iteration no 5476: Loss: 0.2503389073406556, accuracy: 0.9866666666666667\n",
      "iteration no 5477: Loss: 0.250340761364328, accuracy: 0.9866666666666667\n",
      "iteration no 5478: Loss: 0.25033832001194845, accuracy: 0.9866666666666667\n",
      "iteration no 5479: Loss: 0.25033518776572605, accuracy: 0.9866666666666667\n",
      "iteration no 5480: Loss: 0.2503382669653778, accuracy: 0.9866666666666667\n",
      "iteration no 5481: Loss: 0.25033369856094284, accuracy: 0.9866666666666667\n",
      "iteration no 5482: Loss: 0.2503325692801237, accuracy: 0.9866666666666667\n",
      "iteration no 5483: Loss: 0.2503335384075932, accuracy: 0.9866666666666667\n",
      "iteration no 5484: Loss: 0.2503328795336971, accuracy: 0.9866666666666667\n",
      "iteration no 5485: Loss: 0.25032838529699364, accuracy: 0.9866666666666667\n",
      "iteration no 5486: Loss: 0.2503304573979428, accuracy: 0.9866666666666667\n",
      "iteration no 5487: Loss: 0.25032856356625105, accuracy: 0.9866666666666667\n",
      "iteration no 5488: Loss: 0.25032584390264134, accuracy: 0.9866666666666667\n",
      "iteration no 5489: Loss: 0.25032650598741407, accuracy: 0.9866666666666667\n",
      "iteration no 5490: Loss: 0.25032619201907497, accuracy: 0.9866666666666667\n",
      "iteration no 5491: Loss: 0.2503225434991879, accuracy: 0.9866666666666667\n",
      "iteration no 5492: Loss: 0.25032229117839233, accuracy: 0.9866666666666667\n",
      "iteration no 5493: Loss: 0.25032387844374, accuracy: 0.9866666666666667\n",
      "iteration no 5494: Loss: 0.25031904718264364, accuracy: 0.9866666666666667\n",
      "iteration no 5495: Loss: 0.25031997884185136, accuracy: 0.9866666666666667\n",
      "iteration no 5496: Loss: 0.250318835529801, accuracy: 0.9866666666666667\n",
      "iteration no 5497: Loss: 0.250317445956503, accuracy: 0.9866666666666667\n",
      "iteration no 5498: Loss: 0.2503149846637419, accuracy: 0.9866666666666667\n",
      "iteration no 5499: Loss: 0.2503165383628351, accuracy: 0.9866666666666667\n",
      "iteration no 5500: Loss: 0.2503141522899943, accuracy: 0.9866666666666667\n",
      "iteration no 5501: Loss: 0.25031245554888915, accuracy: 0.9866666666666667\n",
      "iteration no 5502: Loss: 0.25031174220954483, accuracy: 0.9866666666666667\n",
      "iteration no 5503: Loss: 0.25031116829096084, accuracy: 0.9866666666666667\n",
      "iteration no 5504: Loss: 0.2503093011142995, accuracy: 0.9866666666666667\n",
      "iteration no 5505: Loss: 0.2503086841670485, accuracy: 0.9866666666666667\n",
      "iteration no 5506: Loss: 0.2503086324021414, accuracy: 0.9866666666666667\n",
      "iteration no 5507: Loss: 0.250304605782974, accuracy: 0.9866666666666667\n",
      "iteration no 5508: Loss: 0.25030583333141454, accuracy: 0.9866666666666667\n",
      "iteration no 5509: Loss: 0.25030497624299997, accuracy: 0.9866666666666667\n",
      "iteration no 5510: Loss: 0.2503029978017977, accuracy: 0.9866666666666667\n",
      "iteration no 5511: Loss: 0.25030079071113565, accuracy: 0.9866666666666667\n",
      "iteration no 5512: Loss: 0.25030269693267326, accuracy: 0.9866666666666667\n",
      "iteration no 5513: Loss: 0.250298360161691, accuracy: 0.9866666666666667\n",
      "iteration no 5514: Loss: 0.2502992129871125, accuracy: 0.9866666666666667\n",
      "iteration no 5515: Loss: 0.2502987646394519, accuracy: 0.9866666666666667\n",
      "iteration no 5516: Loss: 0.2502955754999302, accuracy: 0.9866666666666667\n",
      "iteration no 5517: Loss: 0.25029530452360704, accuracy: 0.9866666666666667\n",
      "iteration no 5518: Loss: 0.2502952734577468, accuracy: 0.9866666666666667\n",
      "iteration no 5519: Loss: 0.25029358513796374, accuracy: 0.9866666666666667\n",
      "iteration no 5520: Loss: 0.2502923623682347, accuracy: 0.9866666666666667\n",
      "iteration no 5521: Loss: 0.2502918419691451, accuracy: 0.9866666666666667\n",
      "iteration no 5522: Loss: 0.25028932834944984, accuracy: 0.9866666666666667\n",
      "iteration no 5523: Loss: 0.2502894962271417, accuracy: 0.9866666666666667\n",
      "iteration no 5524: Loss: 0.2502874392804665, accuracy: 0.9866666666666667\n",
      "iteration no 5525: Loss: 0.2502879853792359, accuracy: 0.9866666666666667\n",
      "iteration no 5526: Loss: 0.25028496609838835, accuracy: 0.9866666666666667\n",
      "iteration no 5527: Loss: 0.2502850793150637, accuracy: 0.9866666666666667\n",
      "iteration no 5528: Loss: 0.250283529712785, accuracy: 0.9866666666666667\n",
      "iteration no 5529: Loss: 0.2502831952633327, accuracy: 0.9866666666666667\n",
      "iteration no 5530: Loss: 0.2502806929759783, accuracy: 0.9866666666666667\n",
      "iteration no 5531: Loss: 0.25028190158368463, accuracy: 0.9866666666666667\n",
      "iteration no 5532: Loss: 0.2502788464472302, accuracy: 0.9866666666666667\n",
      "iteration no 5533: Loss: 0.2502770016220931, accuracy: 0.9866666666666667\n",
      "iteration no 5534: Loss: 0.25027869882814596, accuracy: 0.9866666666666667\n",
      "iteration no 5535: Loss: 0.2502761378208813, accuracy: 0.9866666666666667\n",
      "iteration no 5536: Loss: 0.25027503888434305, accuracy: 0.9866666666666667\n",
      "iteration no 5537: Loss: 0.25027468216791726, accuracy: 0.9866666666666667\n",
      "iteration no 5538: Loss: 0.2502731863831591, accuracy: 0.9866666666666667\n",
      "iteration no 5539: Loss: 0.25027071527936545, accuracy: 0.9866666666666667\n",
      "iteration no 5540: Loss: 0.2502722998972294, accuracy: 0.9866666666666667\n",
      "iteration no 5541: Loss: 0.250268882549409, accuracy: 0.9866666666666667\n",
      "iteration no 5542: Loss: 0.25026906434778295, accuracy: 0.9866666666666667\n",
      "iteration no 5543: Loss: 0.2502667950978952, accuracy: 0.9866666666666667\n",
      "iteration no 5544: Loss: 0.2502671008207216, accuracy: 0.9866666666666667\n",
      "iteration no 5545: Loss: 0.25026560348432375, accuracy: 0.9866666666666667\n",
      "iteration no 5546: Loss: 0.2502644440314864, accuracy: 0.9866666666666667\n",
      "iteration no 5547: Loss: 0.2502628279235398, accuracy: 0.9866666666666667\n",
      "iteration no 5548: Loss: 0.2502625575728156, accuracy: 0.9866666666666667\n",
      "iteration no 5549: Loss: 0.25026132417870983, accuracy: 0.9866666666666667\n",
      "iteration no 5550: Loss: 0.25025971192846563, accuracy: 0.9866666666666667\n",
      "iteration no 5551: Loss: 0.250260102048265, accuracy: 0.9866666666666667\n",
      "iteration no 5552: Loss: 0.25025659752314067, accuracy: 0.9866666666666667\n",
      "iteration no 5553: Loss: 0.250257406084714, accuracy: 0.9866666666666667\n",
      "iteration no 5554: Loss: 0.25025671624596346, accuracy: 0.9866666666666667\n",
      "iteration no 5555: Loss: 0.2502546664159573, accuracy: 0.9866666666666667\n",
      "iteration no 5556: Loss: 0.2502530950058202, accuracy: 0.9866666666666667\n",
      "iteration no 5557: Loss: 0.2502542039640365, accuracy: 0.9866666666666667\n",
      "iteration no 5558: Loss: 0.2502495753281208, accuracy: 0.9866666666666667\n",
      "iteration no 5559: Loss: 0.25025198440185475, accuracy: 0.9866666666666667\n",
      "iteration no 5560: Loss: 0.25025037991551885, accuracy: 0.9866666666666667\n",
      "iteration no 5561: Loss: 0.25024758257452795, accuracy: 0.9866666666666667\n",
      "iteration no 5562: Loss: 0.2502472895288799, accuracy: 0.9866666666666667\n",
      "iteration no 5563: Loss: 0.25024689807703665, accuracy: 0.9866666666666667\n",
      "iteration no 5564: Loss: 0.25024495025727894, accuracy: 0.9866666666666667\n",
      "iteration no 5565: Loss: 0.2502441447142879, accuracy: 0.9866666666666667\n",
      "iteration no 5566: Loss: 0.25024445656698774, accuracy: 0.9866666666666667\n",
      "iteration no 5567: Loss: 0.2502412043427076, accuracy: 0.9866666666666667\n",
      "iteration no 5568: Loss: 0.2502409858653003, accuracy: 0.9866666666666667\n",
      "iteration no 5569: Loss: 0.25023984843444963, accuracy: 0.9866666666666667\n",
      "iteration no 5570: Loss: 0.2502397931297182, accuracy: 0.9866666666666667\n",
      "iteration no 5571: Loss: 0.2502367730981591, accuracy: 0.9866666666666667\n",
      "iteration no 5572: Loss: 0.2502385182404231, accuracy: 0.9866666666666667\n",
      "iteration no 5573: Loss: 0.2502350105209238, accuracy: 0.9866666666666667\n",
      "iteration no 5574: Loss: 0.2502343030657214, accuracy: 0.9866666666666667\n",
      "iteration no 5575: Loss: 0.2502343418404955, accuracy: 0.9866666666666667\n",
      "iteration no 5576: Loss: 0.25023362900015367, accuracy: 0.9866666666666667\n",
      "iteration no 5577: Loss: 0.25023054545471307, accuracy: 0.9866666666666667\n",
      "iteration no 5578: Loss: 0.2502311690449937, accuracy: 0.9866666666666667\n",
      "iteration no 5579: Loss: 0.2502298349218554, accuracy: 0.9866666666666667\n",
      "iteration no 5580: Loss: 0.2502273495407609, accuracy: 0.9866666666666667\n",
      "iteration no 5581: Loss: 0.25022904801383594, accuracy: 0.9866666666666667\n",
      "iteration no 5582: Loss: 0.25022699239272767, accuracy: 0.9866666666666667\n",
      "iteration no 5583: Loss: 0.250224261001438, accuracy: 0.9866666666666667\n",
      "iteration no 5584: Loss: 0.2502239947537314, accuracy: 0.9866666666666667\n",
      "iteration no 5585: Loss: 0.250225657297792, accuracy: 0.9866666666666667\n",
      "iteration no 5586: Loss: 0.2502201415402424, accuracy: 0.9866666666666667\n",
      "iteration no 5587: Loss: 0.2502233099513214, accuracy: 0.9866666666666667\n",
      "iteration no 5588: Loss: 0.2502207811218154, accuracy: 0.9866666666666667\n",
      "iteration no 5589: Loss: 0.2502181076283205, accuracy: 0.9866666666666667\n",
      "iteration no 5590: Loss: 0.25021887872003323, accuracy: 0.9866666666666667\n",
      "iteration no 5591: Loss: 0.2502192451367318, accuracy: 0.9866666666666667\n",
      "iteration no 5592: Loss: 0.2502152198921225, accuracy: 0.9866666666666667\n",
      "iteration no 5593: Loss: 0.25021478173061945, accuracy: 0.9866666666666667\n",
      "iteration no 5594: Loss: 0.2502168623240977, accuracy: 0.9866666666666667\n",
      "iteration no 5595: Loss: 0.250212036598699, accuracy: 0.9866666666666667\n",
      "iteration no 5596: Loss: 0.25021253727339704, accuracy: 0.9866666666666667\n",
      "iteration no 5597: Loss: 0.250212591700568, accuracy: 0.9866666666666667\n",
      "iteration no 5598: Loss: 0.2502104349280263, accuracy: 0.9866666666666667\n",
      "iteration no 5599: Loss: 0.25020799134242083, accuracy: 0.9866666666666667\n",
      "iteration no 5600: Loss: 0.25021113078454166, accuracy: 0.9866666666666667\n",
      "iteration no 5601: Loss: 0.2502066324709621, accuracy: 0.9866666666666667\n",
      "iteration no 5602: Loss: 0.2502055211444261, accuracy: 0.9866666666666667\n",
      "iteration no 5603: Loss: 0.2502077406775532, accuracy: 0.9866666666666667\n",
      "iteration no 5604: Loss: 0.250203755153421, accuracy: 0.9866666666666667\n",
      "iteration no 5605: Loss: 0.250202718423869, accuracy: 0.9866666666666667\n",
      "iteration no 5606: Loss: 0.25020357698195933, accuracy: 0.9866666666666667\n",
      "iteration no 5607: Loss: 0.25020224538119445, accuracy: 0.9866666666666667\n",
      "iteration no 5608: Loss: 0.2501986088711849, accuracy: 0.9866666666666667\n",
      "iteration no 5609: Loss: 0.25020178394192083, accuracy: 0.9866666666666667\n",
      "iteration no 5610: Loss: 0.25019803701859095, accuracy: 0.9866666666666667\n",
      "iteration no 5611: Loss: 0.25019669021323704, accuracy: 0.9866666666666667\n",
      "iteration no 5612: Loss: 0.25019845753309605, accuracy: 0.9866666666666667\n",
      "iteration no 5613: Loss: 0.25019612540677527, accuracy: 0.9866666666666667\n",
      "iteration no 5614: Loss: 0.2501926836292805, accuracy: 0.9866666666666667\n",
      "iteration no 5615: Loss: 0.2501946775974862, accuracy: 0.9866666666666667\n",
      "iteration no 5616: Loss: 0.2501938223088821, accuracy: 0.9866666666666667\n",
      "iteration no 5617: Loss: 0.2501894058435798, accuracy: 0.9866666666666667\n",
      "iteration no 5618: Loss: 0.250193074370872, accuracy: 0.9866666666666667\n",
      "iteration no 5619: Loss: 0.25018920972964015, accuracy: 0.9866666666666667\n",
      "iteration no 5620: Loss: 0.25018726341319764, accuracy: 0.9866666666666667\n",
      "iteration no 5621: Loss: 0.25018905499880834, accuracy: 0.9866666666666667\n",
      "iteration no 5622: Loss: 0.25018840257412067, accuracy: 0.9866666666666667\n",
      "iteration no 5623: Loss: 0.25018396654131936, accuracy: 0.9866666666666667\n",
      "iteration no 5624: Loss: 0.2501851703329684, accuracy: 0.9866666666666667\n",
      "iteration no 5625: Loss: 0.2501855713182862, accuracy: 0.9866666666666667\n",
      "iteration no 5626: Loss: 0.25018161967085656, accuracy: 0.9866666666666667\n",
      "iteration no 5627: Loss: 0.250182849739822, accuracy: 0.9866666666666667\n",
      "iteration no 5628: Loss: 0.2501816257239582, accuracy: 0.9866666666666667\n",
      "iteration no 5629: Loss: 0.2501790714282059, accuracy: 0.9866666666666667\n",
      "iteration no 5630: Loss: 0.25017827790805086, accuracy: 0.9866666666666667\n",
      "iteration no 5631: Loss: 0.25018120451868087, accuracy: 0.9866666666666667\n",
      "iteration no 5632: Loss: 0.25017572819861034, accuracy: 0.9866666666666667\n",
      "iteration no 5633: Loss: 0.2501759678756426, accuracy: 0.9866666666666667\n",
      "iteration no 5634: Loss: 0.250176850596135, accuracy: 0.9866666666666667\n",
      "iteration no 5635: Loss: 0.2501729074894876, accuracy: 0.9866666666666667\n",
      "iteration no 5636: Loss: 0.2501734534651758, accuracy: 0.9866666666666667\n",
      "iteration no 5637: Loss: 0.2501740934556099, accuracy: 0.9866666666666667\n",
      "iteration no 5638: Loss: 0.2501707719701318, accuracy: 0.9866666666666667\n",
      "iteration no 5639: Loss: 0.25016884747883494, accuracy: 0.9866666666666667\n",
      "iteration no 5640: Loss: 0.2501725404848737, accuracy: 0.9866666666666667\n",
      "iteration no 5641: Loss: 0.2501670095293168, accuracy: 0.9866666666666667\n",
      "iteration no 5642: Loss: 0.250167623827647, accuracy: 0.9866666666666667\n",
      "iteration no 5643: Loss: 0.25016842803676237, accuracy: 0.9866666666666667\n",
      "iteration no 5644: Loss: 0.25016485677055295, accuracy: 0.9866666666666667\n",
      "iteration no 5645: Loss: 0.2501636010339713, accuracy: 0.9866666666666667\n",
      "iteration no 5646: Loss: 0.2501657248475132, accuracy: 0.9866666666666667\n",
      "iteration no 5647: Loss: 0.2501625159963815, accuracy: 0.9866666666666667\n",
      "iteration no 5648: Loss: 0.2501600787988657, accuracy: 0.9866666666666667\n",
      "iteration no 5649: Loss: 0.2501640839081538, accuracy: 0.9866666666666667\n",
      "iteration no 5650: Loss: 0.2501582174376824, accuracy: 0.9866666666666667\n",
      "iteration no 5651: Loss: 0.25015835413280524, accuracy: 0.9866666666666667\n",
      "iteration no 5652: Loss: 0.25015988334521305, accuracy: 0.9866666666666667\n",
      "iteration no 5653: Loss: 0.2501572356551487, accuracy: 0.9866666666666667\n",
      "iteration no 5654: Loss: 0.2501545393624953, accuracy: 0.9866666666666667\n",
      "iteration no 5655: Loss: 0.2501573356665303, accuracy: 0.9866666666666667\n",
      "iteration no 5656: Loss: 0.2501534510448874, accuracy: 0.9866666666666667\n",
      "iteration no 5657: Loss: 0.25015182788075385, accuracy: 0.9866666666666667\n",
      "iteration no 5658: Loss: 0.25015548170265955, accuracy: 0.9866666666666667\n",
      "iteration no 5659: Loss: 0.2501506104353782, accuracy: 0.9866666666666667\n",
      "iteration no 5660: Loss: 0.250149046173284, accuracy: 0.9866666666666667\n",
      "iteration no 5661: Loss: 0.25015030305860564, accuracy: 0.9866666666666667\n",
      "iteration no 5662: Loss: 0.25014973994453144, accuracy: 0.9866666666666667\n",
      "iteration no 5663: Loss: 0.2501455325793697, accuracy: 0.9866666666666667\n",
      "iteration no 5664: Loss: 0.25014948506325074, accuracy: 0.9866666666666667\n",
      "iteration no 5665: Loss: 0.25014530087118786, accuracy: 0.9866666666666667\n",
      "iteration no 5666: Loss: 0.2501422804812474, accuracy: 0.9866666666666667\n",
      "iteration no 5667: Loss: 0.2501466102396118, accuracy: 0.9866666666666667\n",
      "iteration no 5668: Loss: 0.2501433561015539, accuracy: 0.9866666666666667\n",
      "iteration no 5669: Loss: 0.25014049042095304, accuracy: 0.9866666666666667\n",
      "iteration no 5670: Loss: 0.2501420405556789, accuracy: 0.9866666666666667\n",
      "iteration no 5671: Loss: 0.2501409803482042, accuracy: 0.9866666666666667\n",
      "iteration no 5672: Loss: 0.2501369090921036, accuracy: 0.9866666666666667\n",
      "iteration no 5673: Loss: 0.25014101011444545, accuracy: 0.9866666666666667\n",
      "iteration no 5674: Loss: 0.25013773184831195, accuracy: 0.9866666666666667\n",
      "iteration no 5675: Loss: 0.2501349457459366, accuracy: 0.9866666666666667\n",
      "iteration no 5676: Loss: 0.2501367121386518, accuracy: 0.9866666666666667\n",
      "iteration no 5677: Loss: 0.25013541297425335, accuracy: 0.9866666666666667\n",
      "iteration no 5678: Loss: 0.2501327160616414, accuracy: 0.9866666666666667\n",
      "iteration no 5679: Loss: 0.250133447981698, accuracy: 0.9866666666666667\n",
      "iteration no 5680: Loss: 0.25013313055197506, accuracy: 0.9866666666666667\n",
      "iteration no 5681: Loss: 0.25012905424857507, accuracy: 0.9866666666666667\n",
      "iteration no 5682: Loss: 0.2501317128614318, accuracy: 0.9866666666666667\n",
      "iteration no 5683: Loss: 0.2501297156149889, accuracy: 0.9866666666666667\n",
      "iteration no 5684: Loss: 0.2501279371204073, accuracy: 0.9866666666666667\n",
      "iteration no 5685: Loss: 0.25012771518065796, accuracy: 0.9866666666666667\n",
      "iteration no 5686: Loss: 0.250127581972306, accuracy: 0.9866666666666667\n",
      "iteration no 5687: Loss: 0.25012467238322145, accuracy: 0.9866666666666667\n",
      "iteration no 5688: Loss: 0.25012568900575954, accuracy: 0.9866666666666667\n",
      "iteration no 5689: Loss: 0.2501243048526423, accuracy: 0.9866666666666667\n",
      "iteration no 5690: Loss: 0.25012169394937556, accuracy: 0.9866666666666667\n",
      "iteration no 5691: Loss: 0.2501238787891745, accuracy: 0.9866666666666667\n",
      "iteration no 5692: Loss: 0.2501197920401037, accuracy: 0.9866666666666667\n",
      "iteration no 5693: Loss: 0.2501210294997368, accuracy: 0.9866666666666667\n",
      "iteration no 5694: Loss: 0.25012018908547096, accuracy: 0.9866666666666667\n",
      "iteration no 5695: Loss: 0.2501182113940572, accuracy: 0.9866666666666667\n",
      "iteration no 5696: Loss: 0.2501174245640744, accuracy: 0.9866666666666667\n",
      "iteration no 5697: Loss: 0.2501177315191576, accuracy: 0.9866666666666667\n",
      "iteration no 5698: Loss: 0.2501146403626, accuracy: 0.9866666666666667\n",
      "iteration no 5699: Loss: 0.25011493843268906, accuracy: 0.9866666666666667\n",
      "iteration no 5700: Loss: 0.2501162606579921, accuracy: 0.9866666666666667\n",
      "iteration no 5701: Loss: 0.25011086931484316, accuracy: 0.9866666666666667\n",
      "iteration no 5702: Loss: 0.25011316668861383, accuracy: 0.9866666666666667\n",
      "iteration no 5703: Loss: 0.2501123424232462, accuracy: 0.9866666666666667\n",
      "iteration no 5704: Loss: 0.250110362876081, accuracy: 0.9866666666666667\n",
      "iteration no 5705: Loss: 0.25010907898006707, accuracy: 0.9866666666666667\n",
      "iteration no 5706: Loss: 0.25011061872688434, accuracy: 0.9866666666666667\n",
      "iteration no 5707: Loss: 0.2501066513034841, accuracy: 0.9866666666666667\n",
      "iteration no 5708: Loss: 0.2501063834442099, accuracy: 0.9866666666666667\n",
      "iteration no 5709: Loss: 0.25010916208688566, accuracy: 0.9866666666666667\n",
      "iteration no 5710: Loss: 0.2501036555879084, accuracy: 0.9866666666666667\n",
      "iteration no 5711: Loss: 0.25010406666699775, accuracy: 0.9866666666666667\n",
      "iteration no 5712: Loss: 0.25010459710681987, accuracy: 0.9866666666666667\n",
      "iteration no 5713: Loss: 0.25010262571368624, accuracy: 0.9866666666666667\n",
      "iteration no 5714: Loss: 0.2501005730601581, accuracy: 0.9866666666666667\n",
      "iteration no 5715: Loss: 0.2501036096021422, accuracy: 0.9866666666666667\n",
      "iteration no 5716: Loss: 0.25009906072940596, accuracy: 0.9866666666666667\n",
      "iteration no 5717: Loss: 0.250098100628446, accuracy: 0.9866666666666667\n",
      "iteration no 5718: Loss: 0.2501002386278596, accuracy: 0.9866666666666667\n",
      "iteration no 5719: Loss: 0.2500967865511222, accuracy: 0.9866666666666667\n",
      "iteration no 5720: Loss: 0.25009641977008723, accuracy: 0.9866666666666667\n",
      "iteration no 5721: Loss: 0.25009602693268307, accuracy: 0.9866666666666667\n",
      "iteration no 5722: Loss: 0.25009559700461476, accuracy: 0.9866666666666667\n",
      "iteration no 5723: Loss: 0.250092658732707, accuracy: 0.9866666666666667\n",
      "iteration no 5724: Loss: 0.2500947011803951, accuracy: 0.9866666666666667\n",
      "iteration no 5725: Loss: 0.2500921056976631, accuracy: 0.9866666666666667\n",
      "iteration no 5726: Loss: 0.2500912353529159, accuracy: 0.9866666666666667\n",
      "iteration no 5727: Loss: 0.25009105856963243, accuracy: 0.9866666666666667\n",
      "iteration no 5728: Loss: 0.2500896945559247, accuracy: 0.9866666666666667\n",
      "iteration no 5729: Loss: 0.2500889643080246, accuracy: 0.9866666666666667\n",
      "iteration no 5730: Loss: 0.2500873469192225, accuracy: 0.9866666666666667\n",
      "iteration no 5731: Loss: 0.250088136314635, accuracy: 0.9866666666666667\n",
      "iteration no 5732: Loss: 0.25008568501590545, accuracy: 0.9866666666666667\n",
      "iteration no 5733: Loss: 0.25008581173174155, accuracy: 0.9866666666666667\n",
      "iteration no 5734: Loss: 0.2500842088547027, accuracy: 0.9866666666666667\n",
      "iteration no 5735: Loss: 0.25008453754138327, accuracy: 0.9866666666666667\n",
      "iteration no 5736: Loss: 0.2500827127381554, accuracy: 0.9866666666666667\n",
      "iteration no 5737: Loss: 0.25008199636091477, accuracy: 0.9866666666666667\n",
      "iteration no 5738: Loss: 0.25008195669040906, accuracy: 0.9866666666666667\n",
      "iteration no 5739: Loss: 0.2500792191262874, accuracy: 0.9866666666666667\n",
      "iteration no 5740: Loss: 0.25007952499608116, accuracy: 0.9866666666666667\n",
      "iteration no 5741: Loss: 0.2500788829807017, accuracy: 0.9866666666666667\n",
      "iteration no 5742: Loss: 0.2500786458964518, accuracy: 0.9866666666666667\n",
      "iteration no 5743: Loss: 0.2500754131005497, accuracy: 0.9866666666666667\n",
      "iteration no 5744: Loss: 0.25007722065021354, accuracy: 0.9866666666666667\n",
      "iteration no 5745: Loss: 0.25007530737371453, accuracy: 0.9866666666666667\n",
      "iteration no 5746: Loss: 0.2500734412555291, accuracy: 0.9866666666666667\n",
      "iteration no 5747: Loss: 0.2500750947291813, accuracy: 0.9866666666666667\n",
      "iteration no 5748: Loss: 0.2500724409680486, accuracy: 0.9866666666666667\n",
      "iteration no 5749: Loss: 0.250070239483027, accuracy: 0.9866666666666667\n",
      "iteration no 5750: Loss: 0.25007174714245967, accuracy: 0.9866666666666667\n",
      "iteration no 5751: Loss: 0.25007179840457916, accuracy: 0.9866666666666667\n",
      "iteration no 5752: Loss: 0.2500664109014363, accuracy: 0.9866666666666667\n",
      "iteration no 5753: Loss: 0.25007107331596196, accuracy: 0.9866666666666667\n",
      "iteration no 5754: Loss: 0.25006792412131496, accuracy: 0.9866666666666667\n",
      "iteration no 5755: Loss: 0.2500647704878621, accuracy: 0.9866666666666667\n",
      "iteration no 5756: Loss: 0.25006740293921637, accuracy: 0.9866666666666667\n",
      "iteration no 5757: Loss: 0.25006587957287524, accuracy: 0.9866666666666667\n",
      "iteration no 5758: Loss: 0.2500627630407134, accuracy: 0.9866666666666667\n",
      "iteration no 5759: Loss: 0.2500635741190542, accuracy: 0.9866666666666667\n",
      "iteration no 5760: Loss: 0.2500646342089893, accuracy: 0.9866666666666667\n",
      "iteration no 5761: Loss: 0.2500591653556498, accuracy: 0.9866666666666667\n",
      "iteration no 5762: Loss: 0.2500627107780316, accuracy: 0.9866666666666667\n",
      "iteration no 5763: Loss: 0.2500611347342303, accuracy: 0.9866666666666667\n",
      "iteration no 5764: Loss: 0.2500576641138982, accuracy: 0.9866666666666667\n",
      "iteration no 5765: Loss: 0.25005884103674, accuracy: 0.9866666666666667\n",
      "iteration no 5766: Loss: 0.2500584163706745, accuracy: 0.9866666666666667\n",
      "iteration no 5767: Loss: 0.2500557620667069, accuracy: 0.9866666666666667\n",
      "iteration no 5768: Loss: 0.2500555415763887, accuracy: 0.9866666666666667\n",
      "iteration no 5769: Loss: 0.25005752096826717, accuracy: 0.9866666666666667\n",
      "iteration no 5770: Loss: 0.25005209155837516, accuracy: 0.9866666666666667\n",
      "iteration no 5771: Loss: 0.25005427763401195, accuracy: 0.9866666666666667\n",
      "iteration no 5772: Loss: 0.2500534961910314, accuracy: 0.9866666666666667\n",
      "iteration no 5773: Loss: 0.2500512406471025, accuracy: 0.9866666666666667\n",
      "iteration no 5774: Loss: 0.25005107221874556, accuracy: 0.9866666666666667\n",
      "iteration no 5775: Loss: 0.25005050807989165, accuracy: 0.9866666666666667\n",
      "iteration no 5776: Loss: 0.25004840638681103, accuracy: 0.9866666666666667\n",
      "iteration no 5777: Loss: 0.2500483985190253, accuracy: 0.9866666666666667\n",
      "iteration no 5778: Loss: 0.25004848025229826, accuracy: 0.9866666666666667\n",
      "iteration no 5779: Loss: 0.25004571232019546, accuracy: 0.9866666666666667\n",
      "iteration no 5780: Loss: 0.25004684939316685, accuracy: 0.9866666666666667\n",
      "iteration no 5781: Loss: 0.2500438157232106, accuracy: 0.9866666666666667\n",
      "iteration no 5782: Loss: 0.2500449055575009, accuracy: 0.9866666666666667\n",
      "iteration no 5783: Loss: 0.25004394395293544, accuracy: 0.9866666666666667\n",
      "iteration no 5784: Loss: 0.2500421600848052, accuracy: 0.9866666666666667\n",
      "iteration no 5785: Loss: 0.2500422611307257, accuracy: 0.9866666666666667\n",
      "iteration no 5786: Loss: 0.2500405762213135, accuracy: 0.9866666666666667\n",
      "iteration no 5787: Loss: 0.25003900591388034, accuracy: 0.9866666666666667\n",
      "iteration no 5788: Loss: 0.25004014613635656, accuracy: 0.9866666666666667\n",
      "iteration no 5789: Loss: 0.25003983501457994, accuracy: 0.9866666666666667\n",
      "iteration no 5790: Loss: 0.25003523776402437, accuracy: 0.9866666666666667\n",
      "iteration no 5791: Loss: 0.25003848585008215, accuracy: 0.9866666666666667\n",
      "iteration no 5792: Loss: 0.25003577884930855, accuracy: 0.9866666666666667\n",
      "iteration no 5793: Loss: 0.2500337945612431, accuracy: 0.9866666666666667\n",
      "iteration no 5794: Loss: 0.2500363279532309, accuracy: 0.9866666666666667\n",
      "iteration no 5795: Loss: 0.25003366955482564, accuracy: 0.9866666666666667\n",
      "iteration no 5796: Loss: 0.25003020658996944, accuracy: 0.9866666666666667\n",
      "iteration no 5797: Loss: 0.25003335198270876, accuracy: 0.9866666666666667\n",
      "iteration no 5798: Loss: 0.250032840216555, accuracy: 0.9866666666666667\n",
      "iteration no 5799: Loss: 0.25002738465648694, accuracy: 0.9866666666666667\n",
      "iteration no 5800: Loss: 0.25003199709009416, accuracy: 0.9866666666666667\n",
      "iteration no 5801: Loss: 0.2500287843237331, accuracy: 0.9866666666666667\n",
      "iteration no 5802: Loss: 0.2500259278328913, accuracy: 0.9866666666666667\n",
      "iteration no 5803: Loss: 0.25002831424852784, accuracy: 0.9866666666666667\n",
      "iteration no 5804: Loss: 0.2500273907113485, accuracy: 0.9866666666666667\n",
      "iteration no 5805: Loss: 0.250023516285103, accuracy: 0.9866666666666667\n",
      "iteration no 5806: Loss: 0.25002502548933586, accuracy: 0.9866666666666667\n",
      "iteration no 5807: Loss: 0.2500245944579197, accuracy: 0.9866666666666667\n",
      "iteration no 5808: Loss: 0.2500203996543853, accuracy: 0.9866666666666667\n",
      "iteration no 5809: Loss: 0.2500241047980861, accuracy: 0.9866666666666667\n",
      "iteration no 5810: Loss: 0.2500205692301605, accuracy: 0.9866666666666667\n",
      "iteration no 5811: Loss: 0.2500197013981208, accuracy: 0.9866666666666667\n",
      "iteration no 5812: Loss: 0.2500193836125577, accuracy: 0.9866666666666667\n",
      "iteration no 5813: Loss: 0.25001873487971216, accuracy: 0.9866666666666667\n",
      "iteration no 5814: Loss: 0.2500173163706738, accuracy: 0.9866666666666667\n",
      "iteration no 5815: Loss: 0.2500164966139275, accuracy: 0.9866666666666667\n",
      "iteration no 5816: Loss: 0.2500163672975062, accuracy: 0.9866666666666667\n",
      "iteration no 5817: Loss: 0.25001419291645466, accuracy: 0.9866666666666667\n",
      "iteration no 5818: Loss: 0.25001434590072996, accuracy: 0.9866666666666667\n",
      "iteration no 5819: Loss: 0.2500122333420325, accuracy: 0.9866666666666667\n",
      "iteration no 5820: Loss: 0.2500138276568904, accuracy: 0.9866666666666667\n",
      "iteration no 5821: Loss: 0.25001058536346277, accuracy: 0.9866666666666667\n",
      "iteration no 5822: Loss: 0.2500107358242629, accuracy: 0.9866666666666667\n",
      "iteration no 5823: Loss: 0.25001010146185887, accuracy: 0.9866666666666667\n",
      "iteration no 5824: Loss: 0.25000788678461916, accuracy: 0.9866666666666667\n",
      "iteration no 5825: Loss: 0.25000840471405816, accuracy: 0.9866666666666667\n",
      "iteration no 5826: Loss: 0.2500080840056328, accuracy: 0.9866666666666667\n",
      "iteration no 5827: Loss: 0.2500062010774502, accuracy: 0.9866666666666667\n",
      "iteration no 5828: Loss: 0.2500042446502888, accuracy: 0.9866666666666667\n",
      "iteration no 5829: Loss: 0.250007522181834, accuracy: 0.9866666666666667\n",
      "iteration no 5830: Loss: 0.25000258252813545, accuracy: 0.9866666666666667\n",
      "iteration no 5831: Loss: 0.25000384279972276, accuracy: 0.9866666666666667\n",
      "iteration no 5832: Loss: 0.25000359450264426, accuracy: 0.9866666666666667\n",
      "iteration no 5833: Loss: 0.2500003307200014, accuracy: 0.9866666666666667\n",
      "iteration no 5834: Loss: 0.25000033674392497, accuracy: 0.9866666666666667\n",
      "iteration no 5835: Loss: 0.2500014343824506, accuracy: 0.9866666666666667\n",
      "iteration no 5836: Loss: 0.24999822331145988, accuracy: 0.9866666666666667\n",
      "iteration no 5837: Loss: 0.24999778792519445, accuracy: 0.9866666666666667\n",
      "iteration no 5838: Loss: 0.2499991325310836, accuracy: 0.9866666666666667\n",
      "iteration no 5839: Loss: 0.24999390912069203, accuracy: 0.9866666666666667\n",
      "iteration no 5840: Loss: 0.249997150665142, accuracy: 0.9866666666666667\n",
      "iteration no 5841: Loss: 0.24999561107475163, accuracy: 0.9866666666666667\n",
      "iteration no 5842: Loss: 0.24999281154119002, accuracy: 0.9866666666666667\n",
      "iteration no 5843: Loss: 0.2499932567641278, accuracy: 0.9866666666666667\n",
      "iteration no 5844: Loss: 0.24999325849445136, accuracy: 0.9866666666666667\n",
      "iteration no 5845: Loss: 0.2499900831977872, accuracy: 0.9866666666666667\n",
      "iteration no 5846: Loss: 0.24999141749317005, accuracy: 0.9866666666666667\n",
      "iteration no 5847: Loss: 0.24999125090602792, accuracy: 0.9866666666666667\n",
      "iteration no 5848: Loss: 0.24998647222584386, accuracy: 0.9866666666666667\n",
      "iteration no 5849: Loss: 0.2499899126169025, accuracy: 0.9866666666666667\n",
      "iteration no 5850: Loss: 0.249986731723378, accuracy: 0.9866666666666667\n",
      "iteration no 5851: Loss: 0.24998665367867168, accuracy: 0.9866666666666667\n",
      "iteration no 5852: Loss: 0.24998571309816725, accuracy: 0.9866666666666667\n",
      "iteration no 5853: Loss: 0.24998535064876845, accuracy: 0.9866666666666667\n",
      "iteration no 5854: Loss: 0.24998313723724258, accuracy: 0.9866666666666667\n",
      "iteration no 5855: Loss: 0.2499829870964228, accuracy: 0.9866666666666667\n",
      "iteration no 5856: Loss: 0.24998300211162272, accuracy: 0.9866666666666667\n",
      "iteration no 5857: Loss: 0.24998119569265248, accuracy: 0.9866666666666667\n",
      "iteration no 5858: Loss: 0.24998087783897008, accuracy: 0.9866666666666667\n",
      "iteration no 5859: Loss: 0.24997884559426164, accuracy: 0.9866666666666667\n",
      "iteration no 5860: Loss: 0.24998008594765314, accuracy: 0.9866666666666667\n",
      "iteration no 5861: Loss: 0.24997708939114838, accuracy: 0.9866666666666667\n",
      "iteration no 5862: Loss: 0.24997783579300636, accuracy: 0.9866666666666667\n",
      "iteration no 5863: Loss: 0.24997736058968517, accuracy: 0.9866666666666667\n",
      "iteration no 5864: Loss: 0.24997479427247282, accuracy: 0.9866666666666667\n",
      "iteration no 5865: Loss: 0.24997421152428628, accuracy: 0.9866666666666667\n",
      "iteration no 5866: Loss: 0.24997540769541501, accuracy: 0.9866666666666667\n",
      "iteration no 5867: Loss: 0.24997283158843203, accuracy: 0.9866666666666667\n",
      "iteration no 5868: Loss: 0.24997188434712364, accuracy: 0.9866666666666667\n",
      "iteration no 5869: Loss: 0.2499742018478973, accuracy: 0.9866666666666667\n",
      "iteration no 5870: Loss: 0.24996827424726603, accuracy: 0.9866666666666667\n",
      "iteration no 5871: Loss: 0.2499711430266917, accuracy: 0.9866666666666667\n",
      "iteration no 5872: Loss: 0.24997042239653683, accuracy: 0.9866666666666667\n",
      "iteration no 5873: Loss: 0.24996737809152736, accuracy: 0.9866666666666667\n",
      "iteration no 5874: Loss: 0.24996775758327794, accuracy: 0.9866666666666667\n",
      "iteration no 5875: Loss: 0.24996763965881133, accuracy: 0.9866666666666667\n",
      "iteration no 5876: Loss: 0.2499638483166709, accuracy: 0.9866666666666667\n",
      "iteration no 5877: Loss: 0.24996653947889652, accuracy: 0.9866666666666667\n",
      "iteration no 5878: Loss: 0.2499664798128511, accuracy: 0.9866666666666667\n",
      "iteration no 5879: Loss: 0.2499605793177963, accuracy: 0.9866666666666667\n",
      "iteration no 5880: Loss: 0.2499640794784372, accuracy: 0.9866666666666667\n",
      "iteration no 5881: Loss: 0.24996199633990313, accuracy: 0.9866666666666667\n",
      "iteration no 5882: Loss: 0.24996014181253373, accuracy: 0.9866666666666667\n",
      "iteration no 5883: Loss: 0.24996141253432927, accuracy: 0.9866666666666667\n",
      "iteration no 5884: Loss: 0.24996015587503478, accuracy: 0.9866666666666667\n",
      "iteration no 5885: Loss: 0.24995618598033706, accuracy: 0.9866666666666667\n",
      "iteration no 5886: Loss: 0.2499592637333995, accuracy: 0.9866666666666667\n",
      "iteration no 5887: Loss: 0.24995760338996592, accuracy: 0.9866666666666667\n",
      "iteration no 5888: Loss: 0.2499548259614224, accuracy: 0.9866666666666667\n",
      "iteration no 5889: Loss: 0.2499569031929893, accuracy: 0.9866666666666667\n",
      "iteration no 5890: Loss: 0.24995343168883072, accuracy: 0.9866666666666667\n",
      "iteration no 5891: Loss: 0.24995396227664654, accuracy: 0.9866666666666667\n",
      "iteration no 5892: Loss: 0.24995289845532492, accuracy: 0.9866666666666667\n",
      "iteration no 5893: Loss: 0.24995263107001991, accuracy: 0.9866666666666667\n",
      "iteration no 5894: Loss: 0.24995103309071431, accuracy: 0.9866666666666667\n",
      "iteration no 5895: Loss: 0.24995062770258686, accuracy: 0.9866666666666667\n",
      "iteration no 5896: Loss: 0.24994929950575143, accuracy: 0.9866666666666667\n",
      "iteration no 5897: Loss: 0.2499494989607423, accuracy: 0.9866666666666667\n",
      "iteration no 5898: Loss: 0.24994844066106675, accuracy: 0.9866666666666667\n",
      "iteration no 5899: Loss: 0.24994567184029376, accuracy: 0.9866666666666667\n",
      "iteration no 5900: Loss: 0.2499487967565367, accuracy: 0.9866666666666667\n",
      "iteration no 5901: Loss: 0.24994398476864688, accuracy: 0.9866666666666667\n",
      "iteration no 5902: Loss: 0.24994508424869907, accuracy: 0.9866666666666667\n",
      "iteration no 5903: Loss: 0.2499454214378415, accuracy: 0.9866666666666667\n",
      "iteration no 5904: Loss: 0.24994262010539853, accuracy: 0.9866666666666667\n",
      "iteration no 5905: Loss: 0.2499431254644216, accuracy: 0.9866666666666667\n",
      "iteration no 5906: Loss: 0.24994216258292312, accuracy: 0.9866666666666667\n",
      "iteration no 5907: Loss: 0.24993913120652367, accuracy: 0.9866666666666667\n",
      "iteration no 5908: Loss: 0.2499409735330873, accuracy: 0.9866666666666667\n",
      "iteration no 5909: Loss: 0.24994100256523077, accuracy: 0.9866666666666667\n",
      "iteration no 5910: Loss: 0.24993579296499555, accuracy: 0.9866666666666667\n",
      "iteration no 5911: Loss: 0.24994035881819554, accuracy: 0.9866666666666667\n",
      "iteration no 5912: Loss: 0.2499362199852247, accuracy: 0.9866666666666667\n",
      "iteration no 5913: Loss: 0.24993417003806115, accuracy: 0.9866666666666667\n",
      "iteration no 5914: Loss: 0.24993809077135024, accuracy: 0.9866666666666667\n",
      "iteration no 5915: Loss: 0.24993502443854948, accuracy: 0.9866666666666667\n",
      "iteration no 5916: Loss: 0.2499315326666379, accuracy: 0.9866666666666667\n",
      "iteration no 5917: Loss: 0.2499344601730591, accuracy: 0.9866666666666667\n",
      "iteration no 5918: Loss: 0.24993230758651458, accuracy: 0.9866666666666667\n",
      "iteration no 5919: Loss: 0.24992993711431757, accuracy: 0.9866666666666667\n",
      "iteration no 5920: Loss: 0.24993399473451483, accuracy: 0.9866666666666667\n",
      "iteration no 5921: Loss: 0.2499280290869424, accuracy: 0.9866666666666667\n",
      "iteration no 5922: Loss: 0.2499288013955095, accuracy: 0.9866666666666667\n",
      "iteration no 5923: Loss: 0.24992957303403174, accuracy: 0.9866666666666667\n",
      "iteration no 5924: Loss: 0.2499273508746329, accuracy: 0.9866666666666667\n",
      "iteration no 5925: Loss: 0.24992692726039464, accuracy: 0.9866666666666667\n",
      "iteration no 5926: Loss: 0.24992650882273887, accuracy: 0.9866666666666667\n",
      "iteration no 5927: Loss: 0.24992417259084937, accuracy: 0.9866666666666667\n",
      "iteration no 5928: Loss: 0.2499240185590586, accuracy: 0.9866666666666667\n",
      "iteration no 5929: Loss: 0.24992594131165474, accuracy: 0.9866666666666667\n",
      "iteration no 5930: Loss: 0.24992177159630552, accuracy: 0.9866666666666667\n",
      "iteration no 5931: Loss: 0.24992287332138144, accuracy: 0.9866666666666667\n",
      "iteration no 5932: Loss: 0.24992105133495818, accuracy: 0.9866666666666667\n",
      "iteration no 5933: Loss: 0.24992061328350648, accuracy: 0.9866666666666667\n",
      "iteration no 5934: Loss: 0.24992040310155791, accuracy: 0.9866666666666667\n",
      "iteration no 5935: Loss: 0.24991874196460132, accuracy: 0.9866666666666667\n",
      "iteration no 5936: Loss: 0.2499195835507199, accuracy: 0.9866666666666667\n",
      "iteration no 5937: Loss: 0.2499161497499411, accuracy: 0.9866666666666667\n",
      "iteration no 5938: Loss: 0.24991707908554095, accuracy: 0.9866666666666667\n",
      "iteration no 5939: Loss: 0.24991701644950226, accuracy: 0.9866666666666667\n",
      "iteration no 5940: Loss: 0.24991541695150685, accuracy: 0.9866666666666667\n",
      "iteration no 5941: Loss: 0.24991339503004595, accuracy: 0.9866666666666667\n",
      "iteration no 5942: Loss: 0.24991547423292781, accuracy: 0.9866666666666667\n",
      "iteration no 5943: Loss: 0.2499124779907534, accuracy: 0.9866666666666667\n",
      "iteration no 5944: Loss: 0.24991077325362127, accuracy: 0.9866666666666667\n",
      "iteration no 5945: Loss: 0.24991492361032536, accuracy: 0.9866666666666667\n",
      "iteration no 5946: Loss: 0.24990851660637584, accuracy: 0.9866666666666667\n",
      "iteration no 5947: Loss: 0.24991110482172066, accuracy: 0.9866666666666667\n",
      "iteration no 5948: Loss: 0.24991049515861963, accuracy: 0.9866666666666667\n",
      "iteration no 5949: Loss: 0.24990692531369577, accuracy: 0.9866666666666667\n",
      "iteration no 5950: Loss: 0.24990873356043286, accuracy: 0.9866666666666667\n",
      "iteration no 5951: Loss: 0.24990831229046356, accuracy: 0.9866666666666667\n",
      "iteration no 5952: Loss: 0.2499047837652612, accuracy: 0.9866666666666667\n",
      "iteration no 5953: Loss: 0.24990557841959504, accuracy: 0.9866666666666667\n",
      "iteration no 5954: Loss: 0.2499064218268564, accuracy: 0.9866666666666667\n",
      "iteration no 5955: Loss: 0.2499015765451496, accuracy: 0.9866666666666667\n",
      "iteration no 5956: Loss: 0.2499056916070898, accuracy: 0.9866666666666667\n",
      "iteration no 5957: Loss: 0.2499025579923853, accuracy: 0.9866666666666667\n",
      "iteration no 5958: Loss: 0.24990080651138363, accuracy: 0.9866666666666667\n",
      "iteration no 5959: Loss: 0.2499018332638387, accuracy: 0.9866666666666667\n",
      "iteration no 5960: Loss: 0.24989992020413973, accuracy: 0.9866666666666667\n",
      "iteration no 5961: Loss: 0.2499002399452686, accuracy: 0.9866666666666667\n",
      "iteration no 5962: Loss: 0.2498982177207031, accuracy: 0.9866666666666667\n",
      "iteration no 5963: Loss: 0.24989823794864385, accuracy: 0.9866666666666667\n",
      "iteration no 5964: Loss: 0.2498968597628961, accuracy: 0.9866666666666667\n",
      "iteration no 5965: Loss: 0.24989706783979287, accuracy: 0.9866666666666667\n",
      "iteration no 5966: Loss: 0.24989518865978816, accuracy: 0.9866666666666667\n",
      "iteration no 5967: Loss: 0.24989656581133457, accuracy: 0.9866666666666667\n",
      "iteration no 5968: Loss: 0.2498937695579388, accuracy: 0.9866666666666667\n",
      "iteration no 5969: Loss: 0.24989258237067544, accuracy: 0.9866666666666667\n",
      "iteration no 5970: Loss: 0.24989442959010105, accuracy: 0.9866666666666667\n",
      "iteration no 5971: Loss: 0.2498906588875716, accuracy: 0.9866666666666667\n",
      "iteration no 5972: Loss: 0.24989317884021406, accuracy: 0.9866666666666667\n",
      "iteration no 5973: Loss: 0.24989061175253327, accuracy: 0.9866666666666667\n",
      "iteration no 5974: Loss: 0.24988840841232485, accuracy: 0.9866666666666667\n",
      "iteration no 5975: Loss: 0.24989009075399943, accuracy: 0.9866666666666667\n",
      "iteration no 5976: Loss: 0.24988875627885992, accuracy: 0.9866666666666667\n",
      "iteration no 5977: Loss: 0.24988601028983365, accuracy: 0.9866666666666667\n",
      "iteration no 5978: Loss: 0.24988830220398248, accuracy: 0.9866666666666667\n",
      "iteration no 5979: Loss: 0.24988689482307627, accuracy: 0.9866666666666667\n",
      "iteration no 5980: Loss: 0.24988252611772313, accuracy: 0.9866666666666667\n",
      "iteration no 5981: Loss: 0.24988781657468587, accuracy: 0.9866666666666667\n",
      "iteration no 5982: Loss: 0.24988326124473897, accuracy: 0.9866666666666667\n",
      "iteration no 5983: Loss: 0.24988262271193556, accuracy: 0.9866666666666667\n",
      "iteration no 5984: Loss: 0.24988440469828194, accuracy: 0.9866666666666667\n",
      "iteration no 5985: Loss: 0.24988034347947907, accuracy: 0.9866666666666667\n",
      "iteration no 5986: Loss: 0.24988095494369933, accuracy: 0.9866666666666667\n",
      "iteration no 5987: Loss: 0.24988126452059195, accuracy: 0.9866666666666667\n",
      "iteration no 5988: Loss: 0.24987884483061418, accuracy: 0.9866666666666667\n",
      "iteration no 5989: Loss: 0.2498785008579975, accuracy: 0.9866666666666667\n",
      "iteration no 5990: Loss: 0.249879619736622, accuracy: 0.9866666666666667\n",
      "iteration no 5991: Loss: 0.24987551751080742, accuracy: 0.9866666666666667\n",
      "iteration no 5992: Loss: 0.24987731303231592, accuracy: 0.9866666666666667\n",
      "iteration no 5993: Loss: 0.24987676352331392, accuracy: 0.9866666666666667\n",
      "iteration no 5994: Loss: 0.2498748644547918, accuracy: 0.9866666666666667\n",
      "iteration no 5995: Loss: 0.24987435786601914, accuracy: 0.9866666666666667\n",
      "iteration no 5996: Loss: 0.24987256968712634, accuracy: 0.9866666666666667\n",
      "iteration no 5997: Loss: 0.24987442615183428, accuracy: 0.9866666666666667\n",
      "iteration no 5998: Loss: 0.24987106656913427, accuracy: 0.9866666666666667\n",
      "iteration no 5999: Loss: 0.2498719969110939, accuracy: 0.9866666666666667\n",
      "iteration no 6000: Loss: 0.24987170003459303, accuracy: 0.9866666666666667\n",
      "iteration no 6001: Loss: 0.2498681467270079, accuracy: 0.9866666666666667\n",
      "iteration no 6002: Loss: 0.24986928856361174, accuracy: 0.9866666666666667\n",
      "iteration no 6003: Loss: 0.24986998783395015, accuracy: 0.9866666666666667\n",
      "iteration no 6004: Loss: 0.24986741954166353, accuracy: 0.9866666666666667\n",
      "iteration no 6005: Loss: 0.24986663711224422, accuracy: 0.9866666666666667\n",
      "iteration no 6006: Loss: 0.24986715988003427, accuracy: 0.9866666666666667\n",
      "iteration no 6007: Loss: 0.24986357588682745, accuracy: 0.9866666666666667\n",
      "iteration no 6008: Loss: 0.2498672983634871, accuracy: 0.9866666666666667\n",
      "iteration no 6009: Loss: 0.24986477677323451, accuracy: 0.9866666666666667\n",
      "iteration no 6010: Loss: 0.24986084615770776, accuracy: 0.9866666666666667\n",
      "iteration no 6011: Loss: 0.24986493203764654, accuracy: 0.9866666666666667\n",
      "iteration no 6012: Loss: 0.24986089222442304, accuracy: 0.9866666666666667\n",
      "iteration no 6013: Loss: 0.24986083099433493, accuracy: 0.9866666666666667\n",
      "iteration no 6014: Loss: 0.24986278233493603, accuracy: 0.9866666666666667\n",
      "iteration no 6015: Loss: 0.24985940025178313, accuracy: 0.9866666666666667\n",
      "iteration no 6016: Loss: 0.24985726602475924, accuracy: 0.9866666666666667\n",
      "iteration no 6017: Loss: 0.24986065239185073, accuracy: 0.9866666666666667\n",
      "iteration no 6018: Loss: 0.24985695353224727, accuracy: 0.9866666666666667\n",
      "iteration no 6019: Loss: 0.24985666362807063, accuracy: 0.9866666666666667\n",
      "iteration no 6020: Loss: 0.249857905163698, accuracy: 0.9866666666666667\n",
      "iteration no 6021: Loss: 0.249852548632227, accuracy: 0.9866666666666667\n",
      "iteration no 6022: Loss: 0.24985670972506765, accuracy: 0.9866666666666667\n",
      "iteration no 6023: Loss: 0.24985416691591059, accuracy: 0.9866666666666667\n",
      "iteration no 6024: Loss: 0.2498524170277568, accuracy: 0.9866666666666667\n",
      "iteration no 6025: Loss: 0.24985360838709827, accuracy: 0.9866666666666667\n",
      "iteration no 6026: Loss: 0.24985144416691793, accuracy: 0.9866666666666667\n",
      "iteration no 6027: Loss: 0.24985012179898483, accuracy: 0.9866666666666667\n",
      "iteration no 6028: Loss: 0.24985156279585313, accuracy: 0.9866666666666667\n",
      "iteration no 6029: Loss: 0.24984918693048042, accuracy: 0.9866666666666667\n",
      "iteration no 6030: Loss: 0.24984838219197997, accuracy: 0.9866666666666667\n",
      "iteration no 6031: Loss: 0.24985010842017574, accuracy: 0.9866666666666667\n",
      "iteration no 6032: Loss: 0.24984478698276527, accuracy: 0.9866666666666667\n",
      "iteration no 6033: Loss: 0.24984878471668914, accuracy: 0.9866666666666667\n",
      "iteration no 6034: Loss: 0.2498465612655092, accuracy: 0.9866666666666667\n",
      "iteration no 6035: Loss: 0.24984316589486416, accuracy: 0.9866666666666667\n",
      "iteration no 6036: Loss: 0.24984786552690091, accuracy: 0.99\n",
      "iteration no 6037: Loss: 0.24984160679337264, accuracy: 0.9866666666666667\n",
      "iteration no 6038: Loss: 0.24984385725649733, accuracy: 0.9866666666666667\n",
      "iteration no 6039: Loss: 0.2498427064995078, accuracy: 0.9866666666666667\n",
      "iteration no 6040: Loss: 0.24984025094006063, accuracy: 0.9866666666666667\n",
      "iteration no 6041: Loss: 0.2498427660734284, accuracy: 0.9866666666666667\n",
      "iteration no 6042: Loss: 0.24983960819003304, accuracy: 0.9866666666666667\n",
      "iteration no 6043: Loss: 0.24983760063669866, accuracy: 0.9866666666666667\n",
      "iteration no 6044: Loss: 0.24983971134003483, accuracy: 0.9866666666666667\n",
      "iteration no 6045: Loss: 0.2498380897352246, accuracy: 0.9866666666666667\n",
      "iteration no 6046: Loss: 0.24983700461022496, accuracy: 0.9866666666666667\n",
      "iteration no 6047: Loss: 0.24983807272141545, accuracy: 0.9866666666666667\n",
      "iteration no 6048: Loss: 0.24983397702618518, accuracy: 0.9866666666666667\n",
      "iteration no 6049: Loss: 0.24983522656266738, accuracy: 0.99\n",
      "iteration no 6050: Loss: 0.2498347922622192, accuracy: 0.9866666666666667\n",
      "iteration no 6051: Loss: 0.2498328042843871, accuracy: 0.9866666666666667\n",
      "iteration no 6052: Loss: 0.24983361983055585, accuracy: 0.9866666666666667\n",
      "iteration no 6053: Loss: 0.24983086844147445, accuracy: 0.9866666666666667\n",
      "iteration no 6054: Loss: 0.24983025296190495, accuracy: 0.9866666666666667\n",
      "iteration no 6055: Loss: 0.24983173103612766, accuracy: 0.9866666666666667\n",
      "iteration no 6056: Loss: 0.24983067699769884, accuracy: 0.9866666666666667\n",
      "iteration no 6057: Loss: 0.24982722609182673, accuracy: 0.9866666666666667\n",
      "iteration no 6058: Loss: 0.24982963914686007, accuracy: 0.9866666666666667\n",
      "iteration no 6059: Loss: 0.24982669152547615, accuracy: 0.99\n",
      "iteration no 6060: Loss: 0.24982659283295786, accuracy: 0.9866666666666667\n",
      "iteration no 6061: Loss: 0.24982862259142824, accuracy: 0.9866666666666667\n",
      "iteration no 6062: Loss: 0.24982227562864875, accuracy: 0.9866666666666667\n",
      "iteration no 6063: Loss: 0.24982685447172176, accuracy: 0.9866666666666667\n",
      "iteration no 6064: Loss: 0.24982295063935736, accuracy: 0.9866666666666667\n",
      "iteration no 6065: Loss: 0.24982270665031436, accuracy: 0.9866666666666667\n",
      "iteration no 6066: Loss: 0.24982443168360569, accuracy: 0.9866666666666667\n",
      "iteration no 6067: Loss: 0.2498203285105733, accuracy: 0.9866666666666667\n",
      "iteration no 6068: Loss: 0.24982173461238505, accuracy: 0.99\n",
      "iteration no 6069: Loss: 0.24981870115063842, accuracy: 0.9866666666666667\n",
      "iteration no 6070: Loss: 0.24982039759946317, accuracy: 0.9866666666666667\n",
      "iteration no 6071: Loss: 0.2498187534771995, accuracy: 0.9866666666666667\n",
      "iteration no 6072: Loss: 0.2498185676922094, accuracy: 0.9866666666666667\n",
      "iteration no 6073: Loss: 0.24981621060744164, accuracy: 0.9866666666666667\n",
      "iteration no 6074: Loss: 0.24981648244192362, accuracy: 0.9866666666666667\n",
      "iteration no 6075: Loss: 0.24981754868867972, accuracy: 0.99\n",
      "iteration no 6076: Loss: 0.24981297195650048, accuracy: 0.9866666666666667\n",
      "iteration no 6077: Loss: 0.24981712315263066, accuracy: 0.9866666666666667\n",
      "iteration no 6078: Loss: 0.24981255636628052, accuracy: 0.9866666666666667\n",
      "iteration no 6079: Loss: 0.24981284295427308, accuracy: 0.9866666666666667\n",
      "iteration no 6080: Loss: 0.2498134189859174, accuracy: 0.99\n",
      "iteration no 6081: Loss: 0.24980925835223433, accuracy: 0.9866666666666667\n",
      "iteration no 6082: Loss: 0.24981347469866477, accuracy: 0.99\n",
      "iteration no 6083: Loss: 0.24980845914919717, accuracy: 0.9866666666666667\n",
      "iteration no 6084: Loss: 0.24980947727392, accuracy: 0.9866666666666667\n",
      "iteration no 6085: Loss: 0.24980949201925423, accuracy: 0.99\n",
      "iteration no 6086: Loss: 0.24980699203313556, accuracy: 0.9866666666666667\n",
      "iteration no 6087: Loss: 0.2498079736417111, accuracy: 0.9866666666666667\n",
      "iteration no 6088: Loss: 0.24980582961357334, accuracy: 0.9866666666666667\n",
      "iteration no 6089: Loss: 0.24980614387012345, accuracy: 0.9866666666666667\n",
      "iteration no 6090: Loss: 0.24980377146095795, accuracy: 0.9866666666666667\n",
      "iteration no 6091: Loss: 0.24980540701162746, accuracy: 0.99\n",
      "iteration no 6092: Loss: 0.24980239460911824, accuracy: 0.9866666666666667\n",
      "iteration no 6093: Loss: 0.2498040683526428, accuracy: 0.9866666666666667\n",
      "iteration no 6094: Loss: 0.24980365579483904, accuracy: 0.99\n",
      "iteration no 6095: Loss: 0.24979866737477727, accuracy: 0.99\n",
      "iteration no 6096: Loss: 0.24980336360305755, accuracy: 0.9866666666666667\n",
      "iteration no 6097: Loss: 0.24979861110926174, accuracy: 0.9866666666666667\n",
      "iteration no 6098: Loss: 0.24980000350658338, accuracy: 0.9866666666666667\n",
      "iteration no 6099: Loss: 0.24979902957621286, accuracy: 0.9866666666666667\n",
      "iteration no 6100: Loss: 0.24979556163722225, accuracy: 0.9866666666666667\n",
      "iteration no 6101: Loss: 0.24979893341301068, accuracy: 0.99\n",
      "iteration no 6102: Loss: 0.2497961813305049, accuracy: 0.9866666666666667\n",
      "iteration no 6103: Loss: 0.2497953075488165, accuracy: 0.9866666666666667\n",
      "iteration no 6104: Loss: 0.24979547549192538, accuracy: 0.99\n",
      "iteration no 6105: Loss: 0.2497935611110939, accuracy: 0.99\n",
      "iteration no 6106: Loss: 0.24979302988484492, accuracy: 0.9866666666666667\n",
      "iteration no 6107: Loss: 0.24979342524931386, accuracy: 0.9866666666666667\n",
      "iteration no 6108: Loss: 0.24979191510661714, accuracy: 0.9866666666666667\n",
      "iteration no 6109: Loss: 0.24979127705483237, accuracy: 0.9866666666666667\n",
      "iteration no 6110: Loss: 0.2497910655306551, accuracy: 0.99\n",
      "iteration no 6111: Loss: 0.2497877555790785, accuracy: 0.99\n",
      "iteration no 6112: Loss: 0.24979074663141815, accuracy: 0.99\n",
      "iteration no 6113: Loss: 0.24978852125790774, accuracy: 0.9866666666666667\n",
      "iteration no 6114: Loss: 0.2497873176848232, accuracy: 0.9866666666666667\n",
      "iteration no 6115: Loss: 0.24978784780110747, accuracy: 0.99\n",
      "iteration no 6116: Loss: 0.24978518275656708, accuracy: 0.99\n",
      "iteration no 6117: Loss: 0.24978659936347666, accuracy: 0.9866666666666667\n",
      "iteration no 6118: Loss: 0.24978524235050642, accuracy: 0.99\n",
      "iteration no 6119: Loss: 0.2497821254820629, accuracy: 0.9866666666666667\n",
      "iteration no 6120: Loss: 0.2497848748508441, accuracy: 0.99\n",
      "iteration no 6121: Loss: 0.24978199720272704, accuracy: 0.99\n",
      "iteration no 6122: Loss: 0.24978208253940032, accuracy: 0.9866666666666667\n",
      "iteration no 6123: Loss: 0.24978210883624175, accuracy: 0.9866666666666667\n",
      "iteration no 6124: Loss: 0.24977984850348317, accuracy: 0.99\n",
      "iteration no 6125: Loss: 0.2497797176935178, accuracy: 0.9866666666666667\n",
      "iteration no 6126: Loss: 0.24977990283589677, accuracy: 0.99\n",
      "iteration no 6127: Loss: 0.24977577312657934, accuracy: 0.99\n",
      "iteration no 6128: Loss: 0.24977959124852572, accuracy: 0.9866666666666667\n",
      "iteration no 6129: Loss: 0.24977769149089546, accuracy: 0.99\n",
      "iteration no 6130: Loss: 0.24977444620586625, accuracy: 0.99\n",
      "iteration no 6131: Loss: 0.2497783120724204, accuracy: 0.99\n",
      "iteration no 6132: Loss: 0.24977257458455254, accuracy: 0.9866666666666667\n",
      "iteration no 6133: Loss: 0.24977543825686033, accuracy: 0.99\n",
      "iteration no 6134: Loss: 0.24977334378377505, accuracy: 0.99\n",
      "iteration no 6135: Loss: 0.2497721514292958, accuracy: 0.9866666666666667\n",
      "iteration no 6136: Loss: 0.24977245530112283, accuracy: 0.99\n",
      "iteration no 6137: Loss: 0.24977000589549786, accuracy: 0.99\n",
      "iteration no 6138: Loss: 0.2497709068351977, accuracy: 0.9866666666666667\n",
      "iteration no 6139: Loss: 0.24976971362275435, accuracy: 0.99\n",
      "iteration no 6140: Loss: 0.2497694141034519, accuracy: 0.99\n",
      "iteration no 6141: Loss: 0.24976607700772985, accuracy: 0.9866666666666667\n",
      "iteration no 6142: Loss: 0.24976987401194242, accuracy: 0.99\n",
      "iteration no 6143: Loss: 0.2497646505634738, accuracy: 0.99\n",
      "iteration no 6144: Loss: 0.24976867139873882, accuracy: 0.9866666666666667\n",
      "iteration no 6145: Loss: 0.24976488291799248, accuracy: 0.99\n",
      "iteration no 6146: Loss: 0.2497636685574857, accuracy: 0.99\n",
      "iteration no 6147: Loss: 0.24976496705209017, accuracy: 0.99\n",
      "iteration no 6148: Loss: 0.2497635484491771, accuracy: 0.9866666666666667\n",
      "iteration no 6149: Loss: 0.24976198904756547, accuracy: 0.99\n",
      "iteration no 6150: Loss: 0.2497630770720483, accuracy: 0.99\n",
      "iteration no 6151: Loss: 0.24975805762747222, accuracy: 0.99\n",
      "iteration no 6152: Loss: 0.24976243434569623, accuracy: 0.99\n",
      "iteration no 6153: Loss: 0.24975816689533958, accuracy: 0.99\n",
      "iteration no 6154: Loss: 0.2497594522282999, accuracy: 0.9866666666666667\n",
      "iteration no 6155: Loss: 0.24975733938908573, accuracy: 0.99\n",
      "iteration no 6156: Loss: 0.24975856404519403, accuracy: 0.99\n",
      "iteration no 6157: Loss: 0.24975447934552875, accuracy: 0.99\n",
      "iteration no 6158: Loss: 0.2497580252738441, accuracy: 0.9866666666666667\n",
      "iteration no 6159: Loss: 0.2497529978671562, accuracy: 0.99\n",
      "iteration no 6160: Loss: 0.24975564050207452, accuracy: 0.99\n",
      "iteration no 6161: Loss: 0.24975342818439072, accuracy: 0.99\n",
      "iteration no 6162: Loss: 0.24975225349128533, accuracy: 0.99\n",
      "iteration no 6163: Loss: 0.2497530253482227, accuracy: 0.99\n",
      "iteration no 6164: Loss: 0.2497521902174788, accuracy: 0.9866666666666667\n",
      "iteration no 6165: Loss: 0.24974882636214557, accuracy: 0.99\n",
      "iteration no 6166: Loss: 0.24975198048423536, accuracy: 0.99\n",
      "iteration no 6167: Loss: 0.24974533936670512, accuracy: 0.99\n",
      "iteration no 6168: Loss: 0.24975149965348709, accuracy: 0.99\n",
      "iteration no 6169: Loss: 0.24974653747944234, accuracy: 0.99\n",
      "iteration no 6170: Loss: 0.24974762462752945, accuracy: 0.9866666666666667\n",
      "iteration no 6171: Loss: 0.24974639660120757, accuracy: 0.99\n",
      "iteration no 6172: Loss: 0.24974575522289716, accuracy: 0.99\n",
      "iteration no 6173: Loss: 0.24974358402546043, accuracy: 0.99\n",
      "iteration no 6174: Loss: 0.24974552002380654, accuracy: 0.9866666666666667\n",
      "iteration no 6175: Loss: 0.24974188246849804, accuracy: 0.99\n",
      "iteration no 6176: Loss: 0.24974420692521107, accuracy: 0.99\n",
      "iteration no 6177: Loss: 0.24974088207740144, accuracy: 0.99\n",
      "iteration no 6178: Loss: 0.24974123316196167, accuracy: 0.99\n",
      "iteration no 6179: Loss: 0.24974066774862647, accuracy: 0.99\n",
      "iteration no 6180: Loss: 0.24974055694592642, accuracy: 0.9866666666666667\n",
      "iteration no 6181: Loss: 0.2497365586057872, accuracy: 0.99\n",
      "iteration no 6182: Loss: 0.24974034614084853, accuracy: 0.99\n",
      "iteration no 6183: Loss: 0.24973477402050637, accuracy: 0.99\n",
      "iteration no 6184: Loss: 0.24973965861564565, accuracy: 0.9866666666666667\n",
      "iteration no 6185: Loss: 0.24973557815057013, accuracy: 0.99\n",
      "iteration no 6186: Loss: 0.24973472975223518, accuracy: 0.99\n",
      "iteration no 6187: Loss: 0.2497354987126824, accuracy: 0.99\n",
      "iteration no 6188: Loss: 0.24973371238469322, accuracy: 0.99\n",
      "iteration no 6189: Loss: 0.24973291962219984, accuracy: 0.99\n",
      "iteration no 6190: Loss: 0.24973353143661142, accuracy: 0.99\n",
      "iteration no 6191: Loss: 0.2497288884262051, accuracy: 0.99\n",
      "iteration no 6192: Loss: 0.2497329952712624, accuracy: 0.99\n",
      "iteration no 6193: Loss: 0.24972843791861146, accuracy: 0.99\n",
      "iteration no 6194: Loss: 0.24973046567411986, accuracy: 0.99\n",
      "iteration no 6195: Loss: 0.24972833655411122, accuracy: 0.99\n",
      "iteration no 6196: Loss: 0.24972858507587303, accuracy: 0.99\n",
      "iteration no 6197: Loss: 0.24972619608131447, accuracy: 0.99\n",
      "iteration no 6198: Loss: 0.24972765472649763, accuracy: 0.99\n",
      "iteration no 6199: Loss: 0.24972377987824257, accuracy: 0.99\n",
      "iteration no 6200: Loss: 0.2497267647724417, accuracy: 0.9866666666666667\n",
      "iteration no 6201: Loss: 0.24972452097681563, accuracy: 0.99\n",
      "iteration no 6202: Loss: 0.2497232320251717, accuracy: 0.99\n",
      "iteration no 6203: Loss: 0.2497234215711236, accuracy: 0.99\n",
      "iteration no 6204: Loss: 0.24972214745269766, accuracy: 0.99\n",
      "iteration no 6205: Loss: 0.2497210871171603, accuracy: 0.99\n",
      "iteration no 6206: Loss: 0.249721975261892, accuracy: 0.99\n",
      "iteration no 6207: Loss: 0.24971651879587659, accuracy: 0.99\n",
      "iteration no 6208: Loss: 0.2497221358117378, accuracy: 0.99\n",
      "iteration no 6209: Loss: 0.24971736069835712, accuracy: 0.99\n",
      "iteration no 6210: Loss: 0.2497190624829693, accuracy: 0.9866666666666667\n",
      "iteration no 6211: Loss: 0.24971808228144438, accuracy: 0.99\n",
      "iteration no 6212: Loss: 0.24971448355622888, accuracy: 0.99\n",
      "iteration no 6213: Loss: 0.24971789965221078, accuracy: 0.99\n",
      "iteration no 6214: Loss: 0.2497142204037085, accuracy: 0.99\n",
      "iteration no 6215: Loss: 0.2497151701187872, accuracy: 0.99\n",
      "iteration no 6216: Loss: 0.24971340168369582, accuracy: 0.99\n",
      "iteration no 6217: Loss: 0.24971256663899838, accuracy: 0.99\n",
      "iteration no 6218: Loss: 0.2497123257140938, accuracy: 0.99\n",
      "iteration no 6219: Loss: 0.24971315157413956, accuracy: 0.99\n",
      "iteration no 6220: Loss: 0.2497092436855446, accuracy: 0.99\n",
      "iteration no 6221: Loss: 0.2497122720550928, accuracy: 0.99\n",
      "iteration no 6222: Loss: 0.24970812660102085, accuracy: 0.99\n",
      "iteration no 6223: Loss: 0.24971043901035056, accuracy: 0.99\n",
      "iteration no 6224: Loss: 0.24970882983408244, accuracy: 0.99\n",
      "iteration no 6225: Loss: 0.24970622700233402, accuracy: 0.99\n",
      "iteration no 6226: Loss: 0.24970748246768526, accuracy: 0.99\n",
      "iteration no 6227: Loss: 0.24970585256180464, accuracy: 0.99\n",
      "iteration no 6228: Loss: 0.24970611599218218, accuracy: 0.99\n",
      "iteration no 6229: Loss: 0.24970550992412258, accuracy: 0.99\n",
      "iteration no 6230: Loss: 0.24970380151002458, accuracy: 0.99\n",
      "iteration no 6231: Loss: 0.24970314791158146, accuracy: 0.99\n",
      "iteration no 6232: Loss: 0.24970446809666946, accuracy: 0.99\n",
      "iteration no 6233: Loss: 0.24970075013254578, accuracy: 0.99\n",
      "iteration no 6234: Loss: 0.24970402867179184, accuracy: 0.99\n",
      "iteration no 6235: Loss: 0.24969917370895572, accuracy: 0.99\n",
      "iteration no 6236: Loss: 0.24970092907945968, accuracy: 0.99\n",
      "iteration no 6237: Loss: 0.2497012243543555, accuracy: 0.99\n",
      "iteration no 6238: Loss: 0.24969707167052058, accuracy: 0.99\n",
      "iteration no 6239: Loss: 0.24970221950096813, accuracy: 0.99\n",
      "iteration no 6240: Loss: 0.24969537375186518, accuracy: 0.99\n",
      "iteration no 6241: Loss: 0.24969836375531773, accuracy: 0.99\n",
      "iteration no 6242: Loss: 0.2496961966974478, accuracy: 0.99\n",
      "iteration no 6243: Loss: 0.2496949055808752, accuracy: 0.99\n",
      "iteration no 6244: Loss: 0.24969733398922456, accuracy: 0.99\n",
      "iteration no 6245: Loss: 0.2496924851280521, accuracy: 0.99\n",
      "iteration no 6246: Loss: 0.2496947656227116, accuracy: 0.99\n",
      "iteration no 6247: Loss: 0.24969216179449855, accuracy: 0.99\n",
      "iteration no 6248: Loss: 0.249692638719527, accuracy: 0.99\n",
      "iteration no 6249: Loss: 0.24969218354376943, accuracy: 0.99\n",
      "iteration no 6250: Loss: 0.24969065870759516, accuracy: 0.99\n",
      "iteration no 6251: Loss: 0.24968993251467614, accuracy: 0.99\n",
      "iteration no 6252: Loss: 0.24968968063022023, accuracy: 0.99\n",
      "iteration no 6253: Loss: 0.24968945251516234, accuracy: 0.99\n",
      "iteration no 6254: Loss: 0.24968746391591606, accuracy: 0.99\n",
      "iteration no 6255: Loss: 0.24968926357736693, accuracy: 0.99\n",
      "iteration no 6256: Loss: 0.24968540344992146, accuracy: 0.99\n",
      "iteration no 6257: Loss: 0.2496878168584819, accuracy: 0.99\n",
      "iteration no 6258: Loss: 0.2496850538165542, accuracy: 0.99\n",
      "iteration no 6259: Loss: 0.2496848224872174, accuracy: 0.99\n",
      "iteration no 6260: Loss: 0.24968589349773251, accuracy: 0.99\n",
      "iteration no 6261: Loss: 0.2496803809048191, accuracy: 0.99\n",
      "iteration no 6262: Loss: 0.24968627309637936, accuracy: 0.99\n",
      "iteration no 6263: Loss: 0.24968087877380954, accuracy: 0.99\n",
      "iteration no 6264: Loss: 0.24968261503004557, accuracy: 0.99\n",
      "iteration no 6265: Loss: 0.24968169431093354, accuracy: 0.99\n",
      "iteration no 6266: Loss: 0.24967811365225076, accuracy: 0.99\n",
      "iteration no 6267: Loss: 0.24968323230489564, accuracy: 0.99\n",
      "iteration no 6268: Loss: 0.2496767042100225, accuracy: 0.99\n",
      "iteration no 6269: Loss: 0.2496803681962918, accuracy: 0.99\n",
      "iteration no 6270: Loss: 0.24967647698342504, accuracy: 0.99\n",
      "iteration no 6271: Loss: 0.24967647376071034, accuracy: 0.99\n",
      "iteration no 6272: Loss: 0.24967897730118743, accuracy: 0.99\n",
      "iteration no 6273: Loss: 0.24967447761775238, accuracy: 0.99\n",
      "iteration no 6274: Loss: 0.2496760009056929, accuracy: 0.99\n",
      "iteration no 6275: Loss: 0.24967299352186695, accuracy: 0.99\n",
      "iteration no 6276: Loss: 0.24967401358802777, accuracy: 0.99\n",
      "iteration no 6277: Loss: 0.24967359929948568, accuracy: 0.99\n",
      "iteration no 6278: Loss: 0.2496726806425389, accuracy: 0.99\n",
      "iteration no 6279: Loss: 0.24967143951899398, accuracy: 0.99\n",
      "iteration no 6280: Loss: 0.24967094353609762, accuracy: 0.99\n",
      "iteration no 6281: Loss: 0.24966989645563542, accuracy: 0.99\n",
      "iteration no 6282: Loss: 0.24967017968833286, accuracy: 0.99\n",
      "iteration no 6283: Loss: 0.24966957640504395, accuracy: 0.99\n",
      "iteration no 6284: Loss: 0.24966729222597647, accuracy: 0.99\n",
      "iteration no 6285: Loss: 0.24966913556391765, accuracy: 0.99\n",
      "iteration no 6286: Loss: 0.24966533836915636, accuracy: 0.99\n",
      "iteration no 6287: Loss: 0.24966777558564912, accuracy: 0.99\n",
      "iteration no 6288: Loss: 0.2496669110056825, accuracy: 0.99\n",
      "iteration no 6289: Loss: 0.24966430456425265, accuracy: 0.99\n",
      "iteration no 6290: Loss: 0.2496661267492566, accuracy: 0.99\n",
      "iteration no 6291: Loss: 0.24966123979660287, accuracy: 0.99\n",
      "iteration no 6292: Loss: 0.24966548997167445, accuracy: 0.99\n",
      "iteration no 6293: Loss: 0.24966209960055052, accuracy: 0.99\n",
      "iteration no 6294: Loss: 0.2496617010587, accuracy: 0.99\n",
      "iteration no 6295: Loss: 0.2496627459576758, accuracy: 0.99\n",
      "iteration no 6296: Loss: 0.24965899621150742, accuracy: 0.99\n",
      "iteration no 6297: Loss: 0.24966141867498157, accuracy: 0.99\n",
      "iteration no 6298: Loss: 0.2496584043421202, accuracy: 0.99\n",
      "iteration no 6299: Loss: 0.24965872126468736, accuracy: 0.99\n",
      "iteration no 6300: Loss: 0.24965853230016571, accuracy: 0.99\n",
      "iteration no 6301: Loss: 0.24965604774717387, accuracy: 0.99\n",
      "iteration no 6302: Loss: 0.249658105267278, accuracy: 0.99\n",
      "iteration no 6303: Loss: 0.24965607175010118, accuracy: 0.99\n",
      "iteration no 6304: Loss: 0.24965503022091054, accuracy: 0.99\n",
      "iteration no 6305: Loss: 0.24965497022661176, accuracy: 0.99\n",
      "iteration no 6306: Loss: 0.24965263374009017, accuracy: 0.99\n",
      "iteration no 6307: Loss: 0.2496546063667585, accuracy: 0.99\n",
      "iteration no 6308: Loss: 0.249653539890345, accuracy: 0.99\n",
      "iteration no 6309: Loss: 0.2496513516048529, accuracy: 0.99\n",
      "iteration no 6310: Loss: 0.249652325594501, accuracy: 0.99\n",
      "iteration no 6311: Loss: 0.24964963421573233, accuracy: 0.99\n",
      "iteration no 6312: Loss: 0.2496517913871587, accuracy: 0.99\n",
      "iteration no 6313: Loss: 0.24964953908608006, accuracy: 0.99\n",
      "iteration no 6314: Loss: 0.24964832662877784, accuracy: 0.99\n",
      "iteration no 6315: Loss: 0.24965041096583568, accuracy: 0.99\n",
      "iteration no 6316: Loss: 0.24964609516936714, accuracy: 0.99\n",
      "iteration no 6317: Loss: 0.24964945705687386, accuracy: 0.99\n",
      "iteration no 6318: Loss: 0.24964581362673777, accuracy: 0.99\n",
      "iteration no 6319: Loss: 0.24964577445937375, accuracy: 0.99\n",
      "iteration no 6320: Loss: 0.2496461089026548, accuracy: 0.99\n",
      "iteration no 6321: Loss: 0.24964360107964342, accuracy: 0.99\n",
      "iteration no 6322: Loss: 0.2496462001862688, accuracy: 0.99\n",
      "iteration no 6323: Loss: 0.2496422592354136, accuracy: 0.99\n",
      "iteration no 6324: Loss: 0.24964352755414274, accuracy: 0.99\n",
      "iteration no 6325: Loss: 0.24964181439943398, accuracy: 0.99\n",
      "iteration no 6326: Loss: 0.24964139373850025, accuracy: 0.99\n",
      "iteration no 6327: Loss: 0.24964205699590652, accuracy: 0.99\n",
      "iteration no 6328: Loss: 0.24963981283827763, accuracy: 0.99\n",
      "iteration no 6329: Loss: 0.24964027673951433, accuracy: 0.99\n",
      "iteration no 6330: Loss: 0.2496382431645713, accuracy: 0.99\n",
      "iteration no 6331: Loss: 0.24963900732486177, accuracy: 0.99\n",
      "iteration no 6332: Loss: 0.2496378273916663, accuracy: 0.99\n",
      "iteration no 6333: Loss: 0.24963814223673608, accuracy: 0.99\n",
      "iteration no 6334: Loss: 0.24963614768356973, accuracy: 0.99\n",
      "iteration no 6335: Loss: 0.24963621428906052, accuracy: 0.99\n",
      "iteration no 6336: Loss: 0.24963517265228236, accuracy: 0.99\n",
      "iteration no 6337: Loss: 0.2496342970428023, accuracy: 0.99\n",
      "iteration no 6338: Loss: 0.24963669388092766, accuracy: 0.99\n",
      "iteration no 6339: Loss: 0.24963225171104475, accuracy: 0.99\n",
      "iteration no 6340: Loss: 0.24963444597187023, accuracy: 0.99\n",
      "iteration no 6341: Loss: 0.24963058501919605, accuracy: 0.99\n",
      "iteration no 6342: Loss: 0.2496331469964545, accuracy: 0.99\n",
      "iteration no 6343: Loss: 0.24963244749809996, accuracy: 0.99\n",
      "iteration no 6344: Loss: 0.24962849430138664, accuracy: 0.99\n",
      "iteration no 6345: Loss: 0.2496326487431401, accuracy: 0.99\n",
      "iteration no 6346: Loss: 0.24962658305511906, accuracy: 0.99\n",
      "iteration no 6347: Loss: 0.24963187946936127, accuracy: 0.99\n",
      "iteration no 6348: Loss: 0.2496272019729623, accuracy: 0.99\n",
      "iteration no 6349: Loss: 0.2496270341575318, accuracy: 0.99\n",
      "iteration no 6350: Loss: 0.24962841901874583, accuracy: 0.99\n",
      "iteration no 6351: Loss: 0.24962412839575, accuracy: 0.99\n",
      "iteration no 6352: Loss: 0.24962927814172753, accuracy: 0.99\n",
      "iteration no 6353: Loss: 0.24962269022311584, accuracy: 0.99\n",
      "iteration no 6354: Loss: 0.24962567025892018, accuracy: 0.99\n",
      "iteration no 6355: Loss: 0.24962447238207597, accuracy: 0.99\n",
      "iteration no 6356: Loss: 0.24962218012023096, accuracy: 0.99\n",
      "iteration no 6357: Loss: 0.24962557049316925, accuracy: 0.99\n",
      "iteration no 6358: Loss: 0.2496199063172891, accuracy: 0.99\n",
      "iteration no 6359: Loss: 0.24962314672914698, accuracy: 0.99\n",
      "iteration no 6360: Loss: 0.24962012622124685, accuracy: 0.99\n",
      "iteration no 6361: Loss: 0.24962074431264752, accuracy: 0.99\n",
      "iteration no 6362: Loss: 0.249620926272745, accuracy: 0.99\n",
      "iteration no 6363: Loss: 0.2496173552376189, accuracy: 0.99\n",
      "iteration no 6364: Loss: 0.24962011905292095, accuracy: 0.99\n",
      "iteration no 6365: Loss: 0.24961752413781965, accuracy: 0.99\n",
      "iteration no 6366: Loss: 0.24961734800697477, accuracy: 0.99\n",
      "iteration no 6367: Loss: 0.24961665050568432, accuracy: 0.99\n",
      "iteration no 6368: Loss: 0.24961528522759008, accuracy: 0.99\n",
      "iteration no 6369: Loss: 0.24961666681132944, accuracy: 0.99\n",
      "iteration no 6370: Loss: 0.24961485709111136, accuracy: 0.99\n",
      "iteration no 6371: Loss: 0.24961447092595834, accuracy: 0.99\n",
      "iteration no 6372: Loss: 0.24961398347358155, accuracy: 0.99\n",
      "iteration no 6373: Loss: 0.24961207708857502, accuracy: 0.99\n",
      "iteration no 6374: Loss: 0.2496132871440483, accuracy: 0.99\n",
      "iteration no 6375: Loss: 0.24961223892066178, accuracy: 0.99\n",
      "iteration no 6376: Loss: 0.24961144663032603, accuracy: 0.99\n",
      "iteration no 6377: Loss: 0.24961198105016982, accuracy: 0.99\n",
      "iteration no 6378: Loss: 0.2496082429326654, accuracy: 0.99\n",
      "iteration no 6379: Loss: 0.24961174528997337, accuracy: 0.99\n",
      "iteration no 6380: Loss: 0.24960808780571964, accuracy: 0.99\n",
      "iteration no 6381: Loss: 0.2496089634993461, accuracy: 0.99\n",
      "iteration no 6382: Loss: 0.2496085757076447, accuracy: 0.99\n",
      "iteration no 6383: Loss: 0.2496057926559896, accuracy: 0.99\n",
      "iteration no 6384: Loss: 0.24960912980776245, accuracy: 0.99\n",
      "iteration no 6385: Loss: 0.24960471067118387, accuracy: 0.99\n",
      "iteration no 6386: Loss: 0.24960699779836742, accuracy: 0.99\n",
      "iteration no 6387: Loss: 0.24960438631940124, accuracy: 0.99\n",
      "iteration no 6388: Loss: 0.2496039876953403, accuracy: 0.99\n",
      "iteration no 6389: Loss: 0.2496050955909194, accuracy: 0.99\n",
      "iteration no 6390: Loss: 0.24960147999390178, accuracy: 0.99\n",
      "iteration no 6391: Loss: 0.2496053784747629, accuracy: 0.99\n",
      "iteration no 6392: Loss: 0.24960040292081012, accuracy: 0.99\n",
      "iteration no 6393: Loss: 0.24960208106585557, accuracy: 0.99\n",
      "iteration no 6394: Loss: 0.24960119531485025, accuracy: 0.99\n",
      "iteration no 6395: Loss: 0.24959984507805744, accuracy: 0.99\n",
      "iteration no 6396: Loss: 0.24960154955932037, accuracy: 0.99\n",
      "iteration no 6397: Loss: 0.24959709121188045, accuracy: 0.99\n",
      "iteration no 6398: Loss: 0.24960015473245128, accuracy: 0.99\n",
      "iteration no 6399: Loss: 0.2495975371905009, accuracy: 0.99\n",
      "iteration no 6400: Loss: 0.24959799177345757, accuracy: 0.99\n",
      "iteration no 6401: Loss: 0.24959703155090945, accuracy: 0.99\n",
      "iteration no 6402: Loss: 0.24959566781803913, accuracy: 0.99\n",
      "iteration no 6403: Loss: 0.24959724673207112, accuracy: 0.99\n",
      "iteration no 6404: Loss: 0.2495942586961834, accuracy: 0.99\n",
      "iteration no 6405: Loss: 0.249595165815872, accuracy: 0.99\n",
      "iteration no 6406: Loss: 0.24959258369582543, accuracy: 0.99\n",
      "iteration no 6407: Loss: 0.2495943243167886, accuracy: 0.99\n",
      "iteration no 6408: Loss: 0.2495936705911383, accuracy: 0.99\n",
      "iteration no 6409: Loss: 0.24959226617944347, accuracy: 0.99\n",
      "iteration no 6410: Loss: 0.2495916017301396, accuracy: 0.99\n",
      "iteration no 6411: Loss: 0.24959056668011148, accuracy: 0.99\n",
      "iteration no 6412: Loss: 0.2495908880030378, accuracy: 0.99\n",
      "iteration no 6413: Loss: 0.24959009299992133, accuracy: 0.99\n",
      "iteration no 6414: Loss: 0.24959052918445912, accuracy: 0.99\n",
      "iteration no 6415: Loss: 0.24958841318231817, accuracy: 0.99\n",
      "iteration no 6416: Loss: 0.24958882522055761, accuracy: 0.99\n",
      "iteration no 6417: Loss: 0.24958717161847754, accuracy: 0.99\n",
      "iteration no 6418: Loss: 0.24958683399867648, accuracy: 0.99\n",
      "iteration no 6419: Loss: 0.24958742274933343, accuracy: 0.99\n",
      "iteration no 6420: Loss: 0.2495852786064477, accuracy: 0.99\n",
      "iteration no 6421: Loss: 0.24958678112552535, accuracy: 0.99\n",
      "iteration no 6422: Loss: 0.24958387171585594, accuracy: 0.99\n",
      "iteration no 6423: Loss: 0.24958552581684448, accuracy: 0.99\n",
      "iteration no 6424: Loss: 0.2495829986869757, accuracy: 0.99\n",
      "iteration no 6425: Loss: 0.2495837838875416, accuracy: 0.99\n",
      "iteration no 6426: Loss: 0.24958295560724947, accuracy: 0.99\n",
      "iteration no 6427: Loss: 0.24958112955966813, accuracy: 0.99\n",
      "iteration no 6428: Loss: 0.24958312790952192, accuracy: 0.99\n",
      "iteration no 6429: Loss: 0.24957931706542896, accuracy: 0.99\n",
      "iteration no 6430: Loss: 0.24958113012006553, accuracy: 0.99\n",
      "iteration no 6431: Loss: 0.24957949163453763, accuracy: 0.99\n",
      "iteration no 6432: Loss: 0.2495798807716269, accuracy: 0.99\n",
      "iteration no 6433: Loss: 0.2495798249679853, accuracy: 0.99\n",
      "iteration no 6434: Loss: 0.24957674913462413, accuracy: 0.99\n",
      "iteration no 6435: Loss: 0.24957904180997417, accuracy: 0.99\n",
      "iteration no 6436: Loss: 0.24957521921221498, accuracy: 0.99\n",
      "iteration no 6437: Loss: 0.2495784437212315, accuracy: 0.99\n",
      "iteration no 6438: Loss: 0.24957574578778527, accuracy: 0.99\n",
      "iteration no 6439: Loss: 0.2495750540258186, accuracy: 0.99\n",
      "iteration no 6440: Loss: 0.24957510356175513, accuracy: 0.99\n",
      "iteration no 6441: Loss: 0.249572694495043, accuracy: 0.99\n",
      "iteration no 6442: Loss: 0.24957537167422778, accuracy: 0.99\n",
      "iteration no 6443: Loss: 0.24957162445705977, accuracy: 0.99\n",
      "iteration no 6444: Loss: 0.24957407452983726, accuracy: 0.99\n",
      "iteration no 6445: Loss: 0.24957167382720633, accuracy: 0.99\n",
      "iteration no 6446: Loss: 0.24957162815524156, accuracy: 0.99\n",
      "iteration no 6447: Loss: 0.24957182242637047, accuracy: 0.99\n",
      "iteration no 6448: Loss: 0.24956920754389839, accuracy: 0.99\n",
      "iteration no 6449: Loss: 0.24957059300439302, accuracy: 0.99\n",
      "iteration no 6450: Loss: 0.24956825217664175, accuracy: 0.99\n",
      "iteration no 6451: Loss: 0.2495700480961473, accuracy: 0.99\n",
      "iteration no 6452: Loss: 0.24956777349438086, accuracy: 0.99\n",
      "iteration no 6453: Loss: 0.24956722782942203, accuracy: 0.99\n",
      "iteration no 6454: Loss: 0.24956697916960446, accuracy: 0.99\n",
      "iteration no 6455: Loss: 0.249566107334478, accuracy: 0.99\n",
      "iteration no 6456: Loss: 0.24956683214422137, accuracy: 0.99\n",
      "iteration no 6457: Loss: 0.24956449057279573, accuracy: 0.99\n",
      "iteration no 6458: Loss: 0.24956526759892467, accuracy: 0.99\n",
      "iteration no 6459: Loss: 0.2495634922113556, accuracy: 0.99\n",
      "iteration no 6460: Loss: 0.24956360200194774, accuracy: 0.99\n",
      "iteration no 6461: Loss: 0.24956336705315357, accuracy: 0.99\n",
      "iteration no 6462: Loss: 0.24956320667698131, accuracy: 0.99\n",
      "iteration no 6463: Loss: 0.2495615342717587, accuracy: 0.99\n",
      "iteration no 6464: Loss: 0.24956170322920485, accuracy: 0.99\n",
      "iteration no 6465: Loss: 0.24956023057222, accuracy: 0.99\n",
      "iteration no 6466: Loss: 0.24956050049518802, accuracy: 0.99\n",
      "iteration no 6467: Loss: 0.2495593359116202, accuracy: 0.99\n",
      "iteration no 6468: Loss: 0.24955964478617976, accuracy: 0.99\n",
      "iteration no 6469: Loss: 0.24955886190931223, accuracy: 0.99\n",
      "iteration no 6470: Loss: 0.2495580013954447, accuracy: 0.99\n",
      "iteration no 6471: Loss: 0.24955807109004008, accuracy: 0.99\n",
      "iteration no 6472: Loss: 0.24955573571766537, accuracy: 0.99\n",
      "iteration no 6473: Loss: 0.24955740623709932, accuracy: 0.99\n",
      "iteration no 6474: Loss: 0.24955500181510587, accuracy: 0.99\n",
      "iteration no 6475: Loss: 0.24955649594978468, accuracy: 0.99\n",
      "iteration no 6476: Loss: 0.24955350208075333, accuracy: 0.99\n",
      "iteration no 6477: Loss: 0.2495543893498724, accuracy: 0.99\n",
      "iteration no 6478: Loss: 0.24955230911802506, accuracy: 0.99\n",
      "iteration no 6479: Loss: 0.24955412899242238, accuracy: 0.99\n",
      "iteration no 6480: Loss: 0.2495524359806614, accuracy: 0.99\n",
      "iteration no 6481: Loss: 0.24955202976804952, accuracy: 0.99\n",
      "iteration no 6482: Loss: 0.2495503720929016, accuracy: 0.99\n",
      "iteration no 6483: Loss: 0.24955162738126915, accuracy: 0.99\n",
      "iteration no 6484: Loss: 0.24954884041372172, accuracy: 0.99\n",
      "iteration no 6485: Loss: 0.24955120601826747, accuracy: 0.99\n",
      "iteration no 6486: Loss: 0.24954716107874592, accuracy: 0.99\n",
      "iteration no 6487: Loss: 0.24955039589361883, accuracy: 0.99\n",
      "iteration no 6488: Loss: 0.24954570366141193, accuracy: 0.99\n",
      "iteration no 6489: Loss: 0.24954910757401175, accuracy: 0.99\n",
      "iteration no 6490: Loss: 0.2495457159829717, accuracy: 0.99\n",
      "iteration no 6491: Loss: 0.2495463983105376, accuracy: 0.99\n",
      "iteration no 6492: Loss: 0.24954541538955188, accuracy: 0.99\n",
      "iteration no 6493: Loss: 0.24954562210177117, accuracy: 0.99\n",
      "iteration no 6494: Loss: 0.24954455857578095, accuracy: 0.99\n",
      "iteration no 6495: Loss: 0.24954449783623434, accuracy: 0.99\n",
      "iteration no 6496: Loss: 0.2495418803012777, accuracy: 0.99\n",
      "iteration no 6497: Loss: 0.24954448808026436, accuracy: 0.99\n",
      "iteration no 6498: Loss: 0.24953987755642842, accuracy: 0.99\n",
      "iteration no 6499: Loss: 0.24954459885769376, accuracy: 0.99\n",
      "iteration no 6500: Loss: 0.249538242628961, accuracy: 0.99\n",
      "iteration no 6501: Loss: 0.24954248918971136, accuracy: 0.99\n",
      "iteration no 6502: Loss: 0.24953943058495023, accuracy: 0.99\n",
      "iteration no 6503: Loss: 0.2495403052003125, accuracy: 0.99\n",
      "iteration no 6504: Loss: 0.2495396083208099, accuracy: 0.99\n",
      "iteration no 6505: Loss: 0.24953716345105392, accuracy: 0.99\n",
      "iteration no 6506: Loss: 0.24953868976150045, accuracy: 0.99\n",
      "iteration no 6507: Loss: 0.24953721816789282, accuracy: 0.99\n",
      "iteration no 6508: Loss: 0.2495362799884458, accuracy: 0.99\n",
      "iteration no 6509: Loss: 0.2495373690405734, accuracy: 0.99\n",
      "iteration no 6510: Loss: 0.2495333252431249, accuracy: 0.99\n",
      "iteration no 6511: Loss: 0.24953723862817917, accuracy: 0.99\n",
      "iteration no 6512: Loss: 0.24953320016392852, accuracy: 0.99\n",
      "iteration no 6513: Loss: 0.24953445208878233, accuracy: 0.99\n",
      "iteration no 6514: Loss: 0.2495334086338255, accuracy: 0.99\n",
      "iteration no 6515: Loss: 0.24953059520655074, accuracy: 0.99\n",
      "iteration no 6516: Loss: 0.24953379798761288, accuracy: 0.99\n",
      "iteration no 6517: Loss: 0.24952869328127905, accuracy: 0.99\n",
      "iteration no 6518: Loss: 0.2495335448344552, accuracy: 0.99\n",
      "iteration no 6519: Loss: 0.24952889691960203, accuracy: 0.99\n",
      "iteration no 6520: Loss: 0.24952995348220935, accuracy: 0.99\n",
      "iteration no 6521: Loss: 0.249529409134921, accuracy: 0.99\n",
      "iteration no 6522: Loss: 0.24952825245145707, accuracy: 0.99\n",
      "iteration no 6523: Loss: 0.24952804581242613, accuracy: 0.99\n",
      "iteration no 6524: Loss: 0.24952728243076688, accuracy: 0.99\n",
      "iteration no 6525: Loss: 0.2495259547972559, accuracy: 0.99\n",
      "iteration no 6526: Loss: 0.24952832255123786, accuracy: 0.99\n",
      "iteration no 6527: Loss: 0.24952404588166666, accuracy: 0.99\n",
      "iteration no 6528: Loss: 0.24952665979245234, accuracy: 0.99\n",
      "iteration no 6529: Loss: 0.2495228552893453, accuracy: 0.99\n",
      "iteration no 6530: Loss: 0.249525661392255, accuracy: 0.99\n",
      "iteration no 6531: Loss: 0.249522307561872, accuracy: 0.99\n",
      "iteration no 6532: Loss: 0.24952281746215152, accuracy: 0.99\n",
      "iteration no 6533: Loss: 0.24952270907578286, accuracy: 0.99\n",
      "iteration no 6534: Loss: 0.24952076907196785, accuracy: 0.99\n",
      "iteration no 6535: Loss: 0.24952112662574596, accuracy: 0.99\n",
      "iteration no 6536: Loss: 0.24951918148446683, accuracy: 0.99\n",
      "iteration no 6537: Loss: 0.249521047325314, accuracy: 0.99\n",
      "iteration no 6538: Loss: 0.24951832676623603, accuracy: 0.99\n",
      "iteration no 6539: Loss: 0.24951924581414378, accuracy: 0.99\n",
      "iteration no 6540: Loss: 0.24951732728027926, accuracy: 0.99\n",
      "iteration no 6541: Loss: 0.24951845614904222, accuracy: 0.99\n",
      "iteration no 6542: Loss: 0.24951578149741166, accuracy: 0.99\n",
      "iteration no 6543: Loss: 0.24951680462798365, accuracy: 0.99\n",
      "iteration no 6544: Loss: 0.2495166610466319, accuracy: 0.99\n",
      "iteration no 6545: Loss: 0.2495156153311413, accuracy: 0.99\n",
      "iteration no 6546: Loss: 0.24951600343501082, accuracy: 0.99\n",
      "iteration no 6547: Loss: 0.24951320678871414, accuracy: 0.99\n",
      "iteration no 6548: Loss: 0.2495136307435793, accuracy: 0.99\n",
      "iteration no 6549: Loss: 0.24951428777479345, accuracy: 0.99\n",
      "iteration no 6550: Loss: 0.24951159228769776, accuracy: 0.99\n",
      "iteration no 6551: Loss: 0.24951388262873253, accuracy: 0.99\n",
      "iteration no 6552: Loss: 0.2495101895868445, accuracy: 0.99\n",
      "iteration no 6553: Loss: 0.24951249488931548, accuracy: 0.99\n",
      "iteration no 6554: Loss: 0.24951134161432792, accuracy: 0.99\n",
      "iteration no 6555: Loss: 0.2495099204243867, accuracy: 0.99\n",
      "iteration no 6556: Loss: 0.2495122591697683, accuracy: 0.99\n",
      "iteration no 6557: Loss: 0.2495065094089684, accuracy: 0.99\n",
      "iteration no 6558: Loss: 0.2495124478818442, accuracy: 0.99\n",
      "iteration no 6559: Loss: 0.24950561938034888, accuracy: 0.99\n",
      "iteration no 6560: Loss: 0.24951148779347457, accuracy: 0.99\n",
      "iteration no 6561: Loss: 0.24950543036088513, accuracy: 0.99\n",
      "iteration no 6562: Loss: 0.24950784457222153, accuracy: 0.99\n",
      "iteration no 6563: Loss: 0.24950715458682357, accuracy: 0.99\n",
      "iteration no 6564: Loss: 0.2495059241897065, accuracy: 0.99\n",
      "iteration no 6565: Loss: 0.24950670572079953, accuracy: 0.99\n",
      "iteration no 6566: Loss: 0.24950375542758113, accuracy: 0.99\n",
      "iteration no 6567: Loss: 0.24950528036218433, accuracy: 0.99\n",
      "iteration no 6568: Loss: 0.2495038401967451, accuracy: 0.99\n",
      "iteration no 6569: Loss: 0.24950334464360896, accuracy: 0.99\n",
      "iteration no 6570: Loss: 0.2495031751635275, accuracy: 0.99\n",
      "iteration no 6571: Loss: 0.24950235229827378, accuracy: 0.99\n",
      "iteration no 6572: Loss: 0.24950168059452232, accuracy: 0.99\n",
      "iteration no 6573: Loss: 0.2495016094798349, accuracy: 0.99\n",
      "iteration no 6574: Loss: 0.2495001489046573, accuracy: 0.99\n",
      "iteration no 6575: Loss: 0.2495017168867455, accuracy: 0.99\n",
      "iteration no 6576: Loss: 0.24949910352769233, accuracy: 0.99\n",
      "iteration no 6577: Loss: 0.2494999991653844, accuracy: 0.99\n",
      "iteration no 6578: Loss: 0.24949902965856713, accuracy: 0.99\n",
      "iteration no 6579: Loss: 0.24949861057880068, accuracy: 0.99\n",
      "iteration no 6580: Loss: 0.2494979298022227, accuracy: 0.99\n",
      "iteration no 6581: Loss: 0.24949790807655461, accuracy: 0.99\n",
      "iteration no 6582: Loss: 0.2494966873237904, accuracy: 0.99\n",
      "iteration no 6583: Loss: 0.24949766244103763, accuracy: 0.99\n",
      "iteration no 6584: Loss: 0.24949594213405696, accuracy: 0.99\n",
      "iteration no 6585: Loss: 0.24949580145725136, accuracy: 0.99\n",
      "iteration no 6586: Loss: 0.24949592211452415, accuracy: 0.99\n",
      "iteration no 6587: Loss: 0.2494942928362065, accuracy: 0.99\n",
      "iteration no 6588: Loss: 0.24949682858953381, accuracy: 0.99\n",
      "iteration no 6589: Loss: 0.24949111345889347, accuracy: 0.99\n",
      "iteration no 6590: Loss: 0.24949590436745636, accuracy: 0.99\n",
      "iteration no 6591: Loss: 0.2494905813451371, accuracy: 0.99\n",
      "iteration no 6592: Loss: 0.24949378346769224, accuracy: 0.99\n",
      "iteration no 6593: Loss: 0.2494929853535784, accuracy: 0.99\n",
      "iteration no 6594: Loss: 0.24949067611967582, accuracy: 0.99\n",
      "iteration no 6595: Loss: 0.24949303473934648, accuracy: 0.99\n",
      "iteration no 6596: Loss: 0.2494879938943752, accuracy: 0.99\n",
      "iteration no 6597: Loss: 0.24949409230488556, accuracy: 0.99\n",
      "iteration no 6598: Loss: 0.24948640634275093, accuracy: 0.99\n",
      "iteration no 6599: Loss: 0.24949200526651677, accuracy: 0.99\n",
      "iteration no 6600: Loss: 0.2494869308081073, accuracy: 0.99\n",
      "iteration no 6601: Loss: 0.24948885613572422, accuracy: 0.99\n",
      "iteration no 6602: Loss: 0.24948861273738399, accuracy: 0.99\n",
      "iteration no 6603: Loss: 0.24948624207059028, accuracy: 0.99\n",
      "iteration no 6604: Loss: 0.24948841978258712, accuracy: 0.99\n",
      "iteration no 6605: Loss: 0.2494845136573896, accuracy: 0.99\n",
      "iteration no 6606: Loss: 0.24948794542483194, accuracy: 0.99\n",
      "iteration no 6607: Loss: 0.24948423910136716, accuracy: 0.99\n",
      "iteration no 6608: Loss: 0.2494855332145235, accuracy: 0.99\n",
      "iteration no 6609: Loss: 0.24948374857295635, accuracy: 0.99\n",
      "iteration no 6610: Loss: 0.2494844433442242, accuracy: 0.99\n",
      "iteration no 6611: Loss: 0.2494827562595773, accuracy: 0.99\n",
      "iteration no 6612: Loss: 0.2494834316827248, accuracy: 0.99\n",
      "iteration no 6613: Loss: 0.2494815255874493, accuracy: 0.99\n",
      "iteration no 6614: Loss: 0.249481948967632, accuracy: 0.99\n",
      "iteration no 6615: Loss: 0.24948242680954652, accuracy: 0.99\n",
      "iteration no 6616: Loss: 0.24948027983968774, accuracy: 0.99\n",
      "iteration no 6617: Loss: 0.24948155757816107, accuracy: 0.99\n",
      "iteration no 6618: Loss: 0.2494785611483734, accuracy: 0.99\n",
      "iteration no 6619: Loss: 0.24948080962226005, accuracy: 0.99\n",
      "iteration no 6620: Loss: 0.2494783322811449, accuracy: 0.99\n",
      "iteration no 6621: Loss: 0.24947918734133812, accuracy: 0.99\n",
      "iteration no 6622: Loss: 0.24947802172970773, accuracy: 0.99\n",
      "iteration no 6623: Loss: 0.2494776481003847, accuracy: 0.99\n",
      "iteration no 6624: Loss: 0.2494784260030688, accuracy: 0.99\n",
      "iteration no 6625: Loss: 0.24947581787375092, accuracy: 0.99\n",
      "iteration no 6626: Loss: 0.24947754698536434, accuracy: 0.99\n",
      "iteration no 6627: Loss: 0.2494753797448812, accuracy: 0.99\n",
      "iteration no 6628: Loss: 0.249475108959815, accuracy: 0.99\n",
      "iteration no 6629: Loss: 0.2494755546013457, accuracy: 0.99\n",
      "iteration no 6630: Loss: 0.24947350703612603, accuracy: 0.99\n",
      "iteration no 6631: Loss: 0.24947554596658472, accuracy: 0.99\n",
      "iteration no 6632: Loss: 0.24947304122628325, accuracy: 0.99\n",
      "iteration no 6633: Loss: 0.24947312140625433, accuracy: 0.99\n",
      "iteration no 6634: Loss: 0.24947469531037522, accuracy: 0.99\n",
      "iteration no 6635: Loss: 0.2494708785914318, accuracy: 0.99\n",
      "iteration no 6636: Loss: 0.24947364063621197, accuracy: 0.99\n",
      "iteration no 6637: Loss: 0.24946948862824378, accuracy: 0.99\n",
      "iteration no 6638: Loss: 0.24947213939727292, accuracy: 0.99\n",
      "iteration no 6639: Loss: 0.2494699854992241, accuracy: 0.99\n",
      "iteration no 6640: Loss: 0.249470497171993, accuracy: 0.99\n",
      "iteration no 6641: Loss: 0.2494706488487417, accuracy: 0.99\n",
      "iteration no 6642: Loss: 0.24946784571879865, accuracy: 0.99\n",
      "iteration no 6643: Loss: 0.2494709186604357, accuracy: 0.99\n",
      "iteration no 6644: Loss: 0.24946672006121373, accuracy: 0.99\n",
      "iteration no 6645: Loss: 0.2494693094085561, accuracy: 0.99\n",
      "iteration no 6646: Loss: 0.24946601540327634, accuracy: 0.99\n",
      "iteration no 6647: Loss: 0.24946709252349683, accuracy: 0.99\n",
      "iteration no 6648: Loss: 0.24946695136733804, accuracy: 0.99\n",
      "iteration no 6649: Loss: 0.24946573816559067, accuracy: 0.99\n",
      "iteration no 6650: Loss: 0.24946707182908023, accuracy: 0.99\n",
      "iteration no 6651: Loss: 0.24946464413166763, accuracy: 0.99\n",
      "iteration no 6652: Loss: 0.24946519501555975, accuracy: 0.99\n",
      "iteration no 6653: Loss: 0.24946420426948118, accuracy: 0.99\n",
      "iteration no 6654: Loss: 0.24946361807833, accuracy: 0.99\n",
      "iteration no 6655: Loss: 0.24946332166724655, accuracy: 0.99\n",
      "iteration no 6656: Loss: 0.2494635604657918, accuracy: 0.99\n",
      "iteration no 6657: Loss: 0.2494617723383902, accuracy: 0.99\n",
      "iteration no 6658: Loss: 0.24946281592006897, accuracy: 0.99\n",
      "iteration no 6659: Loss: 0.24946159494808368, accuracy: 0.99\n",
      "iteration no 6660: Loss: 0.24946139390819502, accuracy: 0.99\n",
      "iteration no 6661: Loss: 0.24946251574159817, accuracy: 0.99\n",
      "iteration no 6662: Loss: 0.24945896401609663, accuracy: 0.99\n",
      "iteration no 6663: Loss: 0.24946234510277213, accuracy: 0.99\n",
      "iteration no 6664: Loss: 0.24945698718778853, accuracy: 0.99\n",
      "iteration no 6665: Loss: 0.2494612805056206, accuracy: 0.99\n",
      "iteration no 6666: Loss: 0.24945714596863788, accuracy: 0.99\n",
      "iteration no 6667: Loss: 0.24945895034866106, accuracy: 0.99\n",
      "iteration no 6668: Loss: 0.24945845373345682, accuracy: 0.99\n",
      "iteration no 6669: Loss: 0.24945641697255944, accuracy: 0.99\n",
      "iteration no 6670: Loss: 0.24945935653631146, accuracy: 0.99\n",
      "iteration no 6671: Loss: 0.24945394485235833, accuracy: 0.99\n",
      "iteration no 6672: Loss: 0.2494590004636373, accuracy: 0.99\n",
      "iteration no 6673: Loss: 0.24945320241081562, accuracy: 0.99\n",
      "iteration no 6674: Loss: 0.24945793122983823, accuracy: 0.99\n",
      "iteration no 6675: Loss: 0.24945346416274844, accuracy: 0.99\n",
      "iteration no 6676: Loss: 0.2494537810378788, accuracy: 0.99\n",
      "iteration no 6677: Loss: 0.24945515289185324, accuracy: 0.99\n",
      "iteration no 6678: Loss: 0.24945195070842424, accuracy: 0.99\n",
      "iteration no 6679: Loss: 0.24945491070840592, accuracy: 0.99\n",
      "iteration no 6680: Loss: 0.24945042614984964, accuracy: 0.99\n",
      "iteration no 6681: Loss: 0.24945305754719174, accuracy: 0.99\n",
      "iteration no 6682: Loss: 0.24945089885153737, accuracy: 0.99\n",
      "iteration no 6683: Loss: 0.24945230194375528, accuracy: 0.99\n",
      "iteration no 6684: Loss: 0.2494509408138339, accuracy: 0.99\n",
      "iteration no 6685: Loss: 0.24945034171299874, accuracy: 0.99\n",
      "iteration no 6686: Loss: 0.24945037020382693, accuracy: 0.99\n",
      "iteration no 6687: Loss: 0.2494496108795274, accuracy: 0.99\n",
      "iteration no 6688: Loss: 0.24945028107907585, accuracy: 0.99\n",
      "iteration no 6689: Loss: 0.2494473965879263, accuracy: 0.99\n",
      "iteration no 6690: Loss: 0.24945022338302308, accuracy: 0.99\n",
      "iteration no 6691: Loss: 0.2494453646698307, accuracy: 0.99\n",
      "iteration no 6692: Loss: 0.24945062170776566, accuracy: 0.99\n",
      "iteration no 6693: Loss: 0.24944492939448015, accuracy: 0.99\n",
      "iteration no 6694: Loss: 0.24944731005011742, accuracy: 0.99\n",
      "iteration no 6695: Loss: 0.24944619549339211, accuracy: 0.99\n",
      "iteration no 6696: Loss: 0.2494456500112573, accuracy: 0.99\n",
      "iteration no 6697: Loss: 0.24944755298539895, accuracy: 0.99\n",
      "iteration no 6698: Loss: 0.2494421510427341, accuracy: 0.99\n",
      "iteration no 6699: Loss: 0.24944720359116232, accuracy: 0.99\n",
      "iteration no 6700: Loss: 0.24944150011348876, accuracy: 0.99\n",
      "iteration no 6701: Loss: 0.2494453944380442, accuracy: 0.99\n",
      "iteration no 6702: Loss: 0.24944336486150664, accuracy: 0.99\n",
      "iteration no 6703: Loss: 0.24944227437312028, accuracy: 0.99\n",
      "iteration no 6704: Loss: 0.24944411670086886, accuracy: 0.99\n",
      "iteration no 6705: Loss: 0.24944019371549042, accuracy: 0.99\n",
      "iteration no 6706: Loss: 0.2494444488733168, accuracy: 0.99\n",
      "iteration no 6707: Loss: 0.24943874404016572, accuracy: 0.99\n",
      "iteration no 6708: Loss: 0.24944292484316055, accuracy: 0.99\n",
      "iteration no 6709: Loss: 0.24943762943882308, accuracy: 0.99\n",
      "iteration no 6710: Loss: 0.24944155418503478, accuracy: 0.99\n",
      "iteration no 6711: Loss: 0.2494385378921946, accuracy: 0.99\n",
      "iteration no 6712: Loss: 0.24943971984633734, accuracy: 0.99\n",
      "iteration no 6713: Loss: 0.24943883408855516, accuracy: 0.99\n",
      "iteration no 6714: Loss: 0.24943740751731783, accuracy: 0.99\n",
      "iteration no 6715: Loss: 0.2494385505997862, accuracy: 0.99\n",
      "iteration no 6716: Loss: 0.24943550207863052, accuracy: 0.99\n",
      "iteration no 6717: Loss: 0.2494383714684404, accuracy: 0.99\n",
      "iteration no 6718: Loss: 0.249435093036315, accuracy: 0.99\n",
      "iteration no 6719: Loss: 0.249436925306795, accuracy: 0.99\n",
      "iteration no 6720: Loss: 0.24943513271183249, accuracy: 0.99\n",
      "iteration no 6721: Loss: 0.24943552107997735, accuracy: 0.99\n",
      "iteration no 6722: Loss: 0.24943478800309746, accuracy: 0.99\n",
      "iteration no 6723: Loss: 0.24943410895866938, accuracy: 0.99\n",
      "iteration no 6724: Loss: 0.24943495753996292, accuracy: 0.99\n",
      "iteration no 6725: Loss: 0.2494321724087747, accuracy: 0.99\n",
      "iteration no 6726: Loss: 0.24943591573585538, accuracy: 0.99\n",
      "iteration no 6727: Loss: 0.2494298151006047, accuracy: 0.99\n",
      "iteration no 6728: Loss: 0.24943535096835012, accuracy: 0.99\n",
      "iteration no 6729: Loss: 0.24942946426188045, accuracy: 0.99\n",
      "iteration no 6730: Loss: 0.24943280413703833, accuracy: 0.99\n",
      "iteration no 6731: Loss: 0.24943157274758543, accuracy: 0.99\n",
      "iteration no 6732: Loss: 0.24942912793557515, accuracy: 0.99\n",
      "iteration no 6733: Loss: 0.24943253962595474, accuracy: 0.99\n",
      "iteration no 6734: Loss: 0.24942661821882534, accuracy: 0.99\n",
      "iteration no 6735: Loss: 0.24943324798330146, accuracy: 0.99\n",
      "iteration no 6736: Loss: 0.24942621993123268, accuracy: 0.99\n",
      "iteration no 6737: Loss: 0.2494316841012711, accuracy: 0.99\n",
      "iteration no 6738: Loss: 0.24942585413469004, accuracy: 0.99\n",
      "iteration no 6739: Loss: 0.24942901555289626, accuracy: 0.99\n",
      "iteration no 6740: Loss: 0.24942790804593484, accuracy: 0.99\n",
      "iteration no 6741: Loss: 0.2494261943309149, accuracy: 0.99\n",
      "iteration no 6742: Loss: 0.2494278863433291, accuracy: 0.99\n",
      "iteration no 6743: Loss: 0.24942369865789737, accuracy: 0.99\n",
      "iteration no 6744: Loss: 0.2494273505592264, accuracy: 0.99\n",
      "iteration no 6745: Loss: 0.24942383958128395, accuracy: 0.99\n",
      "iteration no 6746: Loss: 0.24942556926947843, accuracy: 0.99\n",
      "iteration no 6747: Loss: 0.2494242725343471, accuracy: 0.99\n",
      "iteration no 6748: Loss: 0.24942440748621392, accuracy: 0.99\n",
      "iteration no 6749: Loss: 0.2494235528043095, accuracy: 0.99\n",
      "iteration no 6750: Loss: 0.24942319396925794, accuracy: 0.99\n",
      "iteration no 6751: Loss: 0.24942304203459328, accuracy: 0.99\n",
      "iteration no 6752: Loss: 0.24942191556225426, accuracy: 0.99\n",
      "iteration no 6753: Loss: 0.24942353555758, accuracy: 0.99\n",
      "iteration no 6754: Loss: 0.2494197578303622, accuracy: 0.99\n",
      "iteration no 6755: Loss: 0.24942345793113868, accuracy: 0.99\n",
      "iteration no 6756: Loss: 0.24941873940389359, accuracy: 0.99\n",
      "iteration no 6757: Loss: 0.24942184811120044, accuracy: 0.99\n",
      "iteration no 6758: Loss: 0.24941969167737205, accuracy: 0.99\n",
      "iteration no 6759: Loss: 0.2494197587228296, accuracy: 0.99\n",
      "iteration no 6760: Loss: 0.24942079029958164, accuracy: 0.99\n",
      "iteration no 6761: Loss: 0.24941770902182292, accuracy: 0.99\n",
      "iteration no 6762: Loss: 0.2494214965099667, accuracy: 0.99\n",
      "iteration no 6763: Loss: 0.24941508073420848, accuracy: 0.99\n",
      "iteration no 6764: Loss: 0.24942073460542796, accuracy: 0.99\n",
      "iteration no 6765: Loss: 0.2494149625441942, accuracy: 0.99\n",
      "iteration no 6766: Loss: 0.24941858793478536, accuracy: 0.99\n",
      "iteration no 6767: Loss: 0.24941645262791914, accuracy: 0.99\n",
      "iteration no 6768: Loss: 0.24941472652092925, accuracy: 0.99\n",
      "iteration no 6769: Loss: 0.2494174627740783, accuracy: 0.99\n",
      "iteration no 6770: Loss: 0.24941372880041468, accuracy: 0.99\n",
      "iteration no 6771: Loss: 0.2494171768859223, accuracy: 0.99\n",
      "iteration no 6772: Loss: 0.24941269943821642, accuracy: 0.99\n",
      "iteration no 6773: Loss: 0.24941511939371408, accuracy: 0.99\n",
      "iteration no 6774: Loss: 0.24941211329228952, accuracy: 0.99\n",
      "iteration no 6775: Loss: 0.24941474324771512, accuracy: 0.99\n",
      "iteration no 6776: Loss: 0.2494124236324095, accuracy: 0.99\n",
      "iteration no 6777: Loss: 0.24941211344054667, accuracy: 0.99\n",
      "iteration no 6778: Loss: 0.2494122285053137, accuracy: 0.99\n",
      "iteration no 6779: Loss: 0.24941134330203732, accuracy: 0.99\n",
      "iteration no 6780: Loss: 0.24941263354566004, accuracy: 0.99\n",
      "iteration no 6781: Loss: 0.24941032076775765, accuracy: 0.99\n",
      "iteration no 6782: Loss: 0.24941213111805438, accuracy: 0.99\n",
      "iteration no 6783: Loss: 0.24940789889096052, accuracy: 0.99\n",
      "iteration no 6784: Loss: 0.24941144426616882, accuracy: 0.99\n",
      "iteration no 6785: Loss: 0.24940777165171238, accuracy: 0.99\n",
      "iteration no 6786: Loss: 0.24941000735953894, accuracy: 0.99\n",
      "iteration no 6787: Loss: 0.24940914638790584, accuracy: 0.99\n",
      "iteration no 6788: Loss: 0.24940727004309704, accuracy: 0.99\n",
      "iteration no 6789: Loss: 0.24940930805994857, accuracy: 0.99\n",
      "iteration no 6790: Loss: 0.24940630607113795, accuracy: 0.99\n",
      "iteration no 6791: Loss: 0.24940830439204603, accuracy: 0.99\n",
      "iteration no 6792: Loss: 0.24940522029560622, accuracy: 0.99\n",
      "iteration no 6793: Loss: 0.24940577759132926, accuracy: 0.99\n",
      "iteration no 6794: Loss: 0.24940590312563604, accuracy: 0.99\n",
      "iteration no 6795: Loss: 0.24940506965571446, accuracy: 0.99\n",
      "iteration no 6796: Loss: 0.24940574053865217, accuracy: 0.99\n",
      "iteration no 6797: Loss: 0.24940399250223413, accuracy: 0.99\n",
      "iteration no 6798: Loss: 0.24940539450206306, accuracy: 0.99\n",
      "iteration no 6799: Loss: 0.2494035093398696, accuracy: 0.99\n",
      "iteration no 6800: Loss: 0.24940403571425687, accuracy: 0.99\n",
      "iteration no 6801: Loss: 0.24940284925070483, accuracy: 0.99\n",
      "iteration no 6802: Loss: 0.24940229777208156, accuracy: 0.99\n",
      "iteration no 6803: Loss: 0.24940182635355418, accuracy: 0.99\n",
      "iteration no 6804: Loss: 0.2494016676226169, accuracy: 0.99\n",
      "iteration no 6805: Loss: 0.2494011106218509, accuracy: 0.99\n",
      "iteration no 6806: Loss: 0.24940175268615727, accuracy: 0.99\n",
      "iteration no 6807: Loss: 0.2494001952159764, accuracy: 0.99\n",
      "iteration no 6808: Loss: 0.24940063435308615, accuracy: 0.99\n",
      "iteration no 6809: Loss: 0.24939981654456692, accuracy: 0.99\n",
      "iteration no 6810: Loss: 0.2493994869841274, accuracy: 0.99\n",
      "iteration no 6811: Loss: 0.24939905831456124, accuracy: 0.99\n",
      "iteration no 6812: Loss: 0.2493982724693107, accuracy: 0.99\n",
      "iteration no 6813: Loss: 0.2493984062823494, accuracy: 0.99\n",
      "iteration no 6814: Loss: 0.24939781774555214, accuracy: 0.99\n",
      "iteration no 6815: Loss: 0.2493974908195875, accuracy: 0.99\n",
      "iteration no 6816: Loss: 0.24939781367375397, accuracy: 0.99\n",
      "iteration no 6817: Loss: 0.2493962377352833, accuracy: 0.99\n",
      "iteration no 6818: Loss: 0.24939643927772054, accuracy: 0.99\n",
      "iteration no 6819: Loss: 0.2493964925152377, accuracy: 0.99\n",
      "iteration no 6820: Loss: 0.249394970105681, accuracy: 0.99\n",
      "iteration no 6821: Loss: 0.24939632382151716, accuracy: 0.99\n",
      "iteration no 6822: Loss: 0.24939281934368324, accuracy: 0.99\n",
      "iteration no 6823: Loss: 0.24939553812472154, accuracy: 0.99\n",
      "iteration no 6824: Loss: 0.24939353824641208, accuracy: 0.99\n",
      "iteration no 6825: Loss: 0.24939436524073022, accuracy: 0.99\n",
      "iteration no 6826: Loss: 0.2493944714791846, accuracy: 0.99\n",
      "iteration no 6827: Loss: 0.24939128595993595, accuracy: 0.99\n",
      "iteration no 6828: Loss: 0.24939411599547692, accuracy: 0.99\n",
      "iteration no 6829: Loss: 0.24939071804394375, accuracy: 0.99\n",
      "iteration no 6830: Loss: 0.2493938281390154, accuracy: 0.99\n",
      "iteration no 6831: Loss: 0.24939006449872447, accuracy: 0.99\n",
      "iteration no 6832: Loss: 0.24939095947688278, accuracy: 0.99\n",
      "iteration no 6833: Loss: 0.24939107526964105, accuracy: 0.99\n",
      "iteration no 6834: Loss: 0.24939000958064364, accuracy: 0.99\n",
      "iteration no 6835: Loss: 0.24939189532532152, accuracy: 0.99\n",
      "iteration no 6836: Loss: 0.24938707965772708, accuracy: 0.99\n",
      "iteration no 6837: Loss: 0.24939102171018068, accuracy: 0.99\n",
      "iteration no 6838: Loss: 0.24938757664054406, accuracy: 0.99\n",
      "iteration no 6839: Loss: 0.24938997354692022, accuracy: 0.99\n",
      "iteration no 6840: Loss: 0.24938711880127232, accuracy: 0.99\n",
      "iteration no 6841: Loss: 0.2493882275858197, accuracy: 0.99\n",
      "iteration no 6842: Loss: 0.24938699679625304, accuracy: 0.99\n",
      "iteration no 6843: Loss: 0.24938681590569234, accuracy: 0.99\n",
      "iteration no 6844: Loss: 0.24938719866903325, accuracy: 0.99\n",
      "iteration no 6845: Loss: 0.24938607794605916, accuracy: 0.99\n",
      "iteration no 6846: Loss: 0.24938732749383324, accuracy: 0.99\n",
      "iteration no 6847: Loss: 0.24938329708093343, accuracy: 0.99\n",
      "iteration no 6848: Loss: 0.24938834789103803, accuracy: 0.99\n",
      "iteration no 6849: Loss: 0.2493826716556367, accuracy: 0.99\n",
      "iteration no 6850: Loss: 0.24938593755475108, accuracy: 0.99\n",
      "iteration no 6851: Loss: 0.24938344902569745, accuracy: 0.99\n",
      "iteration no 6852: Loss: 0.249384144022213, accuracy: 0.99\n",
      "iteration no 6853: Loss: 0.24938443792179177, accuracy: 0.99\n",
      "iteration no 6854: Loss: 0.2493813946445319, accuracy: 0.99\n",
      "iteration no 6855: Loss: 0.24938578973067135, accuracy: 0.99\n",
      "iteration no 6856: Loss: 0.2493799203786171, accuracy: 0.99\n",
      "iteration no 6857: Loss: 0.24938478514821258, accuracy: 0.99\n",
      "iteration no 6858: Loss: 0.2493788446145297, accuracy: 0.99\n",
      "iteration no 6859: Loss: 0.24938323578385985, accuracy: 0.99\n",
      "iteration no 6860: Loss: 0.24937971259567737, accuracy: 0.99\n",
      "iteration no 6861: Loss: 0.24938090341500055, accuracy: 0.99\n",
      "iteration no 6862: Loss: 0.24938025114142795, accuracy: 0.99\n",
      "iteration no 6863: Loss: 0.24937871626453054, accuracy: 0.99\n",
      "iteration no 6864: Loss: 0.24938045552119298, accuracy: 0.99\n",
      "iteration no 6865: Loss: 0.2493782200457579, accuracy: 0.99\n",
      "iteration no 6866: Loss: 0.24937941176423362, accuracy: 0.99\n",
      "iteration no 6867: Loss: 0.24937712268257745, accuracy: 0.99\n",
      "iteration no 6868: Loss: 0.24937871834253195, accuracy: 0.99\n",
      "iteration no 6869: Loss: 0.2493761121686368, accuracy: 0.99\n",
      "iteration no 6870: Loss: 0.249378425392767, accuracy: 0.99\n",
      "iteration no 6871: Loss: 0.24937494347448355, accuracy: 0.99\n",
      "iteration no 6872: Loss: 0.24937683715195905, accuracy: 0.99\n",
      "iteration no 6873: Loss: 0.2493755952752807, accuracy: 0.99\n",
      "iteration no 6874: Loss: 0.24937607535307585, accuracy: 0.99\n",
      "iteration no 6875: Loss: 0.24937551618454254, accuracy: 0.99\n",
      "iteration no 6876: Loss: 0.24937396390987143, accuracy: 0.99\n",
      "iteration no 6877: Loss: 0.24937498308927059, accuracy: 0.99\n",
      "iteration no 6878: Loss: 0.24937308729400365, accuracy: 0.99\n",
      "iteration no 6879: Loss: 0.24937426094112558, accuracy: 0.99\n",
      "iteration no 6880: Loss: 0.24937356353798695, accuracy: 0.99\n",
      "iteration no 6881: Loss: 0.24937212192431268, accuracy: 0.99\n",
      "iteration no 6882: Loss: 0.24937278078923397, accuracy: 0.99\n",
      "iteration no 6883: Loss: 0.24937185575126208, accuracy: 0.99\n",
      "iteration no 6884: Loss: 0.24937334820431123, accuracy: 0.99\n",
      "iteration no 6885: Loss: 0.24937162753198025, accuracy: 0.99\n",
      "iteration no 6886: Loss: 0.24937034857512697, accuracy: 0.99\n",
      "iteration no 6887: Loss: 0.2493713671264623, accuracy: 0.99\n",
      "iteration no 6888: Loss: 0.2493691387910119, accuracy: 0.99\n",
      "iteration no 6889: Loss: 0.2493716611128169, accuracy: 0.99\n",
      "iteration no 6890: Loss: 0.2493693830039217, accuracy: 0.99\n",
      "iteration no 6891: Loss: 0.24936874838268702, accuracy: 0.99\n",
      "iteration no 6892: Loss: 0.24937095067061027, accuracy: 0.99\n",
      "iteration no 6893: Loss: 0.2493664729845025, accuracy: 0.99\n",
      "iteration no 6894: Loss: 0.24937167074519798, accuracy: 0.99\n",
      "iteration no 6895: Loss: 0.24936634858806989, accuracy: 0.99\n",
      "iteration no 6896: Loss: 0.24936899357283426, accuracy: 0.99\n",
      "iteration no 6897: Loss: 0.2493658100550808, accuracy: 0.99\n",
      "iteration no 6898: Loss: 0.24936787689109377, accuracy: 0.99\n",
      "iteration no 6899: Loss: 0.24936772373926253, accuracy: 0.99\n",
      "iteration no 6900: Loss: 0.24936568990982416, accuracy: 0.99\n",
      "iteration no 6901: Loss: 0.24936690200475758, accuracy: 0.99\n",
      "iteration no 6902: Loss: 0.24936418004695837, accuracy: 0.99\n",
      "iteration no 6903: Loss: 0.24936676889038595, accuracy: 0.99\n",
      "iteration no 6904: Loss: 0.24936375169172054, accuracy: 0.99\n",
      "iteration no 6905: Loss: 0.24936666544798916, accuracy: 0.99\n",
      "iteration no 6906: Loss: 0.2493625134522558, accuracy: 0.99\n",
      "iteration no 6907: Loss: 0.2493659472202247, accuracy: 0.99\n",
      "iteration no 6908: Loss: 0.2493615027992464, accuracy: 0.99\n",
      "iteration no 6909: Loss: 0.24936490833277636, accuracy: 0.99\n",
      "iteration no 6910: Loss: 0.24936198490044129, accuracy: 0.99\n",
      "iteration no 6911: Loss: 0.249362498184861, accuracy: 0.99\n",
      "iteration no 6912: Loss: 0.24936331888560825, accuracy: 0.99\n",
      "iteration no 6913: Loss: 0.24935961423202724, accuracy: 0.99\n",
      "iteration no 6914: Loss: 0.249363885815926, accuracy: 0.99\n",
      "iteration no 6915: Loss: 0.24935863703524108, accuracy: 0.99\n",
      "iteration no 6916: Loss: 0.24936243298466995, accuracy: 0.99\n",
      "iteration no 6917: Loss: 0.2493588076301881, accuracy: 0.99\n",
      "iteration no 6918: Loss: 0.24936033743006594, accuracy: 0.99\n",
      "iteration no 6919: Loss: 0.24936124785103092, accuracy: 0.99\n",
      "iteration no 6920: Loss: 0.24935866316717276, accuracy: 0.99\n",
      "iteration no 6921: Loss: 0.24936061649259994, accuracy: 0.99\n",
      "iteration no 6922: Loss: 0.24935708455757594, accuracy: 0.99\n",
      "iteration no 6923: Loss: 0.2493595832779149, accuracy: 0.99\n",
      "iteration no 6924: Loss: 0.24935732558457302, accuracy: 0.99\n",
      "iteration no 6925: Loss: 0.2493577915800252, accuracy: 0.99\n",
      "iteration no 6926: Loss: 0.24935716154006743, accuracy: 0.99\n",
      "iteration no 6927: Loss: 0.24935635861715266, accuracy: 0.99\n",
      "iteration no 6928: Loss: 0.24935596303611962, accuracy: 0.99\n",
      "iteration no 6929: Loss: 0.24935689000997613, accuracy: 0.99\n",
      "iteration no 6930: Loss: 0.24935558624755969, accuracy: 0.99\n",
      "iteration no 6931: Loss: 0.24935659090713472, accuracy: 0.99\n",
      "iteration no 6932: Loss: 0.24935355799021125, accuracy: 0.99\n",
      "iteration no 6933: Loss: 0.24935591919750993, accuracy: 0.99\n",
      "iteration no 6934: Loss: 0.24935367421361382, accuracy: 0.99\n",
      "iteration no 6935: Loss: 0.2493547275063317, accuracy: 0.99\n",
      "iteration no 6936: Loss: 0.2493535567421023, accuracy: 0.99\n",
      "iteration no 6937: Loss: 0.2493529336005238, accuracy: 0.99\n",
      "iteration no 6938: Loss: 0.24935287186476846, accuracy: 0.99\n",
      "iteration no 6939: Loss: 0.24935332052911724, accuracy: 0.99\n",
      "iteration no 6940: Loss: 0.24935240040371154, accuracy: 0.99\n",
      "iteration no 6941: Loss: 0.24935152143595107, accuracy: 0.99\n",
      "iteration no 6942: Loss: 0.2493521510461799, accuracy: 0.99\n",
      "iteration no 6943: Loss: 0.2493508533927694, accuracy: 0.99\n",
      "iteration no 6944: Loss: 0.24935289944798025, accuracy: 0.99\n",
      "iteration no 6945: Loss: 0.24934840705545758, accuracy: 0.99\n",
      "iteration no 6946: Loss: 0.24935221961414455, accuracy: 0.99\n",
      "iteration no 6947: Loss: 0.24934844392649444, accuracy: 0.99\n",
      "iteration no 6948: Loss: 0.24935063362698817, accuracy: 0.99\n",
      "iteration no 6949: Loss: 0.24935055015971033, accuracy: 0.99\n",
      "iteration no 6950: Loss: 0.24934755036573003, accuracy: 0.99\n",
      "iteration no 6951: Loss: 0.24935155616545968, accuracy: 0.99\n",
      "iteration no 6952: Loss: 0.24934552136187071, accuracy: 0.99\n",
      "iteration no 6953: Loss: 0.24935173930656296, accuracy: 0.99\n",
      "iteration no 6954: Loss: 0.24934539145976042, accuracy: 0.99\n",
      "iteration no 6955: Loss: 0.24934999662140603, accuracy: 0.99\n",
      "iteration no 6956: Loss: 0.24934483875786984, accuracy: 0.99\n",
      "iteration no 6957: Loss: 0.24934787432621314, accuracy: 0.99\n",
      "iteration no 6958: Loss: 0.24934678935458504, accuracy: 0.99\n",
      "iteration no 6959: Loss: 0.24934587738071196, accuracy: 0.99\n",
      "iteration no 6960: Loss: 0.2493466599372655, accuracy: 0.99\n",
      "iteration no 6961: Loss: 0.249343613246214, accuracy: 0.99\n",
      "iteration no 6962: Loss: 0.2493461097253128, accuracy: 0.99\n",
      "iteration no 6963: Loss: 0.24934323558403465, accuracy: 0.99\n",
      "iteration no 6964: Loss: 0.24934581433498776, accuracy: 0.99\n",
      "iteration no 6965: Loss: 0.2493427961859252, accuracy: 0.99\n",
      "iteration no 6966: Loss: 0.24934488852621461, accuracy: 0.99\n",
      "iteration no 6967: Loss: 0.24934163169811696, accuracy: 0.99\n",
      "iteration no 6968: Loss: 0.24934403369420924, accuracy: 0.99\n",
      "iteration no 6969: Loss: 0.2493421776011306, accuracy: 0.99\n",
      "iteration no 6970: Loss: 0.24934278429776366, accuracy: 0.99\n",
      "iteration no 6971: Loss: 0.24934219972149463, accuracy: 0.99\n",
      "iteration no 6972: Loss: 0.24934082004097802, accuracy: 0.99\n",
      "iteration no 6973: Loss: 0.249341783637868, accuracy: 0.99\n",
      "iteration no 6974: Loss: 0.24934063836008052, accuracy: 0.99\n",
      "iteration no 6975: Loss: 0.24934083894675163, accuracy: 0.99\n",
      "iteration no 6976: Loss: 0.2493406856686906, accuracy: 0.99\n",
      "iteration no 6977: Loss: 0.24933904239165453, accuracy: 0.99\n",
      "iteration no 6978: Loss: 0.2493398651866505, accuracy: 0.99\n",
      "iteration no 6979: Loss: 0.24933916209750245, accuracy: 0.99\n",
      "iteration no 6980: Loss: 0.24933901683974313, accuracy: 0.99\n",
      "iteration no 6981: Loss: 0.2493397720905694, accuracy: 0.99\n",
      "iteration no 6982: Loss: 0.24933646714729663, accuracy: 0.99\n",
      "iteration no 6983: Loss: 0.24933968689554392, accuracy: 0.99\n",
      "iteration no 6984: Loss: 0.24933550393155401, accuracy: 0.99\n",
      "iteration no 6985: Loss: 0.24933989875727897, accuracy: 0.99\n",
      "iteration no 6986: Loss: 0.24933662876735524, accuracy: 0.99\n",
      "iteration no 6987: Loss: 0.2493363964527012, accuracy: 0.99\n",
      "iteration no 6988: Loss: 0.24933798067969856, accuracy: 0.99\n",
      "iteration no 6989: Loss: 0.2493342148968821, accuracy: 0.99\n",
      "iteration no 6990: Loss: 0.24933936034921672, accuracy: 0.99\n",
      "iteration no 6991: Loss: 0.24933319305449483, accuracy: 0.99\n",
      "iteration no 6992: Loss: 0.2493371190662728, accuracy: 0.99\n",
      "iteration no 6993: Loss: 0.24933220821492447, accuracy: 0.99\n",
      "iteration no 6994: Loss: 0.249336568719075, accuracy: 0.99\n",
      "iteration no 6995: Loss: 0.2493341895142286, accuracy: 0.99\n",
      "iteration no 6996: Loss: 0.2493337478863806, accuracy: 0.99\n",
      "iteration no 6997: Loss: 0.24933377161946985, accuracy: 0.99\n",
      "iteration no 6998: Loss: 0.24933220638266917, accuracy: 0.99\n",
      "iteration no 6999: Loss: 0.24933388245898305, accuracy: 0.99\n",
      "iteration no 7000: Loss: 0.24933193187218458, accuracy: 0.99\n",
      "iteration no 7001: Loss: 0.24933292371856292, accuracy: 0.99\n",
      "iteration no 7002: Loss: 0.24933075777403868, accuracy: 0.99\n",
      "iteration no 7003: Loss: 0.24933267620128702, accuracy: 0.99\n",
      "iteration no 7004: Loss: 0.24933009293880237, accuracy: 0.99\n",
      "iteration no 7005: Loss: 0.24933229141952995, accuracy: 0.99\n",
      "iteration no 7006: Loss: 0.24932870303510551, accuracy: 0.99\n",
      "iteration no 7007: Loss: 0.2493309318175952, accuracy: 0.99\n",
      "iteration no 7008: Loss: 0.24932949889820558, accuracy: 0.99\n",
      "iteration no 7009: Loss: 0.24932986666903223, accuracy: 0.99\n",
      "iteration no 7010: Loss: 0.24933072515310967, accuracy: 0.99\n",
      "iteration no 7011: Loss: 0.2493279795681571, accuracy: 0.99\n",
      "iteration no 7012: Loss: 0.24932943950391212, accuracy: 0.99\n",
      "iteration no 7013: Loss: 0.2493279583904006, accuracy: 0.99\n",
      "iteration no 7014: Loss: 0.24932767473973733, accuracy: 0.99\n",
      "iteration no 7015: Loss: 0.24932864955791095, accuracy: 0.99\n",
      "iteration no 7016: Loss: 0.24932559852554342, accuracy: 0.99\n",
      "iteration no 7017: Loss: 0.24932795886144338, accuracy: 0.99\n",
      "iteration no 7018: Loss: 0.24932731825016002, accuracy: 0.99\n",
      "iteration no 7019: Loss: 0.2493254502130431, accuracy: 0.99\n",
      "iteration no 7020: Loss: 0.24932857848209364, accuracy: 0.99\n",
      "iteration no 7021: Loss: 0.24932284286949222, accuracy: 0.99\n",
      "iteration no 7022: Loss: 0.24932809622395288, accuracy: 0.99\n",
      "iteration no 7023: Loss: 0.24932256370117586, accuracy: 0.99\n",
      "iteration no 7024: Loss: 0.24932705730757548, accuracy: 0.99\n",
      "iteration no 7025: Loss: 0.24932359463432613, accuracy: 0.99\n",
      "iteration no 7026: Loss: 0.24932440597806121, accuracy: 0.99\n",
      "iteration no 7027: Loss: 0.2493246544432736, accuracy: 0.99\n",
      "iteration no 7028: Loss: 0.2493223751992884, accuracy: 0.99\n",
      "iteration no 7029: Loss: 0.24932482636838393, accuracy: 0.99\n",
      "iteration no 7030: Loss: 0.24932134323090183, accuracy: 0.99\n",
      "iteration no 7031: Loss: 0.24932379514846298, accuracy: 0.99\n",
      "iteration no 7032: Loss: 0.24932069138042207, accuracy: 0.99\n",
      "iteration no 7033: Loss: 0.2493234545850278, accuracy: 0.99\n",
      "iteration no 7034: Loss: 0.2493204460666938, accuracy: 0.99\n",
      "iteration no 7035: Loss: 0.24932366420279745, accuracy: 0.99\n",
      "iteration no 7036: Loss: 0.2493190423564331, accuracy: 0.99\n",
      "iteration no 7037: Loss: 0.2493218386396201, accuracy: 0.99\n",
      "iteration no 7038: Loss: 0.24931936112030656, accuracy: 0.99\n",
      "iteration no 7039: Loss: 0.24932065427415895, accuracy: 0.99\n",
      "iteration no 7040: Loss: 0.24932130299230768, accuracy: 0.99\n",
      "iteration no 7041: Loss: 0.249318026494572, accuracy: 0.99\n",
      "iteration no 7042: Loss: 0.2493210821653667, accuracy: 0.99\n",
      "iteration no 7043: Loss: 0.24931695814856725, accuracy: 0.99\n",
      "iteration no 7044: Loss: 0.24932045012781642, accuracy: 0.99\n",
      "iteration no 7045: Loss: 0.24931735033776353, accuracy: 0.99\n",
      "iteration no 7046: Loss: 0.2493177948125179, accuracy: 0.99\n",
      "iteration no 7047: Loss: 0.24931737819148864, accuracy: 0.99\n",
      "iteration no 7048: Loss: 0.2493162581731838, accuracy: 0.99\n",
      "iteration no 7049: Loss: 0.24931780622088062, accuracy: 0.99\n",
      "iteration no 7050: Loss: 0.24931731436337723, accuracy: 0.99\n",
      "iteration no 7051: Loss: 0.24931515344990846, accuracy: 0.99\n",
      "iteration no 7052: Loss: 0.2493175777551974, accuracy: 0.99\n",
      "iteration no 7053: Loss: 0.24931319777412922, accuracy: 0.99\n",
      "iteration no 7054: Loss: 0.24931755211841874, accuracy: 0.99\n",
      "iteration no 7055: Loss: 0.24931405697376358, accuracy: 0.99\n",
      "iteration no 7056: Loss: 0.249315421866184, accuracy: 0.99\n",
      "iteration no 7057: Loss: 0.24931526854905287, accuracy: 0.99\n",
      "iteration no 7058: Loss: 0.24931238406622325, accuracy: 0.99\n",
      "iteration no 7059: Loss: 0.2493160735969813, accuracy: 0.99\n",
      "iteration no 7060: Loss: 0.24931135036144939, accuracy: 0.99\n",
      "iteration no 7061: Loss: 0.24931555290318552, accuracy: 0.99\n",
      "iteration no 7062: Loss: 0.24931101693662913, accuracy: 0.99\n",
      "iteration no 7063: Loss: 0.24931338842429082, accuracy: 0.99\n",
      "iteration no 7064: Loss: 0.2493116445203618, accuracy: 0.99\n",
      "iteration no 7065: Loss: 0.24931306538559983, accuracy: 0.99\n",
      "iteration no 7066: Loss: 0.24931175008411824, accuracy: 0.99\n",
      "iteration no 7067: Loss: 0.2493112635798022, accuracy: 0.99\n",
      "iteration no 7068: Loss: 0.2493108790526501, accuracy: 0.99\n",
      "iteration no 7069: Loss: 0.24931057320172784, accuracy: 0.99\n",
      "iteration no 7070: Loss: 0.2493115522615742, accuracy: 0.99\n",
      "iteration no 7071: Loss: 0.2493100233801669, accuracy: 0.99\n",
      "iteration no 7072: Loss: 0.24931048250244559, accuracy: 0.99\n",
      "iteration no 7073: Loss: 0.24930800578062434, accuracy: 0.99\n",
      "iteration no 7074: Loss: 0.24931034112251757, accuracy: 0.99\n",
      "iteration no 7075: Loss: 0.24930838201668887, accuracy: 0.99\n",
      "iteration no 7076: Loss: 0.24930946063721543, accuracy: 0.99\n",
      "iteration no 7077: Loss: 0.24930801083639825, accuracy: 0.99\n",
      "iteration no 7078: Loss: 0.24930757251577684, accuracy: 0.99\n",
      "iteration no 7079: Loss: 0.2493077275436662, accuracy: 0.99\n",
      "iteration no 7080: Loss: 0.2493082967428015, accuracy: 0.99\n",
      "iteration no 7081: Loss: 0.24930665316937295, accuracy: 0.99\n",
      "iteration no 7082: Loss: 0.24930774382499038, accuracy: 0.99\n",
      "iteration no 7083: Loss: 0.24930407383252465, accuracy: 0.99\n",
      "iteration no 7084: Loss: 0.24930844164186838, accuracy: 0.99\n",
      "iteration no 7085: Loss: 0.24930403004227053, accuracy: 0.99\n",
      "iteration no 7086: Loss: 0.24930775655193488, accuracy: 0.99\n",
      "iteration no 7087: Loss: 0.24930562507653775, accuracy: 0.99\n",
      "iteration no 7088: Loss: 0.24930428393167992, accuracy: 0.99\n",
      "iteration no 7089: Loss: 0.249306194741225, accuracy: 0.99\n",
      "iteration no 7090: Loss: 0.2493035214181442, accuracy: 0.99\n",
      "iteration no 7091: Loss: 0.24930591947778646, accuracy: 0.99\n",
      "iteration no 7092: Loss: 0.24930182639212156, accuracy: 0.99\n",
      "iteration no 7093: Loss: 0.2493050362595576, accuracy: 0.99\n",
      "iteration no 7094: Loss: 0.24930216783883702, accuracy: 0.99\n",
      "iteration no 7095: Loss: 0.24930493948953383, accuracy: 0.99\n",
      "iteration no 7096: Loss: 0.24930128157173131, accuracy: 0.99\n",
      "iteration no 7097: Loss: 0.2493033690667897, accuracy: 0.99\n",
      "iteration no 7098: Loss: 0.249301088921642, accuracy: 0.99\n",
      "iteration no 7099: Loss: 0.24930190498033983, accuracy: 0.99\n",
      "iteration no 7100: Loss: 0.2493023842249205, accuracy: 0.99\n",
      "iteration no 7101: Loss: 0.24930094700561922, accuracy: 0.99\n",
      "iteration no 7102: Loss: 0.24930131680141046, accuracy: 0.99\n",
      "iteration no 7103: Loss: 0.24929961345333906, accuracy: 0.99\n",
      "iteration no 7104: Loss: 0.24930087306422027, accuracy: 0.99\n",
      "iteration no 7105: Loss: 0.24929942682435285, accuracy: 0.99\n",
      "iteration no 7106: Loss: 0.24929966079063437, accuracy: 0.99\n",
      "iteration no 7107: Loss: 0.2492996003061314, accuracy: 0.99\n",
      "iteration no 7108: Loss: 0.2492989256882207, accuracy: 0.99\n",
      "iteration no 7109: Loss: 0.24929857420713117, accuracy: 0.99\n",
      "iteration no 7110: Loss: 0.24929911265964724, accuracy: 0.99\n",
      "iteration no 7111: Loss: 0.24929768929255897, accuracy: 0.99\n",
      "iteration no 7112: Loss: 0.24929946709581746, accuracy: 0.99\n",
      "iteration no 7113: Loss: 0.24929506216929917, accuracy: 0.99\n",
      "iteration no 7114: Loss: 0.24929966862111635, accuracy: 0.99\n",
      "iteration no 7115: Loss: 0.2492946845076276, accuracy: 0.99\n",
      "iteration no 7116: Loss: 0.24929942912055453, accuracy: 0.99\n",
      "iteration no 7117: Loss: 0.24929498463498867, accuracy: 0.99\n",
      "iteration no 7118: Loss: 0.2492966939894417, accuracy: 0.99\n",
      "iteration no 7119: Loss: 0.24929635615439033, accuracy: 0.99\n",
      "iteration no 7120: Loss: 0.2492949164582959, accuracy: 0.99\n",
      "iteration no 7121: Loss: 0.24929803787393442, accuracy: 0.99\n",
      "iteration no 7122: Loss: 0.24929271694856733, accuracy: 0.99\n",
      "iteration no 7123: Loss: 0.24929723060515394, accuracy: 0.99\n",
      "iteration no 7124: Loss: 0.24929246677459357, accuracy: 0.99\n",
      "iteration no 7125: Loss: 0.2492956611514338, accuracy: 0.99\n",
      "iteration no 7126: Loss: 0.24929282335054725, accuracy: 0.99\n",
      "iteration no 7127: Loss: 0.2492936897726138, accuracy: 0.99\n",
      "iteration no 7128: Loss: 0.24929299646675201, accuracy: 0.99\n",
      "iteration no 7129: Loss: 0.24929254479426594, accuracy: 0.99\n",
      "iteration no 7130: Loss: 0.24929312299953665, accuracy: 0.99\n",
      "iteration no 7131: Loss: 0.24929292165824557, accuracy: 0.99\n",
      "iteration no 7132: Loss: 0.24929128303659245, accuracy: 0.99\n",
      "iteration no 7133: Loss: 0.2492925879722221, accuracy: 0.99\n",
      "iteration no 7134: Loss: 0.24929066767889851, accuracy: 0.99\n",
      "iteration no 7135: Loss: 0.2492918180530892, accuracy: 0.99\n",
      "iteration no 7136: Loss: 0.24929059932720066, accuracy: 0.99\n",
      "iteration no 7137: Loss: 0.24929104685530928, accuracy: 0.99\n",
      "iteration no 7138: Loss: 0.24929163572955682, accuracy: 0.99\n",
      "iteration no 7139: Loss: 0.24928912977568868, accuracy: 0.99\n",
      "iteration no 7140: Loss: 0.24929109625033194, accuracy: 0.99\n",
      "iteration no 7141: Loss: 0.24928828303067002, accuracy: 0.99\n",
      "iteration no 7142: Loss: 0.2492905620966021, accuracy: 0.99\n",
      "iteration no 7143: Loss: 0.24928670904940387, accuracy: 0.99\n",
      "iteration no 7144: Loss: 0.24929045041299874, accuracy: 0.99\n",
      "iteration no 7145: Loss: 0.24928664882470175, accuracy: 0.99\n",
      "iteration no 7146: Loss: 0.24929026636813098, accuracy: 0.99\n",
      "iteration no 7147: Loss: 0.2492863896894003, accuracy: 0.99\n",
      "iteration no 7148: Loss: 0.24928815810517913, accuracy: 0.99\n",
      "iteration no 7149: Loss: 0.24928637482023913, accuracy: 0.99\n",
      "iteration no 7150: Loss: 0.2492866386359957, accuracy: 0.99\n",
      "iteration no 7151: Loss: 0.2492887718857788, accuracy: 0.99\n",
      "iteration no 7152: Loss: 0.24928480073979792, accuracy: 0.99\n",
      "iteration no 7153: Loss: 0.24928842888160788, accuracy: 0.99\n",
      "iteration no 7154: Loss: 0.2492833694066678, accuracy: 0.99\n",
      "iteration no 7155: Loss: 0.24928743145007518, accuracy: 0.99\n",
      "iteration no 7156: Loss: 0.24928585360411012, accuracy: 0.99\n",
      "iteration no 7157: Loss: 0.24928497683540474, accuracy: 0.99\n",
      "iteration no 7158: Loss: 0.24928400762049654, accuracy: 0.99\n",
      "iteration no 7159: Loss: 0.24928379792374095, accuracy: 0.99\n",
      "iteration no 7160: Loss: 0.24928382510277614, accuracy: 0.99\n",
      "iteration no 7161: Loss: 0.24928521673184678, accuracy: 0.99\n",
      "iteration no 7162: Loss: 0.24928221084600388, accuracy: 0.99\n",
      "iteration no 7163: Loss: 0.24928485604389838, accuracy: 0.99\n",
      "iteration no 7164: Loss: 0.2492812321546904, accuracy: 0.99\n",
      "iteration no 7165: Loss: 0.24928387302299682, accuracy: 0.99\n",
      "iteration no 7166: Loss: 0.24928160873424005, accuracy: 0.99\n",
      "iteration no 7167: Loss: 0.24928275679689949, accuracy: 0.99\n",
      "iteration no 7168: Loss: 0.24928112770967947, accuracy: 0.99\n",
      "iteration no 7169: Loss: 0.2492818192945978, accuracy: 0.99\n",
      "iteration no 7170: Loss: 0.24928138317495233, accuracy: 0.99\n",
      "iteration no 7171: Loss: 0.24928096462076002, accuracy: 0.99\n",
      "iteration no 7172: Loss: 0.249281066012973, accuracy: 0.99\n",
      "iteration no 7173: Loss: 0.24927973859960473, accuracy: 0.99\n",
      "iteration no 7174: Loss: 0.24928142627588978, accuracy: 0.99\n",
      "iteration no 7175: Loss: 0.24927861570036774, accuracy: 0.99\n",
      "iteration no 7176: Loss: 0.2492817604174953, accuracy: 0.99\n",
      "iteration no 7177: Loss: 0.24927733733429575, accuracy: 0.99\n",
      "iteration no 7178: Loss: 0.24928064910032666, accuracy: 0.99\n",
      "iteration no 7179: Loss: 0.2492766468350554, accuracy: 0.99\n",
      "iteration no 7180: Loss: 0.24927967324132538, accuracy: 0.99\n",
      "iteration no 7181: Loss: 0.24927790678440714, accuracy: 0.99\n",
      "iteration no 7182: Loss: 0.24927795047770787, accuracy: 0.99\n",
      "iteration no 7183: Loss: 0.24927816272672798, accuracy: 0.99\n",
      "iteration no 7184: Loss: 0.2492765937972386, accuracy: 0.99\n",
      "iteration no 7185: Loss: 0.2492770938896125, accuracy: 0.99\n",
      "iteration no 7186: Loss: 0.2492775983793563, accuracy: 0.99\n",
      "iteration no 7187: Loss: 0.24927528062214344, accuracy: 0.99\n",
      "iteration no 7188: Loss: 0.24927746874924175, accuracy: 0.99\n",
      "iteration no 7189: Loss: 0.24927399111678333, accuracy: 0.99\n",
      "iteration no 7190: Loss: 0.24927714980765345, accuracy: 0.99\n",
      "iteration no 7191: Loss: 0.24927728853532538, accuracy: 0.99\n",
      "iteration no 7192: Loss: 0.24927420457212696, accuracy: 0.99\n",
      "iteration no 7193: Loss: 0.24927668219596233, accuracy: 0.99\n",
      "iteration no 7194: Loss: 0.24927246109963178, accuracy: 0.99\n",
      "iteration no 7195: Loss: 0.24927621989738308, accuracy: 0.99\n",
      "iteration no 7196: Loss: 0.24927277507306997, accuracy: 0.99\n",
      "iteration no 7197: Loss: 0.24927501771824645, accuracy: 0.99\n",
      "iteration no 7198: Loss: 0.249272254050032, accuracy: 0.99\n",
      "iteration no 7199: Loss: 0.24927421274708678, accuracy: 0.99\n",
      "iteration no 7200: Loss: 0.2492718608931943, accuracy: 0.99\n",
      "iteration no 7201: Loss: 0.24927381666452608, accuracy: 0.99\n",
      "iteration no 7202: Loss: 0.24927176965380804, accuracy: 0.99\n",
      "iteration no 7203: Loss: 0.24927243194203177, accuracy: 0.99\n",
      "iteration no 7204: Loss: 0.24927170186671865, accuracy: 0.99\n",
      "iteration no 7205: Loss: 0.24927165653631222, accuracy: 0.99\n",
      "iteration no 7206: Loss: 0.24927169500016097, accuracy: 0.99\n",
      "iteration no 7207: Loss: 0.2492704679152557, accuracy: 0.99\n",
      "iteration no 7208: Loss: 0.24927096252114297, accuracy: 0.99\n",
      "iteration no 7209: Loss: 0.24927012694941003, accuracy: 0.99\n",
      "iteration no 7210: Loss: 0.249269646188613, accuracy: 0.99\n",
      "iteration no 7211: Loss: 0.24927060422852543, accuracy: 0.99\n",
      "iteration no 7212: Loss: 0.24926952061836022, accuracy: 0.99\n",
      "iteration no 7213: Loss: 0.2492695010777775, accuracy: 0.99\n",
      "iteration no 7214: Loss: 0.24926938227931528, accuracy: 0.99\n",
      "iteration no 7215: Loss: 0.2492677874914176, accuracy: 0.99\n",
      "iteration no 7216: Loss: 0.24927065620834574, accuracy: 0.99\n",
      "iteration no 7217: Loss: 0.24926583591896523, accuracy: 0.99\n",
      "iteration no 7218: Loss: 0.24927183627225458, accuracy: 0.99\n",
      "iteration no 7219: Loss: 0.24926484335804552, accuracy: 0.99\n",
      "iteration no 7220: Loss: 0.24926985846540906, accuracy: 0.99\n",
      "iteration no 7221: Loss: 0.2492659261721713, accuracy: 0.99\n",
      "iteration no 7222: Loss: 0.24926802643164137, accuracy: 0.99\n",
      "iteration no 7223: Loss: 0.24926736192672716, accuracy: 0.99\n",
      "iteration no 7224: Loss: 0.2492647610318304, accuracy: 0.99\n",
      "iteration no 7225: Loss: 0.24926805220950815, accuracy: 0.99\n",
      "iteration no 7226: Loss: 0.2492649895876402, accuracy: 0.99\n",
      "iteration no 7227: Loss: 0.2492667284126287, accuracy: 0.99\n",
      "iteration no 7228: Loss: 0.24926498021318216, accuracy: 0.99\n",
      "iteration no 7229: Loss: 0.2492641605781037, accuracy: 0.99\n",
      "iteration no 7230: Loss: 0.2492650498059487, accuracy: 0.99\n",
      "iteration no 7231: Loss: 0.24926334791270627, accuracy: 0.99\n",
      "iteration no 7232: Loss: 0.24926555240923687, accuracy: 0.99\n",
      "iteration no 7233: Loss: 0.24926467369108957, accuracy: 0.99\n",
      "iteration no 7234: Loss: 0.24926323160744668, accuracy: 0.99\n",
      "iteration no 7235: Loss: 0.24926427507952403, accuracy: 0.99\n",
      "iteration no 7236: Loss: 0.24926191159667677, accuracy: 0.99\n",
      "iteration no 7237: Loss: 0.24926436140709865, accuracy: 0.99\n",
      "iteration no 7238: Loss: 0.24926099334931648, accuracy: 0.99\n",
      "iteration no 7239: Loss: 0.24926385886662789, accuracy: 0.99\n",
      "iteration no 7240: Loss: 0.2492599811703779, accuracy: 0.99\n",
      "iteration no 7241: Loss: 0.24926388407144606, accuracy: 0.99\n",
      "iteration no 7242: Loss: 0.24926033410384713, accuracy: 0.99\n",
      "iteration no 7243: Loss: 0.24926248515279487, accuracy: 0.99\n",
      "iteration no 7244: Loss: 0.24926001129306452, accuracy: 0.99\n",
      "iteration no 7245: Loss: 0.2492607348194976, accuracy: 0.99\n",
      "iteration no 7246: Loss: 0.24926127223815991, accuracy: 0.99\n",
      "iteration no 7247: Loss: 0.24925951809484848, accuracy: 0.99\n",
      "iteration no 7248: Loss: 0.24926163485946054, accuracy: 0.99\n",
      "iteration no 7249: Loss: 0.24925769351985458, accuracy: 0.99\n",
      "iteration no 7250: Loss: 0.249260112558578, accuracy: 0.99\n",
      "iteration no 7251: Loss: 0.249258147326923, accuracy: 0.99\n",
      "iteration no 7252: Loss: 0.2492593925629059, accuracy: 0.99\n",
      "iteration no 7253: Loss: 0.249258694342749, accuracy: 0.99\n",
      "iteration no 7254: Loss: 0.24925721142103674, accuracy: 0.99\n",
      "iteration no 7255: Loss: 0.24925820544818783, accuracy: 0.99\n",
      "iteration no 7256: Loss: 0.249257670456968, accuracy: 0.99\n",
      "iteration no 7257: Loss: 0.24925747058817405, accuracy: 0.99\n",
      "iteration no 7258: Loss: 0.2492591197349984, accuracy: 0.99\n",
      "iteration no 7259: Loss: 0.2492540367975604, accuracy: 0.99\n",
      "iteration no 7260: Loss: 0.24925919305284672, accuracy: 0.99\n",
      "iteration no 7261: Loss: 0.24925344285845108, accuracy: 0.99\n",
      "iteration no 7262: Loss: 0.2492590886783181, accuracy: 0.99\n",
      "iteration no 7263: Loss: 0.24925634537269553, accuracy: 0.99\n",
      "iteration no 7264: Loss: 0.24925602112172002, accuracy: 0.99\n",
      "iteration no 7265: Loss: 0.24925534177963776, accuracy: 0.99\n",
      "iteration no 7266: Loss: 0.24925463344257293, accuracy: 0.99\n",
      "iteration no 7267: Loss: 0.24925562508813331, accuracy: 0.99\n",
      "iteration no 7268: Loss: 0.24925398916719999, accuracy: 0.99\n",
      "iteration no 7269: Loss: 0.24925474565104816, accuracy: 0.99\n",
      "iteration no 7270: Loss: 0.24925319981071983, accuracy: 0.99\n",
      "iteration no 7271: Loss: 0.2492545228750016, accuracy: 0.99\n",
      "iteration no 7272: Loss: 0.24925265905959237, accuracy: 0.99\n",
      "iteration no 7273: Loss: 0.2492539999554464, accuracy: 0.99\n",
      "iteration no 7274: Loss: 0.2492518560612355, accuracy: 0.99\n",
      "iteration no 7275: Loss: 0.24925262507762322, accuracy: 0.99\n",
      "iteration no 7276: Loss: 0.2492520671263297, accuracy: 0.99\n",
      "iteration no 7277: Loss: 0.24925337006541842, accuracy: 0.99\n",
      "iteration no 7278: Loss: 0.2492515114647718, accuracy: 0.99\n",
      "iteration no 7279: Loss: 0.24925190478691345, accuracy: 0.99\n",
      "iteration no 7280: Loss: 0.24925036451461585, accuracy: 0.99\n",
      "iteration no 7281: Loss: 0.24925230684828628, accuracy: 0.99\n",
      "iteration no 7282: Loss: 0.2492488858444994, accuracy: 0.99\n",
      "iteration no 7283: Loss: 0.24925303955381645, accuracy: 0.99\n",
      "iteration no 7284: Loss: 0.24924815499223818, accuracy: 0.99\n",
      "iteration no 7285: Loss: 0.24925132125010968, accuracy: 0.99\n",
      "iteration no 7286: Loss: 0.24924862890070426, accuracy: 0.99\n",
      "iteration no 7287: Loss: 0.24925033147899572, accuracy: 0.99\n",
      "iteration no 7288: Loss: 0.24925018879953492, accuracy: 0.99\n",
      "iteration no 7289: Loss: 0.24924684518812806, accuracy: 0.99\n",
      "iteration no 7290: Loss: 0.24925136598259695, accuracy: 0.99\n",
      "iteration no 7291: Loss: 0.2492466104456438, accuracy: 0.99\n",
      "iteration no 7292: Loss: 0.24925009027344736, accuracy: 0.99\n",
      "iteration no 7293: Loss: 0.24924708328553508, accuracy: 0.99\n",
      "iteration no 7294: Loss: 0.2492475845893229, accuracy: 0.99\n",
      "iteration no 7295: Loss: 0.24924744026186324, accuracy: 0.99\n",
      "iteration no 7296: Loss: 0.2492460480616436, accuracy: 0.99\n",
      "iteration no 7297: Loss: 0.24924792800636975, accuracy: 0.99\n",
      "iteration no 7298: Loss: 0.24924771775956842, accuracy: 0.99\n",
      "iteration no 7299: Loss: 0.24924548775244348, accuracy: 0.99\n",
      "iteration no 7300: Loss: 0.24924767436853104, accuracy: 0.99\n",
      "iteration no 7301: Loss: 0.24924413880966648, accuracy: 0.99\n",
      "iteration no 7302: Loss: 0.24924706526821516, accuracy: 0.99\n",
      "iteration no 7303: Loss: 0.24924405601012656, accuracy: 0.99\n",
      "iteration no 7304: Loss: 0.24924610777237854, accuracy: 0.99\n",
      "iteration no 7305: Loss: 0.24924353697759824, accuracy: 0.99\n",
      "iteration no 7306: Loss: 0.24924558694909943, accuracy: 0.99\n",
      "iteration no 7307: Loss: 0.24924374691129464, accuracy: 0.99\n",
      "iteration no 7308: Loss: 0.24924471135996212, accuracy: 0.99\n",
      "iteration no 7309: Loss: 0.2492431420922832, accuracy: 0.99\n",
      "iteration no 7310: Loss: 0.24924339985542304, accuracy: 0.99\n",
      "iteration no 7311: Loss: 0.24924427132171187, accuracy: 0.99\n",
      "iteration no 7312: Loss: 0.24924199647250694, accuracy: 0.99\n",
      "iteration no 7313: Loss: 0.24924452821507107, accuracy: 0.99\n",
      "iteration no 7314: Loss: 0.24924066331653874, accuracy: 0.99\n",
      "iteration no 7315: Loss: 0.24924359267020466, accuracy: 0.99\n",
      "iteration no 7316: Loss: 0.24924081043594637, accuracy: 0.99\n",
      "iteration no 7317: Loss: 0.24924216316924636, accuracy: 0.99\n",
      "iteration no 7318: Loss: 0.2492413773520652, accuracy: 0.99\n",
      "iteration no 7319: Loss: 0.24924036276855316, accuracy: 0.99\n",
      "iteration no 7320: Loss: 0.2492415244162919, accuracy: 0.99\n",
      "iteration no 7321: Loss: 0.2492405627591602, accuracy: 0.99\n",
      "iteration no 7322: Loss: 0.2492401774822524, accuracy: 0.99\n",
      "iteration no 7323: Loss: 0.24924195238917213, accuracy: 0.99\n",
      "iteration no 7324: Loss: 0.2492382019640691, accuracy: 0.99\n",
      "iteration no 7325: Loss: 0.24924251940127629, accuracy: 0.99\n",
      "iteration no 7326: Loss: 0.24923591484386684, accuracy: 0.99\n",
      "iteration no 7327: Loss: 0.24924209514342022, accuracy: 0.99\n",
      "iteration no 7328: Loss: 0.24923816041694036, accuracy: 0.99\n",
      "iteration no 7329: Loss: 0.2492392015948602, accuracy: 0.99\n",
      "iteration no 7330: Loss: 0.24923909272752953, accuracy: 0.99\n",
      "iteration no 7331: Loss: 0.24923658823346054, accuracy: 0.99\n",
      "iteration no 7332: Loss: 0.2492391723256652, accuracy: 0.99\n",
      "iteration no 7333: Loss: 0.24923632232774728, accuracy: 0.99\n",
      "iteration no 7334: Loss: 0.24923882118252527, accuracy: 0.99\n",
      "iteration no 7335: Loss: 0.2492363843065658, accuracy: 0.99\n",
      "iteration no 7336: Loss: 0.249236185732583, accuracy: 0.99\n",
      "iteration no 7337: Loss: 0.24923689662196308, accuracy: 0.99\n",
      "iteration no 7338: Loss: 0.24923774960480186, accuracy: 0.99\n",
      "iteration no 7339: Loss: 0.24923558354409048, accuracy: 0.99\n",
      "iteration no 7340: Loss: 0.24923556611401937, accuracy: 0.99\n",
      "iteration no 7341: Loss: 0.24923560279927143, accuracy: 0.99\n",
      "iteration no 7342: Loss: 0.24923548784413335, accuracy: 0.99\n",
      "iteration no 7343: Loss: 0.24923543779890656, accuracy: 0.99\n",
      "iteration no 7344: Loss: 0.24923568300810478, accuracy: 0.99\n",
      "iteration no 7345: Loss: 0.24923370714587634, accuracy: 0.99\n",
      "iteration no 7346: Loss: 0.24923615594299875, accuracy: 0.99\n",
      "iteration no 7347: Loss: 0.24923176136386693, accuracy: 0.99\n",
      "iteration no 7348: Loss: 0.24923709838475963, accuracy: 0.99\n",
      "iteration no 7349: Loss: 0.24923082417448944, accuracy: 0.99\n",
      "iteration no 7350: Loss: 0.24923579295476755, accuracy: 0.99\n",
      "iteration no 7351: Loss: 0.24923162201321308, accuracy: 0.99\n",
      "iteration no 7352: Loss: 0.24923322569710243, accuracy: 0.99\n",
      "iteration no 7353: Loss: 0.24923361344101302, accuracy: 0.99\n",
      "iteration no 7354: Loss: 0.24923113973479288, accuracy: 0.99\n",
      "iteration no 7355: Loss: 0.2492340412429039, accuracy: 0.99\n",
      "iteration no 7356: Loss: 0.24923017775461442, accuracy: 0.99\n",
      "iteration no 7357: Loss: 0.24923263453329458, accuracy: 0.99\n",
      "iteration no 7358: Loss: 0.24923100781172358, accuracy: 0.99\n",
      "iteration no 7359: Loss: 0.24923106434960168, accuracy: 0.99\n",
      "iteration no 7360: Loss: 0.24923142377552793, accuracy: 0.99\n",
      "iteration no 7361: Loss: 0.24922853429310182, accuracy: 0.99\n",
      "iteration no 7362: Loss: 0.24923136489980488, accuracy: 0.99\n",
      "iteration no 7363: Loss: 0.2492303855079986, accuracy: 0.99\n",
      "iteration no 7364: Loss: 0.24923008917797385, accuracy: 0.99\n",
      "iteration no 7365: Loss: 0.24923091192522395, accuracy: 0.99\n",
      "iteration no 7366: Loss: 0.2492273770728634, accuracy: 0.99\n",
      "iteration no 7367: Loss: 0.24923147110066435, accuracy: 0.99\n",
      "iteration no 7368: Loss: 0.2492282732219922, accuracy: 0.99\n",
      "iteration no 7369: Loss: 0.24923066466039562, accuracy: 0.99\n",
      "iteration no 7370: Loss: 0.24922638690940857, accuracy: 0.99\n",
      "iteration no 7371: Loss: 0.24923005846366775, accuracy: 0.99\n",
      "iteration no 7372: Loss: 0.2492257771504155, accuracy: 0.99\n",
      "iteration no 7373: Loss: 0.24923035843328661, accuracy: 0.99\n",
      "iteration no 7374: Loss: 0.24922511962771157, accuracy: 0.99\n",
      "iteration no 7375: Loss: 0.24922859501172742, accuracy: 0.99\n",
      "iteration no 7376: Loss: 0.24922672820629432, accuracy: 0.99\n",
      "iteration no 7377: Loss: 0.24922607740815153, accuracy: 0.99\n",
      "iteration no 7378: Loss: 0.24922770741633948, accuracy: 0.99\n",
      "iteration no 7379: Loss: 0.24922535265032425, accuracy: 0.99\n",
      "iteration no 7380: Loss: 0.24922647323561678, accuracy: 0.99\n",
      "iteration no 7381: Loss: 0.24922540318720526, accuracy: 0.99\n",
      "iteration no 7382: Loss: 0.24922457023010025, accuracy: 0.99\n",
      "iteration no 7383: Loss: 0.24922654534420763, accuracy: 0.99\n",
      "iteration no 7384: Loss: 0.24922276645790264, accuracy: 0.99\n",
      "iteration no 7385: Loss: 0.24922687825844164, accuracy: 0.99\n",
      "iteration no 7386: Loss: 0.2492234617340357, accuracy: 0.99\n",
      "iteration no 7387: Loss: 0.24922446881681679, accuracy: 0.99\n",
      "iteration no 7388: Loss: 0.249225625682892, accuracy: 0.99\n",
      "iteration no 7389: Loss: 0.24922257778021437, accuracy: 0.99\n",
      "iteration no 7390: Loss: 0.249225691506987, accuracy: 0.99\n",
      "iteration no 7391: Loss: 0.24922039058380302, accuracy: 0.99\n",
      "iteration no 7392: Loss: 0.2492257033733791, accuracy: 0.99\n",
      "iteration no 7393: Loss: 0.24922139287895334, accuracy: 0.99\n",
      "iteration no 7394: Loss: 0.24922372247108734, accuracy: 0.99\n",
      "iteration no 7395: Loss: 0.24922146788440752, accuracy: 0.99\n",
      "iteration no 7396: Loss: 0.2492214738596961, accuracy: 0.99\n",
      "iteration no 7397: Loss: 0.24922195948138667, accuracy: 0.99\n",
      "iteration no 7398: Loss: 0.2492228940284636, accuracy: 0.99\n",
      "iteration no 7399: Loss: 0.24922059878771466, accuracy: 0.99\n",
      "iteration no 7400: Loss: 0.24922134796185108, accuracy: 0.99\n",
      "iteration no 7401: Loss: 0.24922018789852346, accuracy: 0.99\n",
      "iteration no 7402: Loss: 0.24922123401967977, accuracy: 0.99\n",
      "iteration no 7403: Loss: 0.2492197953996901, accuracy: 0.99\n",
      "iteration no 7404: Loss: 0.24922079817462656, accuracy: 0.99\n",
      "iteration no 7405: Loss: 0.24921849686940561, accuracy: 0.99\n",
      "iteration no 7406: Loss: 0.24922130451951754, accuracy: 0.99\n",
      "iteration no 7407: Loss: 0.24921724251997682, accuracy: 0.99\n",
      "iteration no 7408: Loss: 0.24922138013851328, accuracy: 0.99\n",
      "iteration no 7409: Loss: 0.24921740355501543, accuracy: 0.99\n",
      "iteration no 7410: Loss: 0.24922016056092305, accuracy: 0.99\n",
      "iteration no 7411: Loss: 0.2492188084085657, accuracy: 0.99\n",
      "iteration no 7412: Loss: 0.2492173305382361, accuracy: 0.99\n",
      "iteration no 7413: Loss: 0.2492199266424273, accuracy: 0.99\n",
      "iteration no 7414: Loss: 0.24921613141037358, accuracy: 0.99\n",
      "iteration no 7415: Loss: 0.2492191632723998, accuracy: 0.99\n",
      "iteration no 7416: Loss: 0.24921593926983715, accuracy: 0.99\n",
      "iteration no 7417: Loss: 0.249217498950469, accuracy: 0.99\n",
      "iteration no 7418: Loss: 0.24921658045022327, accuracy: 0.99\n",
      "iteration no 7419: Loss: 0.24921599622844806, accuracy: 0.99\n",
      "iteration no 7420: Loss: 0.24921703500839626, accuracy: 0.99\n",
      "iteration no 7421: Loss: 0.24921436705491679, accuracy: 0.99\n",
      "iteration no 7422: Loss: 0.24921676102871076, accuracy: 0.99\n",
      "iteration no 7423: Loss: 0.24921645106595308, accuracy: 0.99\n",
      "iteration no 7424: Loss: 0.24921438981952931, accuracy: 0.99\n",
      "iteration no 7425: Loss: 0.24921701476200137, accuracy: 0.99\n",
      "iteration no 7426: Loss: 0.2492124864026693, accuracy: 0.99\n",
      "iteration no 7427: Loss: 0.24921725405065323, accuracy: 0.99\n",
      "iteration no 7428: Loss: 0.24921287262348252, accuracy: 0.99\n",
      "iteration no 7429: Loss: 0.24921622891514028, accuracy: 0.99\n",
      "iteration no 7430: Loss: 0.24921210913858233, accuracy: 0.99\n",
      "iteration no 7431: Loss: 0.24921566559754457, accuracy: 0.99\n",
      "iteration no 7432: Loss: 0.24921127801728452, accuracy: 0.99\n",
      "iteration no 7433: Loss: 0.24921498161111005, accuracy: 0.99\n",
      "iteration no 7434: Loss: 0.2492117458745567, accuracy: 0.99\n",
      "iteration no 7435: Loss: 0.24921318600874104, accuracy: 0.99\n",
      "iteration no 7436: Loss: 0.24921331566885507, accuracy: 0.99\n",
      "iteration no 7437: Loss: 0.24921207921436522, accuracy: 0.99\n",
      "iteration no 7438: Loss: 0.24921267329611854, accuracy: 0.99\n",
      "iteration no 7439: Loss: 0.24921143756016811, accuracy: 0.99\n",
      "iteration no 7440: Loss: 0.24921156310715423, accuracy: 0.99\n",
      "iteration no 7441: Loss: 0.24921193529766433, accuracy: 0.99\n",
      "iteration no 7442: Loss: 0.2492093571662465, accuracy: 0.99\n",
      "iteration no 7443: Loss: 0.249212520338285, accuracy: 0.99\n",
      "iteration no 7444: Loss: 0.24920892944745346, accuracy: 0.99\n",
      "iteration no 7445: Loss: 0.2492122331919861, accuracy: 0.99\n",
      "iteration no 7446: Loss: 0.24920922125193862, accuracy: 0.99\n",
      "iteration no 7447: Loss: 0.24920959076560012, accuracy: 0.99\n",
      "iteration no 7448: Loss: 0.24921151985549228, accuracy: 0.99\n",
      "iteration no 7449: Loss: 0.24920753880060323, accuracy: 0.99\n",
      "iteration no 7450: Loss: 0.24921187214897492, accuracy: 0.99\n",
      "iteration no 7451: Loss: 0.24920691876779513, accuracy: 0.99\n",
      "iteration no 7452: Loss: 0.24921044666483916, accuracy: 0.99\n",
      "iteration no 7453: Loss: 0.2492074021905029, accuracy: 0.99\n",
      "iteration no 7454: Loss: 0.24920852262700532, accuracy: 0.99\n",
      "iteration no 7455: Loss: 0.24920810368054852, accuracy: 0.99\n",
      "iteration no 7456: Loss: 0.2492068999854525, accuracy: 0.99\n",
      "iteration no 7457: Loss: 0.249208071497716, accuracy: 0.99\n",
      "iteration no 7458: Loss: 0.24920837041657742, accuracy: 0.99\n",
      "iteration no 7459: Loss: 0.24920667991989814, accuracy: 0.99\n",
      "iteration no 7460: Loss: 0.2492081336492523, accuracy: 0.99\n",
      "iteration no 7461: Loss: 0.24920594513560515, accuracy: 0.99\n",
      "iteration no 7462: Loss: 0.24920708205939035, accuracy: 0.99\n",
      "iteration no 7463: Loss: 0.24920528793076685, accuracy: 0.99\n",
      "iteration no 7464: Loss: 0.2492078423274218, accuracy: 0.99\n",
      "iteration no 7465: Loss: 0.24920406828218716, accuracy: 0.99\n",
      "iteration no 7466: Loss: 0.24920861488410853, accuracy: 0.99\n",
      "iteration no 7467: Loss: 0.24920269497724024, accuracy: 0.99\n",
      "iteration no 7468: Loss: 0.249207463877681, accuracy: 0.99\n",
      "iteration no 7469: Loss: 0.24920285698390574, accuracy: 0.99\n",
      "iteration no 7470: Loss: 0.2492067416821459, accuracy: 0.99\n",
      "iteration no 7471: Loss: 0.24920382847603512, accuracy: 0.99\n",
      "iteration no 7472: Loss: 0.2492032298247584, accuracy: 0.99\n",
      "iteration no 7473: Loss: 0.24920584514174066, accuracy: 0.99\n",
      "iteration no 7474: Loss: 0.24920317528325875, accuracy: 0.99\n",
      "iteration no 7475: Loss: 0.24920443763886563, accuracy: 0.99\n",
      "iteration no 7476: Loss: 0.2492033223796876, accuracy: 0.99\n",
      "iteration no 7477: Loss: 0.2492019881588217, accuracy: 0.99\n",
      "iteration no 7478: Loss: 0.24920384112326704, accuracy: 0.99\n",
      "iteration no 7479: Loss: 0.24920051954408382, accuracy: 0.99\n",
      "iteration no 7480: Loss: 0.24920467540002622, accuracy: 0.99\n",
      "iteration no 7481: Loss: 0.24920036546503843, accuracy: 0.99\n",
      "iteration no 7482: Loss: 0.24920280528038538, accuracy: 0.99\n",
      "iteration no 7483: Loss: 0.24920205492346753, accuracy: 0.99\n",
      "iteration no 7484: Loss: 0.24920038863771504, accuracy: 0.99\n",
      "iteration no 7485: Loss: 0.249203450019719, accuracy: 0.99\n",
      "iteration no 7486: Loss: 0.2491989739316195, accuracy: 0.99\n",
      "iteration no 7487: Loss: 0.24920236265051784, accuracy: 0.99\n",
      "iteration no 7488: Loss: 0.24919890959288488, accuracy: 0.99\n",
      "iteration no 7489: Loss: 0.2492011208236875, accuracy: 0.99\n",
      "iteration no 7490: Loss: 0.2492001435804132, accuracy: 0.99\n",
      "iteration no 7491: Loss: 0.24919946420525793, accuracy: 0.99\n",
      "iteration no 7492: Loss: 0.2491992433924312, accuracy: 0.99\n",
      "iteration no 7493: Loss: 0.24920006875921188, accuracy: 0.99\n",
      "iteration no 7494: Loss: 0.24919961858224893, accuracy: 0.99\n",
      "iteration no 7495: Loss: 0.24919912595081084, accuracy: 0.99\n",
      "iteration no 7496: Loss: 0.2491988281524451, accuracy: 0.99\n",
      "iteration no 7497: Loss: 0.2491979730719185, accuracy: 0.99\n",
      "iteration no 7498: Loss: 0.2491978678039482, accuracy: 0.99\n",
      "iteration no 7499: Loss: 0.2491994145605779, accuracy: 0.99\n",
      "iteration no 7500: Loss: 0.24919643499962918, accuracy: 0.99\n",
      "iteration no 7501: Loss: 0.24920011127716127, accuracy: 0.99\n",
      "iteration no 7502: Loss: 0.24919456926676048, accuracy: 0.99\n",
      "iteration no 7503: Loss: 0.24919982268001356, accuracy: 0.99\n",
      "iteration no 7504: Loss: 0.2491950710757766, accuracy: 0.99\n",
      "iteration no 7505: Loss: 0.24919859310836684, accuracy: 0.99\n",
      "iteration no 7506: Loss: 0.2491948864337323, accuracy: 0.99\n",
      "iteration no 7507: Loss: 0.24919618401825883, accuracy: 0.99\n",
      "iteration no 7508: Loss: 0.24919603765651152, accuracy: 0.99\n",
      "iteration no 7509: Loss: 0.24919535250945046, accuracy: 0.99\n",
      "iteration no 7510: Loss: 0.24919620854188956, accuracy: 0.99\n",
      "iteration no 7511: Loss: 0.2491960866345322, accuracy: 0.99\n",
      "iteration no 7512: Loss: 0.2491931429795501, accuracy: 0.99\n",
      "iteration no 7513: Loss: 0.24919671225423412, accuracy: 0.99\n",
      "iteration no 7514: Loss: 0.2491920399634814, accuracy: 0.99\n",
      "iteration no 7515: Loss: 0.24919722827846152, accuracy: 0.99\n",
      "iteration no 7516: Loss: 0.24919260109873215, accuracy: 0.99\n",
      "iteration no 7517: Loss: 0.24919453743689177, accuracy: 0.99\n",
      "iteration no 7518: Loss: 0.24919409519828206, accuracy: 0.99\n",
      "iteration no 7519: Loss: 0.2491931544251735, accuracy: 0.99\n",
      "iteration no 7520: Loss: 0.2491949624425302, accuracy: 0.99\n",
      "iteration no 7521: Loss: 0.2491916655931584, accuracy: 0.99\n",
      "iteration no 7522: Loss: 0.24919377618517868, accuracy: 0.99\n",
      "iteration no 7523: Loss: 0.24919225448860777, accuracy: 0.99\n",
      "iteration no 7524: Loss: 0.24919246699967598, accuracy: 0.99\n",
      "iteration no 7525: Loss: 0.2491929433655689, accuracy: 0.99\n",
      "iteration no 7526: Loss: 0.24919251551291272, accuracy: 0.99\n",
      "iteration no 7527: Loss: 0.24919141692218044, accuracy: 0.99\n",
      "iteration no 7528: Loss: 0.24919076538394366, accuracy: 0.99\n",
      "iteration no 7529: Loss: 0.2491926774090832, accuracy: 0.99\n",
      "iteration no 7530: Loss: 0.24919023141746444, accuracy: 0.99\n",
      "iteration no 7531: Loss: 0.24919162243033102, accuracy: 0.99\n",
      "iteration no 7532: Loss: 0.24919022097017868, accuracy: 0.99\n",
      "iteration no 7533: Loss: 0.2491905068631199, accuracy: 0.99\n",
      "iteration no 7534: Loss: 0.24919153216337708, accuracy: 0.99\n",
      "iteration no 7535: Loss: 0.24918840014894283, accuracy: 0.99\n",
      "iteration no 7536: Loss: 0.24919275883989625, accuracy: 0.99\n",
      "iteration no 7537: Loss: 0.24918699524809773, accuracy: 0.99\n",
      "iteration no 7538: Loss: 0.24919191955949604, accuracy: 0.99\n",
      "iteration no 7539: Loss: 0.2491879622255449, accuracy: 0.99\n",
      "iteration no 7540: Loss: 0.24918954841901542, accuracy: 0.99\n",
      "iteration no 7541: Loss: 0.24918893812601448, accuracy: 0.99\n",
      "iteration no 7542: Loss: 0.24918686826254038, accuracy: 0.99\n",
      "iteration no 7543: Loss: 0.24918943730347892, accuracy: 0.99\n",
      "iteration no 7544: Loss: 0.2491885497594351, accuracy: 0.99\n",
      "iteration no 7545: Loss: 0.24918783354304155, accuracy: 0.99\n",
      "iteration no 7546: Loss: 0.24918902003450827, accuracy: 0.99\n",
      "iteration no 7547: Loss: 0.2491850533698779, accuracy: 0.99\n",
      "iteration no 7548: Loss: 0.24919092072993498, accuracy: 0.99\n",
      "iteration no 7549: Loss: 0.24918448165284518, accuracy: 0.99\n",
      "iteration no 7550: Loss: 0.24918946243201934, accuracy: 0.99\n",
      "iteration no 7551: Loss: 0.2491849595819715, accuracy: 0.99\n",
      "iteration no 7552: Loss: 0.24918675034691307, accuracy: 0.99\n",
      "iteration no 7553: Loss: 0.24918585508273733, accuracy: 0.99\n",
      "iteration no 7554: Loss: 0.24918532390005427, accuracy: 0.99\n",
      "iteration no 7555: Loss: 0.24918634478194585, accuracy: 0.99\n",
      "iteration no 7556: Loss: 0.24918667663790361, accuracy: 0.99\n",
      "iteration no 7557: Loss: 0.24918437780136748, accuracy: 0.99\n",
      "iteration no 7558: Loss: 0.24918645308888318, accuracy: 0.99\n",
      "iteration no 7559: Loss: 0.24918346460998209, accuracy: 0.99\n",
      "iteration no 7560: Loss: 0.24918693547248166, accuracy: 0.99\n",
      "iteration no 7561: Loss: 0.24918247507476704, accuracy: 0.99\n",
      "iteration no 7562: Loss: 0.24918581085875982, accuracy: 0.99\n",
      "iteration no 7563: Loss: 0.24918280875145574, accuracy: 0.99\n",
      "iteration no 7564: Loss: 0.2491859227678009, accuracy: 0.99\n",
      "iteration no 7565: Loss: 0.24918205168495872, accuracy: 0.99\n",
      "iteration no 7566: Loss: 0.24918476073527168, accuracy: 0.99\n",
      "iteration no 7567: Loss: 0.24918171382590393, accuracy: 0.99\n",
      "iteration no 7568: Loss: 0.24918351559820473, accuracy: 0.99\n",
      "iteration no 7569: Loss: 0.2491832550114818, accuracy: 0.99\n",
      "iteration no 7570: Loss: 0.24918300877169036, accuracy: 0.99\n",
      "iteration no 7571: Loss: 0.24918157064663574, accuracy: 0.99\n",
      "iteration no 7572: Loss: 0.24918212767006065, accuracy: 0.99\n",
      "iteration no 7573: Loss: 0.24918142002928073, accuracy: 0.99\n",
      "iteration no 7574: Loss: 0.24918300389171072, accuracy: 0.99\n",
      "iteration no 7575: Loss: 0.2491798838555177, accuracy: 0.99\n",
      "iteration no 7576: Loss: 0.24918289343874006, accuracy: 0.99\n",
      "iteration no 7577: Loss: 0.24917972671505412, accuracy: 0.99\n",
      "iteration no 7578: Loss: 0.24918163261515366, accuracy: 0.99\n",
      "iteration no 7579: Loss: 0.249180714543833, accuracy: 0.99\n",
      "iteration no 7580: Loss: 0.24917991316114824, accuracy: 0.99\n",
      "iteration no 7581: Loss: 0.24918184685539121, accuracy: 0.99\n",
      "iteration no 7582: Loss: 0.24917890460412723, accuracy: 0.99\n",
      "iteration no 7583: Loss: 0.24918068456474857, accuracy: 0.99\n",
      "iteration no 7584: Loss: 0.2491790757059521, accuracy: 0.99\n",
      "iteration no 7585: Loss: 0.24917990265323756, accuracy: 0.99\n",
      "iteration no 7586: Loss: 0.24917965788025376, accuracy: 0.99\n",
      "iteration no 7587: Loss: 0.24917698241774303, accuracy: 0.99\n",
      "iteration no 7588: Loss: 0.24918045743683287, accuracy: 0.99\n",
      "iteration no 7589: Loss: 0.24917789383823596, accuracy: 0.99\n",
      "iteration no 7590: Loss: 0.24917933192231367, accuracy: 0.99\n",
      "iteration no 7591: Loss: 0.24918070150799188, accuracy: 0.99\n",
      "iteration no 7592: Loss: 0.24917650753027482, accuracy: 0.99\n",
      "iteration no 7593: Loss: 0.24917916008005692, accuracy: 0.99\n",
      "iteration no 7594: Loss: 0.24917676641934172, accuracy: 0.99\n",
      "iteration no 7595: Loss: 0.24917820602118576, accuracy: 0.99\n",
      "iteration no 7596: Loss: 0.24917661582556866, accuracy: 0.99\n",
      "iteration no 7597: Loss: 0.2491765023228234, accuracy: 0.99\n",
      "iteration no 7598: Loss: 0.24917709618290856, accuracy: 0.99\n",
      "iteration no 7599: Loss: 0.24917565151737575, accuracy: 0.99\n",
      "iteration no 7600: Loss: 0.249176871768581, accuracy: 0.99\n",
      "iteration no 7601: Loss: 0.2491759667124684, accuracy: 0.99\n",
      "iteration no 7602: Loss: 0.249175471405777, accuracy: 0.99\n",
      "iteration no 7603: Loss: 0.2491769506974022, accuracy: 0.99\n",
      "iteration no 7604: Loss: 0.24917418470916186, accuracy: 0.99\n",
      "iteration no 7605: Loss: 0.24917646586609277, accuracy: 0.99\n",
      "iteration no 7606: Loss: 0.2491737973379694, accuracy: 0.99\n",
      "iteration no 7607: Loss: 0.24917611614668528, accuracy: 0.99\n",
      "iteration no 7608: Loss: 0.24917361478693606, accuracy: 0.99\n",
      "iteration no 7609: Loss: 0.24917523726131588, accuracy: 0.99\n",
      "iteration no 7610: Loss: 0.24917384475145798, accuracy: 0.99\n",
      "iteration no 7611: Loss: 0.24917471027547647, accuracy: 0.99\n",
      "iteration no 7612: Loss: 0.24917306733784603, accuracy: 0.99\n",
      "iteration no 7613: Loss: 0.24917337562670772, accuracy: 0.99\n",
      "iteration no 7614: Loss: 0.24917388093078616, accuracy: 0.99\n",
      "iteration no 7615: Loss: 0.24917377906757562, accuracy: 0.99\n",
      "iteration no 7616: Loss: 0.24917281726864804, accuracy: 0.99\n",
      "iteration no 7617: Loss: 0.2491731960702304, accuracy: 0.99\n",
      "iteration no 7618: Loss: 0.24917148818506574, accuracy: 0.99\n",
      "iteration no 7619: Loss: 0.2491739959906219, accuracy: 0.99\n",
      "iteration no 7620: Loss: 0.24917142226706118, accuracy: 0.99\n",
      "iteration no 7621: Loss: 0.2491749318036379, accuracy: 0.99\n",
      "iteration no 7622: Loss: 0.24917114307799196, accuracy: 0.99\n",
      "iteration no 7623: Loss: 0.2491718743774141, accuracy: 0.99\n",
      "iteration no 7624: Loss: 0.24917135958682265, accuracy: 0.99\n",
      "iteration no 7625: Loss: 0.24917010683066634, accuracy: 0.99\n",
      "iteration no 7626: Loss: 0.24917280377469458, accuracy: 0.99\n",
      "iteration no 7627: Loss: 0.24916952338824058, accuracy: 0.99\n",
      "iteration no 7628: Loss: 0.24917130349482838, accuracy: 0.99\n",
      "iteration no 7629: Loss: 0.24917108542633598, accuracy: 0.99\n",
      "iteration no 7630: Loss: 0.24916944036523236, accuracy: 0.99\n",
      "iteration no 7631: Loss: 0.24917276557443283, accuracy: 0.99\n",
      "iteration no 7632: Loss: 0.24916736580695859, accuracy: 0.99\n",
      "iteration no 7633: Loss: 0.2491721613792665, accuracy: 0.99\n",
      "iteration no 7634: Loss: 0.24916851013801233, accuracy: 0.99\n",
      "iteration no 7635: Loss: 0.2491702358341392, accuracy: 0.99\n",
      "iteration no 7636: Loss: 0.24916926624544344, accuracy: 0.99\n",
      "iteration no 7637: Loss: 0.24916790073684453, accuracy: 0.99\n",
      "iteration no 7638: Loss: 0.24916955538882218, accuracy: 0.99\n",
      "iteration no 7639: Loss: 0.2491674533191572, accuracy: 0.99\n",
      "iteration no 7640: Loss: 0.24916927261087074, accuracy: 0.99\n",
      "iteration no 7641: Loss: 0.24916968223324415, accuracy: 0.99\n",
      "iteration no 7642: Loss: 0.2491670120693244, accuracy: 0.99\n",
      "iteration no 7643: Loss: 0.24916931077753401, accuracy: 0.99\n",
      "iteration no 7644: Loss: 0.2491658243221465, accuracy: 0.99\n",
      "iteration no 7645: Loss: 0.24916913248212447, accuracy: 0.99\n",
      "iteration no 7646: Loss: 0.24916616718880805, accuracy: 0.99\n",
      "iteration no 7647: Loss: 0.24916774981491285, accuracy: 0.99\n",
      "iteration no 7648: Loss: 0.24916662797086603, accuracy: 0.99\n",
      "iteration no 7649: Loss: 0.24916623959560896, accuracy: 0.99\n",
      "iteration no 7650: Loss: 0.249166977167824, accuracy: 0.99\n",
      "iteration no 7651: Loss: 0.24916596915277622, accuracy: 0.99\n",
      "iteration no 7652: Loss: 0.24916610739615042, accuracy: 0.99\n",
      "iteration no 7653: Loss: 0.2491668269659954, accuracy: 0.99\n",
      "iteration no 7654: Loss: 0.24916508626904746, accuracy: 0.99\n",
      "iteration no 7655: Loss: 0.2491667309610909, accuracy: 0.99\n",
      "iteration no 7656: Loss: 0.24916460450240596, accuracy: 0.99\n",
      "iteration no 7657: Loss: 0.249165798890689, accuracy: 0.99\n",
      "iteration no 7658: Loss: 0.249164136967069, accuracy: 0.99\n",
      "iteration no 7659: Loss: 0.2491674160649779, accuracy: 0.99\n",
      "iteration no 7660: Loss: 0.249163463323662, accuracy: 0.99\n",
      "iteration no 7661: Loss: 0.24916514286190294, accuracy: 0.99\n",
      "iteration no 7662: Loss: 0.24916415323805005, accuracy: 0.99\n",
      "iteration no 7663: Loss: 0.24916304454708255, accuracy: 0.99\n",
      "iteration no 7664: Loss: 0.24916481264618584, accuracy: 0.99\n",
      "iteration no 7665: Loss: 0.24916240821495483, accuracy: 0.99\n",
      "iteration no 7666: Loss: 0.24916460271466073, accuracy: 0.99\n",
      "iteration no 7667: Loss: 0.24916378756867275, accuracy: 0.99\n",
      "iteration no 7668: Loss: 0.24916219149062424, accuracy: 0.99\n",
      "iteration no 7669: Loss: 0.24916498638387263, accuracy: 0.99\n",
      "iteration no 7670: Loss: 0.24916045518059843, accuracy: 0.99\n",
      "iteration no 7671: Loss: 0.24916585267785935, accuracy: 0.99\n",
      "iteration no 7672: Loss: 0.24916049817329278, accuracy: 0.99\n",
      "iteration no 7673: Loss: 0.24916339253312855, accuracy: 0.99\n",
      "iteration no 7674: Loss: 0.24916135383795696, accuracy: 0.99\n",
      "iteration no 7675: Loss: 0.24916155717478644, accuracy: 0.99\n",
      "iteration no 7676: Loss: 0.24916263144000367, accuracy: 0.99\n",
      "iteration no 7677: Loss: 0.24915956435442202, accuracy: 0.99\n",
      "iteration no 7678: Loss: 0.24916289656058555, accuracy: 0.99\n",
      "iteration no 7679: Loss: 0.24916157627670804, accuracy: 0.99\n",
      "iteration no 7680: Loss: 0.24916023643031474, accuracy: 0.99\n",
      "iteration no 7681: Loss: 0.2491627176396634, accuracy: 0.99\n",
      "iteration no 7682: Loss: 0.24915891673444876, accuracy: 0.99\n",
      "iteration no 7683: Loss: 0.2491625084209546, accuracy: 0.99\n",
      "iteration no 7684: Loss: 0.24915845048738233, accuracy: 0.99\n",
      "iteration no 7685: Loss: 0.2491608418077936, accuracy: 0.99\n",
      "iteration no 7686: Loss: 0.24915963753601472, accuracy: 0.99\n",
      "iteration no 7687: Loss: 0.2491593677768235, accuracy: 0.99\n",
      "iteration no 7688: Loss: 0.24915998913167425, accuracy: 0.99\n",
      "iteration no 7689: Loss: 0.24915792488434368, accuracy: 0.99\n",
      "iteration no 7690: Loss: 0.24915940761684505, accuracy: 0.99\n",
      "iteration no 7691: Loss: 0.24915917902473217, accuracy: 0.99\n",
      "iteration no 7692: Loss: 0.24915872261047445, accuracy: 0.99\n",
      "iteration no 7693: Loss: 0.2491594502339554, accuracy: 0.99\n",
      "iteration no 7694: Loss: 0.2491571569050271, accuracy: 0.99\n",
      "iteration no 7695: Loss: 0.24915930908950634, accuracy: 0.99\n",
      "iteration no 7696: Loss: 0.2491568882295956, accuracy: 0.99\n",
      "iteration no 7697: Loss: 0.24915914574459402, accuracy: 0.99\n",
      "iteration no 7698: Loss: 0.24915708991491736, accuracy: 0.99\n",
      "iteration no 7699: Loss: 0.24915777424971952, accuracy: 0.99\n",
      "iteration no 7700: Loss: 0.24915694046400544, accuracy: 0.99\n",
      "iteration no 7701: Loss: 0.24915709213414622, accuracy: 0.99\n",
      "iteration no 7702: Loss: 0.2491567044578163, accuracy: 0.99\n",
      "iteration no 7703: Loss: 0.24915683342101425, accuracy: 0.99\n",
      "iteration no 7704: Loss: 0.2491565746855167, accuracy: 0.99\n",
      "iteration no 7705: Loss: 0.24915630548636294, accuracy: 0.99\n",
      "iteration no 7706: Loss: 0.249156256878664, accuracy: 0.99\n",
      "iteration no 7707: Loss: 0.24915648924960537, accuracy: 0.99\n",
      "iteration no 7708: Loss: 0.2491548909152526, accuracy: 0.99\n",
      "iteration no 7709: Loss: 0.24915868664206578, accuracy: 0.99\n",
      "iteration no 7710: Loss: 0.24915361819820864, accuracy: 0.99\n",
      "iteration no 7711: Loss: 0.24915675090226624, accuracy: 0.99\n",
      "iteration no 7712: Loss: 0.24915460776257148, accuracy: 0.99\n",
      "iteration no 7713: Loss: 0.2491538164868305, accuracy: 0.99\n",
      "iteration no 7714: Loss: 0.2491549741565359, accuracy: 0.99\n",
      "iteration no 7715: Loss: 0.2491526808050958, accuracy: 0.99\n",
      "iteration no 7716: Loss: 0.24915575786477548, accuracy: 0.99\n",
      "iteration no 7717: Loss: 0.2491534704150437, accuracy: 0.99\n",
      "iteration no 7718: Loss: 0.24915361435977684, accuracy: 0.99\n",
      "iteration no 7719: Loss: 0.24915481169265552, accuracy: 0.99\n",
      "iteration no 7720: Loss: 0.24915169416170635, accuracy: 0.99\n",
      "iteration no 7721: Loss: 0.24915581289262095, accuracy: 0.99\n",
      "iteration no 7722: Loss: 0.24915091619988722, accuracy: 0.99\n",
      "iteration no 7723: Loss: 0.24915426673536106, accuracy: 0.99\n",
      "iteration no 7724: Loss: 0.24915178592078677, accuracy: 0.99\n",
      "iteration no 7725: Loss: 0.249151773266656, accuracy: 0.99\n",
      "iteration no 7726: Loss: 0.24915325099521723, accuracy: 0.99\n",
      "iteration no 7727: Loss: 0.2491508731389237, accuracy: 0.99\n",
      "iteration no 7728: Loss: 0.24915262664121865, accuracy: 0.99\n",
      "iteration no 7729: Loss: 0.2491509165795429, accuracy: 0.99\n",
      "iteration no 7730: Loss: 0.24915143129208095, accuracy: 0.99\n",
      "iteration no 7731: Loss: 0.24915301108929994, accuracy: 0.99\n",
      "iteration no 7732: Loss: 0.24914999732622617, accuracy: 0.99\n",
      "iteration no 7733: Loss: 0.2491521459312517, accuracy: 0.99\n",
      "iteration no 7734: Loss: 0.24914890313063415, accuracy: 0.99\n",
      "iteration no 7735: Loss: 0.24915187000605513, accuracy: 0.99\n",
      "iteration no 7736: Loss: 0.24915009713491082, accuracy: 0.99\n",
      "iteration no 7737: Loss: 0.24915032144841698, accuracy: 0.99\n",
      "iteration no 7738: Loss: 0.24914992170614192, accuracy: 0.99\n",
      "iteration no 7739: Loss: 0.24914895821599203, accuracy: 0.99\n",
      "iteration no 7740: Loss: 0.2491496945296603, accuracy: 0.99\n",
      "iteration no 7741: Loss: 0.2491490895954064, accuracy: 0.99\n",
      "iteration no 7742: Loss: 0.24914930557853676, accuracy: 0.99\n",
      "iteration no 7743: Loss: 0.24914967048447334, accuracy: 0.99\n",
      "iteration no 7744: Loss: 0.24914828840044895, accuracy: 0.99\n",
      "iteration no 7745: Loss: 0.24914895050631203, accuracy: 0.99\n",
      "iteration no 7746: Loss: 0.24914767078417244, accuracy: 0.99\n",
      "iteration no 7747: Loss: 0.2491488664434569, accuracy: 0.99\n",
      "iteration no 7748: Loss: 0.2491474197376352, accuracy: 0.99\n",
      "iteration no 7749: Loss: 0.24914998961698132, accuracy: 0.99\n",
      "iteration no 7750: Loss: 0.24914629706816124, accuracy: 0.99\n",
      "iteration no 7751: Loss: 0.24914755763107502, accuracy: 0.99\n",
      "iteration no 7752: Loss: 0.24914764268690964, accuracy: 0.99\n",
      "iteration no 7753: Loss: 0.24914481081142884, accuracy: 0.99\n",
      "iteration no 7754: Loss: 0.24914826801849388, accuracy: 0.99\n",
      "iteration no 7755: Loss: 0.24914479661188232, accuracy: 0.99\n",
      "iteration no 7756: Loss: 0.24914750267675945, accuracy: 0.99\n",
      "iteration no 7757: Loss: 0.24914643410844556, accuracy: 0.99\n",
      "iteration no 7758: Loss: 0.24914464646009854, accuracy: 0.99\n",
      "iteration no 7759: Loss: 0.24914763163209966, accuracy: 0.99\n",
      "iteration no 7760: Loss: 0.24914331605147738, accuracy: 0.99\n",
      "iteration no 7761: Loss: 0.249147192538335, accuracy: 0.99\n",
      "iteration no 7762: Loss: 0.24914428242218356, accuracy: 0.99\n",
      "iteration no 7763: Loss: 0.24914426748025578, accuracy: 0.99\n",
      "iteration no 7764: Loss: 0.2491453579003489, accuracy: 0.99\n",
      "iteration no 7765: Loss: 0.24914226603179362, accuracy: 0.99\n",
      "iteration no 7766: Loss: 0.24914584578561766, accuracy: 0.99\n",
      "iteration no 7767: Loss: 0.24914304388206998, accuracy: 0.99\n",
      "iteration no 7768: Loss: 0.249144402511213, accuracy: 0.99\n",
      "iteration no 7769: Loss: 0.2491443102210171, accuracy: 0.99\n",
      "iteration no 7770: Loss: 0.24914204919554384, accuracy: 0.99\n",
      "iteration no 7771: Loss: 0.24914524403440208, accuracy: 0.99\n",
      "iteration no 7772: Loss: 0.24914105367598255, accuracy: 0.99\n",
      "iteration no 7773: Loss: 0.2491446657495022, accuracy: 0.99\n",
      "iteration no 7774: Loss: 0.24914183160357656, accuracy: 0.99\n",
      "iteration no 7775: Loss: 0.24914192590536868, accuracy: 0.99\n",
      "iteration no 7776: Loss: 0.24914292520713877, accuracy: 0.99\n",
      "iteration no 7777: Loss: 0.24914050451325026, accuracy: 0.99\n",
      "iteration no 7778: Loss: 0.24914275467520342, accuracy: 0.99\n",
      "iteration no 7779: Loss: 0.24914039766155938, accuracy: 0.99\n",
      "iteration no 7780: Loss: 0.2491423699169793, accuracy: 0.99\n",
      "iteration no 7781: Loss: 0.24914166605975674, accuracy: 0.99\n",
      "iteration no 7782: Loss: 0.24914015144801974, accuracy: 0.99\n",
      "iteration no 7783: Loss: 0.24914183627114606, accuracy: 0.99\n",
      "iteration no 7784: Loss: 0.24913979343517, accuracy: 0.99\n",
      "iteration no 7785: Loss: 0.24914129801430474, accuracy: 0.99\n",
      "iteration no 7786: Loss: 0.24913989179034918, accuracy: 0.99\n",
      "iteration no 7787: Loss: 0.24913975601668997, accuracy: 0.99\n",
      "iteration no 7788: Loss: 0.24913975616224399, accuracy: 0.99\n",
      "iteration no 7789: Loss: 0.2491394721093964, accuracy: 0.99\n",
      "iteration no 7790: Loss: 0.24913944441532967, accuracy: 0.99\n",
      "iteration no 7791: Loss: 0.2491382088377856, accuracy: 0.99\n",
      "iteration no 7792: Loss: 0.24913944842157443, accuracy: 0.99\n",
      "iteration no 7793: Loss: 0.2491382771075023, accuracy: 0.99\n",
      "iteration no 7794: Loss: 0.24914027546428785, accuracy: 0.99\n",
      "iteration no 7795: Loss: 0.2491381022306557, accuracy: 0.99\n",
      "iteration no 7796: Loss: 0.24913821937134947, accuracy: 0.99\n",
      "iteration no 7797: Loss: 0.24913905213672793, accuracy: 0.99\n",
      "iteration no 7798: Loss: 0.24913564478514877, accuracy: 0.99\n",
      "iteration no 7799: Loss: 0.24913939244828842, accuracy: 0.99\n",
      "iteration no 7800: Loss: 0.24913676971933263, accuracy: 0.99\n",
      "iteration no 7801: Loss: 0.24913759508657224, accuracy: 0.99\n",
      "iteration no 7802: Loss: 0.24913711837046137, accuracy: 0.99\n",
      "iteration no 7803: Loss: 0.2491345964326213, accuracy: 0.99\n",
      "iteration no 7804: Loss: 0.24913824708390098, accuracy: 0.99\n",
      "iteration no 7805: Loss: 0.2491344961059305, accuracy: 0.99\n",
      "iteration no 7806: Loss: 0.24913804077862856, accuracy: 0.99\n",
      "iteration no 7807: Loss: 0.24913581034426363, accuracy: 0.99\n",
      "iteration no 7808: Loss: 0.24913555860009523, accuracy: 0.99\n",
      "iteration no 7809: Loss: 0.24913655995121745, accuracy: 0.99\n",
      "iteration no 7810: Loss: 0.24913349617947467, accuracy: 0.99\n",
      "iteration no 7811: Loss: 0.24913740183620864, accuracy: 0.99\n",
      "iteration no 7812: Loss: 0.24913419215045932, accuracy: 0.99\n",
      "iteration no 7813: Loss: 0.24913498458206018, accuracy: 0.99\n",
      "iteration no 7814: Loss: 0.2491347339001342, accuracy: 0.99\n",
      "iteration no 7815: Loss: 0.24913232073620678, accuracy: 0.99\n",
      "iteration no 7816: Loss: 0.24913632656380247, accuracy: 0.99\n",
      "iteration no 7817: Loss: 0.2491334378454392, accuracy: 0.99\n",
      "iteration no 7818: Loss: 0.24913459019652887, accuracy: 0.99\n",
      "iteration no 7819: Loss: 0.24913339239282892, accuracy: 0.99\n",
      "iteration no 7820: Loss: 0.24913349282757957, accuracy: 0.99\n",
      "iteration no 7821: Loss: 0.24913319577658116, accuracy: 0.99\n",
      "iteration no 7822: Loss: 0.24913288512252843, accuracy: 0.99\n",
      "iteration no 7823: Loss: 0.24913293061992361, accuracy: 0.99\n",
      "iteration no 7824: Loss: 0.24913274043499573, accuracy: 0.99\n",
      "iteration no 7825: Loss: 0.24913280787470704, accuracy: 0.99\n",
      "iteration no 7826: Loss: 0.24913211680314326, accuracy: 0.99\n",
      "iteration no 7827: Loss: 0.24913163086535578, accuracy: 0.99\n",
      "iteration no 7828: Loss: 0.24913189262543356, accuracy: 0.99\n",
      "iteration no 7829: Loss: 0.24913090725777431, accuracy: 0.99\n",
      "iteration no 7830: Loss: 0.24913290247548206, accuracy: 0.99\n",
      "iteration no 7831: Loss: 0.2491306484605357, accuracy: 0.99\n",
      "iteration no 7832: Loss: 0.2491315120825109, accuracy: 0.99\n",
      "iteration no 7833: Loss: 0.24913042474455113, accuracy: 0.99\n",
      "iteration no 7834: Loss: 0.24913072428484817, accuracy: 0.99\n",
      "iteration no 7835: Loss: 0.2491307287542966, accuracy: 0.99\n",
      "iteration no 7836: Loss: 0.24913021743011582, accuracy: 0.99\n",
      "iteration no 7837: Loss: 0.24913071428254974, accuracy: 0.99\n",
      "iteration no 7838: Loss: 0.2491293356290572, accuracy: 0.99\n",
      "iteration no 7839: Loss: 0.24912963587993925, accuracy: 0.99\n",
      "iteration no 7840: Loss: 0.24912958632671528, accuracy: 0.99\n",
      "iteration no 7841: Loss: 0.2491281934104924, accuracy: 0.99\n",
      "iteration no 7842: Loss: 0.24913069001657312, accuracy: 0.99\n",
      "iteration no 7843: Loss: 0.24912703057374502, accuracy: 0.99\n",
      "iteration no 7844: Loss: 0.24913027170633995, accuracy: 0.99\n",
      "iteration no 7845: Loss: 0.24912754393499087, accuracy: 0.99\n",
      "iteration no 7846: Loss: 0.2491292801195748, accuracy: 0.99\n",
      "iteration no 7847: Loss: 0.24912882417173304, accuracy: 0.99\n",
      "iteration no 7848: Loss: 0.2491266267927747, accuracy: 0.99\n",
      "iteration no 7849: Loss: 0.2491290335041838, accuracy: 0.99\n",
      "iteration no 7850: Loss: 0.2491269244655852, accuracy: 0.99\n",
      "iteration no 7851: Loss: 0.24912783774317343, accuracy: 0.99\n",
      "iteration no 7852: Loss: 0.24912896177111987, accuracy: 0.99\n",
      "iteration no 7853: Loss: 0.24912483178795852, accuracy: 0.99\n",
      "iteration no 7854: Loss: 0.24912879180836858, accuracy: 0.99\n",
      "iteration no 7855: Loss: 0.24912500276381128, accuracy: 0.99\n",
      "iteration no 7856: Loss: 0.24912751141590284, accuracy: 0.99\n",
      "iteration no 7857: Loss: 0.24912680022567169, accuracy: 0.99\n",
      "iteration no 7858: Loss: 0.24912584332849486, accuracy: 0.99\n",
      "iteration no 7859: Loss: 0.24912682126338417, accuracy: 0.99\n",
      "iteration no 7860: Loss: 0.24912445974901576, accuracy: 0.99\n",
      "iteration no 7861: Loss: 0.2491260512513166, accuracy: 0.99\n",
      "iteration no 7862: Loss: 0.24912621829663717, accuracy: 0.99\n",
      "iteration no 7863: Loss: 0.24912387435856748, accuracy: 0.99\n",
      "iteration no 7864: Loss: 0.24912645458244728, accuracy: 0.99\n",
      "iteration no 7865: Loss: 0.24912305854715547, accuracy: 0.99\n",
      "iteration no 7866: Loss: 0.2491257127771137, accuracy: 0.99\n",
      "iteration no 7867: Loss: 0.2491232380446242, accuracy: 0.99\n",
      "iteration no 7868: Loss: 0.24912530145861173, accuracy: 0.99\n",
      "iteration no 7869: Loss: 0.24912444691213234, accuracy: 0.99\n",
      "iteration no 7870: Loss: 0.2491238805804103, accuracy: 0.99\n",
      "iteration no 7871: Loss: 0.24912372903215818, accuracy: 0.99\n",
      "iteration no 7872: Loss: 0.24912312783321638, accuracy: 0.99\n",
      "iteration no 7873: Loss: 0.2491232426144907, accuracy: 0.99\n",
      "iteration no 7874: Loss: 0.24912381895928082, accuracy: 0.99\n",
      "iteration no 7875: Loss: 0.2491219059943401, accuracy: 0.99\n",
      "iteration no 7876: Loss: 0.2491237131273708, accuracy: 0.99\n",
      "iteration no 7877: Loss: 0.24912107159648425, accuracy: 0.99\n",
      "iteration no 7878: Loss: 0.24912329436909347, accuracy: 0.99\n",
      "iteration no 7879: Loss: 0.24912105773893395, accuracy: 0.99\n",
      "iteration no 7880: Loss: 0.24912363189938416, accuracy: 0.99\n",
      "iteration no 7881: Loss: 0.24912111979407145, accuracy: 0.99\n",
      "iteration no 7882: Loss: 0.24912206049978758, accuracy: 0.99\n",
      "iteration no 7883: Loss: 0.249120698258346, accuracy: 0.99\n",
      "iteration no 7884: Loss: 0.2491219642876203, accuracy: 0.99\n",
      "iteration no 7885: Loss: 0.24912107343414158, accuracy: 0.99\n",
      "iteration no 7886: Loss: 0.24912150671993472, accuracy: 0.99\n",
      "iteration no 7887: Loss: 0.24912132512694032, accuracy: 0.99\n",
      "iteration no 7888: Loss: 0.24912103141570655, accuracy: 0.99\n",
      "iteration no 7889: Loss: 0.24911921466444564, accuracy: 0.99\n",
      "iteration no 7890: Loss: 0.24912187550726123, accuracy: 0.99\n",
      "iteration no 7891: Loss: 0.24911697173999076, accuracy: 0.99\n",
      "iteration no 7892: Loss: 0.24912270812670204, accuracy: 0.99\n",
      "iteration no 7893: Loss: 0.24911777906879015, accuracy: 0.99\n",
      "iteration no 7894: Loss: 0.24912102262259872, accuracy: 0.99\n",
      "iteration no 7895: Loss: 0.24911940607584993, accuracy: 0.99\n",
      "iteration no 7896: Loss: 0.2491186939884254, accuracy: 0.99\n",
      "iteration no 7897: Loss: 0.24912013460787963, accuracy: 0.99\n",
      "iteration no 7898: Loss: 0.24911788216852357, accuracy: 0.99\n",
      "iteration no 7899: Loss: 0.24911884526598552, accuracy: 0.99\n",
      "iteration no 7900: Loss: 0.24911880940953157, accuracy: 0.99\n",
      "iteration no 7901: Loss: 0.24911666857516945, accuracy: 0.99\n",
      "iteration no 7902: Loss: 0.24912003477973266, accuracy: 0.99\n",
      "iteration no 7903: Loss: 0.2491142833386428, accuracy: 0.99\n",
      "iteration no 7904: Loss: 0.2491205621565547, accuracy: 0.99\n",
      "iteration no 7905: Loss: 0.24911534706657323, accuracy: 0.99\n",
      "iteration no 7906: Loss: 0.24911876270623629, accuracy: 0.99\n",
      "iteration no 7907: Loss: 0.24911746940583368, accuracy: 0.99\n",
      "iteration no 7908: Loss: 0.24911620380147212, accuracy: 0.99\n",
      "iteration no 7909: Loss: 0.249117699861424, accuracy: 0.99\n",
      "iteration no 7910: Loss: 0.2491164002663794, accuracy: 0.99\n",
      "iteration no 7911: Loss: 0.24911582294480478, accuracy: 0.99\n",
      "iteration no 7912: Loss: 0.2491171941063921, accuracy: 0.99\n",
      "iteration no 7913: Loss: 0.24911351584857727, accuracy: 0.99\n",
      "iteration no 7914: Loss: 0.24911836022826797, accuracy: 0.99\n",
      "iteration no 7915: Loss: 0.24911443901612937, accuracy: 0.99\n",
      "iteration no 7916: Loss: 0.24911702725539298, accuracy: 0.99\n",
      "iteration no 7917: Loss: 0.24911337661408484, accuracy: 0.99\n",
      "iteration no 7918: Loss: 0.24911677190784084, accuracy: 0.99\n",
      "iteration no 7919: Loss: 0.2491135773991074, accuracy: 0.99\n",
      "iteration no 7920: Loss: 0.24911578226864695, accuracy: 0.99\n",
      "iteration no 7921: Loss: 0.2491137183438757, accuracy: 0.99\n",
      "iteration no 7922: Loss: 0.24911484761874722, accuracy: 0.99\n",
      "iteration no 7923: Loss: 0.24911410924180888, accuracy: 0.99\n",
      "iteration no 7924: Loss: 0.24911442946747808, accuracy: 0.99\n",
      "iteration no 7925: Loss: 0.24911279404121645, accuracy: 0.99\n",
      "iteration no 7926: Loss: 0.249114355751789, accuracy: 0.99\n",
      "iteration no 7927: Loss: 0.24911193891026567, accuracy: 0.99\n",
      "iteration no 7928: Loss: 0.24911517790497312, accuracy: 0.99\n",
      "iteration no 7929: Loss: 0.24911078985509882, accuracy: 0.99\n",
      "iteration no 7930: Loss: 0.24911489154850636, accuracy: 0.99\n",
      "iteration no 7931: Loss: 0.24911084351530632, accuracy: 0.99\n",
      "iteration no 7932: Loss: 0.24911430765983622, accuracy: 0.99\n",
      "iteration no 7933: Loss: 0.24911159434010355, accuracy: 0.99\n",
      "iteration no 7934: Loss: 0.24911238787619228, accuracy: 0.99\n",
      "iteration no 7935: Loss: 0.24911194901431166, accuracy: 0.99\n",
      "iteration no 7936: Loss: 0.2491120466121597, accuracy: 0.99\n",
      "iteration no 7937: Loss: 0.24911148251197626, accuracy: 0.99\n",
      "iteration no 7938: Loss: 0.24911276768859292, accuracy: 0.99\n",
      "iteration no 7939: Loss: 0.2491090171305116, accuracy: 0.99\n",
      "iteration no 7940: Loss: 0.249113444975426, accuracy: 0.99\n",
      "iteration no 7941: Loss: 0.24910779423577464, accuracy: 0.99\n",
      "iteration no 7942: Loss: 0.2491137359436456, accuracy: 0.99\n",
      "iteration no 7943: Loss: 0.24910910039812273, accuracy: 0.99\n",
      "iteration no 7944: Loss: 0.249112066083243, accuracy: 0.99\n",
      "iteration no 7945: Loss: 0.24911156420208938, accuracy: 0.99\n",
      "iteration no 7946: Loss: 0.2491096153071933, accuracy: 0.99\n",
      "iteration no 7947: Loss: 0.24910962901180028, accuracy: 0.99\n",
      "iteration no 7948: Loss: 0.24911122217253173, accuracy: 0.99\n",
      "iteration no 7949: Loss: 0.2491072418968711, accuracy: 0.99\n",
      "iteration no 7950: Loss: 0.24911196469308589, accuracy: 0.99\n",
      "iteration no 7951: Loss: 0.24910581223421097, accuracy: 0.99\n",
      "iteration no 7952: Loss: 0.24911127192637988, accuracy: 0.99\n",
      "iteration no 7953: Loss: 0.24910627960923748, accuracy: 0.99\n",
      "iteration no 7954: Loss: 0.24911012588590245, accuracy: 0.99\n",
      "iteration no 7955: Loss: 0.24910782150899607, accuracy: 0.99\n",
      "iteration no 7956: Loss: 0.24910758935825378, accuracy: 0.99\n",
      "iteration no 7957: Loss: 0.24910807325701595, accuracy: 0.99\n",
      "iteration no 7958: Loss: 0.24910652835824093, accuracy: 0.99\n",
      "iteration no 7959: Loss: 0.2491082573064634, accuracy: 0.99\n",
      "iteration no 7960: Loss: 0.2491078228319612, accuracy: 0.99\n",
      "iteration no 7961: Loss: 0.24910639896711068, accuracy: 0.99\n",
      "iteration no 7962: Loss: 0.24910851492038238, accuracy: 0.99\n",
      "iteration no 7963: Loss: 0.24910557246969178, accuracy: 0.99\n",
      "iteration no 7964: Loss: 0.2491079920496705, accuracy: 0.99\n",
      "iteration no 7965: Loss: 0.24910508497118283, accuracy: 0.99\n",
      "iteration no 7966: Loss: 0.249107119179456, accuracy: 0.99\n",
      "iteration no 7967: Loss: 0.24910620425661661, accuracy: 0.99\n",
      "iteration no 7968: Loss: 0.2491070070097759, accuracy: 0.99\n",
      "iteration no 7969: Loss: 0.2491049342315673, accuracy: 0.99\n",
      "iteration no 7970: Loss: 0.24910534903614312, accuracy: 0.99\n",
      "iteration no 7971: Loss: 0.24910625171816442, accuracy: 0.99\n",
      "iteration no 7972: Loss: 0.24910419405931622, accuracy: 0.99\n",
      "iteration no 7973: Loss: 0.24910630595210206, accuracy: 0.99\n",
      "iteration no 7974: Loss: 0.24910367732615174, accuracy: 0.99\n",
      "iteration no 7975: Loss: 0.24910537936005916, accuracy: 0.99\n",
      "iteration no 7976: Loss: 0.24910576988690486, accuracy: 0.99\n",
      "iteration no 7977: Loss: 0.2491033832023259, accuracy: 0.99\n",
      "iteration no 7978: Loss: 0.2491065858984163, accuracy: 0.99\n",
      "iteration no 7979: Loss: 0.24910259805143148, accuracy: 0.99\n",
      "iteration no 7980: Loss: 0.24910462268838524, accuracy: 0.99\n",
      "iteration no 7981: Loss: 0.2491034496938895, accuracy: 0.99\n",
      "iteration no 7982: Loss: 0.24910311281862113, accuracy: 0.99\n",
      "iteration no 7983: Loss: 0.24910439113542227, accuracy: 0.99\n",
      "iteration no 7984: Loss: 0.2491012430040463, accuracy: 0.99\n",
      "iteration no 7985: Loss: 0.24910481450924082, accuracy: 0.99\n",
      "iteration no 7986: Loss: 0.24910145499888306, accuracy: 0.99\n",
      "iteration no 7987: Loss: 0.24910372864782448, accuracy: 0.99\n",
      "iteration no 7988: Loss: 0.24910296903213364, accuracy: 0.99\n",
      "iteration no 7989: Loss: 0.2491013317704861, accuracy: 0.99\n",
      "iteration no 7990: Loss: 0.2491045937800213, accuracy: 0.99\n",
      "iteration no 7991: Loss: 0.24910070966290537, accuracy: 0.99\n",
      "iteration no 7992: Loss: 0.24910303334551417, accuracy: 0.99\n",
      "iteration no 7993: Loss: 0.24910279142962988, accuracy: 0.99\n",
      "iteration no 7994: Loss: 0.24909952614904463, accuracy: 0.99\n",
      "iteration no 7995: Loss: 0.24910322747237085, accuracy: 0.99\n",
      "iteration no 7996: Loss: 0.2490994261756424, accuracy: 0.99\n",
      "iteration no 7997: Loss: 0.24910261488300686, accuracy: 0.99\n",
      "iteration no 7998: Loss: 0.24910092383246568, accuracy: 0.99\n",
      "iteration no 7999: Loss: 0.24910027814847108, accuracy: 0.99\n",
      "iteration no 8000: Loss: 0.24910101825279476, accuracy: 0.99\n",
      "iteration no 8001: Loss: 0.24909976047236465, accuracy: 0.99\n",
      "iteration no 8002: Loss: 0.24910072807000733, accuracy: 0.99\n",
      "iteration no 8003: Loss: 0.2491002957483583, accuracy: 0.99\n",
      "iteration no 8004: Loss: 0.2490988473020691, accuracy: 0.99\n",
      "iteration no 8005: Loss: 0.24910094576614028, accuracy: 0.99\n",
      "iteration no 8006: Loss: 0.24909735500683133, accuracy: 0.99\n",
      "iteration no 8007: Loss: 0.24910093776809283, accuracy: 0.99\n",
      "iteration no 8008: Loss: 0.2490976649560376, accuracy: 0.99\n",
      "iteration no 8009: Loss: 0.2491007282962272, accuracy: 0.99\n",
      "iteration no 8010: Loss: 0.2490987314900569, accuracy: 0.99\n",
      "iteration no 8011: Loss: 0.2490984911272174, accuracy: 0.99\n",
      "iteration no 8012: Loss: 0.24909853237862115, accuracy: 0.99\n",
      "iteration no 8013: Loss: 0.24909821949928612, accuracy: 0.99\n",
      "iteration no 8014: Loss: 0.24909865222851188, accuracy: 0.99\n",
      "iteration no 8015: Loss: 0.24909850573456943, accuracy: 0.99\n",
      "iteration no 8016: Loss: 0.24909669725980982, accuracy: 0.99\n",
      "iteration no 8017: Loss: 0.24909890934077372, accuracy: 0.99\n",
      "iteration no 8018: Loss: 0.24909672465465454, accuracy: 0.99\n",
      "iteration no 8019: Loss: 0.24909864944717988, accuracy: 0.99\n",
      "iteration no 8020: Loss: 0.24909549812650086, accuracy: 0.99\n",
      "iteration no 8021: Loss: 0.24909952950190384, accuracy: 0.99\n",
      "iteration no 8022: Loss: 0.249095000038644, accuracy: 0.99\n",
      "iteration no 8023: Loss: 0.24909793783285378, accuracy: 0.99\n",
      "iteration no 8024: Loss: 0.24909489899360793, accuracy: 0.99\n",
      "iteration no 8025: Loss: 0.24909727604858442, accuracy: 0.99\n",
      "iteration no 8026: Loss: 0.2490972557540535, accuracy: 0.99\n",
      "iteration no 8027: Loss: 0.24909595099901888, accuracy: 0.99\n",
      "iteration no 8028: Loss: 0.24909600292294562, accuracy: 0.99\n",
      "iteration no 8029: Loss: 0.24909524367864144, accuracy: 0.99\n",
      "iteration no 8030: Loss: 0.24909437276209007, accuracy: 0.99\n",
      "iteration no 8031: Loss: 0.24909716747077398, accuracy: 0.99\n",
      "iteration no 8032: Loss: 0.2490927913703121, accuracy: 0.99\n",
      "iteration no 8033: Loss: 0.2490977355650536, accuracy: 0.99\n",
      "iteration no 8034: Loss: 0.24909236141611957, accuracy: 0.99\n",
      "iteration no 8035: Loss: 0.24909670856961222, accuracy: 0.99\n",
      "iteration no 8036: Loss: 0.2490953740417181, accuracy: 0.99\n",
      "iteration no 8037: Loss: 0.24909409575321395, accuracy: 0.99\n",
      "iteration no 8038: Loss: 0.24909519842446243, accuracy: 0.99\n",
      "iteration no 8039: Loss: 0.24909440597071908, accuracy: 0.99\n",
      "iteration no 8040: Loss: 0.24909317498364064, accuracy: 0.99\n",
      "iteration no 8041: Loss: 0.2490950098671092, accuracy: 0.99\n",
      "iteration no 8042: Loss: 0.24909088917625322, accuracy: 0.99\n",
      "iteration no 8043: Loss: 0.24909639076918444, accuracy: 0.99\n",
      "iteration no 8044: Loss: 0.24909091097419522, accuracy: 0.99\n",
      "iteration no 8045: Loss: 0.24909614977996694, accuracy: 0.99\n",
      "iteration no 8046: Loss: 0.24909165944301176, accuracy: 0.99\n",
      "iteration no 8047: Loss: 0.2490934016365503, accuracy: 0.99\n",
      "iteration no 8048: Loss: 0.24909355542591008, accuracy: 0.99\n",
      "iteration no 8049: Loss: 0.24909264207853615, accuracy: 0.99\n",
      "iteration no 8050: Loss: 0.2490930375537875, accuracy: 0.99\n",
      "iteration no 8051: Loss: 0.2490926143758635, accuracy: 0.99\n",
      "iteration no 8052: Loss: 0.24909083974213306, accuracy: 0.99\n",
      "iteration no 8053: Loss: 0.24909414437821187, accuracy: 0.99\n",
      "iteration no 8054: Loss: 0.2490895007595224, accuracy: 0.99\n",
      "iteration no 8055: Loss: 0.24909403452387935, accuracy: 0.99\n",
      "iteration no 8056: Loss: 0.24908906455510527, accuracy: 0.99\n",
      "iteration no 8057: Loss: 0.24909401156423455, accuracy: 0.99\n",
      "iteration no 8058: Loss: 0.2490906808383245, accuracy: 0.99\n",
      "iteration no 8059: Loss: 0.24909225382465222, accuracy: 0.99\n",
      "iteration no 8060: Loss: 0.24909081585694326, accuracy: 0.99\n",
      "iteration no 8061: Loss: 0.24909132828667663, accuracy: 0.99\n",
      "iteration no 8062: Loss: 0.24909074140939025, accuracy: 0.99\n",
      "iteration no 8063: Loss: 0.24909177988101297, accuracy: 0.99\n",
      "iteration no 8064: Loss: 0.24908914887518643, accuracy: 0.99\n",
      "iteration no 8065: Loss: 0.24909233124936675, accuracy: 0.99\n",
      "iteration no 8066: Loss: 0.24908931325968456, accuracy: 0.99\n",
      "iteration no 8067: Loss: 0.2490916862949905, accuracy: 0.99\n",
      "iteration no 8068: Loss: 0.24908743372820047, accuracy: 0.99\n",
      "iteration no 8069: Loss: 0.24909285729391, accuracy: 0.99\n",
      "iteration no 8070: Loss: 0.2490875753801784, accuracy: 0.99\n",
      "iteration no 8071: Loss: 0.2490911983042579, accuracy: 0.99\n",
      "iteration no 8072: Loss: 0.2490876240905164, accuracy: 0.99\n",
      "iteration no 8073: Loss: 0.24908974119687047, accuracy: 0.99\n",
      "iteration no 8074: Loss: 0.24908973635582102, accuracy: 0.99\n",
      "iteration no 8075: Loss: 0.24908845492645487, accuracy: 0.99\n",
      "iteration no 8076: Loss: 0.2490895523043537, accuracy: 0.99\n",
      "iteration no 8077: Loss: 0.24908772087203507, accuracy: 0.99\n",
      "iteration no 8078: Loss: 0.2490887781716537, accuracy: 0.99\n",
      "iteration no 8079: Loss: 0.24908947082767674, accuracy: 0.99\n",
      "iteration no 8080: Loss: 0.2490873232345316, accuracy: 0.99\n",
      "iteration no 8081: Loss: 0.24909052907082516, accuracy: 0.99\n",
      "iteration no 8082: Loss: 0.24908641731246917, accuracy: 0.99\n",
      "iteration no 8083: Loss: 0.2490894233357394, accuracy: 0.99\n",
      "iteration no 8084: Loss: 0.2490868159689803, accuracy: 0.99\n",
      "iteration no 8085: Loss: 0.24908763763054878, accuracy: 0.99\n",
      "iteration no 8086: Loss: 0.2490882748712091, accuracy: 0.99\n",
      "iteration no 8087: Loss: 0.249085933870905, accuracy: 0.99\n",
      "iteration no 8088: Loss: 0.24908873449695557, accuracy: 0.99\n",
      "iteration no 8089: Loss: 0.24908566144147126, accuracy: 0.99\n",
      "iteration no 8090: Loss: 0.24908769631527083, accuracy: 0.99\n",
      "iteration no 8091: Loss: 0.24908937250572172, accuracy: 0.99\n",
      "iteration no 8092: Loss: 0.2490857898453736, accuracy: 0.99\n",
      "iteration no 8093: Loss: 0.24908833988169943, accuracy: 0.99\n",
      "iteration no 8094: Loss: 0.249085141744658, accuracy: 0.99\n",
      "iteration no 8095: Loss: 0.2490860594247422, accuracy: 0.99\n",
      "iteration no 8096: Loss: 0.24908719316256434, accuracy: 0.99\n",
      "iteration no 8097: Loss: 0.249084898984486, accuracy: 0.99\n",
      "iteration no 8098: Loss: 0.2490872728857918, accuracy: 0.99\n",
      "iteration no 8099: Loss: 0.24908375353767964, accuracy: 0.99\n",
      "iteration no 8100: Loss: 0.2490870186473605, accuracy: 0.99\n",
      "iteration no 8101: Loss: 0.24908570412403755, accuracy: 0.99\n",
      "iteration no 8102: Loss: 0.2490852059884463, accuracy: 0.99\n",
      "iteration no 8103: Loss: 0.24908638880524772, accuracy: 0.99\n",
      "iteration no 8104: Loss: 0.24908421721013682, accuracy: 0.99\n",
      "iteration no 8105: Loss: 0.24908571084867398, accuracy: 0.99\n",
      "iteration no 8106: Loss: 0.24908413325012493, accuracy: 0.99\n",
      "iteration no 8107: Loss: 0.24908400887852183, accuracy: 0.99\n",
      "iteration no 8108: Loss: 0.24908605043485021, accuracy: 0.99\n",
      "iteration no 8109: Loss: 0.24908345922408281, accuracy: 0.99\n",
      "iteration no 8110: Loss: 0.24908522861465632, accuracy: 0.99\n",
      "iteration no 8111: Loss: 0.2490823001182149, accuracy: 0.99\n",
      "iteration no 8112: Loss: 0.24908495561941535, accuracy: 0.99\n",
      "iteration no 8113: Loss: 0.2490843879314781, accuracy: 0.99\n",
      "iteration no 8114: Loss: 0.24908410601940084, accuracy: 0.99\n",
      "iteration no 8115: Loss: 0.2490840584087829, accuracy: 0.99\n",
      "iteration no 8116: Loss: 0.24908296588450987, accuracy: 0.99\n",
      "iteration no 8117: Loss: 0.24908367136505083, accuracy: 0.99\n",
      "iteration no 8118: Loss: 0.24908317531541269, accuracy: 0.99\n",
      "iteration no 8119: Loss: 0.24908285444898226, accuracy: 0.99\n",
      "iteration no 8120: Loss: 0.24908374721555707, accuracy: 0.99\n",
      "iteration no 8121: Loss: 0.24908200316686083, accuracy: 0.99\n",
      "iteration no 8122: Loss: 0.24908292960361533, accuracy: 0.99\n",
      "iteration no 8123: Loss: 0.24908122218781753, accuracy: 0.99\n",
      "iteration no 8124: Loss: 0.2490839563792123, accuracy: 0.99\n",
      "iteration no 8125: Loss: 0.24908208605005455, accuracy: 0.99\n",
      "iteration no 8126: Loss: 0.24908401458027124, accuracy: 0.99\n",
      "iteration no 8127: Loss: 0.24908142141681183, accuracy: 0.99\n",
      "iteration no 8128: Loss: 0.2490817631739196, accuracy: 0.99\n",
      "iteration no 8129: Loss: 0.24908287056343745, accuracy: 0.99\n",
      "iteration no 8130: Loss: 0.24908093693687589, accuracy: 0.99\n",
      "iteration no 8131: Loss: 0.24908292722489211, accuracy: 0.99\n",
      "iteration no 8132: Loss: 0.24908107325006879, accuracy: 0.99\n",
      "iteration no 8133: Loss: 0.24908035103713114, accuracy: 0.99\n",
      "iteration no 8134: Loss: 0.24908235300825127, accuracy: 0.99\n",
      "iteration no 8135: Loss: 0.24907811243544287, accuracy: 0.99\n",
      "iteration no 8136: Loss: 0.249083931183648, accuracy: 0.99\n",
      "iteration no 8137: Loss: 0.24907828726028675, accuracy: 0.99\n",
      "iteration no 8138: Loss: 0.24908251317944477, accuracy: 0.99\n",
      "iteration no 8139: Loss: 0.24908010137915393, accuracy: 0.99\n",
      "iteration no 8140: Loss: 0.24907969863261203, accuracy: 0.99\n",
      "iteration no 8141: Loss: 0.24908152476920023, accuracy: 0.99\n",
      "iteration no 8142: Loss: 0.24907923821882788, accuracy: 0.99\n",
      "iteration no 8143: Loss: 0.24908075914572333, accuracy: 0.99\n",
      "iteration no 8144: Loss: 0.249080189201913, accuracy: 0.99\n",
      "iteration no 8145: Loss: 0.24907793947539922, accuracy: 0.99\n",
      "iteration no 8146: Loss: 0.2490809594661901, accuracy: 0.99\n",
      "iteration no 8147: Loss: 0.24907676470773432, accuracy: 0.99\n",
      "iteration no 8148: Loss: 0.2490826280822956, accuracy: 0.99\n",
      "iteration no 8149: Loss: 0.24907826904311725, accuracy: 0.99\n",
      "iteration no 8150: Loss: 0.24908005982363557, accuracy: 0.99\n",
      "iteration no 8151: Loss: 0.2490784717407495, accuracy: 0.99\n",
      "iteration no 8152: Loss: 0.24907891130171292, accuracy: 0.99\n",
      "iteration no 8153: Loss: 0.24907868044727238, accuracy: 0.99\n",
      "iteration no 8154: Loss: 0.24907864492515824, accuracy: 0.99\n",
      "iteration no 8155: Loss: 0.24907744108580154, accuracy: 0.99\n",
      "iteration no 8156: Loss: 0.24907958312521258, accuracy: 0.99\n",
      "iteration no 8157: Loss: 0.24907641438192857, accuracy: 0.99\n",
      "iteration no 8158: Loss: 0.24907908117916283, accuracy: 0.99\n",
      "iteration no 8159: Loss: 0.24907576126200429, accuracy: 0.99\n",
      "iteration no 8160: Loss: 0.24907929425139919, accuracy: 0.99\n",
      "iteration no 8161: Loss: 0.2490773927581269, accuracy: 0.99\n",
      "iteration no 8162: Loss: 0.24907818921345054, accuracy: 0.99\n",
      "iteration no 8163: Loss: 0.24907644003583976, accuracy: 0.99\n",
      "iteration no 8164: Loss: 0.24907744801403636, accuracy: 0.99\n",
      "iteration no 8165: Loss: 0.2490766406663825, accuracy: 0.99\n",
      "iteration no 8166: Loss: 0.24907765612992355, accuracy: 0.99\n",
      "iteration no 8167: Loss: 0.24907542038050956, accuracy: 0.99\n",
      "iteration no 8168: Loss: 0.24907750473898116, accuracy: 0.99\n",
      "iteration no 8169: Loss: 0.24907508835591582, accuracy: 0.99\n",
      "iteration no 8170: Loss: 0.24907742810986072, accuracy: 0.99\n",
      "iteration no 8171: Loss: 0.24907406551188233, accuracy: 0.99\n",
      "iteration no 8172: Loss: 0.24907767729989366, accuracy: 0.99\n",
      "iteration no 8173: Loss: 0.24907418750259755, accuracy: 0.99\n",
      "iteration no 8174: Loss: 0.24907813268318554, accuracy: 0.99\n",
      "iteration no 8175: Loss: 0.24907413084555657, accuracy: 0.99\n",
      "iteration no 8176: Loss: 0.24907648977205926, accuracy: 0.99\n",
      "iteration no 8177: Loss: 0.2490756216130337, accuracy: 0.99\n",
      "iteration no 8178: Loss: 0.24907478907736486, accuracy: 0.99\n",
      "iteration no 8179: Loss: 0.24907514312873347, accuracy: 0.99\n",
      "iteration no 8180: Loss: 0.2490746701394419, accuracy: 0.99\n",
      "iteration no 8181: Loss: 0.24907388963098204, accuracy: 0.99\n",
      "iteration no 8182: Loss: 0.24907628127067447, accuracy: 0.99\n",
      "iteration no 8183: Loss: 0.24907140645530718, accuracy: 0.99\n",
      "iteration no 8184: Loss: 0.24907732428827026, accuracy: 0.99\n",
      "iteration no 8185: Loss: 0.24907061648917325, accuracy: 0.99\n",
      "iteration no 8186: Loss: 0.24907647267715893, accuracy: 0.99\n",
      "iteration no 8187: Loss: 0.24907280590365086, accuracy: 0.99\n",
      "iteration no 8188: Loss: 0.24907420296387556, accuracy: 0.99\n",
      "iteration no 8189: Loss: 0.24907334508312373, accuracy: 0.99\n",
      "iteration no 8190: Loss: 0.2490716784836533, accuracy: 0.99\n",
      "iteration no 8191: Loss: 0.24907464150183134, accuracy: 0.99\n",
      "iteration no 8192: Loss: 0.249072584487594, accuracy: 0.99\n",
      "iteration no 8193: Loss: 0.24907370583461616, accuracy: 0.99\n",
      "iteration no 8194: Loss: 0.2490750931677537, accuracy: 0.99\n",
      "iteration no 8195: Loss: 0.24907118155022853, accuracy: 0.99\n",
      "iteration no 8196: Loss: 0.2490740263121058, accuracy: 0.99\n",
      "iteration no 8197: Loss: 0.24907104742672354, accuracy: 0.99\n",
      "iteration no 8198: Loss: 0.2490729734002105, accuracy: 0.99\n",
      "iteration no 8199: Loss: 0.2490725945789804, accuracy: 0.99\n",
      "iteration no 8200: Loss: 0.24907103836164732, accuracy: 0.99\n",
      "iteration no 8201: Loss: 0.24907246076457806, accuracy: 0.99\n",
      "iteration no 8202: Loss: 0.24906960029820757, accuracy: 0.99\n",
      "iteration no 8203: Loss: 0.24907304226834462, accuracy: 0.99\n",
      "iteration no 8204: Loss: 0.24907115653270145, accuracy: 0.99\n",
      "iteration no 8205: Loss: 0.24907121487635786, accuracy: 0.99\n",
      "iteration no 8206: Loss: 0.24907158106752716, accuracy: 0.99\n",
      "iteration no 8207: Loss: 0.2490698868721211, accuracy: 0.99\n",
      "iteration no 8208: Loss: 0.24907112242954343, accuracy: 0.99\n",
      "iteration no 8209: Loss: 0.24907062517768902, accuracy: 0.99\n",
      "iteration no 8210: Loss: 0.24907033798492773, accuracy: 0.99\n",
      "iteration no 8211: Loss: 0.24907134540496095, accuracy: 0.99\n",
      "iteration no 8212: Loss: 0.24906914398594257, accuracy: 0.99\n",
      "iteration no 8213: Loss: 0.24907069995567854, accuracy: 0.99\n",
      "iteration no 8214: Loss: 0.249068818408227, accuracy: 0.99\n",
      "iteration no 8215: Loss: 0.24907126079715386, accuracy: 0.99\n",
      "iteration no 8216: Loss: 0.24906888304962835, accuracy: 0.99\n",
      "iteration no 8217: Loss: 0.2490704849458889, accuracy: 0.99\n",
      "iteration no 8218: Loss: 0.24906947286865982, accuracy: 0.99\n",
      "iteration no 8219: Loss: 0.24907024625696347, accuracy: 0.99\n",
      "iteration no 8220: Loss: 0.24906937345698177, accuracy: 0.99\n",
      "iteration no 8221: Loss: 0.24906906261554929, accuracy: 0.99\n",
      "iteration no 8222: Loss: 0.24906961605614694, accuracy: 0.99\n",
      "iteration no 8223: Loss: 0.24906818518602453, accuracy: 0.99\n",
      "iteration no 8224: Loss: 0.24906846870969934, accuracy: 0.99\n",
      "iteration no 8225: Loss: 0.24906866154004265, accuracy: 0.99\n",
      "iteration no 8226: Loss: 0.24906726373697974, accuracy: 0.99\n",
      "iteration no 8227: Loss: 0.24907022123412054, accuracy: 0.99\n",
      "iteration no 8228: Loss: 0.24906575480736665, accuracy: 0.99\n",
      "iteration no 8229: Loss: 0.24906999790142026, accuracy: 0.99\n",
      "iteration no 8230: Loss: 0.24906573736700643, accuracy: 0.99\n",
      "iteration no 8231: Loss: 0.2490692343602614, accuracy: 0.99\n",
      "iteration no 8232: Loss: 0.24906877059434704, accuracy: 0.99\n",
      "iteration no 8233: Loss: 0.24906659875266873, accuracy: 0.99\n",
      "iteration no 8234: Loss: 0.24906808963059884, accuracy: 0.99\n",
      "iteration no 8235: Loss: 0.24906717763628056, accuracy: 0.99\n",
      "iteration no 8236: Loss: 0.249066404420494, accuracy: 0.99\n",
      "iteration no 8237: Loss: 0.24906854179644577, accuracy: 0.99\n",
      "iteration no 8238: Loss: 0.24906421584612445, accuracy: 0.99\n",
      "iteration no 8239: Loss: 0.2490695437348193, accuracy: 0.99\n",
      "iteration no 8240: Loss: 0.2490632088779764, accuracy: 0.99\n",
      "iteration no 8241: Loss: 0.24906901563861222, accuracy: 0.99\n",
      "iteration no 8242: Loss: 0.24906524585901307, accuracy: 0.99\n",
      "iteration no 8243: Loss: 0.24906689403284166, accuracy: 0.99\n",
      "iteration no 8244: Loss: 0.24906641922159478, accuracy: 0.99\n",
      "iteration no 8245: Loss: 0.24906525772371185, accuracy: 0.99\n",
      "iteration no 8246: Loss: 0.24906652078060523, accuracy: 0.99\n",
      "iteration no 8247: Loss: 0.24906553635493323, accuracy: 0.99\n",
      "iteration no 8248: Loss: 0.2490646382550433, accuracy: 0.99\n",
      "iteration no 8249: Loss: 0.24906717940769424, accuracy: 0.99\n",
      "iteration no 8250: Loss: 0.24906233791677923, accuracy: 0.99\n",
      "iteration no 8251: Loss: 0.24906794496688714, accuracy: 0.99\n",
      "iteration no 8252: Loss: 0.24906196962813681, accuracy: 0.99\n",
      "iteration no 8253: Loss: 0.2490667313439858, accuracy: 0.99\n",
      "iteration no 8254: Loss: 0.24906404903696014, accuracy: 0.99\n",
      "iteration no 8255: Loss: 0.24906528231300606, accuracy: 0.99\n",
      "iteration no 8256: Loss: 0.2490645426597747, accuracy: 0.99\n",
      "iteration no 8257: Loss: 0.2490654108469098, accuracy: 0.99\n",
      "iteration no 8258: Loss: 0.2490632730199942, accuracy: 0.99\n",
      "iteration no 8259: Loss: 0.2490649581990553, accuracy: 0.99\n",
      "iteration no 8260: Loss: 0.24906368706948856, accuracy: 0.99\n",
      "iteration no 8261: Loss: 0.24906499941569119, accuracy: 0.99\n",
      "iteration no 8262: Loss: 0.2490624037874633, accuracy: 0.99\n",
      "iteration no 8263: Loss: 0.24906478180171493, accuracy: 0.99\n",
      "iteration no 8264: Loss: 0.24906137531071246, accuracy: 0.99\n",
      "iteration no 8265: Loss: 0.24906503979615507, accuracy: 0.99\n",
      "iteration no 8266: Loss: 0.24906180695272911, accuracy: 0.99\n",
      "iteration no 8267: Loss: 0.2490644879393573, accuracy: 0.99\n",
      "iteration no 8268: Loss: 0.24906163347538646, accuracy: 0.99\n",
      "iteration no 8269: Loss: 0.24906327640217862, accuracy: 0.99\n",
      "iteration no 8270: Loss: 0.2490619095227739, accuracy: 0.99\n",
      "iteration no 8271: Loss: 0.24906245745666752, accuracy: 0.99\n",
      "iteration no 8272: Loss: 0.24906275466459998, accuracy: 0.99\n",
      "iteration no 8273: Loss: 0.2490624693852963, accuracy: 0.99\n",
      "iteration no 8274: Loss: 0.2490616826757585, accuracy: 0.99\n",
      "iteration no 8275: Loss: 0.24906227225285038, accuracy: 0.99\n",
      "iteration no 8276: Loss: 0.24906090867391922, accuracy: 0.99\n",
      "iteration no 8277: Loss: 0.249064746606043, accuracy: 0.99\n",
      "iteration no 8278: Loss: 0.24906062306953225, accuracy: 0.99\n",
      "iteration no 8279: Loss: 0.24906227685791366, accuracy: 0.99\n",
      "iteration no 8280: Loss: 0.24906113420153975, accuracy: 0.99\n",
      "iteration no 8281: Loss: 0.24905993199776752, accuracy: 0.99\n",
      "iteration no 8282: Loss: 0.24906259381014617, accuracy: 0.99\n",
      "iteration no 8283: Loss: 0.24905931808179693, accuracy: 0.99\n",
      "iteration no 8284: Loss: 0.2490627967992833, accuracy: 0.99\n",
      "iteration no 8285: Loss: 0.2490610835525492, accuracy: 0.99\n",
      "iteration no 8286: Loss: 0.24905979737807482, accuracy: 0.99\n",
      "iteration no 8287: Loss: 0.24906206768471845, accuracy: 0.99\n",
      "iteration no 8288: Loss: 0.24905835921675623, accuracy: 0.99\n",
      "iteration no 8289: Loss: 0.24906287060916898, accuracy: 0.99\n",
      "iteration no 8290: Loss: 0.24905929842927277, accuracy: 0.99\n",
      "iteration no 8291: Loss: 0.24906018901260038, accuracy: 0.99\n",
      "iteration no 8292: Loss: 0.24905981749516004, accuracy: 0.99\n",
      "iteration no 8293: Loss: 0.24905826911734777, accuracy: 0.99\n",
      "iteration no 8294: Loss: 0.2490617758542411, accuracy: 0.99\n",
      "iteration no 8295: Loss: 0.24905790542747838, accuracy: 0.99\n",
      "iteration no 8296: Loss: 0.2490610360633738, accuracy: 0.99\n",
      "iteration no 8297: Loss: 0.24906025473205406, accuracy: 0.99\n",
      "iteration no 8298: Loss: 0.24905864137908423, accuracy: 0.99\n",
      "iteration no 8299: Loss: 0.2490609372159855, accuracy: 0.99\n",
      "iteration no 8300: Loss: 0.24905831016856225, accuracy: 0.99\n",
      "iteration no 8301: Loss: 0.2490595277009061, accuracy: 0.99\n",
      "iteration no 8302: Loss: 0.2490588950085304, accuracy: 0.99\n",
      "iteration no 8303: Loss: 0.24905738541317582, accuracy: 0.99\n",
      "iteration no 8304: Loss: 0.24905941736840562, accuracy: 0.99\n",
      "iteration no 8305: Loss: 0.24905740970281395, accuracy: 0.99\n",
      "iteration no 8306: Loss: 0.24905950242426617, accuracy: 0.99\n",
      "iteration no 8307: Loss: 0.24905785540880582, accuracy: 0.99\n",
      "iteration no 8308: Loss: 0.249057868900622, accuracy: 0.99\n",
      "iteration no 8309: Loss: 0.24905833530322027, accuracy: 0.99\n",
      "iteration no 8310: Loss: 0.24905736824842417, accuracy: 0.99\n",
      "iteration no 8311: Loss: 0.24905876114180114, accuracy: 0.99\n",
      "iteration no 8312: Loss: 0.24905695016917512, accuracy: 0.99\n",
      "iteration no 8313: Loss: 0.24905728910734254, accuracy: 0.99\n",
      "iteration no 8314: Loss: 0.24905748905488945, accuracy: 0.99\n",
      "iteration no 8315: Loss: 0.24905643149521206, accuracy: 0.99\n",
      "iteration no 8316: Loss: 0.24905816075074833, accuracy: 0.99\n",
      "iteration no 8317: Loss: 0.24905598156052977, accuracy: 0.99\n",
      "iteration no 8318: Loss: 0.249057208369738, accuracy: 0.99\n",
      "iteration no 8319: Loss: 0.2490562782307774, accuracy: 0.99\n",
      "iteration no 8320: Loss: 0.24905705202257625, accuracy: 0.99\n",
      "iteration no 8321: Loss: 0.2490568909035211, accuracy: 0.99\n",
      "iteration no 8322: Loss: 0.24905635090661343, accuracy: 0.99\n",
      "iteration no 8323: Loss: 0.24905653176986614, accuracy: 0.99\n",
      "iteration no 8324: Loss: 0.24905554011049846, accuracy: 0.99\n",
      "iteration no 8325: Loss: 0.24905625189626274, accuracy: 0.99\n",
      "iteration no 8326: Loss: 0.24905605718745574, accuracy: 0.99\n",
      "iteration no 8327: Loss: 0.24905656706133178, accuracy: 0.99\n",
      "iteration no 8328: Loss: 0.24905662134847334, accuracy: 0.99\n",
      "iteration no 8329: Loss: 0.24905407770612847, accuracy: 0.99\n",
      "iteration no 8330: Loss: 0.2490570462670168, accuracy: 0.99\n",
      "iteration no 8331: Loss: 0.2490534624163989, accuracy: 0.99\n",
      "iteration no 8332: Loss: 0.24905685966432867, accuracy: 0.99\n",
      "iteration no 8333: Loss: 0.24905456212567026, accuracy: 0.99\n",
      "iteration no 8334: Loss: 0.24905553641696332, accuracy: 0.99\n",
      "iteration no 8335: Loss: 0.24905581344490854, accuracy: 0.99\n",
      "iteration no 8336: Loss: 0.24905344167046067, accuracy: 0.99\n",
      "iteration no 8337: Loss: 0.24905629893927894, accuracy: 0.99\n",
      "iteration no 8338: Loss: 0.24905398914120286, accuracy: 0.99\n",
      "iteration no 8339: Loss: 0.2490542577684886, accuracy: 0.99\n",
      "iteration no 8340: Loss: 0.24905510017252436, accuracy: 0.99\n",
      "iteration no 8341: Loss: 0.24905223779577326, accuracy: 0.99\n",
      "iteration no 8342: Loss: 0.24905579426978966, accuracy: 0.99\n",
      "iteration no 8343: Loss: 0.24905262034932857, accuracy: 0.99\n",
      "iteration no 8344: Loss: 0.24905499757499247, accuracy: 0.99\n",
      "iteration no 8345: Loss: 0.24905547516487908, accuracy: 0.99\n",
      "iteration no 8346: Loss: 0.2490525309470119, accuracy: 0.99\n",
      "iteration no 8347: Loss: 0.2490547497189544, accuracy: 0.99\n",
      "iteration no 8348: Loss: 0.24905261801054784, accuracy: 0.99\n",
      "iteration no 8349: Loss: 0.24905358731157412, accuracy: 0.99\n",
      "iteration no 8350: Loss: 0.24905390576884928, accuracy: 0.99\n",
      "iteration no 8351: Loss: 0.2490515127977694, accuracy: 0.99\n",
      "iteration no 8352: Loss: 0.24905453377221987, accuracy: 0.99\n",
      "iteration no 8353: Loss: 0.24905084165965513, accuracy: 0.99\n",
      "iteration no 8354: Loss: 0.24905385690326404, accuracy: 0.99\n",
      "iteration no 8355: Loss: 0.2490530694469339, accuracy: 0.99\n",
      "iteration no 8356: Loss: 0.24905164452084014, accuracy: 0.99\n",
      "iteration no 8357: Loss: 0.2490535858366649, accuracy: 0.99\n",
      "iteration no 8358: Loss: 0.24905078349943652, accuracy: 0.99\n",
      "iteration no 8359: Loss: 0.24905290947654085, accuracy: 0.99\n",
      "iteration no 8360: Loss: 0.24905185603758906, accuracy: 0.99\n",
      "iteration no 8361: Loss: 0.24905131137839673, accuracy: 0.99\n",
      "iteration no 8362: Loss: 0.2490532273430061, accuracy: 0.99\n",
      "iteration no 8363: Loss: 0.2490502336470763, accuracy: 0.99\n",
      "iteration no 8364: Loss: 0.24905253536733188, accuracy: 0.99\n",
      "iteration no 8365: Loss: 0.24904990964686338, accuracy: 0.99\n",
      "iteration no 8366: Loss: 0.2490519948945044, accuracy: 0.99\n",
      "iteration no 8367: Loss: 0.2490525283234842, accuracy: 0.99\n",
      "iteration no 8368: Loss: 0.24905051034527126, accuracy: 0.99\n",
      "iteration no 8369: Loss: 0.24905192603618562, accuracy: 0.99\n",
      "iteration no 8370: Loss: 0.24904952068185343, accuracy: 0.99\n",
      "iteration no 8371: Loss: 0.24905114816164284, accuracy: 0.99\n",
      "iteration no 8372: Loss: 0.24905154530240015, accuracy: 0.99\n",
      "iteration no 8373: Loss: 0.24904971582683133, accuracy: 0.99\n",
      "iteration no 8374: Loss: 0.24905121119923387, accuracy: 0.99\n",
      "iteration no 8375: Loss: 0.24904981872763451, accuracy: 0.99\n",
      "iteration no 8376: Loss: 0.24905120465852992, accuracy: 0.99\n",
      "iteration no 8377: Loss: 0.2490491727249235, accuracy: 0.99\n",
      "iteration no 8378: Loss: 0.24905150006361076, accuracy: 0.99\n",
      "iteration no 8379: Loss: 0.24904914452262283, accuracy: 0.99\n",
      "iteration no 8380: Loss: 0.24904978895671304, accuracy: 0.99\n",
      "iteration no 8381: Loss: 0.24904957667345068, accuracy: 0.99\n",
      "iteration no 8382: Loss: 0.24904919880416176, accuracy: 0.99\n",
      "iteration no 8383: Loss: 0.24905034100847181, accuracy: 0.99\n",
      "iteration no 8384: Loss: 0.24904901508789135, accuracy: 0.99\n",
      "iteration no 8385: Loss: 0.24904946197867028, accuracy: 0.99\n",
      "iteration no 8386: Loss: 0.24904842384401427, accuracy: 0.99\n",
      "iteration no 8387: Loss: 0.24904770229833387, accuracy: 0.99\n",
      "iteration no 8388: Loss: 0.24905087250248395, accuracy: 0.99\n",
      "iteration no 8389: Loss: 0.24904771043846097, accuracy: 0.99\n",
      "iteration no 8390: Loss: 0.24904942038564573, accuracy: 0.99\n",
      "iteration no 8391: Loss: 0.2490475721866256, accuracy: 0.99\n",
      "iteration no 8392: Loss: 0.24904801503135904, accuracy: 0.99\n",
      "iteration no 8393: Loss: 0.2490516829177163, accuracy: 0.99\n",
      "iteration no 8394: Loss: 0.2490472852888975, accuracy: 0.99\n",
      "iteration no 8395: Loss: 0.24904884415940512, accuracy: 0.99\n",
      "iteration no 8396: Loss: 0.24904792652579943, accuracy: 0.99\n",
      "iteration no 8397: Loss: 0.2490461179418445, accuracy: 0.99\n",
      "iteration no 8398: Loss: 0.2490491477681578, accuracy: 0.99\n",
      "iteration no 8399: Loss: 0.24904567922940032, accuracy: 0.99\n",
      "iteration no 8400: Loss: 0.24904949700584225, accuracy: 0.99\n",
      "iteration no 8401: Loss: 0.2490470000722117, accuracy: 0.99\n",
      "iteration no 8402: Loss: 0.24904670662701223, accuracy: 0.99\n",
      "iteration no 8403: Loss: 0.2490480640078293, accuracy: 0.99\n",
      "iteration no 8404: Loss: 0.24904559637582097, accuracy: 0.99\n",
      "iteration no 8405: Loss: 0.24904950611113824, accuracy: 0.99\n",
      "iteration no 8406: Loss: 0.24904594363948027, accuracy: 0.99\n",
      "iteration no 8407: Loss: 0.24904680536080764, accuracy: 0.99\n",
      "iteration no 8408: Loss: 0.24904651506790026, accuracy: 0.99\n",
      "iteration no 8409: Loss: 0.24904440305689818, accuracy: 0.99\n",
      "iteration no 8410: Loss: 0.24904886324304548, accuracy: 0.99\n",
      "iteration no 8411: Loss: 0.24904609553923873, accuracy: 0.99\n",
      "iteration no 8412: Loss: 0.24904625838681954, accuracy: 0.99\n",
      "iteration no 8413: Loss: 0.2490462751930852, accuracy: 0.99\n",
      "iteration no 8414: Loss: 0.24904557658067888, accuracy: 0.99\n",
      "iteration no 8415: Loss: 0.24904617698556714, accuracy: 0.99\n",
      "iteration no 8416: Loss: 0.24904515546737718, accuracy: 0.99\n",
      "iteration no 8417: Loss: 0.2490454999958987, accuracy: 0.99\n",
      "iteration no 8418: Loss: 0.24904555545658516, accuracy: 0.99\n",
      "iteration no 8419: Loss: 0.24904496489831174, accuracy: 0.99\n",
      "iteration no 8420: Loss: 0.24904537430921664, accuracy: 0.99\n",
      "iteration no 8421: Loss: 0.24904410952086886, accuracy: 0.99\n",
      "iteration no 8422: Loss: 0.24904542926912707, accuracy: 0.99\n",
      "iteration no 8423: Loss: 0.24904449865075062, accuracy: 0.99\n",
      "iteration no 8424: Loss: 0.2490449302191729, accuracy: 0.99\n",
      "iteration no 8425: Loss: 0.2490449685560875, accuracy: 0.99\n",
      "iteration no 8426: Loss: 0.24904400472688093, accuracy: 0.99\n",
      "iteration no 8427: Loss: 0.24904490239876595, accuracy: 0.99\n",
      "iteration no 8428: Loss: 0.24904383000463087, accuracy: 0.99\n",
      "iteration no 8429: Loss: 0.24904382816288367, accuracy: 0.99\n",
      "iteration no 8430: Loss: 0.24904403393698382, accuracy: 0.99\n",
      "iteration no 8431: Loss: 0.2490435981456537, accuracy: 0.99\n",
      "iteration no 8432: Loss: 0.24904421591475207, accuracy: 0.99\n",
      "iteration no 8433: Loss: 0.24904312364396247, accuracy: 0.99\n",
      "iteration no 8434: Loss: 0.24904395873518986, accuracy: 0.99\n",
      "iteration no 8435: Loss: 0.24904359144829252, accuracy: 0.99\n",
      "iteration no 8436: Loss: 0.24904338992818753, accuracy: 0.99\n",
      "iteration no 8437: Loss: 0.24904333998749983, accuracy: 0.99\n",
      "iteration no 8438: Loss: 0.24904230260988863, accuracy: 0.99\n",
      "iteration no 8439: Loss: 0.24904356766571592, accuracy: 0.99\n",
      "iteration no 8440: Loss: 0.2490428750027133, accuracy: 0.99\n",
      "iteration no 8441: Loss: 0.24904418113518084, accuracy: 0.99\n",
      "iteration no 8442: Loss: 0.2490425230336272, accuracy: 0.99\n",
      "iteration no 8443: Loss: 0.2490420292834229, accuracy: 0.99\n",
      "iteration no 8444: Loss: 0.24904343877658797, accuracy: 0.99\n",
      "iteration no 8445: Loss: 0.24904020549757816, accuracy: 0.99\n",
      "iteration no 8446: Loss: 0.24904383017506262, accuracy: 0.99\n",
      "iteration no 8447: Loss: 0.24904086676443063, accuracy: 0.99\n",
      "iteration no 8448: Loss: 0.2490424826175708, accuracy: 0.99\n",
      "iteration no 8449: Loss: 0.24904295562287382, accuracy: 0.99\n",
      "iteration no 8450: Loss: 0.24903987639991698, accuracy: 0.99\n",
      "iteration no 8451: Loss: 0.24904409152503226, accuracy: 0.99\n",
      "iteration no 8452: Loss: 0.24904020797867074, accuracy: 0.99\n",
      "iteration no 8453: Loss: 0.24904218147136492, accuracy: 0.99\n",
      "iteration no 8454: Loss: 0.24904141841816285, accuracy: 0.99\n",
      "iteration no 8455: Loss: 0.24903946665810608, accuracy: 0.99\n",
      "iteration no 8456: Loss: 0.24904252321714238, accuracy: 0.99\n",
      "iteration no 8457: Loss: 0.24903907543360954, accuracy: 0.99\n",
      "iteration no 8458: Loss: 0.24904229605685824, accuracy: 0.99\n",
      "iteration no 8459: Loss: 0.2490423358568999, accuracy: 0.99\n",
      "iteration no 8460: Loss: 0.24903964965563508, accuracy: 0.99\n",
      "iteration no 8461: Loss: 0.2490417677733702, accuracy: 0.99\n",
      "iteration no 8462: Loss: 0.2490394173732145, accuracy: 0.99\n",
      "iteration no 8463: Loss: 0.2490403471860208, accuracy: 0.99\n",
      "iteration no 8464: Loss: 0.24904033150108518, accuracy: 0.99\n",
      "iteration no 8465: Loss: 0.24903914559538742, accuracy: 0.99\n",
      "iteration no 8466: Loss: 0.2490412370504677, accuracy: 0.99\n",
      "iteration no 8467: Loss: 0.24903801022861155, accuracy: 0.99\n",
      "iteration no 8468: Loss: 0.24904075209563659, accuracy: 0.99\n",
      "iteration no 8469: Loss: 0.24903986618942547, accuracy: 0.99\n",
      "iteration no 8470: Loss: 0.24903883677740618, accuracy: 0.99\n",
      "iteration no 8471: Loss: 0.24904113024070634, accuracy: 0.99\n",
      "iteration no 8472: Loss: 0.24903762704406662, accuracy: 0.99\n",
      "iteration no 8473: Loss: 0.24904004699612609, accuracy: 0.99\n",
      "iteration no 8474: Loss: 0.24903843350775645, accuracy: 0.99\n",
      "iteration no 8475: Loss: 0.24903864280555155, accuracy: 0.99\n",
      "iteration no 8476: Loss: 0.24903979602980247, accuracy: 0.99\n",
      "iteration no 8477: Loss: 0.24903779194905118, accuracy: 0.99\n",
      "iteration no 8478: Loss: 0.24903933088610272, accuracy: 0.99\n",
      "iteration no 8479: Loss: 0.24903780039623502, accuracy: 0.99\n",
      "iteration no 8480: Loss: 0.24903842216821903, accuracy: 0.99\n",
      "iteration no 8481: Loss: 0.24903978416810826, accuracy: 0.99\n",
      "iteration no 8482: Loss: 0.2490375126857142, accuracy: 0.99\n",
      "iteration no 8483: Loss: 0.24903933808942902, accuracy: 0.99\n",
      "iteration no 8484: Loss: 0.24903652594833764, accuracy: 0.99\n",
      "iteration no 8485: Loss: 0.24903786423810084, accuracy: 0.99\n",
      "iteration no 8486: Loss: 0.24903825717792483, accuracy: 0.99\n",
      "iteration no 8487: Loss: 0.2490372095051566, accuracy: 0.99\n",
      "iteration no 8488: Loss: 0.24903794638725105, accuracy: 0.99\n",
      "iteration no 8489: Loss: 0.24903730151254766, accuracy: 0.99\n",
      "iteration no 8490: Loss: 0.24903775155809857, accuracy: 0.99\n",
      "iteration no 8491: Loss: 0.24903677240614056, accuracy: 0.99\n",
      "iteration no 8492: Loss: 0.24903793772191515, accuracy: 0.99\n",
      "iteration no 8493: Loss: 0.24903623416642787, accuracy: 0.99\n",
      "iteration no 8494: Loss: 0.249036727913876, accuracy: 0.99\n",
      "iteration no 8495: Loss: 0.2490370096746107, accuracy: 0.99\n",
      "iteration no 8496: Loss: 0.24903564809578627, accuracy: 0.99\n",
      "iteration no 8497: Loss: 0.2490378567591297, accuracy: 0.99\n",
      "iteration no 8498: Loss: 0.2490354499033795, accuracy: 0.99\n",
      "iteration no 8499: Loss: 0.24903662528960543, accuracy: 0.99\n",
      "iteration no 8500: Loss: 0.2490359943313598, accuracy: 0.99\n",
      "iteration no 8501: Loss: 0.24903505042033675, accuracy: 0.99\n",
      "iteration no 8502: Loss: 0.24903729252148793, accuracy: 0.99\n",
      "iteration no 8503: Loss: 0.24903514630668439, accuracy: 0.99\n",
      "iteration no 8504: Loss: 0.2490361712317271, accuracy: 0.99\n",
      "iteration no 8505: Loss: 0.24903549036727074, accuracy: 0.99\n",
      "iteration no 8506: Loss: 0.2490343873701225, accuracy: 0.99\n",
      "iteration no 8507: Loss: 0.24903854167625428, accuracy: 0.99\n",
      "iteration no 8508: Loss: 0.2490347801123251, accuracy: 0.99\n",
      "iteration no 8509: Loss: 0.24903610006282745, accuracy: 0.99\n",
      "iteration no 8510: Loss: 0.24903495119782632, accuracy: 0.99\n",
      "iteration no 8511: Loss: 0.2490334027546241, accuracy: 0.99\n",
      "iteration no 8512: Loss: 0.24903647136906826, accuracy: 0.99\n",
      "iteration no 8513: Loss: 0.24903355382132356, accuracy: 0.99\n",
      "iteration no 8514: Loss: 0.2490358795731183, accuracy: 0.99\n",
      "iteration no 8515: Loss: 0.24903482402153337, accuracy: 0.99\n",
      "iteration no 8516: Loss: 0.2490331188384492, accuracy: 0.99\n",
      "iteration no 8517: Loss: 0.24903610423820183, accuracy: 0.99\n",
      "iteration no 8518: Loss: 0.2490322759095816, accuracy: 0.99\n",
      "iteration no 8519: Loss: 0.24903604362474474, accuracy: 0.99\n",
      "iteration no 8520: Loss: 0.2490332897891135, accuracy: 0.99\n",
      "iteration no 8521: Loss: 0.24903353217080215, accuracy: 0.99\n",
      "iteration no 8522: Loss: 0.24903437153856772, accuracy: 0.99\n",
      "iteration no 8523: Loss: 0.24903169739992997, accuracy: 0.99\n",
      "iteration no 8524: Loss: 0.24903535527170795, accuracy: 0.99\n",
      "iteration no 8525: Loss: 0.2490328600100961, accuracy: 0.99\n",
      "iteration no 8526: Loss: 0.2490333973832506, accuracy: 0.99\n",
      "iteration no 8527: Loss: 0.2490346421167473, accuracy: 0.99\n",
      "iteration no 8528: Loss: 0.2490308812611306, accuracy: 0.99\n",
      "iteration no 8529: Loss: 0.24903593553282521, accuracy: 0.99\n",
      "iteration no 8530: Loss: 0.24903301530351715, accuracy: 0.99\n",
      "iteration no 8531: Loss: 0.24903322970452896, accuracy: 0.99\n",
      "iteration no 8532: Loss: 0.24903263419154922, accuracy: 0.99\n",
      "iteration no 8533: Loss: 0.2490319749523094, accuracy: 0.99\n",
      "iteration no 8534: Loss: 0.24903268381148164, accuracy: 0.99\n",
      "iteration no 8535: Loss: 0.24903230796348952, accuracy: 0.99\n",
      "iteration no 8536: Loss: 0.24903242340316223, accuracy: 0.99\n",
      "iteration no 8537: Loss: 0.24903298414457706, accuracy: 0.99\n",
      "iteration no 8538: Loss: 0.24903143691380053, accuracy: 0.99\n",
      "iteration no 8539: Loss: 0.24903249995860532, accuracy: 0.99\n",
      "iteration no 8540: Loss: 0.2490309757199214, accuracy: 0.99\n",
      "iteration no 8541: Loss: 0.24903259689309837, accuracy: 0.99\n",
      "iteration no 8542: Loss: 0.2490317919056126, accuracy: 0.99\n",
      "iteration no 8543: Loss: 0.24903187160805046, accuracy: 0.99\n",
      "iteration no 8544: Loss: 0.24903075136375036, accuracy: 0.99\n",
      "iteration no 8545: Loss: 0.24903116808666748, accuracy: 0.99\n",
      "iteration no 8546: Loss: 0.24903117491212226, accuracy: 0.99\n",
      "iteration no 8547: Loss: 0.2490315457432457, accuracy: 0.99\n",
      "iteration no 8548: Loss: 0.2490321321485224, accuracy: 0.99\n",
      "iteration no 8549: Loss: 0.2490305710079671, accuracy: 0.99\n",
      "iteration no 8550: Loss: 0.2490302444805194, accuracy: 0.99\n",
      "iteration no 8551: Loss: 0.24903252013531246, accuracy: 0.99\n",
      "iteration no 8552: Loss: 0.24902927311088174, accuracy: 0.99\n",
      "iteration no 8553: Loss: 0.2490326268605171, accuracy: 0.99\n",
      "iteration no 8554: Loss: 0.2490287892831034, accuracy: 0.99\n",
      "iteration no 8555: Loss: 0.24903097860715398, accuracy: 0.99\n",
      "iteration no 8556: Loss: 0.24903063518016208, accuracy: 0.99\n",
      "iteration no 8557: Loss: 0.2490290725744201, accuracy: 0.99\n",
      "iteration no 8558: Loss: 0.24903140920446157, accuracy: 0.99\n",
      "iteration no 8559: Loss: 0.24902886588928436, accuracy: 0.99\n",
      "iteration no 8560: Loss: 0.24903030875507637, accuracy: 0.99\n",
      "iteration no 8561: Loss: 0.24903063507969397, accuracy: 0.99\n",
      "iteration no 8562: Loss: 0.24902817238964287, accuracy: 0.99\n",
      "iteration no 8563: Loss: 0.24903221520161153, accuracy: 0.99\n",
      "iteration no 8564: Loss: 0.24902751050973104, accuracy: 0.99\n",
      "iteration no 8565: Loss: 0.24903089417065474, accuracy: 0.99\n",
      "iteration no 8566: Loss: 0.24902833925834428, accuracy: 0.99\n",
      "iteration no 8567: Loss: 0.24902882949739996, accuracy: 0.99\n",
      "iteration no 8568: Loss: 0.24902988168328513, accuracy: 0.99\n",
      "iteration no 8569: Loss: 0.24902750338070045, accuracy: 0.99\n",
      "iteration no 8570: Loss: 0.2490297583064155, accuracy: 0.99\n",
      "iteration no 8571: Loss: 0.24902814406814228, accuracy: 0.99\n",
      "iteration no 8572: Loss: 0.24902834448416025, accuracy: 0.99\n",
      "iteration no 8573: Loss: 0.24903038513432443, accuracy: 0.99\n",
      "iteration no 8574: Loss: 0.24902625597241496, accuracy: 0.99\n",
      "iteration no 8575: Loss: 0.24903079661640537, accuracy: 0.99\n",
      "iteration no 8576: Loss: 0.24902640706303264, accuracy: 0.99\n",
      "iteration no 8577: Loss: 0.24902870714324238, accuracy: 0.99\n",
      "iteration no 8578: Loss: 0.24902820477708487, accuracy: 0.99\n",
      "iteration no 8579: Loss: 0.24902643502301008, accuracy: 0.99\n",
      "iteration no 8580: Loss: 0.24902906164984426, accuracy: 0.99\n",
      "iteration no 8581: Loss: 0.2490264684936533, accuracy: 0.99\n",
      "iteration no 8582: Loss: 0.2490280065881407, accuracy: 0.99\n",
      "iteration no 8583: Loss: 0.24902988385170977, accuracy: 0.99\n",
      "iteration no 8584: Loss: 0.24902595375740672, accuracy: 0.99\n",
      "iteration no 8585: Loss: 0.24902895732468505, accuracy: 0.99\n",
      "iteration no 8586: Loss: 0.24902605251279009, accuracy: 0.99\n",
      "iteration no 8587: Loss: 0.2490270720884194, accuracy: 0.99\n",
      "iteration no 8588: Loss: 0.24902724913698981, accuracy: 0.99\n",
      "iteration no 8589: Loss: 0.2490265543732549, accuracy: 0.99\n",
      "iteration no 8590: Loss: 0.2490273700275643, accuracy: 0.99\n",
      "iteration no 8591: Loss: 0.24902588679726426, accuracy: 0.99\n",
      "iteration no 8592: Loss: 0.2490266116621949, accuracy: 0.99\n",
      "iteration no 8593: Loss: 0.24902748115833218, accuracy: 0.99\n",
      "iteration no 8594: Loss: 0.2490259195602544, accuracy: 0.99\n",
      "iteration no 8595: Loss: 0.24902832901745744, accuracy: 0.99\n",
      "iteration no 8596: Loss: 0.24902420292096178, accuracy: 0.99\n",
      "iteration no 8597: Loss: 0.24902728150777306, accuracy: 0.99\n",
      "iteration no 8598: Loss: 0.249025107954775, accuracy: 0.99\n",
      "iteration no 8599: Loss: 0.24902607924115824, accuracy: 0.99\n",
      "iteration no 8600: Loss: 0.24902627335383823, accuracy: 0.99\n",
      "iteration no 8601: Loss: 0.24902608726106257, accuracy: 0.99\n",
      "iteration no 8602: Loss: 0.24902547842256897, accuracy: 0.99\n",
      "iteration no 8603: Loss: 0.24902493827429714, accuracy: 0.99\n",
      "iteration no 8604: Loss: 0.24902579451318002, accuracy: 0.99\n",
      "iteration no 8605: Loss: 0.24902515707627373, accuracy: 0.99\n",
      "iteration no 8606: Loss: 0.24902483795265784, accuracy: 0.99\n",
      "iteration no 8607: Loss: 0.24902578511253054, accuracy: 0.99\n",
      "iteration no 8608: Loss: 0.24902349331490842, accuracy: 0.99\n",
      "iteration no 8609: Loss: 0.2490264964834648, accuracy: 0.99\n",
      "iteration no 8610: Loss: 0.24902357589718874, accuracy: 0.99\n",
      "iteration no 8611: Loss: 0.24902609308879647, accuracy: 0.99\n",
      "iteration no 8612: Loss: 0.24902371389451722, accuracy: 0.99\n",
      "iteration no 8613: Loss: 0.24902382578145144, accuracy: 0.99\n",
      "iteration no 8614: Loss: 0.249024796013934, accuracy: 0.99\n",
      "iteration no 8615: Loss: 0.24902413443937493, accuracy: 0.99\n",
      "iteration no 8616: Loss: 0.24902459305926386, accuracy: 0.99\n",
      "iteration no 8617: Loss: 0.24902468397885358, accuracy: 0.99\n",
      "iteration no 8618: Loss: 0.24902287798497535, accuracy: 0.99\n",
      "iteration no 8619: Loss: 0.2490260115911296, accuracy: 0.99\n",
      "iteration no 8620: Loss: 0.24902225172998144, accuracy: 0.99\n",
      "iteration no 8621: Loss: 0.2490255223996999, accuracy: 0.99\n",
      "iteration no 8622: Loss: 0.24902214442725465, accuracy: 0.99\n",
      "iteration no 8623: Loss: 0.24902378274141662, accuracy: 0.99\n",
      "iteration no 8624: Loss: 0.24902450061572806, accuracy: 0.99\n",
      "iteration no 8625: Loss: 0.2490221824867478, accuracy: 0.99\n",
      "iteration no 8626: Loss: 0.24902415064958544, accuracy: 0.99\n",
      "iteration no 8627: Loss: 0.24902403704537057, accuracy: 0.99\n",
      "iteration no 8628: Loss: 0.24902140528037014, accuracy: 0.99\n",
      "iteration no 8629: Loss: 0.24902539756237765, accuracy: 0.99\n",
      "iteration no 8630: Loss: 0.24902008350770388, accuracy: 0.99\n",
      "iteration no 8631: Loss: 0.24902503385508173, accuracy: 0.99\n",
      "iteration no 8632: Loss: 0.24902149374295238, accuracy: 0.99\n",
      "iteration no 8633: Loss: 0.24902272951687093, accuracy: 0.99\n",
      "iteration no 8634: Loss: 0.2490225584774899, accuracy: 0.99\n",
      "iteration no 8635: Loss: 0.2490208277199627, accuracy: 0.99\n",
      "iteration no 8636: Loss: 0.24902316588182188, accuracy: 0.99\n",
      "iteration no 8637: Loss: 0.24902227772001123, accuracy: 0.99\n",
      "iteration no 8638: Loss: 0.24902163676045325, accuracy: 0.99\n",
      "iteration no 8639: Loss: 0.24902347649877687, accuracy: 0.99\n",
      "iteration no 8640: Loss: 0.24901956123605812, accuracy: 0.99\n",
      "iteration no 8641: Loss: 0.24902418699850004, accuracy: 0.99\n",
      "iteration no 8642: Loss: 0.24902001682122984, accuracy: 0.99\n",
      "iteration no 8643: Loss: 0.24902285915543515, accuracy: 0.99\n",
      "iteration no 8644: Loss: 0.24902095909675087, accuracy: 0.99\n",
      "iteration no 8645: Loss: 0.2490204946033236, accuracy: 0.99\n",
      "iteration no 8646: Loss: 0.24902195812529498, accuracy: 0.99\n",
      "iteration no 8647: Loss: 0.24902181661780726, accuracy: 0.99\n",
      "iteration no 8648: Loss: 0.2490212968724685, accuracy: 0.99\n",
      "iteration no 8649: Loss: 0.24902175003878074, accuracy: 0.99\n",
      "iteration no 8650: Loss: 0.2490200975600884, accuracy: 0.99\n",
      "iteration no 8651: Loss: 0.2490216940296479, accuracy: 0.99\n",
      "iteration no 8652: Loss: 0.24901901945117566, accuracy: 0.99\n",
      "iteration no 8653: Loss: 0.24902203952441976, accuracy: 0.99\n",
      "iteration no 8654: Loss: 0.2490199075599963, accuracy: 0.99\n",
      "iteration no 8655: Loss: 0.24902147758969198, accuracy: 0.99\n",
      "iteration no 8656: Loss: 0.24901932546573274, accuracy: 0.99\n",
      "iteration no 8657: Loss: 0.249020270817572, accuracy: 0.99\n",
      "iteration no 8658: Loss: 0.24901964053238312, accuracy: 0.99\n",
      "iteration no 8659: Loss: 0.24902078203964995, accuracy: 0.99\n",
      "iteration no 8660: Loss: 0.24901960605547946, accuracy: 0.99\n",
      "iteration no 8661: Loss: 0.2490201761664067, accuracy: 0.99\n",
      "iteration no 8662: Loss: 0.24901835308200249, accuracy: 0.99\n",
      "iteration no 8663: Loss: 0.2490205895869804, accuracy: 0.99\n",
      "iteration no 8664: Loss: 0.24901882112126184, accuracy: 0.99\n",
      "iteration no 8665: Loss: 0.2490223199046888, accuracy: 0.99\n",
      "iteration no 8666: Loss: 0.24901757357735194, accuracy: 0.99\n",
      "iteration no 8667: Loss: 0.24901989417388015, accuracy: 0.99\n",
      "iteration no 8668: Loss: 0.2490186837769331, accuracy: 0.99\n",
      "iteration no 8669: Loss: 0.24901803402019462, accuracy: 0.99\n",
      "iteration no 8670: Loss: 0.24902017953755895, accuracy: 0.99\n",
      "iteration no 8671: Loss: 0.24901833123764847, accuracy: 0.99\n",
      "iteration no 8672: Loss: 0.24901862392988983, accuracy: 0.99\n",
      "iteration no 8673: Loss: 0.24901994912070047, accuracy: 0.99\n",
      "iteration no 8674: Loss: 0.24901646266879168, accuracy: 0.99\n",
      "iteration no 8675: Loss: 0.249021664942557, accuracy: 0.99\n",
      "iteration no 8676: Loss: 0.24901618040749735, accuracy: 0.99\n",
      "iteration no 8677: Loss: 0.249020486098614, accuracy: 0.99\n",
      "iteration no 8678: Loss: 0.24901693587722923, accuracy: 0.99\n",
      "iteration no 8679: Loss: 0.24901750044906418, accuracy: 0.99\n",
      "iteration no 8680: Loss: 0.24901871863225905, accuracy: 0.99\n",
      "iteration no 8681: Loss: 0.2490169701330654, accuracy: 0.99\n",
      "iteration no 8682: Loss: 0.24901880643083707, accuracy: 0.99\n",
      "iteration no 8683: Loss: 0.2490182850360192, accuracy: 0.99\n",
      "iteration no 8684: Loss: 0.24901591999638623, accuracy: 0.99\n",
      "iteration no 8685: Loss: 0.24902059495318385, accuracy: 0.99\n",
      "iteration no 8686: Loss: 0.2490148508519741, accuracy: 0.99\n",
      "iteration no 8687: Loss: 0.2490201240627158, accuracy: 0.99\n",
      "iteration no 8688: Loss: 0.24901560602393746, accuracy: 0.99\n",
      "iteration no 8689: Loss: 0.24901733659223008, accuracy: 0.99\n",
      "iteration no 8690: Loss: 0.24901706785120858, accuracy: 0.99\n",
      "iteration no 8691: Loss: 0.24901569369016083, accuracy: 0.99\n",
      "iteration no 8692: Loss: 0.24901844089575448, accuracy: 0.99\n",
      "iteration no 8693: Loss: 0.24901850444093826, accuracy: 0.99\n",
      "iteration no 8694: Loss: 0.2490149924611662, accuracy: 0.99\n",
      "iteration no 8695: Loss: 0.24901828701369239, accuracy: 0.99\n",
      "iteration no 8696: Loss: 0.24901478424404072, accuracy: 0.99\n",
      "iteration no 8697: Loss: 0.24901804102767186, accuracy: 0.99\n",
      "iteration no 8698: Loss: 0.24901541405000577, accuracy: 0.99\n",
      "iteration no 8699: Loss: 0.24901667433721789, accuracy: 0.99\n",
      "iteration no 8700: Loss: 0.24901547724747608, accuracy: 0.99\n",
      "iteration no 8701: Loss: 0.24901633018397215, accuracy: 0.99\n",
      "iteration no 8702: Loss: 0.2490161141075883, accuracy: 0.99\n",
      "iteration no 8703: Loss: 0.24901623923575839, accuracy: 0.99\n",
      "iteration no 8704: Loss: 0.2490146983922007, accuracy: 0.99\n",
      "iteration no 8705: Loss: 0.24901682730483743, accuracy: 0.99\n",
      "iteration no 8706: Loss: 0.24901415761201198, accuracy: 0.99\n",
      "iteration no 8707: Loss: 0.24901740400533515, accuracy: 0.99\n",
      "iteration no 8708: Loss: 0.2490137408045922, accuracy: 0.99\n",
      "iteration no 8709: Loss: 0.24901645136124223, accuracy: 0.99\n",
      "iteration no 8710: Loss: 0.24901431965195028, accuracy: 0.99\n",
      "iteration no 8711: Loss: 0.24901581370849207, accuracy: 0.99\n",
      "iteration no 8712: Loss: 0.24901460333741915, accuracy: 0.99\n",
      "iteration no 8713: Loss: 0.24901520805233213, accuracy: 0.99\n",
      "iteration no 8714: Loss: 0.2490139422954159, accuracy: 0.99\n",
      "iteration no 8715: Loss: 0.24901572267999678, accuracy: 0.99\n",
      "iteration no 8716: Loss: 0.24901462279530956, accuracy: 0.99\n",
      "iteration no 8717: Loss: 0.24901539832683606, accuracy: 0.99\n",
      "iteration no 8718: Loss: 0.24901337622746242, accuracy: 0.99\n",
      "iteration no 8719: Loss: 0.24901675104213605, accuracy: 0.99\n",
      "iteration no 8720: Loss: 0.24901196436918094, accuracy: 0.99\n",
      "iteration no 8721: Loss: 0.24901615158724272, accuracy: 0.99\n",
      "iteration no 8722: Loss: 0.24901226498603873, accuracy: 0.99\n",
      "iteration no 8723: Loss: 0.2490151914894536, accuracy: 0.99\n",
      "iteration no 8724: Loss: 0.24901374128117842, accuracy: 0.99\n",
      "iteration no 8725: Loss: 0.24901310188096126, accuracy: 0.99\n",
      "iteration no 8726: Loss: 0.24901388986960582, accuracy: 0.99\n",
      "iteration no 8727: Loss: 0.2490134243513319, accuracy: 0.99\n",
      "iteration no 8728: Loss: 0.24901385312655666, accuracy: 0.99\n",
      "iteration no 8729: Loss: 0.24901515987536565, accuracy: 0.99\n",
      "iteration no 8730: Loss: 0.24901081875936815, accuracy: 0.99\n",
      "iteration no 8731: Loss: 0.2490159095281711, accuracy: 0.99\n",
      "iteration no 8732: Loss: 0.2490106687768663, accuracy: 0.99\n",
      "iteration no 8733: Loss: 0.249015476210429, accuracy: 0.99\n",
      "iteration no 8734: Loss: 0.2490122919134687, accuracy: 0.99\n",
      "iteration no 8735: Loss: 0.24901244459780014, accuracy: 0.99\n",
      "iteration no 8736: Loss: 0.24901311434779624, accuracy: 0.99\n",
      "iteration no 8737: Loss: 0.2490117749054802, accuracy: 0.99\n",
      "iteration no 8738: Loss: 0.24901355939006953, accuracy: 0.99\n",
      "iteration no 8739: Loss: 0.24901523254208036, accuracy: 0.99\n",
      "iteration no 8740: Loss: 0.24901019144038536, accuracy: 0.99\n",
      "iteration no 8741: Loss: 0.2490150443671065, accuracy: 0.99\n",
      "iteration no 8742: Loss: 0.2490103643160579, accuracy: 0.99\n",
      "iteration no 8743: Loss: 0.24901364507824497, accuracy: 0.99\n",
      "iteration no 8744: Loss: 0.24901180665465344, accuracy: 0.99\n",
      "iteration no 8745: Loss: 0.24901171628616064, accuracy: 0.99\n",
      "iteration no 8746: Loss: 0.24901217066725595, accuracy: 0.99\n",
      "iteration no 8747: Loss: 0.2490114991497907, accuracy: 0.99\n",
      "iteration no 8748: Loss: 0.24901180697170305, accuracy: 0.99\n",
      "iteration no 8749: Loss: 0.24901338289797126, accuracy: 0.99\n",
      "iteration no 8750: Loss: 0.24901003316680825, accuracy: 0.99\n",
      "iteration no 8751: Loss: 0.2490140104068696, accuracy: 0.99\n",
      "iteration no 8752: Loss: 0.24900880768962394, accuracy: 0.99\n",
      "iteration no 8753: Loss: 0.24901343481523097, accuracy: 0.99\n",
      "iteration no 8754: Loss: 0.24901038233762351, accuracy: 0.99\n",
      "iteration no 8755: Loss: 0.24901158452965874, accuracy: 0.99\n",
      "iteration no 8756: Loss: 0.24901083138789304, accuracy: 0.99\n",
      "iteration no 8757: Loss: 0.24901058865420136, accuracy: 0.99\n",
      "iteration no 8758: Loss: 0.24901081127588004, accuracy: 0.99\n",
      "iteration no 8759: Loss: 0.24901130558330248, accuracy: 0.99\n",
      "iteration no 8760: Loss: 0.24900967147912445, accuracy: 0.99\n",
      "iteration no 8761: Loss: 0.24901271223465474, accuracy: 0.99\n",
      "iteration no 8762: Loss: 0.2490084642545465, accuracy: 0.99\n",
      "iteration no 8763: Loss: 0.2490122578136999, accuracy: 0.99\n",
      "iteration no 8764: Loss: 0.2490086355934582, accuracy: 0.99\n",
      "iteration no 8765: Loss: 0.2490111750461591, accuracy: 0.99\n",
      "iteration no 8766: Loss: 0.24900972188764886, accuracy: 0.99\n",
      "iteration no 8767: Loss: 0.2490119641714219, accuracy: 0.99\n",
      "iteration no 8768: Loss: 0.24900921011776433, accuracy: 0.99\n",
      "iteration no 8769: Loss: 0.2490098094074137, accuracy: 0.99\n",
      "iteration no 8770: Loss: 0.2490099688747764, accuracy: 0.99\n",
      "iteration no 8771: Loss: 0.24900995275893079, accuracy: 0.99\n",
      "iteration no 8772: Loss: 0.2490096031694951, accuracy: 0.99\n",
      "iteration no 8773: Loss: 0.24901039893647992, accuracy: 0.99\n",
      "iteration no 8774: Loss: 0.2490078646942714, accuracy: 0.99\n",
      "iteration no 8775: Loss: 0.2490118370800449, accuracy: 0.99\n",
      "iteration no 8776: Loss: 0.24900770780271683, accuracy: 0.99\n",
      "iteration no 8777: Loss: 0.24901141190841528, accuracy: 0.99\n",
      "iteration no 8778: Loss: 0.2490074483370399, accuracy: 0.99\n",
      "iteration no 8779: Loss: 0.24900984360161182, accuracy: 0.99\n",
      "iteration no 8780: Loss: 0.24900850288075238, accuracy: 0.99\n",
      "iteration no 8781: Loss: 0.24900874475238405, accuracy: 0.99\n",
      "iteration no 8782: Loss: 0.2490091594835035, accuracy: 0.99\n",
      "iteration no 8783: Loss: 0.24900913742134362, accuracy: 0.99\n",
      "iteration no 8784: Loss: 0.24900804099899304, accuracy: 0.99\n",
      "iteration no 8785: Loss: 0.24901024766410018, accuracy: 0.99\n",
      "iteration no 8786: Loss: 0.24900622507372072, accuracy: 0.99\n",
      "iteration no 8787: Loss: 0.24901102843955997, accuracy: 0.99\n",
      "iteration no 8788: Loss: 0.24900603708187247, accuracy: 0.99\n",
      "iteration no 8789: Loss: 0.24900999281222713, accuracy: 0.99\n",
      "iteration no 8790: Loss: 0.2490071386503832, accuracy: 0.99\n",
      "iteration no 8791: Loss: 0.24900717193501248, accuracy: 0.99\n",
      "iteration no 8792: Loss: 0.24900849126724411, accuracy: 0.99\n",
      "iteration no 8793: Loss: 0.24900814072077082, accuracy: 0.99\n",
      "iteration no 8794: Loss: 0.24900798852884895, accuracy: 0.99\n",
      "iteration no 8795: Loss: 0.24901060766487243, accuracy: 0.99\n",
      "iteration no 8796: Loss: 0.24900432679879658, accuracy: 0.99\n",
      "iteration no 8797: Loss: 0.24901040992969503, accuracy: 0.99\n",
      "iteration no 8798: Loss: 0.2490059632437689, accuracy: 0.99\n",
      "iteration no 8799: Loss: 0.24900825196393495, accuracy: 0.99\n",
      "iteration no 8800: Loss: 0.24900682551802586, accuracy: 0.99\n",
      "iteration no 8801: Loss: 0.24900610427109052, accuracy: 0.99\n",
      "iteration no 8802: Loss: 0.24900737474711993, accuracy: 0.99\n",
      "iteration no 8803: Loss: 0.24900721978135326, accuracy: 0.99\n",
      "iteration no 8804: Loss: 0.24900651901169413, accuracy: 0.99\n",
      "iteration no 8805: Loss: 0.24900829025065632, accuracy: 0.99\n",
      "iteration no 8806: Loss: 0.24900438484279108, accuracy: 0.99\n",
      "iteration no 8807: Loss: 0.249008841826366, accuracy: 0.99\n",
      "iteration no 8808: Loss: 0.24900440450424663, accuracy: 0.99\n",
      "iteration no 8809: Loss: 0.2490085953392664, accuracy: 0.99\n",
      "iteration no 8810: Loss: 0.24900548791701607, accuracy: 0.99\n",
      "iteration no 8811: Loss: 0.2490061026702493, accuracy: 0.99\n",
      "iteration no 8812: Loss: 0.249006164331431, accuracy: 0.99\n",
      "iteration no 8813: Loss: 0.24900569442530382, accuracy: 0.99\n",
      "iteration no 8814: Loss: 0.2490064333370702, accuracy: 0.99\n",
      "iteration no 8815: Loss: 0.24900783653683592, accuracy: 0.99\n",
      "iteration no 8816: Loss: 0.24900393382772354, accuracy: 0.99\n",
      "iteration no 8817: Loss: 0.24900779005302637, accuracy: 0.99\n",
      "iteration no 8818: Loss: 0.24900283046516974, accuracy: 0.99\n",
      "iteration no 8819: Loss: 0.24900805010414634, accuracy: 0.99\n",
      "iteration no 8820: Loss: 0.24900462305505242, accuracy: 0.99\n",
      "iteration no 8821: Loss: 0.24900563203433138, accuracy: 0.99\n",
      "iteration no 8822: Loss: 0.24900507119749746, accuracy: 0.99\n",
      "iteration no 8823: Loss: 0.24900637363524897, accuracy: 0.99\n",
      "iteration no 8824: Loss: 0.24900517742625616, accuracy: 0.99\n",
      "iteration no 8825: Loss: 0.24900564537200942, accuracy: 0.99\n",
      "iteration no 8826: Loss: 0.24900472195960663, accuracy: 0.99\n",
      "iteration no 8827: Loss: 0.24900502492786628, accuracy: 0.99\n",
      "iteration no 8828: Loss: 0.2490037309538493, accuracy: 0.99\n",
      "iteration no 8829: Loss: 0.24900623622857881, accuracy: 0.99\n",
      "iteration no 8830: Loss: 0.2490033767761422, accuracy: 0.99\n",
      "iteration no 8831: Loss: 0.24900655425969284, accuracy: 0.99\n",
      "iteration no 8832: Loss: 0.24900272577724913, accuracy: 0.99\n",
      "iteration no 8833: Loss: 0.2490054010117005, accuracy: 0.99\n",
      "iteration no 8834: Loss: 0.24900338419558457, accuracy: 0.99\n",
      "iteration no 8835: Loss: 0.24900458213247967, accuracy: 0.99\n",
      "iteration no 8836: Loss: 0.24900418178358677, accuracy: 0.99\n",
      "iteration no 8837: Loss: 0.24900456774076785, accuracy: 0.99\n",
      "iteration no 8838: Loss: 0.2490029556634516, accuracy: 0.99\n",
      "iteration no 8839: Loss: 0.24900476330688226, accuracy: 0.99\n",
      "iteration no 8840: Loss: 0.24900252408127754, accuracy: 0.99\n",
      "iteration no 8841: Loss: 0.2490061984802083, accuracy: 0.99\n",
      "iteration no 8842: Loss: 0.24900177772107202, accuracy: 0.99\n",
      "iteration no 8843: Loss: 0.24900478636608023, accuracy: 0.99\n",
      "iteration no 8844: Loss: 0.2490019360513785, accuracy: 0.99\n",
      "iteration no 8845: Loss: 0.2490039378117872, accuracy: 0.99\n",
      "iteration no 8846: Loss: 0.2490036165903156, accuracy: 0.99\n",
      "iteration no 8847: Loss: 0.24900309480461338, accuracy: 0.99\n",
      "iteration no 8848: Loss: 0.2490027691399499, accuracy: 0.99\n",
      "iteration no 8849: Loss: 0.24900330938892146, accuracy: 0.99\n",
      "iteration no 8850: Loss: 0.2490020994499916, accuracy: 0.99\n",
      "iteration no 8851: Loss: 0.24900675128352984, accuracy: 0.99\n",
      "iteration no 8852: Loss: 0.24900039572733457, accuracy: 0.99\n",
      "iteration no 8853: Loss: 0.24900423942812966, accuracy: 0.99\n",
      "iteration no 8854: Loss: 0.24900186158477744, accuracy: 0.99\n",
      "iteration no 8855: Loss: 0.2490019795068103, accuracy: 0.99\n",
      "iteration no 8856: Loss: 0.24900333519603374, accuracy: 0.99\n",
      "iteration no 8857: Loss: 0.249001277269845, accuracy: 0.99\n",
      "iteration no 8858: Loss: 0.24900287508742996, accuracy: 0.99\n",
      "iteration no 8859: Loss: 0.24900349855237788, accuracy: 0.99\n",
      "iteration no 8860: Loss: 0.24900025239253068, accuracy: 0.99\n",
      "iteration no 8861: Loss: 0.24900471012349482, accuracy: 0.99\n",
      "iteration no 8862: Loss: 0.24899921697317562, accuracy: 0.99\n",
      "iteration no 8863: Loss: 0.24900447378306712, accuracy: 0.99\n",
      "iteration no 8864: Loss: 0.2490005746612912, accuracy: 0.99\n",
      "iteration no 8865: Loss: 0.24900163853170265, accuracy: 0.99\n",
      "iteration no 8866: Loss: 0.24900195908047135, accuracy: 0.99\n",
      "iteration no 8867: Loss: 0.24900056137845655, accuracy: 0.99\n",
      "iteration no 8868: Loss: 0.24900264905437797, accuracy: 0.99\n",
      "iteration no 8869: Loss: 0.24900217138905606, accuracy: 0.99\n",
      "iteration no 8870: Loss: 0.24899983398837566, accuracy: 0.99\n",
      "iteration no 8871: Loss: 0.24900419673573104, accuracy: 0.99\n",
      "iteration no 8872: Loss: 0.24899856669925716, accuracy: 0.99\n",
      "iteration no 8873: Loss: 0.2490041243057834, accuracy: 0.99\n",
      "iteration no 8874: Loss: 0.24899954077564782, accuracy: 0.99\n",
      "iteration no 8875: Loss: 0.24900124192173148, accuracy: 0.99\n",
      "iteration no 8876: Loss: 0.24900072722208105, accuracy: 0.99\n",
      "iteration no 8877: Loss: 0.24899985579175601, accuracy: 0.99\n",
      "iteration no 8878: Loss: 0.2490017161920717, accuracy: 0.99\n",
      "iteration no 8879: Loss: 0.24900187824816492, accuracy: 0.99\n",
      "iteration no 8880: Loss: 0.24899995075299616, accuracy: 0.99\n",
      "iteration no 8881: Loss: 0.24900155414363423, accuracy: 0.99\n",
      "iteration no 8882: Loss: 0.24899905694115812, accuracy: 0.99\n",
      "iteration no 8883: Loss: 0.24900115097116465, accuracy: 0.99\n",
      "iteration no 8884: Loss: 0.24899935211998422, accuracy: 0.99\n",
      "iteration no 8885: Loss: 0.2490014458775454, accuracy: 0.99\n",
      "iteration no 8886: Loss: 0.2489992266197189, accuracy: 0.99\n",
      "iteration no 8887: Loss: 0.2490003089728939, accuracy: 0.99\n",
      "iteration no 8888: Loss: 0.24899919482929062, accuracy: 0.99\n",
      "iteration no 8889: Loss: 0.2490001837345675, accuracy: 0.99\n",
      "iteration no 8890: Loss: 0.2489996052158317, accuracy: 0.99\n",
      "iteration no 8891: Loss: 0.24900060379451083, accuracy: 0.99\n",
      "iteration no 8892: Loss: 0.24899787267753265, accuracy: 0.99\n",
      "iteration no 8893: Loss: 0.2490009186735247, accuracy: 0.99\n",
      "iteration no 8894: Loss: 0.24899805794336077, accuracy: 0.99\n",
      "iteration no 8895: Loss: 0.24900076403255966, accuracy: 0.99\n",
      "iteration no 8896: Loss: 0.2489984343243685, accuracy: 0.99\n",
      "iteration no 8897: Loss: 0.2489992096555558, accuracy: 0.99\n",
      "iteration no 8898: Loss: 0.2489984705457431, accuracy: 0.99\n",
      "iteration no 8899: Loss: 0.2489992614054016, accuracy: 0.99\n",
      "iteration no 8900: Loss: 0.2489989737261945, accuracy: 0.99\n",
      "iteration no 8901: Loss: 0.24900005148215348, accuracy: 0.99\n",
      "iteration no 8902: Loss: 0.24899762039973294, accuracy: 0.99\n",
      "iteration no 8903: Loss: 0.24899958026974872, accuracy: 0.99\n",
      "iteration no 8904: Loss: 0.2489970244199715, accuracy: 0.99\n",
      "iteration no 8905: Loss: 0.249000305553703, accuracy: 0.99\n",
      "iteration no 8906: Loss: 0.24899755798550355, accuracy: 0.99\n",
      "iteration no 8907: Loss: 0.24900037194910069, accuracy: 0.99\n",
      "iteration no 8908: Loss: 0.2489973414294488, accuracy: 0.99\n",
      "iteration no 8909: Loss: 0.24899756006267784, accuracy: 0.99\n",
      "iteration no 8910: Loss: 0.24899896220310475, accuracy: 0.99\n",
      "iteration no 8911: Loss: 0.24899728878487642, accuracy: 0.99\n",
      "iteration no 8912: Loss: 0.2489983899196569, accuracy: 0.99\n",
      "iteration no 8913: Loss: 0.2489981229702944, accuracy: 0.99\n",
      "iteration no 8914: Loss: 0.24899625947928453, accuracy: 0.99\n",
      "iteration no 8915: Loss: 0.24900030746144353, accuracy: 0.99\n",
      "iteration no 8916: Loss: 0.2489956990305952, accuracy: 0.99\n",
      "iteration no 8917: Loss: 0.2489992974137152, accuracy: 0.99\n",
      "iteration no 8918: Loss: 0.24899599186518834, accuracy: 0.99\n",
      "iteration no 8919: Loss: 0.24899754772015048, accuracy: 0.99\n",
      "iteration no 8920: Loss: 0.24899755052363085, accuracy: 0.99\n",
      "iteration no 8921: Loss: 0.24899659287806486, accuracy: 0.99\n",
      "iteration no 8922: Loss: 0.24899778165823291, accuracy: 0.99\n",
      "iteration no 8923: Loss: 0.24899681890057296, accuracy: 0.99\n",
      "iteration no 8924: Loss: 0.24899598557833313, accuracy: 0.99\n",
      "iteration no 8925: Loss: 0.24899895966863922, accuracy: 0.99\n",
      "iteration no 8926: Loss: 0.24899464259560958, accuracy: 0.99\n",
      "iteration no 8927: Loss: 0.24899906650589002, accuracy: 0.99\n",
      "iteration no 8928: Loss: 0.24899487490070116, accuracy: 0.99\n",
      "iteration no 8929: Loss: 0.24899717686370293, accuracy: 0.99\n",
      "iteration no 8930: Loss: 0.24899763526998708, accuracy: 0.99\n",
      "iteration no 8931: Loss: 0.24899546000630446, accuracy: 0.99\n",
      "iteration no 8932: Loss: 0.24899709242887155, accuracy: 0.99\n",
      "iteration no 8933: Loss: 0.248997132343036, accuracy: 0.99\n",
      "iteration no 8934: Loss: 0.24899414396469835, accuracy: 0.99\n",
      "iteration no 8935: Loss: 0.24899877094633147, accuracy: 0.99\n",
      "iteration no 8936: Loss: 0.2489937367079195, accuracy: 0.99\n",
      "iteration no 8937: Loss: 0.24899831786127552, accuracy: 0.99\n",
      "iteration no 8938: Loss: 0.24899484968923488, accuracy: 0.99\n",
      "iteration no 8939: Loss: 0.24899523772720775, accuracy: 0.99\n",
      "iteration no 8940: Loss: 0.24899616166987776, accuracy: 0.99\n",
      "iteration no 8941: Loss: 0.2489944696432388, accuracy: 0.99\n",
      "iteration no 8942: Loss: 0.24899644918880479, accuracy: 0.99\n",
      "iteration no 8943: Loss: 0.24899570647045244, accuracy: 0.99\n",
      "iteration no 8944: Loss: 0.24899414011192195, accuracy: 0.99\n",
      "iteration no 8945: Loss: 0.2489968954887967, accuracy: 0.99\n",
      "iteration no 8946: Loss: 0.24899340362810865, accuracy: 0.99\n",
      "iteration no 8947: Loss: 0.2489974772990688, accuracy: 0.99\n",
      "iteration no 8948: Loss: 0.24899363557094958, accuracy: 0.99\n",
      "iteration no 8949: Loss: 0.24899517017162512, accuracy: 0.99\n",
      "iteration no 8950: Loss: 0.2489949383084653, accuracy: 0.99\n",
      "iteration no 8951: Loss: 0.2489939758344601, accuracy: 0.99\n",
      "iteration no 8952: Loss: 0.24899557331198097, accuracy: 0.99\n",
      "iteration no 8953: Loss: 0.24899435206265538, accuracy: 0.99\n",
      "iteration no 8954: Loss: 0.24899397524688743, accuracy: 0.99\n",
      "iteration no 8955: Loss: 0.24899622995070037, accuracy: 0.99\n",
      "iteration no 8956: Loss: 0.24899254532198017, accuracy: 0.99\n",
      "iteration no 8957: Loss: 0.24899659275718194, accuracy: 0.99\n",
      "iteration no 8958: Loss: 0.24899384184273324, accuracy: 0.99\n",
      "iteration no 8959: Loss: 0.24899541191781388, accuracy: 0.99\n",
      "iteration no 8960: Loss: 0.24899320561341923, accuracy: 0.99\n",
      "iteration no 8961: Loss: 0.2489942813818064, accuracy: 0.99\n",
      "iteration no 8962: Loss: 0.24899323980460025, accuracy: 0.99\n",
      "iteration no 8963: Loss: 0.24899446454235327, accuracy: 0.99\n",
      "iteration no 8964: Loss: 0.24899406164681426, accuracy: 0.99\n",
      "iteration no 8965: Loss: 0.24899395989206913, accuracy: 0.99\n",
      "iteration no 8966: Loss: 0.24899289650792225, accuracy: 0.99\n",
      "iteration no 8967: Loss: 0.24899385852514477, accuracy: 0.99\n",
      "iteration no 8968: Loss: 0.2489931192048001, accuracy: 0.99\n",
      "iteration no 8969: Loss: 0.24899546160079436, accuracy: 0.99\n",
      "iteration no 8970: Loss: 0.24899216998569015, accuracy: 0.99\n",
      "iteration no 8971: Loss: 0.24899404840355527, accuracy: 0.99\n",
      "iteration no 8972: Loss: 0.24899220775388126, accuracy: 0.99\n",
      "iteration no 8973: Loss: 0.24899346891428387, accuracy: 0.99\n",
      "iteration no 8974: Loss: 0.24899469031786153, accuracy: 0.99\n",
      "iteration no 8975: Loss: 0.2489925158150108, accuracy: 0.99\n",
      "iteration no 8976: Loss: 0.24899292331394776, accuracy: 0.99\n",
      "iteration no 8977: Loss: 0.24899483920244495, accuracy: 0.99\n",
      "iteration no 8978: Loss: 0.248990600953167, accuracy: 0.99\n",
      "iteration no 8979: Loss: 0.2489959595493777, accuracy: 0.99\n",
      "iteration no 8980: Loss: 0.24899074735474844, accuracy: 0.99\n",
      "iteration no 8981: Loss: 0.24899358688689655, accuracy: 0.99\n",
      "iteration no 8982: Loss: 0.2489923843426385, accuracy: 0.99\n",
      "iteration no 8983: Loss: 0.24899132425888806, accuracy: 0.99\n",
      "iteration no 8984: Loss: 0.24899344863004344, accuracy: 0.99\n",
      "iteration no 8985: Loss: 0.2489911337662605, accuracy: 0.99\n",
      "iteration no 8986: Loss: 0.24899311747222017, accuracy: 0.99\n",
      "iteration no 8987: Loss: 0.24899294718711407, accuracy: 0.99\n",
      "iteration no 8988: Loss: 0.2489901437541599, accuracy: 0.99\n",
      "iteration no 8989: Loss: 0.2489943653485847, accuracy: 0.99\n",
      "iteration no 8990: Loss: 0.2489899521539043, accuracy: 0.99\n",
      "iteration no 8991: Loss: 0.24899390586883194, accuracy: 0.99\n",
      "iteration no 8992: Loss: 0.2489911859775064, accuracy: 0.99\n",
      "iteration no 8993: Loss: 0.24899084076873573, accuracy: 0.99\n",
      "iteration no 8994: Loss: 0.24899253216131817, accuracy: 0.99\n",
      "iteration no 8995: Loss: 0.2489898873087958, accuracy: 0.99\n",
      "iteration no 8996: Loss: 0.24899274834683666, accuracy: 0.99\n",
      "iteration no 8997: Loss: 0.2489932869262898, accuracy: 0.99\n",
      "iteration no 8998: Loss: 0.24898958966240153, accuracy: 0.99\n",
      "iteration no 8999: Loss: 0.24899344265324408, accuracy: 0.99\n",
      "iteration no 9000: Loss: 0.24898977847947973, accuracy: 0.99\n",
      "iteration no 9001: Loss: 0.24899174693648973, accuracy: 0.99\n",
      "iteration no 9002: Loss: 0.2489911732027465, accuracy: 0.99\n",
      "iteration no 9003: Loss: 0.24899005250056172, accuracy: 0.99\n",
      "iteration no 9004: Loss: 0.24899136098266372, accuracy: 0.99\n",
      "iteration no 9005: Loss: 0.24899008573186224, accuracy: 0.99\n",
      "iteration no 9006: Loss: 0.2489910895460095, accuracy: 0.99\n",
      "iteration no 9007: Loss: 0.24899241248785486, accuracy: 0.99\n",
      "iteration no 9008: Loss: 0.24898925152217016, accuracy: 0.99\n",
      "iteration no 9009: Loss: 0.2489920260465489, accuracy: 0.99\n",
      "iteration no 9010: Loss: 0.24898863890743722, accuracy: 0.99\n",
      "iteration no 9011: Loss: 0.24899119093439204, accuracy: 0.99\n",
      "iteration no 9012: Loss: 0.24899012251225652, accuracy: 0.99\n",
      "iteration no 9013: Loss: 0.2489896349892905, accuracy: 0.99\n",
      "iteration no 9014: Loss: 0.2489903118132521, accuracy: 0.99\n",
      "iteration no 9015: Loss: 0.2489889298746253, accuracy: 0.99\n",
      "iteration no 9016: Loss: 0.24899052714107628, accuracy: 0.99\n",
      "iteration no 9017: Loss: 0.2489911720106413, accuracy: 0.99\n",
      "iteration no 9018: Loss: 0.24898877786854456, accuracy: 0.99\n",
      "iteration no 9019: Loss: 0.2489914136527209, accuracy: 0.99\n",
      "iteration no 9020: Loss: 0.24898755188900387, accuracy: 0.99\n",
      "iteration no 9021: Loss: 0.2489911594553858, accuracy: 0.99\n",
      "iteration no 9022: Loss: 0.24898917742548127, accuracy: 0.99\n",
      "iteration no 9023: Loss: 0.24898883342812342, accuracy: 0.99\n",
      "iteration no 9024: Loss: 0.24898952880543343, accuracy: 0.99\n",
      "iteration no 9025: Loss: 0.2489880498056934, accuracy: 0.99\n",
      "iteration no 9026: Loss: 0.24899003791173652, accuracy: 0.99\n",
      "iteration no 9027: Loss: 0.24899028130542725, accuracy: 0.99\n",
      "iteration no 9028: Loss: 0.24898791082580984, accuracy: 0.99\n",
      "iteration no 9029: Loss: 0.2489905618155168, accuracy: 0.99\n",
      "iteration no 9030: Loss: 0.24898795426464926, accuracy: 0.99\n",
      "iteration no 9031: Loss: 0.2489901878933942, accuracy: 0.99\n",
      "iteration no 9032: Loss: 0.24898788537369868, accuracy: 0.99\n",
      "iteration no 9033: Loss: 0.2489894640980046, accuracy: 0.99\n",
      "iteration no 9034: Loss: 0.2489874843840804, accuracy: 0.99\n",
      "iteration no 9035: Loss: 0.24898865363048192, accuracy: 0.99\n",
      "iteration no 9036: Loss: 0.24898866928882568, accuracy: 0.99\n",
      "iteration no 9037: Loss: 0.2489879351922512, accuracy: 0.99\n",
      "iteration no 9038: Loss: 0.24898856040389447, accuracy: 0.99\n",
      "iteration no 9039: Loss: 0.2489882676755309, accuracy: 0.99\n",
      "iteration no 9040: Loss: 0.24898774692038989, accuracy: 0.99\n",
      "iteration no 9041: Loss: 0.24898904872430472, accuracy: 0.99\n",
      "iteration no 9042: Loss: 0.24898695612258484, accuracy: 0.99\n",
      "iteration no 9043: Loss: 0.24898958944766486, accuracy: 0.99\n",
      "iteration no 9044: Loss: 0.24898691451973426, accuracy: 0.99\n",
      "iteration no 9045: Loss: 0.2489876712752036, accuracy: 0.99\n",
      "iteration no 9046: Loss: 0.24898742966215384, accuracy: 0.99\n",
      "iteration no 9047: Loss: 0.24898713072914555, accuracy: 0.99\n",
      "iteration no 9048: Loss: 0.24898819425393748, accuracy: 0.99\n",
      "iteration no 9049: Loss: 0.2489875682423281, accuracy: 0.99\n",
      "iteration no 9050: Loss: 0.24898725751265843, accuracy: 0.99\n",
      "iteration no 9051: Loss: 0.2489879845649881, accuracy: 0.99\n",
      "iteration no 9052: Loss: 0.24898611486828792, accuracy: 0.99\n",
      "iteration no 9053: Loss: 0.24898981398384504, accuracy: 0.99\n",
      "iteration no 9054: Loss: 0.24898658311182936, accuracy: 0.99\n",
      "iteration no 9055: Loss: 0.24898675503770415, accuracy: 0.99\n",
      "iteration no 9056: Loss: 0.2489875168726526, accuracy: 0.99\n",
      "iteration no 9057: Loss: 0.2489848815035054, accuracy: 0.99\n",
      "iteration no 9058: Loss: 0.24898876056355823, accuracy: 0.99\n",
      "iteration no 9059: Loss: 0.24898652701795665, accuracy: 0.99\n",
      "iteration no 9060: Loss: 0.24898629149855278, accuracy: 0.99\n",
      "iteration no 9061: Loss: 0.2489880751596497, accuracy: 0.99\n",
      "iteration no 9062: Loss: 0.2489839747030973, accuracy: 0.99\n",
      "iteration no 9063: Loss: 0.24898873057997423, accuracy: 0.99\n",
      "iteration no 9064: Loss: 0.2489855518705064, accuracy: 0.99\n",
      "iteration no 9065: Loss: 0.2489866159652907, accuracy: 0.99\n",
      "iteration no 9066: Loss: 0.2489865038090558, accuracy: 0.99\n",
      "iteration no 9067: Loss: 0.24898380483076835, accuracy: 0.99\n",
      "iteration no 9068: Loss: 0.24898778831443788, accuracy: 0.99\n",
      "iteration no 9069: Loss: 0.2489857549618245, accuracy: 0.99\n",
      "iteration no 9070: Loss: 0.2489859089526625, accuracy: 0.99\n",
      "iteration no 9071: Loss: 0.24898726665210869, accuracy: 0.99\n",
      "iteration no 9072: Loss: 0.24898329579737183, accuracy: 0.99\n",
      "iteration no 9073: Loss: 0.2489878474905477, accuracy: 0.99\n",
      "iteration no 9074: Loss: 0.24898483894509563, accuracy: 0.99\n",
      "iteration no 9075: Loss: 0.2489858974186122, accuracy: 0.99\n",
      "iteration no 9076: Loss: 0.24898582435982586, accuracy: 0.99\n",
      "iteration no 9077: Loss: 0.24898326868652282, accuracy: 0.99\n",
      "iteration no 9078: Loss: 0.24898688984584594, accuracy: 0.99\n",
      "iteration no 9079: Loss: 0.24898490556267905, accuracy: 0.99\n",
      "iteration no 9080: Loss: 0.24898541397911847, accuracy: 0.99\n",
      "iteration no 9081: Loss: 0.24898732645660204, accuracy: 0.99\n",
      "iteration no 9082: Loss: 0.24898325122627796, accuracy: 0.99\n",
      "iteration no 9083: Loss: 0.24898639170811526, accuracy: 0.99\n",
      "iteration no 9084: Loss: 0.24898446894205672, accuracy: 0.99\n",
      "iteration no 9085: Loss: 0.24898467544722114, accuracy: 0.99\n",
      "iteration no 9086: Loss: 0.24898544108324844, accuracy: 0.99\n",
      "iteration no 9087: Loss: 0.24898344066187716, accuracy: 0.99\n",
      "iteration no 9088: Loss: 0.24898537729809905, accuracy: 0.99\n",
      "iteration no 9089: Loss: 0.2489840232010423, accuracy: 0.99\n",
      "iteration no 9090: Loss: 0.2489842638845205, accuracy: 0.99\n",
      "iteration no 9091: Loss: 0.24898589808107552, accuracy: 0.99\n",
      "iteration no 9092: Loss: 0.24898280107503518, accuracy: 0.99\n",
      "iteration no 9093: Loss: 0.24898553747781482, accuracy: 0.99\n",
      "iteration no 9094: Loss: 0.24898322092002662, accuracy: 0.99\n",
      "iteration no 9095: Loss: 0.24898399049680425, accuracy: 0.99\n",
      "iteration no 9096: Loss: 0.2489849985590187, accuracy: 0.99\n",
      "iteration no 9097: Loss: 0.24898414623896636, accuracy: 0.99\n",
      "iteration no 9098: Loss: 0.24898390321735686, accuracy: 0.99\n",
      "iteration no 9099: Loss: 0.24898306465890216, accuracy: 0.99\n",
      "iteration no 9100: Loss: 0.24898441803929897, accuracy: 0.99\n",
      "iteration no 9101: Loss: 0.2489835315880184, accuracy: 0.99\n",
      "iteration no 9102: Loss: 0.24898368622205957, accuracy: 0.99\n",
      "iteration no 9103: Loss: 0.24898368229943604, accuracy: 0.99\n",
      "iteration no 9104: Loss: 0.2489826560048393, accuracy: 0.99\n",
      "iteration no 9105: Loss: 0.24898458172899474, accuracy: 0.99\n",
      "iteration no 9106: Loss: 0.24898269561041075, accuracy: 0.99\n",
      "iteration no 9107: Loss: 0.24898370832992, accuracy: 0.99\n",
      "iteration no 9108: Loss: 0.24898269787368627, accuracy: 0.99\n",
      "iteration no 9109: Loss: 0.24898273614575067, accuracy: 0.99\n",
      "iteration no 9110: Loss: 0.24898383131550683, accuracy: 0.99\n",
      "iteration no 9111: Loss: 0.24898266943065228, accuracy: 0.99\n",
      "iteration no 9112: Loss: 0.24898302164331773, accuracy: 0.99\n",
      "iteration no 9113: Loss: 0.24898330538371066, accuracy: 0.99\n",
      "iteration no 9114: Loss: 0.2489823195717168, accuracy: 0.99\n",
      "iteration no 9115: Loss: 0.24898380127909864, accuracy: 0.99\n",
      "iteration no 9116: Loss: 0.24898191369173495, accuracy: 0.99\n",
      "iteration no 9117: Loss: 0.24898279737241774, accuracy: 0.99\n",
      "iteration no 9118: Loss: 0.24898217910656112, accuracy: 0.99\n",
      "iteration no 9119: Loss: 0.24898186101243952, accuracy: 0.99\n",
      "iteration no 9120: Loss: 0.24898454937421927, accuracy: 0.99\n",
      "iteration no 9121: Loss: 0.24898197195558794, accuracy: 0.99\n",
      "iteration no 9122: Loss: 0.24898234216218457, accuracy: 0.99\n",
      "iteration no 9123: Loss: 0.24898378480713718, accuracy: 0.99\n",
      "iteration no 9124: Loss: 0.24898001781554707, accuracy: 0.99\n",
      "iteration no 9125: Loss: 0.2489840280378875, accuracy: 0.99\n",
      "iteration no 9126: Loss: 0.24898073749355087, accuracy: 0.99\n",
      "iteration no 9127: Loss: 0.24898214305098715, accuracy: 0.99\n",
      "iteration no 9128: Loss: 0.24898233870442343, accuracy: 0.99\n",
      "iteration no 9129: Loss: 0.2489798187892262, accuracy: 0.99\n",
      "iteration no 9130: Loss: 0.24898350010840892, accuracy: 0.99\n",
      "iteration no 9131: Loss: 0.24898069116498023, accuracy: 0.99\n",
      "iteration no 9132: Loss: 0.24898186752775187, accuracy: 0.99\n",
      "iteration no 9133: Loss: 0.2489825506688662, accuracy: 0.99\n",
      "iteration no 9134: Loss: 0.2489798018923975, accuracy: 0.99\n",
      "iteration no 9135: Loss: 0.24898344756912733, accuracy: 0.99\n",
      "iteration no 9136: Loss: 0.24898025739738663, accuracy: 0.99\n",
      "iteration no 9137: Loss: 0.24898159933778038, accuracy: 0.99\n",
      "iteration no 9138: Loss: 0.24898153427595343, accuracy: 0.99\n",
      "iteration no 9139: Loss: 0.24897905807852927, accuracy: 0.99\n",
      "iteration no 9140: Loss: 0.24898301600734365, accuracy: 0.99\n",
      "iteration no 9141: Loss: 0.24898004577089577, accuracy: 0.99\n",
      "iteration no 9142: Loss: 0.2489812262122248, accuracy: 0.99\n",
      "iteration no 9143: Loss: 0.24898295625086214, accuracy: 0.99\n",
      "iteration no 9144: Loss: 0.2489789709958594, accuracy: 0.99\n",
      "iteration no 9145: Loss: 0.24898235415781114, accuracy: 0.99\n",
      "iteration no 9146: Loss: 0.24898048464327266, accuracy: 0.99\n",
      "iteration no 9147: Loss: 0.248979380192844, accuracy: 0.99\n",
      "iteration no 9148: Loss: 0.24898156186320503, accuracy: 0.99\n",
      "iteration no 9149: Loss: 0.24897846015869157, accuracy: 0.99\n",
      "iteration no 9150: Loss: 0.24898241584118713, accuracy: 0.99\n",
      "iteration no 9151: Loss: 0.24898025024522277, accuracy: 0.99\n",
      "iteration no 9152: Loss: 0.2489796157274094, accuracy: 0.99\n",
      "iteration no 9153: Loss: 0.24898163102806165, accuracy: 0.99\n",
      "iteration no 9154: Loss: 0.2489785706489798, accuracy: 0.99\n",
      "iteration no 9155: Loss: 0.24898205894309605, accuracy: 0.99\n",
      "iteration no 9156: Loss: 0.24897963601607143, accuracy: 0.99\n",
      "iteration no 9157: Loss: 0.248979421542111, accuracy: 0.99\n",
      "iteration no 9158: Loss: 0.24898057572684462, accuracy: 0.99\n",
      "iteration no 9159: Loss: 0.24897760059103868, accuracy: 0.99\n",
      "iteration no 9160: Loss: 0.24898160701949323, accuracy: 0.99\n",
      "iteration no 9161: Loss: 0.24897855930119578, accuracy: 0.99\n",
      "iteration no 9162: Loss: 0.24897957113134617, accuracy: 0.99\n",
      "iteration no 9163: Loss: 0.24898070246269133, accuracy: 0.99\n",
      "iteration no 9164: Loss: 0.24897805704627174, accuracy: 0.99\n",
      "iteration no 9165: Loss: 0.24898129104137368, accuracy: 0.99\n",
      "iteration no 9166: Loss: 0.24897943672555053, accuracy: 0.99\n",
      "iteration no 9167: Loss: 0.2489793649542899, accuracy: 0.99\n",
      "iteration no 9168: Loss: 0.2489792286456199, accuracy: 0.99\n",
      "iteration no 9169: Loss: 0.24897788456018163, accuracy: 0.99\n",
      "iteration no 9170: Loss: 0.24897924890159318, accuracy: 0.99\n",
      "iteration no 9171: Loss: 0.2489788216372479, accuracy: 0.99\n",
      "iteration no 9172: Loss: 0.2489793404335704, accuracy: 0.99\n",
      "iteration no 9173: Loss: 0.24897869118116994, accuracy: 0.99\n",
      "iteration no 9174: Loss: 0.2489781038288878, accuracy: 0.99\n",
      "iteration no 9175: Loss: 0.2489784617503, accuracy: 0.99\n",
      "iteration no 9176: Loss: 0.24897884379932544, accuracy: 0.99\n",
      "iteration no 9177: Loss: 0.2489782770949542, accuracy: 0.99\n",
      "iteration no 9178: Loss: 0.2489787722165968, accuracy: 0.99\n",
      "iteration no 9179: Loss: 0.24897745070271704, accuracy: 0.99\n",
      "iteration no 9180: Loss: 0.2489779703943381, accuracy: 0.99\n",
      "iteration no 9181: Loss: 0.24897825199499826, accuracy: 0.99\n",
      "iteration no 9182: Loss: 0.24897899690720154, accuracy: 0.99\n",
      "iteration no 9183: Loss: 0.24897792353686904, accuracy: 0.99\n",
      "iteration no 9184: Loss: 0.24897762011910995, accuracy: 0.99\n",
      "iteration no 9185: Loss: 0.24897766171419047, accuracy: 0.99\n",
      "iteration no 9186: Loss: 0.24897767155062472, accuracy: 0.99\n",
      "iteration no 9187: Loss: 0.24897827012849638, accuracy: 0.99\n",
      "iteration no 9188: Loss: 0.24897788811848057, accuracy: 0.99\n",
      "iteration no 9189: Loss: 0.24897836121930728, accuracy: 0.99\n",
      "iteration no 9190: Loss: 0.24897812336330175, accuracy: 0.99\n",
      "iteration no 9191: Loss: 0.24897572289432413, accuracy: 0.99\n",
      "iteration no 9192: Loss: 0.24897957666545062, accuracy: 0.99\n",
      "iteration no 9193: Loss: 0.24897609893148048, accuracy: 0.99\n",
      "iteration no 9194: Loss: 0.2489782786531598, accuracy: 0.99\n",
      "iteration no 9195: Loss: 0.24897796985751558, accuracy: 0.99\n",
      "iteration no 9196: Loss: 0.24897618695384954, accuracy: 0.99\n",
      "iteration no 9197: Loss: 0.24897865327690444, accuracy: 0.99\n",
      "iteration no 9198: Loss: 0.2489762540363778, accuracy: 0.99\n",
      "iteration no 9199: Loss: 0.2489773565354683, accuracy: 0.99\n",
      "iteration no 9200: Loss: 0.24897715314475022, accuracy: 0.99\n",
      "iteration no 9201: Loss: 0.24897510142103063, accuracy: 0.99\n",
      "iteration no 9202: Loss: 0.2489786678047856, accuracy: 0.99\n",
      "iteration no 9203: Loss: 0.24897569074824955, accuracy: 0.99\n",
      "iteration no 9204: Loss: 0.24897796461253088, accuracy: 0.99\n",
      "iteration no 9205: Loss: 0.2489783405552873, accuracy: 0.99\n",
      "iteration no 9206: Loss: 0.24897489721781152, accuracy: 0.99\n",
      "iteration no 9207: Loss: 0.24897802417135606, accuracy: 0.99\n",
      "iteration no 9208: Loss: 0.2489765298838374, accuracy: 0.99\n",
      "iteration no 9209: Loss: 0.24897515522944297, accuracy: 0.99\n",
      "iteration no 9210: Loss: 0.24897754258377824, accuracy: 0.99\n",
      "iteration no 9211: Loss: 0.2489734019996746, accuracy: 0.99\n",
      "iteration no 9212: Loss: 0.2489785796646165, accuracy: 0.99\n",
      "iteration no 9213: Loss: 0.24897593530308404, accuracy: 0.99\n",
      "iteration no 9214: Loss: 0.24897604591576528, accuracy: 0.99\n",
      "iteration no 9215: Loss: 0.24897722768883615, accuracy: 0.99\n",
      "iteration no 9216: Loss: 0.24897390930204957, accuracy: 0.99\n",
      "iteration no 9217: Loss: 0.24897744884381362, accuracy: 0.99\n",
      "iteration no 9218: Loss: 0.2489756155103653, accuracy: 0.99\n",
      "iteration no 9219: Loss: 0.24897457662955896, accuracy: 0.99\n",
      "iteration no 9220: Loss: 0.24897693155103873, accuracy: 0.99\n",
      "iteration no 9221: Loss: 0.24897274468157893, accuracy: 0.99\n",
      "iteration no 9222: Loss: 0.2489779344105152, accuracy: 0.99\n",
      "iteration no 9223: Loss: 0.24897488483563343, accuracy: 0.99\n",
      "iteration no 9224: Loss: 0.24897514035647092, accuracy: 0.99\n",
      "iteration no 9225: Loss: 0.2489766012543531, accuracy: 0.99\n",
      "iteration no 9226: Loss: 0.24897349691402415, accuracy: 0.99\n",
      "iteration no 9227: Loss: 0.24897646840936247, accuracy: 0.99\n",
      "iteration no 9228: Loss: 0.2489759863764159, accuracy: 0.99\n",
      "iteration no 9229: Loss: 0.2489740110997648, accuracy: 0.99\n",
      "iteration no 9230: Loss: 0.24897617272894845, accuracy: 0.99\n",
      "iteration no 9231: Loss: 0.248973313001578, accuracy: 0.99\n",
      "iteration no 9232: Loss: 0.2489756345157141, accuracy: 0.99\n",
      "iteration no 9233: Loss: 0.24897488159386327, accuracy: 0.99\n",
      "iteration no 9234: Loss: 0.248974617840171, accuracy: 0.99\n",
      "iteration no 9235: Loss: 0.248975217778261, accuracy: 0.99\n",
      "iteration no 9236: Loss: 0.2489735548132651, accuracy: 0.99\n",
      "iteration no 9237: Loss: 0.2489746188543009, accuracy: 0.99\n",
      "iteration no 9238: Loss: 0.24897492561657086, accuracy: 0.99\n",
      "iteration no 9239: Loss: 0.2489730479948335, accuracy: 0.99\n",
      "iteration no 9240: Loss: 0.2489757674147054, accuracy: 0.99\n",
      "iteration no 9241: Loss: 0.24897225640507734, accuracy: 0.99\n",
      "iteration no 9242: Loss: 0.24897530893164, accuracy: 0.99\n",
      "iteration no 9243: Loss: 0.24897425848665802, accuracy: 0.99\n",
      "iteration no 9244: Loss: 0.24897387739073074, accuracy: 0.99\n",
      "iteration no 9245: Loss: 0.24897485841863767, accuracy: 0.99\n",
      "iteration no 9246: Loss: 0.24897297034739133, accuracy: 0.99\n",
      "iteration no 9247: Loss: 0.24897399554890487, accuracy: 0.99\n",
      "iteration no 9248: Loss: 0.2489742968830024, accuracy: 0.99\n",
      "iteration no 9249: Loss: 0.24897224200991475, accuracy: 0.99\n",
      "iteration no 9250: Loss: 0.24897493658379266, accuracy: 0.99\n",
      "iteration no 9251: Loss: 0.24897153064783334, accuracy: 0.99\n",
      "iteration no 9252: Loss: 0.24897465070719196, accuracy: 0.99\n",
      "iteration no 9253: Loss: 0.2489736880527274, accuracy: 0.99\n",
      "iteration no 9254: Loss: 0.24897321419871082, accuracy: 0.99\n",
      "iteration no 9255: Loss: 0.2489736307814413, accuracy: 0.99\n",
      "iteration no 9256: Loss: 0.24897338477560027, accuracy: 0.99\n",
      "iteration no 9257: Loss: 0.24897378924239083, accuracy: 0.99\n",
      "iteration no 9258: Loss: 0.24897302256377318, accuracy: 0.99\n",
      "iteration no 9259: Loss: 0.2489726336580434, accuracy: 0.99\n",
      "iteration no 9260: Loss: 0.2489726424964746, accuracy: 0.99\n",
      "iteration no 9261: Loss: 0.2489720860432612, accuracy: 0.99\n",
      "iteration no 9262: Loss: 0.24897411339045838, accuracy: 0.99\n",
      "iteration no 9263: Loss: 0.24897158446587875, accuracy: 0.99\n",
      "iteration no 9264: Loss: 0.248973846035828, accuracy: 0.99\n",
      "iteration no 9265: Loss: 0.24897134343605387, accuracy: 0.99\n",
      "iteration no 9266: Loss: 0.248972859949406, accuracy: 0.99\n",
      "iteration no 9267: Loss: 0.24897249923354753, accuracy: 0.99\n",
      "iteration no 9268: Loss: 0.24897260527220483, accuracy: 0.99\n",
      "iteration no 9269: Loss: 0.24897228350169504, accuracy: 0.99\n",
      "iteration no 9270: Loss: 0.24897187607405333, accuracy: 0.99\n",
      "iteration no 9271: Loss: 0.24897143779072256, accuracy: 0.99\n",
      "iteration no 9272: Loss: 0.24897324096490892, accuracy: 0.99\n",
      "iteration no 9273: Loss: 0.24897072499407796, accuracy: 0.99\n",
      "iteration no 9274: Loss: 0.2489732775472793, accuracy: 0.99\n",
      "iteration no 9275: Loss: 0.24897065404421978, accuracy: 0.99\n",
      "iteration no 9276: Loss: 0.24897241591644817, accuracy: 0.99\n",
      "iteration no 9277: Loss: 0.24897170836216548, accuracy: 0.99\n",
      "iteration no 9278: Loss: 0.2489720310731231, accuracy: 0.99\n",
      "iteration no 9279: Loss: 0.2489733740672388, accuracy: 0.99\n",
      "iteration no 9280: Loss: 0.24897200110794213, accuracy: 0.99\n",
      "iteration no 9281: Loss: 0.24896960936731366, accuracy: 0.99\n",
      "iteration no 9282: Loss: 0.24897333913818367, accuracy: 0.99\n",
      "iteration no 9283: Loss: 0.2489686393796111, accuracy: 0.99\n",
      "iteration no 9284: Loss: 0.2489740154066031, accuracy: 0.99\n",
      "iteration no 9285: Loss: 0.24897038441701141, accuracy: 0.99\n",
      "iteration no 9286: Loss: 0.24897107185735778, accuracy: 0.99\n",
      "iteration no 9287: Loss: 0.24897204568134146, accuracy: 0.99\n",
      "iteration no 9288: Loss: 0.24897003270408424, accuracy: 0.99\n",
      "iteration no 9289: Loss: 0.2489724132644276, accuracy: 0.99\n",
      "iteration no 9290: Loss: 0.24897115202894002, accuracy: 0.99\n",
      "iteration no 9291: Loss: 0.24896889748880202, accuracy: 0.99\n",
      "iteration no 9292: Loss: 0.24897254948909192, accuracy: 0.99\n",
      "iteration no 9293: Loss: 0.24896791999422518, accuracy: 0.99\n",
      "iteration no 9294: Loss: 0.2489737174711193, accuracy: 0.99\n",
      "iteration no 9295: Loss: 0.24897111252540305, accuracy: 0.99\n",
      "iteration no 9296: Loss: 0.24897005396218588, accuracy: 0.99\n",
      "iteration no 9297: Loss: 0.248971125155018, accuracy: 0.99\n",
      "iteration no 9298: Loss: 0.24896990745394398, accuracy: 0.99\n",
      "iteration no 9299: Loss: 0.24897026731890093, accuracy: 0.99\n",
      "iteration no 9300: Loss: 0.2489716232211468, accuracy: 0.99\n",
      "iteration no 9301: Loss: 0.24896781614809793, accuracy: 0.99\n",
      "iteration no 9302: Loss: 0.24897207938166388, accuracy: 0.99\n",
      "iteration no 9303: Loss: 0.2489678657927914, accuracy: 0.99\n",
      "iteration no 9304: Loss: 0.24897156379502988, accuracy: 0.99\n",
      "iteration no 9305: Loss: 0.2489701754169708, accuracy: 0.99\n",
      "iteration no 9306: Loss: 0.24896961199340917, accuracy: 0.99\n",
      "iteration no 9307: Loss: 0.2489705452997248, accuracy: 0.99\n",
      "iteration no 9308: Loss: 0.2489688485773884, accuracy: 0.99\n",
      "iteration no 9309: Loss: 0.2489693472338419, accuracy: 0.99\n",
      "iteration no 9310: Loss: 0.24897106031580896, accuracy: 0.99\n",
      "iteration no 9311: Loss: 0.24896739782189553, accuracy: 0.99\n",
      "iteration no 9312: Loss: 0.24897148184377071, accuracy: 0.99\n",
      "iteration no 9313: Loss: 0.24896713894637607, accuracy: 0.99\n",
      "iteration no 9314: Loss: 0.2489704810986565, accuracy: 0.99\n",
      "iteration no 9315: Loss: 0.2489698699634223, accuracy: 0.99\n",
      "iteration no 9316: Loss: 0.2489694248627346, accuracy: 0.99\n",
      "iteration no 9317: Loss: 0.24896973831755942, accuracy: 0.99\n",
      "iteration no 9318: Loss: 0.24896828269280213, accuracy: 0.99\n",
      "iteration no 9319: Loss: 0.24896825196416472, accuracy: 0.99\n",
      "iteration no 9320: Loss: 0.24897062499659076, accuracy: 0.99\n",
      "iteration no 9321: Loss: 0.2489670577043263, accuracy: 0.99\n",
      "iteration no 9322: Loss: 0.24897087446374677, accuracy: 0.99\n",
      "iteration no 9323: Loss: 0.24896771369310688, accuracy: 0.99\n",
      "iteration no 9324: Loss: 0.24897030499240205, accuracy: 0.99\n",
      "iteration no 9325: Loss: 0.2489681702318472, accuracy: 0.99\n",
      "iteration no 9326: Loss: 0.24896949987457684, accuracy: 0.99\n",
      "iteration no 9327: Loss: 0.24896758603444236, accuracy: 0.99\n",
      "iteration no 9328: Loss: 0.24896877333492418, accuracy: 0.99\n",
      "iteration no 9329: Loss: 0.24896786250674163, accuracy: 0.99\n",
      "iteration no 9330: Loss: 0.24896908957736846, accuracy: 0.99\n",
      "iteration no 9331: Loss: 0.24896746517057378, accuracy: 0.99\n",
      "iteration no 9332: Loss: 0.24896886544502794, accuracy: 0.99\n",
      "iteration no 9333: Loss: 0.24896653015607018, accuracy: 0.99\n",
      "iteration no 9334: Loss: 0.24896953907375824, accuracy: 0.99\n",
      "iteration no 9335: Loss: 0.2489672233124745, accuracy: 0.99\n",
      "iteration no 9336: Loss: 0.2489688070087765, accuracy: 0.99\n",
      "iteration no 9337: Loss: 0.24896691623668177, accuracy: 0.99\n",
      "iteration no 9338: Loss: 0.24896796870721383, accuracy: 0.99\n",
      "iteration no 9339: Loss: 0.24896727216490594, accuracy: 0.99\n",
      "iteration no 9340: Loss: 0.2489684188143751, accuracy: 0.99\n",
      "iteration no 9341: Loss: 0.2489664635433776, accuracy: 0.99\n",
      "iteration no 9342: Loss: 0.2489683312453276, accuracy: 0.99\n",
      "iteration no 9343: Loss: 0.24896621109468148, accuracy: 0.99\n",
      "iteration no 9344: Loss: 0.24896895242423195, accuracy: 0.99\n",
      "iteration no 9345: Loss: 0.24896631460724417, accuracy: 0.99\n",
      "iteration no 9346: Loss: 0.24896930760713443, accuracy: 0.99\n",
      "iteration no 9347: Loss: 0.24896723866598558, accuracy: 0.99\n",
      "iteration no 9348: Loss: 0.24896677367058767, accuracy: 0.99\n",
      "iteration no 9349: Loss: 0.24896794846172057, accuracy: 0.99\n",
      "iteration no 9350: Loss: 0.24896659751564792, accuracy: 0.99\n",
      "iteration no 9351: Loss: 0.24896638418132105, accuracy: 0.99\n",
      "iteration no 9352: Loss: 0.24896832028596816, accuracy: 0.99\n",
      "iteration no 9353: Loss: 0.24896445295899522, accuracy: 0.99\n",
      "iteration no 9354: Loss: 0.24896953103860558, accuracy: 0.99\n",
      "iteration no 9355: Loss: 0.24896441490387433, accuracy: 0.99\n",
      "iteration no 9356: Loss: 0.24896826704244618, accuracy: 0.99\n",
      "iteration no 9357: Loss: 0.2489667824058257, accuracy: 0.99\n",
      "iteration no 9358: Loss: 0.24896643285792436, accuracy: 0.99\n",
      "iteration no 9359: Loss: 0.24896719672306244, accuracy: 0.99\n",
      "iteration no 9360: Loss: 0.24896584858624637, accuracy: 0.99\n",
      "iteration no 9361: Loss: 0.24896586725120204, accuracy: 0.99\n",
      "iteration no 9362: Loss: 0.2489687084629543, accuracy: 0.99\n",
      "iteration no 9363: Loss: 0.24896352538741434, accuracy: 0.99\n",
      "iteration no 9364: Loss: 0.24896896947500072, accuracy: 0.99\n",
      "iteration no 9365: Loss: 0.2489651064769572, accuracy: 0.99\n",
      "iteration no 9366: Loss: 0.24896652695489013, accuracy: 0.99\n",
      "iteration no 9367: Loss: 0.24896671505704182, accuracy: 0.99\n",
      "iteration no 9368: Loss: 0.24896512662577272, accuracy: 0.99\n",
      "iteration no 9369: Loss: 0.24896631384988394, accuracy: 0.99\n",
      "iteration no 9370: Loss: 0.24896650151836164, accuracy: 0.99\n",
      "iteration no 9371: Loss: 0.24896386592926656, accuracy: 0.99\n",
      "iteration no 9372: Loss: 0.24896769027563237, accuracy: 0.99\n",
      "iteration no 9373: Loss: 0.24896235883765036, accuracy: 0.99\n",
      "iteration no 9374: Loss: 0.2489686325283103, accuracy: 0.99\n",
      "iteration no 9375: Loss: 0.24896448232532647, accuracy: 0.99\n",
      "iteration no 9376: Loss: 0.24896604225528818, accuracy: 0.99\n",
      "iteration no 9377: Loss: 0.24896591667551357, accuracy: 0.99\n",
      "iteration no 9378: Loss: 0.2489642292604884, accuracy: 0.99\n",
      "iteration no 9379: Loss: 0.24896603143214413, accuracy: 0.99\n",
      "iteration no 9380: Loss: 0.24896575335059923, accuracy: 0.99\n",
      "iteration no 9381: Loss: 0.24896344731181358, accuracy: 0.99\n",
      "iteration no 9382: Loss: 0.24896708983699412, accuracy: 0.99\n",
      "iteration no 9383: Loss: 0.24896128948296328, accuracy: 0.99\n",
      "iteration no 9384: Loss: 0.24896825350423485, accuracy: 0.99\n",
      "iteration no 9385: Loss: 0.2489651112980873, accuracy: 0.99\n",
      "iteration no 9386: Loss: 0.2489655052342118, accuracy: 0.99\n",
      "iteration no 9387: Loss: 0.24896420286847987, accuracy: 0.99\n",
      "iteration no 9388: Loss: 0.24896477061689118, accuracy: 0.99\n",
      "iteration no 9389: Loss: 0.24896398957504817, accuracy: 0.99\n",
      "iteration no 9390: Loss: 0.24896614683925672, accuracy: 0.99\n",
      "iteration no 9391: Loss: 0.24896294648783973, accuracy: 0.99\n",
      "iteration no 9392: Loss: 0.2489661711005441, accuracy: 0.99\n",
      "iteration no 9393: Loss: 0.248961809069078, accuracy: 0.99\n",
      "iteration no 9394: Loss: 0.24896588569569034, accuracy: 0.99\n",
      "iteration no 9395: Loss: 0.24896409828638755, accuracy: 0.99\n",
      "iteration no 9396: Loss: 0.24896480323972356, accuracy: 0.99\n",
      "iteration no 9397: Loss: 0.24896405833785407, accuracy: 0.99\n",
      "iteration no 9398: Loss: 0.24896398710699724, accuracy: 0.99\n",
      "iteration no 9399: Loss: 0.24896321976063307, accuracy: 0.99\n",
      "iteration no 9400: Loss: 0.2489654657749441, accuracy: 0.99\n",
      "iteration no 9401: Loss: 0.24896188168029798, accuracy: 0.99\n",
      "iteration no 9402: Loss: 0.24896597262047016, accuracy: 0.99\n",
      "iteration no 9403: Loss: 0.24896119193393068, accuracy: 0.99\n",
      "iteration no 9404: Loss: 0.2489653725328509, accuracy: 0.99\n",
      "iteration no 9405: Loss: 0.24896316474861835, accuracy: 0.99\n",
      "iteration no 9406: Loss: 0.24896438188338144, accuracy: 0.99\n",
      "iteration no 9407: Loss: 0.24896368976162672, accuracy: 0.99\n",
      "iteration no 9408: Loss: 0.24896323841530688, accuracy: 0.99\n",
      "iteration no 9409: Loss: 0.24896257118607726, accuracy: 0.99\n",
      "iteration no 9410: Loss: 0.24896450057706482, accuracy: 0.99\n",
      "iteration no 9411: Loss: 0.24896126645494746, accuracy: 0.99\n",
      "iteration no 9412: Loss: 0.24896547430063584, accuracy: 0.99\n",
      "iteration no 9413: Loss: 0.2489619389265352, accuracy: 0.99\n",
      "iteration no 9414: Loss: 0.2489654529475717, accuracy: 0.99\n",
      "iteration no 9415: Loss: 0.24896080811292698, accuracy: 0.99\n",
      "iteration no 9416: Loss: 0.24896470571386395, accuracy: 0.99\n",
      "iteration no 9417: Loss: 0.2489618910594601, accuracy: 0.99\n",
      "iteration no 9418: Loss: 0.24896359501214116, accuracy: 0.99\n",
      "iteration no 9419: Loss: 0.24896223368869813, accuracy: 0.99\n",
      "iteration no 9420: Loss: 0.2489630721691682, accuracy: 0.99\n",
      "iteration no 9421: Loss: 0.2489613812372269, accuracy: 0.99\n",
      "iteration no 9422: Loss: 0.2489639761195927, accuracy: 0.99\n",
      "iteration no 9423: Loss: 0.2489608062580629, accuracy: 0.99\n",
      "iteration no 9424: Loss: 0.2489646099947817, accuracy: 0.99\n",
      "iteration no 9425: Loss: 0.24896057591280357, accuracy: 0.99\n",
      "iteration no 9426: Loss: 0.24896353480232494, accuracy: 0.99\n",
      "iteration no 9427: Loss: 0.24896127585535288, accuracy: 0.99\n",
      "iteration no 9428: Loss: 0.24896324453178414, accuracy: 0.99\n",
      "iteration no 9429: Loss: 0.24896289959587484, accuracy: 0.99\n",
      "iteration no 9430: Loss: 0.24896260231280953, accuracy: 0.99\n",
      "iteration no 9431: Loss: 0.2489603661375494, accuracy: 0.99\n",
      "iteration no 9432: Loss: 0.24896425477804035, accuracy: 0.99\n",
      "iteration no 9433: Loss: 0.2489592408384877, accuracy: 0.99\n",
      "iteration no 9434: Loss: 0.2489646827813784, accuracy: 0.99\n",
      "iteration no 9435: Loss: 0.24895972085321802, accuracy: 0.99\n",
      "iteration no 9436: Loss: 0.2489624572663066, accuracy: 0.99\n",
      "iteration no 9437: Loss: 0.24896132080382505, accuracy: 0.99\n",
      "iteration no 9438: Loss: 0.2489606687014127, accuracy: 0.99\n",
      "iteration no 9439: Loss: 0.24896294925122683, accuracy: 0.99\n",
      "iteration no 9440: Loss: 0.24896077378421816, accuracy: 0.99\n",
      "iteration no 9441: Loss: 0.24896162363432567, accuracy: 0.99\n",
      "iteration no 9442: Loss: 0.24896239467072775, accuracy: 0.99\n",
      "iteration no 9443: Loss: 0.2489596718840596, accuracy: 0.99\n",
      "iteration no 9444: Loss: 0.24896449957658234, accuracy: 0.99\n",
      "iteration no 9445: Loss: 0.2489598265433891, accuracy: 0.99\n",
      "iteration no 9446: Loss: 0.2489621552313004, accuracy: 0.99\n",
      "iteration no 9447: Loss: 0.24896174400965887, accuracy: 0.99\n",
      "iteration no 9448: Loss: 0.24895969423385472, accuracy: 0.99\n",
      "iteration no 9449: Loss: 0.24896257685283454, accuracy: 0.99\n",
      "iteration no 9450: Loss: 0.24896040429312727, accuracy: 0.99\n",
      "iteration no 9451: Loss: 0.24896056311370318, accuracy: 0.99\n",
      "iteration no 9452: Loss: 0.24896172990404533, accuracy: 0.99\n",
      "iteration no 9453: Loss: 0.24895915131555726, accuracy: 0.99\n",
      "iteration no 9454: Loss: 0.24896276570938208, accuracy: 0.99\n",
      "iteration no 9455: Loss: 0.2489599825666003, accuracy: 0.99\n",
      "iteration no 9456: Loss: 0.2489603764591813, accuracy: 0.99\n",
      "iteration no 9457: Loss: 0.24896089495512597, accuracy: 0.99\n",
      "iteration no 9458: Loss: 0.24895841078162395, accuracy: 0.99\n",
      "iteration no 9459: Loss: 0.2489625009761814, accuracy: 0.99\n",
      "iteration no 9460: Loss: 0.24895906051868744, accuracy: 0.99\n",
      "iteration no 9461: Loss: 0.24896060499431094, accuracy: 0.99\n",
      "iteration no 9462: Loss: 0.2489608012943813, accuracy: 0.99\n",
      "iteration no 9463: Loss: 0.24895858659100148, accuracy: 0.99\n",
      "iteration no 9464: Loss: 0.24896183656422022, accuracy: 0.99\n",
      "iteration no 9465: Loss: 0.24895943204282103, accuracy: 0.99\n",
      "iteration no 9466: Loss: 0.24895976448948193, accuracy: 0.99\n",
      "iteration no 9467: Loss: 0.2489604700245, accuracy: 0.99\n",
      "iteration no 9468: Loss: 0.24895764775912516, accuracy: 0.99\n",
      "iteration no 9469: Loss: 0.24896180974371723, accuracy: 0.99\n",
      "iteration no 9470: Loss: 0.2489595334892044, accuracy: 0.99\n",
      "iteration no 9471: Loss: 0.24896057777763772, accuracy: 0.99\n",
      "iteration no 9472: Loss: 0.24895933205295828, accuracy: 0.99\n",
      "iteration no 9473: Loss: 0.24895936304177202, accuracy: 0.99\n",
      "iteration no 9474: Loss: 0.24895912770739603, accuracy: 0.99\n",
      "iteration no 9475: Loss: 0.24895982593266108, accuracy: 0.99\n",
      "iteration no 9476: Loss: 0.24895897048350257, accuracy: 0.99\n",
      "iteration no 9477: Loss: 0.24895998757391613, accuracy: 0.99\n",
      "iteration no 9478: Loss: 0.24895774508351215, accuracy: 0.99\n",
      "iteration no 9479: Loss: 0.248959809618219, accuracy: 0.99\n",
      "iteration no 9480: Loss: 0.2489584332571188, accuracy: 0.99\n",
      "iteration no 9481: Loss: 0.24896007772658715, accuracy: 0.99\n",
      "iteration no 9482: Loss: 0.24895888899322866, accuracy: 0.99\n",
      "iteration no 9483: Loss: 0.2489587845939414, accuracy: 0.99\n",
      "iteration no 9484: Loss: 0.24895840747595432, accuracy: 0.99\n",
      "iteration no 9485: Loss: 0.2489591122074655, accuracy: 0.99\n",
      "iteration no 9486: Loss: 0.24895823957414312, accuracy: 0.99\n",
      "iteration no 9487: Loss: 0.2489597482236787, accuracy: 0.99\n",
      "iteration no 9488: Loss: 0.2489571949932667, accuracy: 0.99\n",
      "iteration no 9489: Loss: 0.24895943170093382, accuracy: 0.99\n",
      "iteration no 9490: Loss: 0.2489574405141846, accuracy: 0.99\n",
      "iteration no 9491: Loss: 0.24895958160750048, accuracy: 0.99\n",
      "iteration no 9492: Loss: 0.2489581245952453, accuracy: 0.99\n",
      "iteration no 9493: Loss: 0.24895950082364535, accuracy: 0.99\n",
      "iteration no 9494: Loss: 0.2489583929924012, accuracy: 0.99\n",
      "iteration no 9495: Loss: 0.24895788113304457, accuracy: 0.99\n",
      "iteration no 9496: Loss: 0.24895869520371067, accuracy: 0.99\n",
      "iteration no 9497: Loss: 0.24895824048226267, accuracy: 0.99\n",
      "iteration no 9498: Loss: 0.24895732452209973, accuracy: 0.99\n",
      "iteration no 9499: Loss: 0.24895953939114618, accuracy: 0.99\n",
      "iteration no 9500: Loss: 0.2489558007271831, accuracy: 0.99\n",
      "iteration no 9501: Loss: 0.24895969477997648, accuracy: 0.99\n",
      "iteration no 9502: Loss: 0.24895603781224673, accuracy: 0.99\n",
      "iteration no 9503: Loss: 0.24895885025992903, accuracy: 0.99\n",
      "iteration no 9504: Loss: 0.24895752905306506, accuracy: 0.99\n",
      "iteration no 9505: Loss: 0.24895764282049682, accuracy: 0.99\n",
      "iteration no 9506: Loss: 0.2489572017105003, accuracy: 0.99\n",
      "iteration no 9507: Loss: 0.2489577411214285, accuracy: 0.99\n",
      "iteration no 9508: Loss: 0.2489564238251488, accuracy: 0.99\n",
      "iteration no 9509: Loss: 0.24895887318823054, accuracy: 0.99\n",
      "iteration no 9510: Loss: 0.24895530430096474, accuracy: 0.99\n",
      "iteration no 9511: Loss: 0.24895900903494236, accuracy: 0.99\n",
      "iteration no 9512: Loss: 0.24895531081834452, accuracy: 0.99\n",
      "iteration no 9513: Loss: 0.24895839127639866, accuracy: 0.99\n",
      "iteration no 9514: Loss: 0.2489566945122766, accuracy: 0.99\n",
      "iteration no 9515: Loss: 0.24895760805460626, accuracy: 0.99\n",
      "iteration no 9516: Loss: 0.2489575223769827, accuracy: 0.99\n",
      "iteration no 9517: Loss: 0.24895782118544507, accuracy: 0.99\n",
      "iteration no 9518: Loss: 0.24895475555010937, accuracy: 0.99\n",
      "iteration no 9519: Loss: 0.2489594464558905, accuracy: 0.99\n",
      "iteration no 9520: Loss: 0.24895361425971363, accuracy: 0.99\n",
      "iteration no 9521: Loss: 0.248959371590965, accuracy: 0.99\n",
      "iteration no 9522: Loss: 0.24895539893416388, accuracy: 0.99\n",
      "iteration no 9523: Loss: 0.24895721087790876, accuracy: 0.99\n",
      "iteration no 9524: Loss: 0.2489574848721437, accuracy: 0.99\n",
      "iteration no 9525: Loss: 0.24895606601234788, accuracy: 0.99\n",
      "iteration no 9526: Loss: 0.24895635246529524, accuracy: 0.99\n",
      "iteration no 9527: Loss: 0.24895725009111355, accuracy: 0.99\n",
      "iteration no 9528: Loss: 0.24895368641140436, accuracy: 0.99\n",
      "iteration no 9529: Loss: 0.24895903155261978, accuracy: 0.99\n",
      "iteration no 9530: Loss: 0.24895311538790382, accuracy: 0.99\n",
      "iteration no 9531: Loss: 0.24895878249699577, accuracy: 0.99\n",
      "iteration no 9532: Loss: 0.24895582341519512, accuracy: 0.99\n",
      "iteration no 9533: Loss: 0.24895542195091913, accuracy: 0.99\n",
      "iteration no 9534: Loss: 0.24895642201844242, accuracy: 0.99\n",
      "iteration no 9535: Loss: 0.24895560880265472, accuracy: 0.99\n",
      "iteration no 9536: Loss: 0.24895552702115412, accuracy: 0.99\n",
      "iteration no 9537: Loss: 0.24895710802543314, accuracy: 0.99\n",
      "iteration no 9538: Loss: 0.24895364344374019, accuracy: 0.99\n",
      "iteration no 9539: Loss: 0.24895776260987612, accuracy: 0.99\n",
      "iteration no 9540: Loss: 0.2489539406642367, accuracy: 0.99\n",
      "iteration no 9541: Loss: 0.24895626297602835, accuracy: 0.99\n",
      "iteration no 9542: Loss: 0.24895543935644235, accuracy: 0.99\n",
      "iteration no 9543: Loss: 0.24895387341167713, accuracy: 0.99\n",
      "iteration no 9544: Loss: 0.2489563167716319, accuracy: 0.99\n",
      "iteration no 9545: Loss: 0.2489546297243294, accuracy: 0.99\n",
      "iteration no 9546: Loss: 0.2489555095723291, accuracy: 0.99\n",
      "iteration no 9547: Loss: 0.24895611026108178, accuracy: 0.99\n",
      "iteration no 9548: Loss: 0.2489534119058995, accuracy: 0.99\n",
      "iteration no 9549: Loss: 0.24895626030539572, accuracy: 0.99\n",
      "iteration no 9550: Loss: 0.24895397971254773, accuracy: 0.99\n",
      "iteration no 9551: Loss: 0.2489551606237602, accuracy: 0.99\n",
      "iteration no 9552: Loss: 0.24895547362297768, accuracy: 0.99\n",
      "iteration no 9553: Loss: 0.2489530815559095, accuracy: 0.99\n",
      "iteration no 9554: Loss: 0.24895574916853586, accuracy: 0.99\n",
      "iteration no 9555: Loss: 0.2489551918250143, accuracy: 0.99\n",
      "iteration no 9556: Loss: 0.24895557969858415, accuracy: 0.99\n",
      "iteration no 9557: Loss: 0.24895442777691124, accuracy: 0.99\n",
      "iteration no 9558: Loss: 0.24895430625189235, accuracy: 0.99\n",
      "iteration no 9559: Loss: 0.24895386550261617, accuracy: 0.99\n",
      "iteration no 9560: Loss: 0.24895443438797416, accuracy: 0.99\n",
      "iteration no 9561: Loss: 0.24895440462870141, accuracy: 0.99\n",
      "iteration no 9562: Loss: 0.24895491101360673, accuracy: 0.99\n",
      "iteration no 9563: Loss: 0.24895333204511627, accuracy: 0.99\n",
      "iteration no 9564: Loss: 0.24895435232762986, accuracy: 0.99\n",
      "iteration no 9565: Loss: 0.24895323733564315, accuracy: 0.99\n",
      "iteration no 9566: Loss: 0.24895500268309195, accuracy: 0.99\n",
      "iteration no 9567: Loss: 0.24895387808482927, accuracy: 0.99\n",
      "iteration no 9568: Loss: 0.24895361333630045, accuracy: 0.99\n",
      "iteration no 9569: Loss: 0.24895379592209627, accuracy: 0.99\n",
      "iteration no 9570: Loss: 0.2489536867976998, accuracy: 0.99\n",
      "iteration no 9571: Loss: 0.2489536397618432, accuracy: 0.99\n",
      "iteration no 9572: Loss: 0.24895464132100714, accuracy: 0.99\n",
      "iteration no 9573: Loss: 0.24895241057090323, accuracy: 0.99\n",
      "iteration no 9574: Loss: 0.24895431578411986, accuracy: 0.99\n",
      "iteration no 9575: Loss: 0.24895226616071361, accuracy: 0.99\n",
      "iteration no 9576: Loss: 0.2489546697356016, accuracy: 0.99\n",
      "iteration no 9577: Loss: 0.24895331682989952, accuracy: 0.99\n",
      "iteration no 9578: Loss: 0.24895466386508086, accuracy: 0.99\n",
      "iteration no 9579: Loss: 0.24895352412604949, accuracy: 0.99\n",
      "iteration no 9580: Loss: 0.2489526383152153, accuracy: 0.99\n",
      "iteration no 9581: Loss: 0.2489536022429221, accuracy: 0.99\n",
      "iteration no 9582: Loss: 0.24895305307366444, accuracy: 0.99\n",
      "iteration no 9583: Loss: 0.24895287275352757, accuracy: 0.99\n",
      "iteration no 9584: Loss: 0.2489540482979926, accuracy: 0.99\n",
      "iteration no 9585: Loss: 0.24895074989531368, accuracy: 0.99\n",
      "iteration no 9586: Loss: 0.24895474575182044, accuracy: 0.99\n",
      "iteration no 9587: Loss: 0.24895139380627046, accuracy: 0.99\n",
      "iteration no 9588: Loss: 0.24895395029536393, accuracy: 0.99\n",
      "iteration no 9589: Loss: 0.24895236617490066, accuracy: 0.99\n",
      "iteration no 9590: Loss: 0.2489529264824385, accuracy: 0.99\n",
      "iteration no 9591: Loss: 0.2489523554568644, accuracy: 0.99\n",
      "iteration no 9592: Loss: 0.2489526357159995, accuracy: 0.99\n",
      "iteration no 9593: Loss: 0.24895190146588497, accuracy: 0.99\n",
      "iteration no 9594: Loss: 0.24895388202567498, accuracy: 0.99\n",
      "iteration no 9595: Loss: 0.24895011596990713, accuracy: 0.99\n",
      "iteration no 9596: Loss: 0.2489541182115058, accuracy: 0.99\n",
      "iteration no 9597: Loss: 0.24895029718890385, accuracy: 0.99\n",
      "iteration no 9598: Loss: 0.24895339862048793, accuracy: 0.99\n",
      "iteration no 9599: Loss: 0.24895182237533742, accuracy: 0.99\n",
      "iteration no 9600: Loss: 0.24895251458752288, accuracy: 0.99\n",
      "iteration no 9601: Loss: 0.24895293765064802, accuracy: 0.99\n",
      "iteration no 9602: Loss: 0.24895272292020157, accuracy: 0.99\n",
      "iteration no 9603: Loss: 0.24894982968513008, accuracy: 0.99\n",
      "iteration no 9604: Loss: 0.2489546708994636, accuracy: 0.99\n",
      "iteration no 9605: Loss: 0.24894812771220093, accuracy: 0.99\n",
      "iteration no 9606: Loss: 0.2489545564626191, accuracy: 0.99\n",
      "iteration no 9607: Loss: 0.24895059075783305, accuracy: 0.99\n",
      "iteration no 9608: Loss: 0.24895188431707366, accuracy: 0.99\n",
      "iteration no 9609: Loss: 0.24895257919069613, accuracy: 0.99\n",
      "iteration no 9610: Loss: 0.24895098115113545, accuracy: 0.99\n",
      "iteration no 9611: Loss: 0.24895139605228273, accuracy: 0.99\n",
      "iteration no 9612: Loss: 0.2489524495051153, accuracy: 0.99\n",
      "iteration no 9613: Loss: 0.24894872140445995, accuracy: 0.99\n",
      "iteration no 9614: Loss: 0.24895446076346528, accuracy: 0.99\n",
      "iteration no 9615: Loss: 0.24894819755327546, accuracy: 0.99\n",
      "iteration no 9616: Loss: 0.24895344238447384, accuracy: 0.99\n",
      "iteration no 9617: Loss: 0.24895147502411252, accuracy: 0.99\n",
      "iteration no 9618: Loss: 0.2489502789799449, accuracy: 0.99\n",
      "iteration no 9619: Loss: 0.24895156883186453, accuracy: 0.99\n",
      "iteration no 9620: Loss: 0.24895066816620331, accuracy: 0.99\n",
      "iteration no 9621: Loss: 0.24895080673855097, accuracy: 0.99\n",
      "iteration no 9622: Loss: 0.24895253518554944, accuracy: 0.99\n",
      "iteration no 9623: Loss: 0.24894899275921817, accuracy: 0.99\n",
      "iteration no 9624: Loss: 0.24895259537921638, accuracy: 0.99\n",
      "iteration no 9625: Loss: 0.24894930247419633, accuracy: 0.99\n",
      "iteration no 9626: Loss: 0.24895096525169774, accuracy: 0.99\n",
      "iteration no 9627: Loss: 0.24895112795041988, accuracy: 0.99\n",
      "iteration no 9628: Loss: 0.24894912039211436, accuracy: 0.99\n",
      "iteration no 9629: Loss: 0.24895162172113097, accuracy: 0.99\n",
      "iteration no 9630: Loss: 0.24894964149830506, accuracy: 0.99\n",
      "iteration no 9631: Loss: 0.24895069010652637, accuracy: 0.99\n",
      "iteration no 9632: Loss: 0.24895122156359667, accuracy: 0.99\n",
      "iteration no 9633: Loss: 0.24894891446282827, accuracy: 0.99\n",
      "iteration no 9634: Loss: 0.24895157010892416, accuracy: 0.99\n",
      "iteration no 9635: Loss: 0.24894919321387282, accuracy: 0.99\n",
      "iteration no 9636: Loss: 0.24895017054474344, accuracy: 0.99\n",
      "iteration no 9637: Loss: 0.24895085060158756, accuracy: 0.99\n",
      "iteration no 9638: Loss: 0.24894805608653447, accuracy: 0.99\n",
      "iteration no 9639: Loss: 0.24895144002157807, accuracy: 0.99\n",
      "iteration no 9640: Loss: 0.24894970227423252, accuracy: 0.99\n",
      "iteration no 9641: Loss: 0.24895075036520042, accuracy: 0.99\n",
      "iteration no 9642: Loss: 0.2489494570807808, accuracy: 0.99\n",
      "iteration no 9643: Loss: 0.24894955270654867, accuracy: 0.99\n",
      "iteration no 9644: Loss: 0.24894959578894443, accuracy: 0.99\n",
      "iteration no 9645: Loss: 0.24894972739179966, accuracy: 0.99\n",
      "iteration no 9646: Loss: 0.24894920665804815, accuracy: 0.99\n",
      "iteration no 9647: Loss: 0.24895019539961838, accuracy: 0.99\n",
      "iteration no 9648: Loss: 0.2489480476360813, accuracy: 0.99\n",
      "iteration no 9649: Loss: 0.2489502249551187, accuracy: 0.99\n",
      "iteration no 9650: Loss: 0.24894825495233847, accuracy: 0.99\n",
      "iteration no 9651: Loss: 0.24895025891618033, accuracy: 0.99\n",
      "iteration no 9652: Loss: 0.2489492493381236, accuracy: 0.99\n",
      "iteration no 9653: Loss: 0.24894902534970328, accuracy: 0.99\n",
      "iteration no 9654: Loss: 0.2489491719275148, accuracy: 0.99\n",
      "iteration no 9655: Loss: 0.24894927504649667, accuracy: 0.99\n",
      "iteration no 9656: Loss: 0.2489479116668139, accuracy: 0.99\n",
      "iteration no 9657: Loss: 0.24895002279928696, accuracy: 0.99\n",
      "iteration no 9658: Loss: 0.2489473754695693, accuracy: 0.99\n",
      "iteration no 9659: Loss: 0.24895018468540997, accuracy: 0.99\n",
      "iteration no 9660: Loss: 0.24894762944554139, accuracy: 0.99\n",
      "iteration no 9661: Loss: 0.24894955059751622, accuracy: 0.99\n",
      "iteration no 9662: Loss: 0.24894819875482863, accuracy: 0.99\n",
      "iteration no 9663: Loss: 0.24895000092400862, accuracy: 0.99\n",
      "iteration no 9664: Loss: 0.24894855105214814, accuracy: 0.99\n",
      "iteration no 9665: Loss: 0.24894843686735588, accuracy: 0.99\n",
      "iteration no 9666: Loss: 0.24894821693467162, accuracy: 0.99\n",
      "iteration no 9667: Loss: 0.24894841003124168, accuracy: 0.99\n",
      "iteration no 9668: Loss: 0.24894781639865943, accuracy: 0.99\n",
      "iteration no 9669: Loss: 0.24895000427590164, accuracy: 0.99\n",
      "iteration no 9670: Loss: 0.24894596542751507, accuracy: 0.99\n",
      "iteration no 9671: Loss: 0.2489498155175882, accuracy: 0.99\n",
      "iteration no 9672: Loss: 0.24894611434301434, accuracy: 0.99\n",
      "iteration no 9673: Loss: 0.24894937505651898, accuracy: 0.99\n",
      "iteration no 9674: Loss: 0.24894781667987614, accuracy: 0.99\n",
      "iteration no 9675: Loss: 0.24894857419996874, accuracy: 0.99\n",
      "iteration no 9676: Loss: 0.24894721922879876, accuracy: 0.99\n",
      "iteration no 9677: Loss: 0.24894812717948367, accuracy: 0.99\n",
      "iteration no 9678: Loss: 0.24894664846777567, accuracy: 0.99\n",
      "iteration no 9679: Loss: 0.24894961125592235, accuracy: 0.99\n",
      "iteration no 9680: Loss: 0.2489454315778662, accuracy: 0.99\n",
      "iteration no 9681: Loss: 0.2489492484191787, accuracy: 0.99\n",
      "iteration no 9682: Loss: 0.24894555468806212, accuracy: 0.99\n",
      "iteration no 9683: Loss: 0.24894844347225736, accuracy: 0.99\n",
      "iteration no 9684: Loss: 0.2489470084965556, accuracy: 0.99\n",
      "iteration no 9685: Loss: 0.24894750786933478, accuracy: 0.99\n",
      "iteration no 9686: Loss: 0.24894822137686334, accuracy: 0.99\n",
      "iteration no 9687: Loss: 0.24894751110043334, accuracy: 0.99\n",
      "iteration no 9688: Loss: 0.2489462404486767, accuracy: 0.99\n",
      "iteration no 9689: Loss: 0.24894939992088266, accuracy: 0.99\n",
      "iteration no 9690: Loss: 0.2489446093193315, accuracy: 0.99\n",
      "iteration no 9691: Loss: 0.24894891843599098, accuracy: 0.99\n",
      "iteration no 9692: Loss: 0.24894611446153847, accuracy: 0.99\n",
      "iteration no 9693: Loss: 0.24894638025183427, accuracy: 0.99\n",
      "iteration no 9694: Loss: 0.24894787345348807, accuracy: 0.99\n",
      "iteration no 9695: Loss: 0.2489449215556892, accuracy: 0.99\n",
      "iteration no 9696: Loss: 0.24894832480060214, accuracy: 0.99\n",
      "iteration no 9697: Loss: 0.248946375304472, accuracy: 0.99\n",
      "iteration no 9698: Loss: 0.24894619445203578, accuracy: 0.99\n",
      "iteration no 9699: Loss: 0.2489481458389095, accuracy: 0.99\n",
      "iteration no 9700: Loss: 0.2489447829501466, accuracy: 0.99\n",
      "iteration no 9701: Loss: 0.24894809457849895, accuracy: 0.99\n",
      "iteration no 9702: Loss: 0.24894610965577457, accuracy: 0.99\n",
      "iteration no 9703: Loss: 0.24894519591303543, accuracy: 0.99\n",
      "iteration no 9704: Loss: 0.24894762858651892, accuracy: 0.99\n",
      "iteration no 9705: Loss: 0.2489441586214538, accuracy: 0.99\n",
      "iteration no 9706: Loss: 0.24894816375122808, accuracy: 0.99\n",
      "iteration no 9707: Loss: 0.24894526834330133, accuracy: 0.99\n",
      "iteration no 9708: Loss: 0.248945784551814, accuracy: 0.99\n",
      "iteration no 9709: Loss: 0.24894865947915812, accuracy: 0.99\n",
      "iteration no 9710: Loss: 0.24894603935241866, accuracy: 0.99\n",
      "iteration no 9711: Loss: 0.24894588801280665, accuracy: 0.99\n",
      "iteration no 9712: Loss: 0.2489468537140181, accuracy: 0.99\n",
      "iteration no 9713: Loss: 0.24894332572808925, accuracy: 0.99\n",
      "iteration no 9714: Loss: 0.24894784131389852, accuracy: 0.99\n",
      "iteration no 9715: Loss: 0.24894422980082032, accuracy: 0.99\n",
      "iteration no 9716: Loss: 0.24894681316503836, accuracy: 0.99\n",
      "iteration no 9717: Loss: 0.24894582535858314, accuracy: 0.99\n",
      "iteration no 9718: Loss: 0.24894468798383312, accuracy: 0.99\n",
      "iteration no 9719: Loss: 0.2489466759768007, accuracy: 0.99\n",
      "iteration no 9720: Loss: 0.2489455592885596, accuracy: 0.99\n",
      "iteration no 9721: Loss: 0.24894480945818595, accuracy: 0.99\n",
      "iteration no 9722: Loss: 0.24894676489709894, accuracy: 0.99\n",
      "iteration no 9723: Loss: 0.2489422557160152, accuracy: 0.99\n",
      "iteration no 9724: Loss: 0.24894770336647132, accuracy: 0.99\n",
      "iteration no 9725: Loss: 0.24894475965354262, accuracy: 0.99\n",
      "iteration no 9726: Loss: 0.2489465596883878, accuracy: 0.99\n",
      "iteration no 9727: Loss: 0.24894444862914167, accuracy: 0.99\n",
      "iteration no 9728: Loss: 0.24894529972334148, accuracy: 0.99\n",
      "iteration no 9729: Loss: 0.24894430033480813, accuracy: 0.99\n",
      "iteration no 9730: Loss: 0.2489458014002217, accuracy: 0.99\n",
      "iteration no 9731: Loss: 0.2489436691981643, accuracy: 0.99\n",
      "iteration no 9732: Loss: 0.24894629282761138, accuracy: 0.99\n",
      "iteration no 9733: Loss: 0.24894262120998345, accuracy: 0.99\n",
      "iteration no 9734: Loss: 0.24894608261026208, accuracy: 0.99\n",
      "iteration no 9735: Loss: 0.24894337867171146, accuracy: 0.99\n",
      "iteration no 9736: Loss: 0.24894590536204425, accuracy: 0.99\n",
      "iteration no 9737: Loss: 0.24894401105191072, accuracy: 0.99\n",
      "iteration no 9738: Loss: 0.24894464000890487, accuracy: 0.99\n",
      "iteration no 9739: Loss: 0.24894370742038308, accuracy: 0.99\n",
      "iteration no 9740: Loss: 0.24894544584917622, accuracy: 0.99\n",
      "iteration no 9741: Loss: 0.24894275082231038, accuracy: 0.99\n",
      "iteration no 9742: Loss: 0.24894626278722454, accuracy: 0.99\n",
      "iteration no 9743: Loss: 0.24894195752670284, accuracy: 0.99\n",
      "iteration no 9744: Loss: 0.2489455704017135, accuracy: 0.99\n",
      "iteration no 9745: Loss: 0.2489429814866434, accuracy: 0.99\n",
      "iteration no 9746: Loss: 0.2489445505348764, accuracy: 0.99\n",
      "iteration no 9747: Loss: 0.24894398325194753, accuracy: 0.99\n",
      "iteration no 9748: Loss: 0.24894476811251576, accuracy: 0.99\n",
      "iteration no 9749: Loss: 0.24894419084909142, accuracy: 0.99\n",
      "iteration no 9750: Loss: 0.2489430007926302, accuracy: 0.99\n",
      "iteration no 9751: Loss: 0.24894420486989252, accuracy: 0.99\n",
      "iteration no 9752: Loss: 0.24894334301625065, accuracy: 0.99\n",
      "iteration no 9753: Loss: 0.24894342091734928, accuracy: 0.99\n",
      "iteration no 9754: Loss: 0.24894435087762068, accuracy: 0.99\n",
      "iteration no 9755: Loss: 0.24894281484501735, accuracy: 0.99\n",
      "iteration no 9756: Loss: 0.2489443584470407, accuracy: 0.99\n",
      "iteration no 9757: Loss: 0.24894275685722173, accuracy: 0.99\n",
      "iteration no 9758: Loss: 0.2489431627996565, accuracy: 0.99\n",
      "iteration no 9759: Loss: 0.24894382519824132, accuracy: 0.99\n",
      "iteration no 9760: Loss: 0.2489422906256869, accuracy: 0.99\n",
      "iteration no 9761: Loss: 0.24894400299036254, accuracy: 0.99\n",
      "iteration no 9762: Loss: 0.24894270362305437, accuracy: 0.99\n",
      "iteration no 9763: Loss: 0.24894324770781662, accuracy: 0.99\n",
      "iteration no 9764: Loss: 0.24894353031612787, accuracy: 0.99\n",
      "iteration no 9765: Loss: 0.24894294431949174, accuracy: 0.99\n",
      "iteration no 9766: Loss: 0.24894421058318206, accuracy: 0.99\n",
      "iteration no 9767: Loss: 0.2489430650348776, accuracy: 0.99\n",
      "iteration no 9768: Loss: 0.24894131477071518, accuracy: 0.99\n",
      "iteration no 9769: Loss: 0.2489450073606716, accuracy: 0.99\n",
      "iteration no 9770: Loss: 0.248940320859514, accuracy: 0.99\n",
      "iteration no 9771: Loss: 0.24894470902833093, accuracy: 0.99\n",
      "iteration no 9772: Loss: 0.24894241988803895, accuracy: 0.99\n",
      "iteration no 9773: Loss: 0.2489425133871953, accuracy: 0.99\n",
      "iteration no 9774: Loss: 0.24894380135397265, accuracy: 0.99\n",
      "iteration no 9775: Loss: 0.24894157954470336, accuracy: 0.99\n",
      "iteration no 9776: Loss: 0.2489430171771284, accuracy: 0.99\n",
      "iteration no 9777: Loss: 0.2489430188873586, accuracy: 0.99\n",
      "iteration no 9778: Loss: 0.24894056061001363, accuracy: 0.99\n",
      "iteration no 9779: Loss: 0.24894458116807905, accuracy: 0.99\n",
      "iteration no 9780: Loss: 0.24893960102425375, accuracy: 0.99\n",
      "iteration no 9781: Loss: 0.2489441757039558, accuracy: 0.99\n",
      "iteration no 9782: Loss: 0.24894102687632955, accuracy: 0.99\n",
      "iteration no 9783: Loss: 0.24894204227041794, accuracy: 0.99\n",
      "iteration no 9784: Loss: 0.2489427926244022, accuracy: 0.99\n",
      "iteration no 9785: Loss: 0.2489413307506334, accuracy: 0.99\n",
      "iteration no 9786: Loss: 0.24894193985662633, accuracy: 0.99\n",
      "iteration no 9787: Loss: 0.2489428205603042, accuracy: 0.99\n",
      "iteration no 9788: Loss: 0.24893949523534617, accuracy: 0.99\n",
      "iteration no 9789: Loss: 0.24894443941947747, accuracy: 0.99\n",
      "iteration no 9790: Loss: 0.2489387617702315, accuracy: 0.99\n",
      "iteration no 9791: Loss: 0.2489436698692644, accuracy: 0.99\n",
      "iteration no 9792: Loss: 0.24894055247050023, accuracy: 0.99\n",
      "iteration no 9793: Loss: 0.24894161887578842, accuracy: 0.99\n",
      "iteration no 9794: Loss: 0.24894233293952805, accuracy: 0.99\n",
      "iteration no 9795: Loss: 0.24894134047994743, accuracy: 0.99\n",
      "iteration no 9796: Loss: 0.24894139161806464, accuracy: 0.99\n",
      "iteration no 9797: Loss: 0.24894399188160643, accuracy: 0.99\n",
      "iteration no 9798: Loss: 0.24893799317055132, accuracy: 0.99\n",
      "iteration no 9799: Loss: 0.24894396466690633, accuracy: 0.99\n",
      "iteration no 9800: Loss: 0.24893963309137718, accuracy: 0.99\n",
      "iteration no 9801: Loss: 0.2489413474109305, accuracy: 0.99\n",
      "iteration no 9802: Loss: 0.24894165276338742, accuracy: 0.99\n",
      "iteration no 9803: Loss: 0.2489395896984541, accuracy: 0.99\n",
      "iteration no 9804: Loss: 0.24894188829761052, accuracy: 0.99\n",
      "iteration no 9805: Loss: 0.24894071982485252, accuracy: 0.99\n",
      "iteration no 9806: Loss: 0.2489405554065045, accuracy: 0.99\n",
      "iteration no 9807: Loss: 0.2489427832058273, accuracy: 0.99\n",
      "iteration no 9808: Loss: 0.2489387073691946, accuracy: 0.99\n",
      "iteration no 9809: Loss: 0.24894277246885949, accuracy: 0.99\n",
      "iteration no 9810: Loss: 0.24893962576817913, accuracy: 0.99\n",
      "iteration no 9811: Loss: 0.24894036698520672, accuracy: 0.99\n",
      "iteration no 9812: Loss: 0.2489416358893844, accuracy: 0.99\n",
      "iteration no 9813: Loss: 0.24893845966768877, accuracy: 0.99\n",
      "iteration no 9814: Loss: 0.2489422534666263, accuracy: 0.99\n",
      "iteration no 9815: Loss: 0.248940759533671, accuracy: 0.99\n",
      "iteration no 9816: Loss: 0.2489407668269144, accuracy: 0.99\n",
      "iteration no 9817: Loss: 0.248940967941289, accuracy: 0.99\n",
      "iteration no 9818: Loss: 0.24893946622068636, accuracy: 0.99\n",
      "iteration no 9819: Loss: 0.2489402594125664, accuracy: 0.99\n",
      "iteration no 9820: Loss: 0.24894032594591026, accuracy: 0.99\n",
      "iteration no 9821: Loss: 0.2489394002579816, accuracy: 0.99\n",
      "iteration no 9822: Loss: 0.24894137349715229, accuracy: 0.99\n",
      "iteration no 9823: Loss: 0.24893856815407578, accuracy: 0.99\n",
      "iteration no 9824: Loss: 0.24894078061152292, accuracy: 0.99\n",
      "iteration no 9825: Loss: 0.24893916753144435, accuracy: 0.99\n",
      "iteration no 9826: Loss: 0.24894020345933088, accuracy: 0.99\n",
      "iteration no 9827: Loss: 0.24893995534722668, accuracy: 0.99\n",
      "iteration no 9828: Loss: 0.24893920000082614, accuracy: 0.99\n",
      "iteration no 9829: Loss: 0.2489397031585318, accuracy: 0.99\n",
      "iteration no 9830: Loss: 0.24894036338047326, accuracy: 0.99\n",
      "iteration no 9831: Loss: 0.248938310637756, accuracy: 0.99\n",
      "iteration no 9832: Loss: 0.24894120039179507, accuracy: 0.99\n",
      "iteration no 9833: Loss: 0.2489387786275042, accuracy: 0.99\n",
      "iteration no 9834: Loss: 0.24894120729277325, accuracy: 0.99\n",
      "iteration no 9835: Loss: 0.24893739235440854, accuracy: 0.99\n",
      "iteration no 9836: Loss: 0.24894058190372695, accuracy: 0.99\n",
      "iteration no 9837: Loss: 0.24893776850789964, accuracy: 0.99\n",
      "iteration no 9838: Loss: 0.24894003363419298, accuracy: 0.99\n",
      "iteration no 9839: Loss: 0.24893947457074989, accuracy: 0.99\n",
      "iteration no 9840: Loss: 0.2489396020970836, accuracy: 0.99\n",
      "iteration no 9841: Loss: 0.24893818102979215, accuracy: 0.99\n",
      "iteration no 9842: Loss: 0.24893960580507996, accuracy: 0.99\n",
      "iteration no 9843: Loss: 0.2489374083614428, accuracy: 0.99\n",
      "iteration no 9844: Loss: 0.2489410189082604, accuracy: 0.99\n",
      "iteration no 9845: Loss: 0.24893707694219389, accuracy: 0.99\n",
      "iteration no 9846: Loss: 0.2489400251821995, accuracy: 0.99\n",
      "iteration no 9847: Loss: 0.24893730619541082, accuracy: 0.99\n",
      "iteration no 9848: Loss: 0.24894004143635085, accuracy: 0.99\n",
      "iteration no 9849: Loss: 0.24893965146251146, accuracy: 0.99\n",
      "iteration no 9850: Loss: 0.24893895266022567, accuracy: 0.99\n",
      "iteration no 9851: Loss: 0.24893709031757152, accuracy: 0.99\n",
      "iteration no 9852: Loss: 0.2489405639809822, accuracy: 0.99\n",
      "iteration no 9853: Loss: 0.2489359752040496, accuracy: 0.99\n",
      "iteration no 9854: Loss: 0.24894106260719756, accuracy: 0.99\n",
      "iteration no 9855: Loss: 0.24893638095972542, accuracy: 0.99\n",
      "iteration no 9856: Loss: 0.2489391036125876, accuracy: 0.99\n",
      "iteration no 9857: Loss: 0.2489377884125345, accuracy: 0.99\n",
      "iteration no 9858: Loss: 0.2489370938887387, accuracy: 0.99\n",
      "iteration no 9859: Loss: 0.24893948458996898, accuracy: 0.99\n",
      "iteration no 9860: Loss: 0.24893732697700988, accuracy: 0.99\n",
      "iteration no 9861: Loss: 0.24893807355845013, accuracy: 0.99\n",
      "iteration no 9862: Loss: 0.24893884344798467, accuracy: 0.99\n",
      "iteration no 9863: Loss: 0.24893611010832298, accuracy: 0.99\n",
      "iteration no 9864: Loss: 0.24893972828916489, accuracy: 0.99\n",
      "iteration no 9865: Loss: 0.24893663343849748, accuracy: 0.99\n",
      "iteration no 9866: Loss: 0.24893809279491935, accuracy: 0.99\n",
      "iteration no 9867: Loss: 0.24893783013338874, accuracy: 0.99\n",
      "iteration no 9868: Loss: 0.2489359151562834, accuracy: 0.99\n",
      "iteration no 9869: Loss: 0.24893944369676596, accuracy: 0.99\n",
      "iteration no 9870: Loss: 0.24893665780861488, accuracy: 0.99\n",
      "iteration no 9871: Loss: 0.2489380679211104, accuracy: 0.99\n",
      "iteration no 9872: Loss: 0.24893945158554726, accuracy: 0.99\n",
      "iteration no 9873: Loss: 0.2489360519676373, accuracy: 0.99\n",
      "iteration no 9874: Loss: 0.24893831408153871, accuracy: 0.99\n",
      "iteration no 9875: Loss: 0.24893795887796205, accuracy: 0.99\n",
      "iteration no 9876: Loss: 0.24893550433731226, accuracy: 0.99\n",
      "iteration no 9877: Loss: 0.24893888970033048, accuracy: 0.99\n",
      "iteration no 9878: Loss: 0.24893456799098268, accuracy: 0.99\n",
      "iteration no 9879: Loss: 0.24893921267962948, accuracy: 0.99\n",
      "iteration no 9880: Loss: 0.24893690946273478, accuracy: 0.99\n",
      "iteration no 9881: Loss: 0.24893655250652683, accuracy: 0.99\n",
      "iteration no 9882: Loss: 0.24893807039624888, accuracy: 0.99\n",
      "iteration no 9883: Loss: 0.24893573580572387, accuracy: 0.99\n",
      "iteration no 9884: Loss: 0.24893740654988006, accuracy: 0.99\n",
      "iteration no 9885: Loss: 0.24893788652244586, accuracy: 0.99\n",
      "iteration no 9886: Loss: 0.24893446762340687, accuracy: 0.99\n",
      "iteration no 9887: Loss: 0.2489388437534173, accuracy: 0.99\n",
      "iteration no 9888: Loss: 0.24893362107214345, accuracy: 0.99\n",
      "iteration no 9889: Loss: 0.2489388129531207, accuracy: 0.99\n",
      "iteration no 9890: Loss: 0.24893621007522826, accuracy: 0.99\n",
      "iteration no 9891: Loss: 0.24893606693968606, accuracy: 0.99\n",
      "iteration no 9892: Loss: 0.24893761583282203, accuracy: 0.99\n",
      "iteration no 9893: Loss: 0.2489357833370353, accuracy: 0.99\n",
      "iteration no 9894: Loss: 0.24893606992721637, accuracy: 0.99\n",
      "iteration no 9895: Loss: 0.2489376993937374, accuracy: 0.99\n",
      "iteration no 9896: Loss: 0.24893345313365262, accuracy: 0.99\n",
      "iteration no 9897: Loss: 0.24893898729014555, accuracy: 0.99\n",
      "iteration no 9898: Loss: 0.24893295309993932, accuracy: 0.99\n",
      "iteration no 9899: Loss: 0.24893836710898842, accuracy: 0.99\n",
      "iteration no 9900: Loss: 0.24893667060202346, accuracy: 0.99\n",
      "iteration no 9901: Loss: 0.24893662512519815, accuracy: 0.99\n",
      "iteration no 9902: Loss: 0.24893564195975287, accuracy: 0.99\n",
      "iteration no 9903: Loss: 0.24893619912819903, accuracy: 0.99\n",
      "iteration no 9904: Loss: 0.2489346895635854, accuracy: 0.99\n",
      "iteration no 9905: Loss: 0.24893747134121377, accuracy: 0.99\n",
      "iteration no 9906: Loss: 0.24893402990199703, accuracy: 0.99\n",
      "iteration no 9907: Loss: 0.24893720800819627, accuracy: 0.99\n",
      "iteration no 9908: Loss: 0.24893452404065453, accuracy: 0.99\n",
      "iteration no 9909: Loss: 0.24893574058053994, accuracy: 0.99\n",
      "iteration no 9910: Loss: 0.24893592190277206, accuracy: 0.99\n",
      "iteration no 9911: Loss: 0.24893613337616993, accuracy: 0.99\n",
      "iteration no 9912: Loss: 0.24893577021845423, accuracy: 0.99\n",
      "iteration no 9913: Loss: 0.24893472043455164, accuracy: 0.99\n",
      "iteration no 9914: Loss: 0.2489358481790252, accuracy: 0.99\n",
      "iteration no 9915: Loss: 0.24893480924119643, accuracy: 0.99\n",
      "iteration no 9916: Loss: 0.24893538230380766, accuracy: 0.99\n",
      "iteration no 9917: Loss: 0.2489355682192499, accuracy: 0.99\n",
      "iteration no 9918: Loss: 0.24893495390449666, accuracy: 0.99\n",
      "iteration no 9919: Loss: 0.24893553905812088, accuracy: 0.99\n",
      "iteration no 9920: Loss: 0.24893491863760575, accuracy: 0.99\n",
      "iteration no 9921: Loss: 0.24893446276731662, accuracy: 0.99\n",
      "iteration no 9922: Loss: 0.2489355262035993, accuracy: 0.99\n",
      "iteration no 9923: Loss: 0.24893410553667242, accuracy: 0.99\n",
      "iteration no 9924: Loss: 0.24893562197889468, accuracy: 0.99\n",
      "iteration no 9925: Loss: 0.24893406183855338, accuracy: 0.99\n",
      "iteration no 9926: Loss: 0.24893487389822178, accuracy: 0.99\n",
      "iteration no 9927: Loss: 0.24893470579108246, accuracy: 0.99\n",
      "iteration no 9928: Loss: 0.24893526542724925, accuracy: 0.99\n",
      "iteration no 9929: Loss: 0.24893443159583936, accuracy: 0.99\n",
      "iteration no 9930: Loss: 0.24893484974012625, accuracy: 0.99\n",
      "iteration no 9931: Loss: 0.24893312000786944, accuracy: 0.99\n",
      "iteration no 9932: Loss: 0.24893572252112656, accuracy: 0.99\n",
      "iteration no 9933: Loss: 0.24893337770250912, accuracy: 0.99\n",
      "iteration no 9934: Loss: 0.24893545949517404, accuracy: 0.99\n",
      "iteration no 9935: Loss: 0.24893322991819505, accuracy: 0.99\n",
      "iteration no 9936: Loss: 0.2489345347308822, accuracy: 0.99\n",
      "iteration no 9937: Loss: 0.24893395687137385, accuracy: 0.99\n",
      "iteration no 9938: Loss: 0.2489354101199424, accuracy: 0.99\n",
      "iteration no 9939: Loss: 0.24893466065924783, accuracy: 0.99\n",
      "iteration no 9940: Loss: 0.24893492623297342, accuracy: 0.99\n",
      "iteration no 9941: Loss: 0.24893151120532087, accuracy: 0.99\n",
      "iteration no 9942: Loss: 0.24893657839025346, accuracy: 0.99\n",
      "iteration no 9943: Loss: 0.2489311757494944, accuracy: 0.99\n",
      "iteration no 9944: Loss: 0.24893622635898072, accuracy: 0.99\n",
      "iteration no 9945: Loss: 0.2489321911358428, accuracy: 0.99\n",
      "iteration no 9946: Loss: 0.24893380919702818, accuracy: 0.99\n",
      "iteration no 9947: Loss: 0.24893396853972788, accuracy: 0.99\n",
      "iteration no 9948: Loss: 0.2489331250766152, accuracy: 0.99\n",
      "iteration no 9949: Loss: 0.24893411002956667, accuracy: 0.99\n",
      "iteration no 9950: Loss: 0.2489336720357667, accuracy: 0.99\n",
      "iteration no 9951: Loss: 0.24893228321344507, accuracy: 0.99\n",
      "iteration no 9952: Loss: 0.2489354101512165, accuracy: 0.99\n",
      "iteration no 9953: Loss: 0.24893218207538248, accuracy: 0.99\n",
      "iteration no 9954: Loss: 0.24893483779731762, accuracy: 0.99\n",
      "iteration no 9955: Loss: 0.24893235558434293, accuracy: 0.99\n",
      "iteration no 9956: Loss: 0.24893268741216804, accuracy: 0.99\n",
      "iteration no 9957: Loss: 0.24893549398842285, accuracy: 0.99\n",
      "iteration no 9958: Loss: 0.2489321497655446, accuracy: 0.99\n",
      "iteration no 9959: Loss: 0.24893378894450705, accuracy: 0.99\n",
      "iteration no 9960: Loss: 0.24893398549577736, accuracy: 0.99\n",
      "iteration no 9961: Loss: 0.2489314798865072, accuracy: 0.99\n",
      "iteration no 9962: Loss: 0.24893514785359236, accuracy: 0.99\n",
      "iteration no 9963: Loss: 0.24893203843119047, accuracy: 0.99\n",
      "iteration no 9964: Loss: 0.24893304343486083, accuracy: 0.99\n",
      "iteration no 9965: Loss: 0.2489338379585413, accuracy: 0.99\n",
      "iteration no 9966: Loss: 0.24893026529983087, accuracy: 0.99\n",
      "iteration no 9967: Loss: 0.24893499043408543, accuracy: 0.99\n",
      "iteration no 9968: Loss: 0.24893058267431883, accuracy: 0.99\n",
      "iteration no 9969: Loss: 0.2489342139874554, accuracy: 0.99\n",
      "iteration no 9970: Loss: 0.24893295047316905, accuracy: 0.99\n",
      "iteration no 9971: Loss: 0.248931231212932, accuracy: 0.99\n",
      "iteration no 9972: Loss: 0.24893412040593973, accuracy: 0.99\n",
      "iteration no 9973: Loss: 0.24893224118361995, accuracy: 0.99\n",
      "iteration no 9974: Loss: 0.24893204227727808, accuracy: 0.99\n",
      "iteration no 9975: Loss: 0.24893356972219038, accuracy: 0.99\n",
      "iteration no 9976: Loss: 0.2489292423306526, accuracy: 0.99\n",
      "iteration no 9977: Loss: 0.2489351154927102, accuracy: 0.99\n",
      "iteration no 9978: Loss: 0.24892964790756988, accuracy: 0.99\n",
      "iteration no 9979: Loss: 0.24893393752995313, accuracy: 0.99\n",
      "iteration no 9980: Loss: 0.2489334038402352, accuracy: 0.99\n",
      "iteration no 9981: Loss: 0.2489320539977546, accuracy: 0.99\n",
      "iteration no 9982: Loss: 0.24893192337711748, accuracy: 0.99\n",
      "iteration no 9983: Loss: 0.24893273890148743, accuracy: 0.99\n",
      "iteration no 9984: Loss: 0.24892988593702872, accuracy: 0.99\n",
      "iteration no 9985: Loss: 0.24893430292745244, accuracy: 0.99\n",
      "iteration no 9986: Loss: 0.24892934488405544, accuracy: 0.99\n",
      "iteration no 9987: Loss: 0.2489337634447961, accuracy: 0.99\n",
      "iteration no 9988: Loss: 0.24893027272762278, accuracy: 0.99\n",
      "iteration no 9989: Loss: 0.24893156441746256, accuracy: 0.99\n",
      "iteration no 9990: Loss: 0.24893227162114712, accuracy: 0.99\n",
      "iteration no 9991: Loss: 0.24893058087145717, accuracy: 0.99\n",
      "iteration no 9992: Loss: 0.24893210052821457, accuracy: 0.99\n",
      "iteration no 9993: Loss: 0.24893129674096132, accuracy: 0.99\n",
      "iteration no 9994: Loss: 0.2489304864279799, accuracy: 0.99\n",
      "iteration no 9995: Loss: 0.24893310845622882, accuracy: 0.99\n",
      "iteration no 9996: Loss: 0.24892995527182096, accuracy: 0.99\n",
      "iteration no 9997: Loss: 0.24893282714236856, accuracy: 0.99\n",
      "iteration no 9998: Loss: 0.24893047051812145, accuracy: 0.99\n",
      "iteration no 9999: Loss: 0.24893018802992856, accuracy: 0.99\n",
      "iteration no 10000: Loss: 0.24893243196119821, accuracy: 0.99\n"
     ]
    }
   ],
   "source": [
    "# hyperparamters\n",
    "h = 100   # number of hidden states\n",
    "iterations = 10000\n",
    "step_size = 0.5\n",
    "reg = 1e-3\n",
    "\n",
    "# constants\n",
    "n_examples = X.shape[0]\n",
    "\n",
    "# initialize the weights\n",
    "W1 = np.random.randn(D,h)*0.01\n",
    "W2 = np.random.randn(h,K)*0.01\n",
    "\n",
    "b1 = np.zeros((1,h))\n",
    "b2 = np.zeros((1,K))\n",
    "\n",
    "# training loop\n",
    "for iter in range(iterations):\n",
    "\n",
    "    # calculate the scores\n",
    "    hidden_scores = np.dot(X, W1) + b1\n",
    "    hidden_scores = np.maximum(hidden_scores, 0) # relu activation function\n",
    "    scores = np.dot(hidden_scores, W2) + b2\n",
    "\n",
    "    # applying softmax\n",
    "    exp_scores = np.exp(scores)\n",
    "    probs = exp_scores/np.sum(exp_scores, axis=1, keepdims=True)\n",
    "\n",
    "    # loss calculation\n",
    "    loss = -np.sum(np.log(probs[range(n_examples), y]))/n_examples + 0.5*reg*np.sum(W1*W1) + 0.5*reg*np.sum(W2*W2)\n",
    "\n",
    "    # calculate gradients\n",
    "    dscores2 = probs\n",
    "    dscores2[range(n_examples), y]-=1\n",
    "    dscores2 = dscores2/n_examples\n",
    "\n",
    "    db2 = np.sum(dscores2, axis=0, keepdims=True)\n",
    "    dW2 = np.dot(hidden_scores.T, dscores2) \n",
    "\n",
    "    dscores1 = np.dot(dscores2, W2.T)\n",
    "    dscores1[hidden_scores==0] = 0       # backprop relu activation non-linearity\n",
    "\n",
    "    db1 = np.sum(dscores1, axis=0, keepdims=True)\n",
    "    dW1 = np.dot(X.T, dscores1)\n",
    "\n",
    "    dW2 += reg*W2\n",
    "    dW1 += reg*W1\n",
    "\n",
    "    # performing gradient descent\n",
    "    b2 = b2 - step_size * db2\n",
    "    W2 = W2 - step_size * dW2\n",
    "    b1 = b1 - step_size * db1\n",
    "    W1 = W1 - step_size * dW1\n",
    "\n",
    "\n",
    "    # accuracy calculation\n",
    "    hidden_scores = np.dot(X, W1) + b1\n",
    "    hidden_scores = np.maximum(hidden_scores, 0) # relu activation function\n",
    "    scores = np.dot(hidden_scores, W2) + b2\n",
    "    predictions = np.argmax(scores, axis=1)\n",
    "\n",
    "    accuracy = np.sum(predictions == y)/n_examples\n",
    "\n",
    "    print(f'iteration no {iter+1}: Loss: {loss}, accuracy: {accuracy}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<center><b>ACHIEVED ACCURACY OF 99% </b><center>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
